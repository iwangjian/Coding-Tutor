{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "type": "method", "class_name": "Quad", "function_name": "upload_to_texture", "dependency_all": "# Intra-class Dependency:\neasyvolcap.utils.gl_utils.Quad.H\n\neasyvolcap.utils.gl_utils.Quad.W\n\neasyvolcap.utils.gl_utils.Quad.tex\n\n", "dependency_sampled": "# Intra-class Dependency:\neasyvolcap.utils.gl_utils.Quad.tex\n\n", "contexts_above": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from easyvolcap.utils.egl_utils import create_opengl_context, eglContextManager  # must be imported before OpenGL.GL\n    from easyvolcap.runners.volumetric_video_viewer import VolumetricVideoViewer\n\nimport os\nimport sys\nimport glm\nimport torch\nimport ctypes\nimport numpy as np\n\nfrom torch import nn\nfrom enum import Enum, auto\nfrom types import MethodType\nfrom typing import Dict, Union, List\nfrom glm import vec2, vec3, vec4, mat3, mat4, mat4x3, mat2x3  # This is actually highly optimized\n\nfrom easyvolcap.utils.console_utils import *\nfrom easyvolcap.utils.base_utils import dotdict\nfrom easyvolcap.utils.viewer_utils import Camera\nfrom easyvolcap.utils.bound_utils import get_bounds\nfrom easyvolcap.utils.chunk_utils import multi_gather\nfrom easyvolcap.utils.color_utils import cm_cpu_store\nfrom easyvolcap.utils.ray_utils import create_meshgrid\nfrom easyvolcap.utils.depth_utils import depth_curve_fn\nfrom easyvolcap.utils.gaussian_utils import rgb2sh0, sh02rgb\nfrom easyvolcap.utils.nerf_utils import volume_rendering, raw2alpha\nfrom easyvolcap.utils.data_utils import load_pts, load_mesh, to_cuda, add_batch\nfrom easyvolcap.utils.cuda_utils import CHECK_CUDART_ERROR, FORMAT_CUDART_ERROR\nfrom easyvolcap.utils.net_utils import typed, torch_dtype_to_numpy_dtype, load_pretrained\nfrom easyvolcap.utils.fcds_utils import prepare_feedback_transform, get_opencv_camera_params\n\n\n# fmt: off\n# Environment variable messaging\n# Need to export EGL_DEVICE_ID before trying to import egl\n# And we need to consider the case when we're performing distributed training\n# from easyvolcap.engine import cfg, args  # FIXME: GLOBAL IMPORTS\nif 'easyvolcap.engine' in sys.modules and \\\n    (sys.modules['easyvolcap.engine'].args.type != 'gui' or \\\n        sys.modules['easyvolcap.engine'].cfg.viewer_cfg.type != 'VolumetricVideoViewer'): # FIXME: GLOBAL VARIABLES\n    try:\n        from easyvolcap.utils.egl_utils import create_opengl_context, eglContextManager\n    except Exception as e:\n        log(yellow(f'Could not import EGL related modules. {type(e).__name__}: {e}'))\n        os.environ['PYOPENGL_PLATFORM'] = ''\n\ndef is_wsl2():\n    \"\"\"Returns True if the current environment is WSL2, False otherwise.\"\"\"\n    return exists(\"/etc/wsl.conf\") and os.environ.get(\"WSL_DISTRO_NAME\")\n\nif is_wsl2():\n    os.environ['PYOPENGL_PLATFORM'] = 'glx'\n\nimport OpenGL.GL as gl\n\ntry:\n    from OpenGL.GL import shaders\nexcept Exception as e:\n    print(f'WARNING: OpenGL shaders import error encountered, please install the latest PyOpenGL from github using:')\n    print(f'pip install git+https://github.com/mcfletch/pyopengl')\n    raise e\n# fmt: on\n\n\ndef linearize_depth(d, n: float, f: float):\n    # 0-1 -> -1,1\n    # ndc -> view\n    return (2.0 * n * f) / (f + n - (d * 2 - 1) * (f - n))\n\n\ndef common_opengl_options():\n    # Use program point size\n    gl.glEnable(gl.GL_PROGRAM_POINT_SIZE)\n\n    # Performs face culling\n    gl.glEnable(gl.GL_CULL_FACE)\n    gl.glCullFace(gl.GL_BACK)\n\n    # Performs alpha trans testing\n    # gl.glEnable(gl.GL_ALPHA_TEST)\n    try: gl.glEnable(gl.GL_ALPHA_TEST)\n    except gl.GLError as e: pass\n\n    # Performs z-buffer testing\n    gl.glEnable(gl.GL_DEPTH_TEST)\n    # gl.glDepthMask(gl.GL_TRUE)\n    gl.glDepthFunc(gl.GL_LEQUAL)\n    # gl.glDepthRange(-1.0, 1.0)\n    gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n    # Enable some masking tests\n    gl.glEnable(gl.GL_SCISSOR_TEST)\n\n    # Enable this to correctly render points\n    # https://community.khronos.org/t/gl-point-sprite-gone-in-3-2/59310\n    # gl.glEnable(gl.GL_POINT_SPRITE)  # MARK: ONLY SPRITE IS WORKING FOR NOW\n    try: gl.glEnable(gl.GL_POINT_SPRITE)  # MARK: ONLY SPRITE IS WORKING FOR NOW\n    except gl.GLError as e: pass\n    # gl.glEnable(gl.GL_POINT_SMOOTH) # MARK: ONLY SPRITE IS WORKING FOR NOW\n\n    # # Configure how we store the pixels in memory for our subsequent reading of the FBO to store the rendering into memory.\n    # # The second argument specifies that our pixels will be in bytes.\n    # gl.glPixelStorei(gl.GL_PACK_ALIGNMENT, 1)\n\n\ndef load_shader_source(file: str = 'splat.frag'):\n    # Ideally we can just specify the shader name instead of an variable\n    if not exists(file):\n        file = f'{dirname(__file__)}/shaders/{file}'\n    if not exists(file):\n        file = file.replace('shaders/', '')\n    if not exists(file):\n        raise RuntimeError(f'Shader file: {file} does not exist')\n    with open(file, 'r') as f:\n        return f.read()\n\n\ndef use_gl_program(program: Union[shaders.ShaderProgram, dict]):\n    if isinstance(program, dict):\n        # Recompile the program if the user supplied sources\n        program = dotdict(program)\n        program = shaders.compileProgram(\n            shaders.compileShader(program.VERT_SHADER_SRC, gl.GL_VERTEX_SHADER),\n            shaders.compileShader(program.FRAG_SHADER_SRC, gl.GL_FRAGMENT_SHADER)\n        )\n    return gl.glUseProgram(program)\n\n\nclass Mesh:\n    class RenderType(Enum):\n        POINTS = 1\n        LINES = 2\n        TRIS = 3\n        QUADS = 4  # TODO: Support quad loading\n        STRIPS = 5\n\n    # Helper class to render a mesh on opengl\n    # This implementation should only be used for debug visualization\n    # Since no differentiable mechanism will be added\n    # We recommend using nvdiffrast and pytorch3d's point renderer directly if you will to optimize these structures directly\n\n    def __init__(self,\n                 verts: torch.Tensor = torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 1]]),  # need to call update after update\n                 faces: torch.Tensor = torch.tensor([[0, 1, 2]]),  # need to call update after update\n                 colors: torch.Tensor = None,\n                 normals: torch.Tensor = None,\n                 scalars: dotdict[str, torch.Tensor] = dotdict(),\n                 render_type: RenderType = RenderType.TRIS,\n\n                 # Misc info\n                 name: str = 'mesh',\n                 filename: str = '',\n                 visible: bool = True,\n\n                 # Render options\n                 shade_flat: bool = False,  # smooth shading\n                 point_radius: float = 0.015,\n                 render_normal: bool = False,\n\n                 # Storage options\n                 store_device: str = 'cpu',\n                 compute_device: str = 'cuda',\n                 vert_sizes=[3, 3, 3],  # pos + color + norm\n\n                 # Init options\n                 est_normal_thresh: int = 100000,\n\n                 # Ignore unused input\n                 **kwargs,\n                 ) -> None:\n        super().__init__()\n        self.name = name\n        self.visible = visible\n        self.render_type = render_type\n\n        self.shade_flat = shade_flat\n        self.point_radius = point_radius\n        self.render_normal = render_normal\n\n        self.store_device = store_device\n        self.compute_device = compute_device\n        self.vert_sizes = vert_sizes\n\n        self.est_normal_thresh = est_normal_thresh\n\n        # Uniform and program\n        self.compile_shaders()\n        self.uniforms = dotdict()  # uniform values\n\n        # Before initialization\n        self.max_verts = 0\n        self.max_faces = 0\n\n        # OpenGL data\n        if filename: self.load_from_file(filename)\n        else: self.load_from_data(verts, faces, colors, normals, scalars)\n\n    def compile_shaders(self):\n        try:\n            self.mesh_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('mesh.vert'), gl.GL_VERTEX_SHADER),\n                shaders.compileShader(load_shader_source('mesh.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n            self.point_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('point.vert'), gl.GL_VERTEX_SHADER),\n                shaders.compileShader(load_shader_source('point.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n        except Exception as e:\n            print(str(e).encode('utf-8').decode('unicode_escape'))\n            raise e\n\n    @property\n    def n_verts_bytes(self):\n        return len(self.verts) * self.vert_size * self.verts.element_size()\n\n    @property\n    def n_faces_bytes(self):\n        return len(self.faces) * self.face_size * self.faces.element_size()\n\n    @property\n    def verts_data(self):  # a heavy copy operation\n        verts = torch.cat([self.verts, self.colors, self.normals], dim=-1).ravel().numpy()  # MARK: Maybe sync\n        verts = np.asarray(verts, dtype=np.float32, order='C')\n        return verts\n\n    @property\n    def faces_data(self):  # a heavy copy operation\n        faces = self.faces.ravel().numpy()  # N, 3\n        faces = np.asarray(faces, dtype=np.uint32, order='C')\n        return faces\n\n    @property\n    def face_size(self):\n        return self.render_type.value\n\n    @property\n    def vert_size(self):\n        return sum(self.vert_sizes)\n\n    def load_from_file(self, filename: str = 'assets/meshes/bunny.ply'):\n        verts, faces, colors, normals, scalars = self.load_data_from_file(filename)\n        self.load_from_data(verts, faces, colors, normals, scalars)\n\n    def load_data_from_file(self, filename: str = 'assets/meshes/bunny.ply'):\n        self.name = os.path.split(filename)[-1]\n        verts, faces, colors, normals, scalars = None, None, None, None, None\n        verts, faces = load_mesh(filename, device=self.store_device)\n        if not len(faces):\n            verts, colors, normals, scalars = load_pts(filename)\n            self.render_type = Mesh.RenderType.POINTS\n        else:\n            self.render_type = Mesh.RenderType(faces.shape[-1])  # use value\n        return verts, faces, colors, normals, scalars\n\n    def load_from_data(self, verts: torch.Tensor, faces: torch.Tensor, colors: torch.Tensor = None, normals: torch.Tensor = None, scalars: dotdict[str, torch.Tensor] = dotdict()):\n        # Data type conversion\n        verts = torch.as_tensor(verts)  # convert to tensor if input is of other types\n        if verts.dtype == torch.float32:\n            pass  # supports this for now\n        elif verts.dtype == torch.float16:\n            pass  # supports this for now\n        else:\n            verts = verts.type(torch.float)  # convert to float32 if input is of higher precision\n        gl_dtype = gl.GL_FLOAT if verts.dtype == torch.float else gl.GL_HALF_FLOAT\n        self.vert_gl_types = [gl_dtype] * len(self.vert_sizes)\n\n        # Prepare main mesh data: vertices and faces\n        self.verts = torch.as_tensor(verts, device=self.store_device)\n        self.faces = torch.as_tensor(faces, device=self.store_device, dtype=torch.int32)  # NOTE: No uint32 support\n\n        # Prepare colors and normals\n        if colors is not None:\n            self.colors = torch.as_tensor(colors, device=self.store_device, dtype=self.verts.dtype)\n        else:\n            bounds = get_bounds(self.verts[None])[0]\n            self.colors = (self.verts - bounds[0]) / (bounds[1] - bounds[0])\n        if normals is not None:\n            self.normals = torch.as_tensor(normals, device=self.store_device, dtype=self.verts.dtype)\n        else:\n            self.estimate_vertex_normals()\n\n        # Prepare other scalars\n        if scalars is not None:\n            for k, v in scalars.items():\n                setattr(self, k, torch.as_tensor(v, device=self.store_device, dtype=self.verts.dtype))  # is this ok?\n\n        # Prepare OpenGL related buffer\n        self.update_gl_buffers()\n\n    def estimate_vertex_normals(self):\n        def est_pcd_norms():\n            if self.verts.dtype == torch.half:\n                self.normals = self.verts\n            else:\n                from pytorch3d.structures import Pointclouds, Meshes\n                pcd = Pointclouds([self.verts]).to(self.compute_device)\n                self.normals = pcd.estimate_normals()[0].cpu().to(self.verts.dtype)  # no batch dim\n\n        def est_tri_norms():\n            if self.verts.dtype == torch.half:\n                self.normals = self.verts\n            else:\n                from pytorch3d.structures import Pointclouds, Meshes\n                mesh = Meshes([self.verts], [self.faces]).to(self.compute_device)\n                self.normals = mesh.verts_normals_packed().cpu().to(self.verts.dtype)  # no batch dim\n\n        if not len(self.verts) > self.est_normal_thresh:\n            if self.render_type == Mesh.RenderType.TRIS: est_tri_norms()\n            elif self.render_type == Mesh.RenderType.POINTS: est_pcd_norms()\n            else:\n                # log(yellow(f'Unsupported mesh type: {self.render_type} for normal estimation, skipping'))\n                self.normals = self.verts\n        else:\n            # log(yellow(f'Number of points for mesh too large: {len(self.verts)} > {self.est_normal_thresh}, skipping normal estimation'))\n            self.normals = self.verts\n\n    def offscreen_render(self, eglctx: \"eglContextManager\", camera: Camera):\n        eglctx.resize(camera.W, camera.H)\n        self.render(camera)\n\n    def render(self, camera: Camera):\n        if not self.visible: return\n\n        # For point rendering\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glUseProgram(self.point_program)\n            self.use_gl_program(self.point_program)\n        else:\n            gl.glUseProgram(self.mesh_program)\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(0)\n\n    def use_gl_program(self, program: shaders.ShaderProgram):\n        use_gl_program(program)\n        self.uniforms.shade_flat = gl.glGetUniformLocation(program, \"shade_flat\")\n        self.uniforms.point_radius = gl.glGetUniformLocation(program, \"point_radius\")\n        self.uniforms.render_normal = gl.glGetUniformLocation(program, \"render_normal\")\n        self.uniforms.H = gl.glGetUniformLocation(program, \"H\")\n        self.uniforms.W = gl.glGetUniformLocation(program, \"W\")\n        self.uniforms.n = gl.glGetUniformLocation(program, \"n\")\n        self.uniforms.f = gl.glGetUniformLocation(program, \"f\")\n        self.uniforms.P = gl.glGetUniformLocation(program, \"P\")\n        self.uniforms.K = gl.glGetUniformLocation(program, \"K\")\n        self.uniforms.V = gl.glGetUniformLocation(program, \"V\")\n        self.uniforms.M = gl.glGetUniformLocation(program, \"M\")\n\n    def upload_gl_uniforms(self, camera: Camera):\n        K = camera.gl_ixt  # hold the reference\n        V = camera.gl_ext  # hold the reference\n        M = glm.identity(mat4)\n        P = K * V * M\n\n        gl.glUniform1i(self.uniforms.shade_flat, self.shade_flat)\n        gl.glUniform1f(self.uniforms.point_radius, self.point_radius)\n        gl.glUniform1i(self.uniforms.render_normal, self.render_normal)\n        gl.glUniform1i(self.uniforms.H, camera.H)  # o2w\n        gl.glUniform1i(self.uniforms.W, camera.W)  # o2w\n        gl.glUniform1f(self.uniforms.n, camera.n)  # o2w\n        gl.glUniform1f(self.uniforms.f, camera.f)  # o2w\n        gl.glUniformMatrix4fv(self.uniforms.P, 1, gl.GL_FALSE, glm.value_ptr(P))  # o2clip\n        gl.glUniformMatrix4fv(self.uniforms.K, 1, gl.GL_FALSE, glm.value_ptr(K))  # c2clip\n        gl.glUniformMatrix4fv(self.uniforms.V, 1, gl.GL_FALSE, glm.value_ptr(V))  # w2c\n        gl.glUniformMatrix4fv(self.uniforms.M, 1, gl.GL_FALSE, glm.value_ptr(M))  # o2w\n\n    def update_gl_buffers(self):\n        # Might be overwritten\n        self.resize_buffers(len(self.verts) if hasattr(self, 'verts') else 0,\n                            len(self.faces) if hasattr(self, 'faces') else 0)  # maybe repeated\n\n        if hasattr(self, 'verts'):\n            gl.glBindBuffer(gl.GL_ARRAY_BUFFER, self.vbo)\n            gl.glBufferSubData(gl.GL_ARRAY_BUFFER, 0, self.n_verts_bytes, self.verts_data)  # hold the reference\n        if hasattr(self, 'faces'):\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glBufferSubData(gl.GL_ELEMENT_ARRAY_BUFFER, 0, self.n_faces_bytes, self.faces_data)\n\n    def resize_buffers(self, v: int = 0, f: int = 0):\n        if v > self.max_verts or f > self.max_faces:\n            if v > self.max_verts: self.max_verts = v\n            if f > self.max_faces: self.max_faces = f\n            self.init_gl_buffers(v, f)\n\n    def init_gl_buffers(self, v: int = 0, f: int = 0):\n        # This will only init the corresponding buffer object\n        n_verts_bytes = v * self.vert_size * self.verts.element_size() if v > 0 else self.n_verts_bytes\n        n_faces_bytes = f * self.face_size * self.faces.element_size() if f > 0 else self.n_faces_bytes\n\n        # Housekeeping\n        if hasattr(self, 'vao'):\n            gl.glDeleteVertexArrays(1, [self.vao])\n            gl.glDeleteBuffers(2, [self.vbo, self.ebo])\n\n        self.vao = gl.glGenVertexArrays(1)\n        self.vbo = gl.glGenBuffers(1)\n        self.ebo = gl.glGenBuffers(1)\n\n        gl.glBindVertexArray(self.vao)\n        gl.glBindBuffer(gl.GL_ARRAY_BUFFER, self.vbo)\n        gl.glBufferData(gl.GL_ARRAY_BUFFER, n_verts_bytes, ctypes.c_void_p(0), gl.GL_DYNAMIC_DRAW)  # NOTE: Using pointers here won't work\n\n        # https://stackoverflow.com/questions/67195932/pyopengl-cannot-render-any-vao\n        cumsum = 0\n        for i, (s, t) in enumerate(zip(self.vert_sizes, self.vert_gl_types)):\n            gl.glVertexAttribPointer(i, s, t, gl.GL_FALSE, self.vert_size * self.verts.element_size(), ctypes.c_void_p(cumsum * self.verts.element_size()))  # we use 32 bit float\n            gl.glEnableVertexAttribArray(i)\n            cumsum += s\n\n        if n_faces_bytes > 0:\n            # Some implementation has no faces, we dangerously ignore ebo here, assuming they will never be used\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glBufferData(gl.GL_ELEMENT_ARRAY_BUFFER, n_faces_bytes, ctypes.c_void_p(0), gl.GL_DYNAMIC_DRAW)\n            gl.glBindVertexArray(0)\n\n    def render_imgui(mesh, viewer: 'VolumetricVideoViewer', batch: dotdict):\n        from imgui_bundle import imgui\n        from easyvolcap.utils.imgui_utils import push_button_color, pop_button_color\n\n        i = batch.i\n        will_delete = batch.will_delete\n        slider_width = batch.slider_width\n\n        imgui.push_item_width(slider_width * 0.5)\n        mesh.name = imgui.input_text(f'Mesh name##{i}', mesh.name)[1]\n\n        if imgui.begin_combo(f'Mesh type##{i}', mesh.render_type.name):\n            for t in Mesh.RenderType:\n                if imgui.selectable(t.name, mesh.render_type == t)[1]:\n                    mesh.render_type = t  # construct enum from name\n                if mesh.render_type == t:\n                    imgui.set_item_default_focus()\n            imgui.end_combo()\n        imgui.pop_item_width()\n\n        if hasattr(mesh, 'point_radius'):\n            mesh.point_radius = imgui.slider_float(f'Point radius##{i}', mesh.point_radius, 0.0005, 3.0)[1]  # 0.1mm\n\n        if hasattr(mesh, 'pts_per_pix'):\n            mesh.pts_per_pix = imgui.slider_int('Point per pixel', mesh.pts_per_pix, 0, 60)[1]  # 0.1mm\n\n        if hasattr(mesh, 'shade_flat'):\n            push_button_color(0x55cc33ff if not mesh.shade_flat else 0x8855aaff)\n            if imgui.button(f'Smooth##{i}' if not mesh.shade_flat else f' Flat ##{i}'):\n                mesh.shade_flat = not mesh.shade_flat\n            pop_button_color()\n\n        if hasattr(mesh, 'render_normal'):\n            imgui.same_line()\n            push_button_color(0x55cc33ff if not mesh.render_normal else 0x8855aaff)\n            if imgui.button(f'Color ##{i}' if not mesh.render_normal else f'Normal##{i}'):\n                mesh.render_normal = not mesh.render_normal\n            pop_button_color()\n\n        if hasattr(mesh, 'visible'):\n            imgui.same_line()\n            push_button_color(0x55cc33ff if not mesh.visible else 0x8855aaff)\n            if imgui.button(f'Show##{i}' if not mesh.visible else f'Hide##{i}'):\n                mesh.visible = not mesh.visible\n            pop_button_color()\n\n        # Render the delete button\n        imgui.same_line()\n        push_button_color(0xff5533ff)\n        if imgui.button(f'Delete##{i}'):\n            will_delete.append(i)\n        pop_button_color()\n\n\nclass Quad(Mesh):\n    # A shared texture for CUDA (pytorch) and OpenGL\n    # Could be rendererd to screen using blitting or just drawing a quad\n    def __init__(self,\n                 H: int = 256, W: int = 256,\n                 use_quad_draw: bool = True,\n                 use_quad_cuda: bool = True,\n                 compose: bool = False,\n                 compose_power: float = 1.0,\n                 ):  # the texture to blip\n        self.use_quad_draw = use_quad_draw\n        self.use_quad_cuda = use_quad_cuda\n        self.vert_sizes = [3]  # only position\n        self.vert_gl_types = [gl.GL_FLOAT]  # only position\n        self.render_type = Mesh.RenderType.STRIPS  # remove side effects of settings _type\n        self.max_verts, self.max_faces = 0, 0\n        self.verts = torch.as_tensor([[-1., -1., 0.5],\n                                      [1., -1., 0.5],\n                                      [-1., 1., 0.5],\n                                      [1., 1., 0.5],])\n        self.update_gl_buffers()\n        self.compile_shaders()\n\n        self.max_H, self.max_W = H, W\n        self.H, self.W = H, W\n        self.compose = compose\n        self.compose_power = compose_power\n\n        self.init_texture()\n\n    @property\n    def n_faces_bytes(self): return 0\n\n    def use_gl_program(self, program: shaders.ShaderProgram):\n        super().use_gl_program(program)\n        self.uniforms.tex = gl.glGetUniformLocation(program, 'tex')\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glUniform1i(self.uniforms.tex, 0)\n\n    def compile_shaders(self):\n        try:\n            self.quad_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('quad.vert'), gl.GL_VERTEX_SHADER),\n                shaders.compileShader(load_shader_source('quad.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n        except Exception as e:\n            print(str(e).encode('utf-8').decode('unicode_escape'))\n            raise e\n\n    def resize_textures(self, H: int, W: int):  # analogy to update_gl_buffers\n        self.H, self.W = H, W\n        if self.H > self.max_H or self.W > self.max_W:  # max got updated\n            self.max_H, self.max_W = max(int(self.H * 1.05), self.max_H), max(int(self.W * 1.05), self.max_W)\n            self.init_texture()\n\n    def init_texture(self):\n        if hasattr(self, 'cu_tex'):\n            from cuda import cudart\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_tex))\n\n        if hasattr(self, 'fbo'):\n            gl.glDeleteFramebuffers(1, [self.fbo])\n            gl.glDeleteTextures(1, [self.tex])\n\n        # Init the texture to be blit onto the screen\n        self.tex = gl.glGenTextures(1)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_RGBA8, self.max_W, self.max_H, 0, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ctypes.c_void_p(0))\n        gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n        gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n        # Init the framebuffer object if explicit blitting is used (slower than drawing quad)\n        self.fbo = gl.glGenFramebuffers(1)\n        old_fbo = gl.glGetIntegerv(gl.GL_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)\n        gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT0, gl.GL_TEXTURE_2D, self.tex, 0)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, old_fbo)\n\n        if self.use_quad_cuda:\n            from cuda import cudart\n            if self.compose:\n                # Both reading and writing of this resource is required\n                flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsNone\n            else:\n                flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsWriteDiscard\n            try:\n                self.cu_tex = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.tex, gl.GL_TEXTURE_2D, flags))\n            except RuntimeError as e:\n                log(red('Failed to initialize Quad with CUDA-GL interop, will use slow upload: '), e)\n                self.use_quad_cuda = False\n\n    def copy_to_texture(self, image: torch.Tensor, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n        if not self.use_quad_cuda:\n            self.upload_to_texture(image)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if image.shape[-1] == 3:\n            image = torch.cat([image, image.new_ones(image.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        if self.compose:\n            \"\"\"\n            Blit current framebuffer to this texture (self.tex)\n            Read content of this texture into a cuda buffer\n            Perform alpha blending based on the frame's alpha channel\n            Copy the blended image back into the texture (self.tex)\n            \"\"\"\n            old = gl.glGetInteger(gl.GL_DRAW_FRAMEBUFFER_BINDING)\n            gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n            gl.glBlitFramebuffer(x, y, w, h,\n                                 x, y, w, h,\n                                 gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n            gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, old)\n\n            buffer = torch.empty_like(image)\n            CHECK_CUDART_ERROR(cudart.cudaMemcpy2DFromArrayAsync(buffer.data_ptr(),  # dst\n                                                                 w * 4 * buffer.element_size(),  # dpitch\n                                                                 cu_tex_arr,  # src\n                                                                 x * 4 * image.element_size(),  # wOffset\n                                                                 y,  # hOffset\n                                                                 w * 4 * buffer.element_size(),  # width Width of matrix transfer (columns in bytes)\n                                                                 h,  # height\n                                                                 kind,  # kind\n                                                                 torch.cuda.current_stream().cuda_stream))  # stream\n\n            # cv2.imwrite('image.png', image.flip(0).detach().cpu().numpy()[..., [2,1,0,3]])\n            alpha = image[..., -1:] / 255\n            image[..., :-1] = buffer[..., :-1] * (1 - alpha ** self.compose_power) + image[..., :-1] * alpha  # storing float into int\n            image[..., -1:] = buffer[..., -1:] + image[..., -1:]\n            image = image.clip(0, 255)\n\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * image.element_size(),\n                                                           y,\n                                                           image.data_ptr(),\n                                                           w * 4 * image.element_size(),  # differently sized\n                                                           w * 4 * image.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n\n", "contexts_below": "\n    @property\n    def verts_data(self):  # a heavy copy operation\n        verts = self.verts.ravel().detach().cpu().numpy()  # MARK: Maybe sync\n        verts = np.asarray(verts, dtype=np.float32, order='C')\n        return verts\n\n    def render(self, camera: Camera = None):\n        self.draw()  # no uploading needed\n\n    def draw(self, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n        \"\"\"\n        Upload the texture instead of the camera\n        This respects the OpenGL convension of lower left corners\n        \"\"\"\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        gl.glBindVertexArray(0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n    def blit(self, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n        \"\"\"\n        This respects the OpenGL convension of lower left corners\n        \"\"\"\n        w = w or self.W\n        h = h or self.H\n        old = gl.glGetInteger(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # write buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, x + w, y + h,  # the height is flipped\n                             x, y, x + w, y + h,  # the height is flipped\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n\n\nclass UQuad(Mesh):\n    \"\"\"\n    Responsible for initializing textures with a single value\n    or blitting a texture to a framebuffer (possibly better done with blit instead of quad drawing)\n    Effectively clearing the texture for real, see: https://stackoverflow.com/questions/37335281/is-glcleargl-color-buffer-bit-preferred-before-a-whole-frame-buffer-overwritte\n    \"\"\"\n\n    def __init__(self):\n        self.n_blit_values = 3\n        self.vert_sizes = [3]  # only position\n        self.vert_gl_types = [gl.GL_FLOAT]  # only position\n        self.max_verts, self.max_faces = 0, 0\n        self.verts = torch.as_tensor([[-1., -1., 0.5],\n                                      [1., -1., 0.5],\n                                      [-1., 1., 0.5],\n                                      [1., 1., 0.5],])\n        self.compile_shaders()\n        self.uniforms = dotdict()  # uniform values\n        self.use_gl_programs(self.quad_program)\n        self.update_gl_buffers()\n\n    @property\n    def n_faces_bytes(self): return 0\n\n    @property\n    def verts_data(self):  # a heavy copy operation\n        verts = self.verts.ravel().detach().cpu().numpy()  # MARK: Maybe sync\n        verts = np.asarray(verts, dtype=np.float32, order='C')\n        return verts\n\n    def use_gl_programs(self, program: shaders.ShaderProgram):\n        for i in range(self.n_blit_values):\n            self.uniforms[f'value{i}'] = gl.glGetUniformLocation(program, f'value{i}')\n        for i in range(self.n_blit_values):\n            self.uniforms[f'use_tex{i}'] = gl.glGetUniformLocation(program, f'use_tex{i}')\n\n        gl.glUseProgram(self.program)  # use a different program\n\n        for i in range(self.n_blit_values):\n            self.uniforms[f'tex{i}'] = gl.glGetUniformLocation(program, f'tex{i}')\n            gl.glUniform1i(self.uniforms[f'tex{i}'], i)\n\n    def upload_gl_uniforms(self, values: List[List[float]], use_texs: List[bool]):\n        for i, v in enumerate(values):\n            v = vec4(v)  # HACK: Hold the reference for this upload\n            gl.glUniform4fv(self.uniforms[f'value{i}'], 1, glm.value_ptr(v))  # as float array\n        for i, v in enumerate(use_texs):\n            gl.glUniform1i(self.uniforms[f'use_tex{i}'], v)\n\n    def compile_shaders(self):\n        try:\n            self.quad_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('uquad.vert'), gl.GL_VERTEX_SHADER),\n                shaders.compileShader(load_shader_source('uquad.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n        except Exception as e:\n            print(str(e).encode('utf-8').decode('unicode_escape'))\n            raise e\n\n    def draw(self, values: List[List[float]] = [], use_texs=[]):\n        \"\"\"\n        This function will render 'value' to the currently bound framebuffer, up to six outputs\n        \"\"\"\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        self.upload_gl_uniforms(values, use_texs)  # should be a noop\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n\nclass DQuad(UQuad):\n    def compile_shaders(self):\n        try:\n            self.quad_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('dquad.vert'), gl.GL_VERTEX_SHADER),\n                shaders.compileShader(load_shader_source('dquad.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n        except Exception as e:\n            print(str(e).encode('utf-8').decode('unicode_escape'))\n            raise e\n\n    def draw(self, values: List[List[float]] = [], use_texs=[]):\n        old_function = gl.glGetIntegerv(gl.GL_DEPTH_FUNC)\n        gl.glDepthFunc(gl.GL_ALWAYS)\n        super().draw(values, use_texs)\n        gl.glDepthFunc(old_function)\n\n\ndef hardware_rendering_framebuffer(H: int, W: int, gl_tex_dtype=gl.GL_RGBA16F):\n    # Prepare for write frame buffers\n    color_buffer = gl.glGenTextures(1)\n    depth_upper = gl.glGenTextures(1)\n    depth_lower = gl.glGenTextures(1)\n    depth_attach = gl.glGenTextures(1)\n    fbo = gl.glGenFramebuffers(1)  # generate 1 framebuffer, storereference in fb\n\n    # Init the texture (call the resizing function), will simply allocate empty memory\n    # The internal format describes how the texture shall be stored in the GPU. The format describes how the format of your pixel data in client memory (together with the type parameter).\n    gl.glBindTexture(gl.GL_TEXTURE_2D, color_buffer)\n    gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl_tex_dtype, W, H, 0, gl.GL_RGBA, gl.GL_FLOAT, ctypes.c_void_p(0))  # 16 * 4\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n    gl.glBindTexture(gl.GL_TEXTURE_2D, depth_upper)\n    gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_R32F, W, H, 0, gl.GL_RED, gl.GL_FLOAT, ctypes.c_void_p(0))  # 32\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n    gl.glBindTexture(gl.GL_TEXTURE_2D, depth_lower)\n    gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_R32F, W, H, 0, gl.GL_RED, gl.GL_FLOAT, ctypes.c_void_p(0))  # 32\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n    gl.glBindTexture(gl.GL_TEXTURE_2D, depth_attach)\n    gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_DEPTH_COMPONENT24, W, H, 0, gl.GL_DEPTH_COMPONENT, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # 32\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n    # Bind texture to fbo\n    gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, fbo)\n    gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT0, gl.GL_TEXTURE_2D, color_buffer, 0)  # location 0\n    gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT1, gl.GL_TEXTURE_2D, depth_upper, 0)  # location 1\n    gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT2, gl.GL_TEXTURE_2D, depth_lower, 0)  # location 1\n    gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_DEPTH_ATTACHMENT, gl.GL_TEXTURE_2D, depth_attach, 0)\n    gl.glDrawBuffers(3, [gl.GL_COLOR_ATTACHMENT0, gl.GL_COLOR_ATTACHMENT1, gl.GL_COLOR_ATTACHMENT2])\n\n    # Check framebuffer status\n    if gl.glCheckFramebufferStatus(gl.GL_FRAMEBUFFER) != gl.GL_FRAMEBUFFER_COMPLETE:\n        log(red('Framebuffer not complete, exiting...'))\n        raise RuntimeError('Incomplete framebuffer')\n\n    # Restore the original state\n    gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n\n    return color_buffer, depth_upper, depth_lower, depth_attach, fbo\n\n\ndef hareward_peeling_framebuffer(H: int, W: int):\n    # Prepare for write frame buffers\n    index_buffer = gl.glGenTextures(1)\n    depth_lower = gl.glGenTextures(1)\n    depth_attach = gl.glGenTextures(1)\n    fbo = gl.glGenFramebuffers(1)  # generate 1 framebuffer, storereference in fb\n\n    # Init the texture (call the resizing function), will simply allocate empty memory\n    # The internal format describes how the texture shall be stored in the GPU. The format describes how the format of your pixel data in client memory (together with the type parameter).\n    gl.glBindTexture(gl.GL_TEXTURE_2D, index_buffer)\n    gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_R32I, W, H, 0, gl.GL_RED_INTEGER, gl.GL_INT, ctypes.c_void_p(0))  # 32\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n    gl.glBindTexture(gl.GL_TEXTURE_2D, depth_lower)\n    gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_R32F, W, H, 0, gl.GL_RED, gl.GL_FLOAT, ctypes.c_void_p(0))  # 32\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n    gl.glBindTexture(gl.GL_TEXTURE_2D, depth_attach)\n    gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_DEPTH_COMPONENT24, W, H, 0, gl.GL_DEPTH_COMPONENT, gl.GL_FLOAT, ctypes.c_void_p(0))  # 32\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n    gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n\n    # Bind texture to fbo\n    gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, fbo)\n    gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT0, gl.GL_TEXTURE_2D, index_buffer, 0)  # location 1\n    gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT1, gl.GL_TEXTURE_2D, depth_lower, 0)  # location 1\n    gl.glFramebufferTexture2D(gl.GL_FRAMEBUFFER, gl.GL_DEPTH_ATTACHMENT, gl.GL_TEXTURE_2D, depth_attach, 0)\n    gl.glDrawBuffers(2, [gl.GL_COLOR_ATTACHMENT0, gl.GL_COLOR_ATTACHMENT1])\n\n    # Check framebuffer status\n    if gl.glCheckFramebufferStatus(gl.GL_FRAMEBUFFER) != gl.GL_FRAMEBUFFER_COMPLETE:\n        log(red('Framebuffer not complete, exiting...'))\n        raise RuntimeError('Incomplete framebuffer')\n\n    # Restore the original state\n    gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n\n    return index_buffer, depth_lower, depth_attach, fbo\n\n\nclass Gaussian(Mesh):\n    def __init__(self,\n                 filename: str = 'assets/meshes/zju3dv.npz',\n\n                 gaussian_cfg: dotdict = dotdict(),\n                 quad_cfg: dotdict = dotdict(),\n\n                 view_depth: bool = False,  # show depth or show color\n                 dpt_cm: str = 'linear',\n\n                 H: int = 1024,\n                 W: int = 1024,\n                 **kwargs,\n                 ):\n        # Import Gaussian Model\n        from easyvolcap.engine.registry import call_from_cfg\n        from easyvolcap.utils.gaussian_utils import GaussianModel\n\n        # Housekeeping\n        super().__init__(**kwargs)\n        self.name = split(filename)[-1]\n\n        # Init Gaussian related models, for now only the first gaussian model is supported\n        if filename.endswith('.npz') or filename.endswith('.pt') or filename.endswith('.pth'):\n            # Load from GaussianTSampler\n            pretrained, _ = load_pretrained(filename)  # loaded model and updated path (maybe)\n            pretrained = pretrained.model\n            state_dict = dotdict()\n            for k, v in pretrained.items():\n                if k.startswith('sampler.pcds.0'):\n                    state_dict[k.replace('sampler.pcds.0.', '')] = v\n\n            # Load the parameters into the gaussian model\n            self.gaussian_model: GaussianModel = call_from_cfg(GaussianModel, gaussian_cfg)  # init empty gaussian model\n            self.gaussian_model.load_state_dict(state_dict)  # load the first gaussian model\n            self.gaussian_model.cuda()  # move the parameters to GPU\n\n        elif filename.endswith('.ply'):\n            # Load raw GaussianModel\n            # pts, rgb, norm, scalars = load_pts(filename)\n            self.gaussian_model: GaussianModel = call_from_cfg(GaussianModel, gaussian_cfg)  # init empty gaussian model\n            self.gaussian_model.load_ply(filename)  # load the original gaussian model\n            self.gaussian_model.cuda()\n        else:\n            raise NotImplementedError\n\n        # Init rendering quad\n        self.quad: Quad = call_from_cfg(Quad, quad_cfg, H=H, W=W)\n\n        # Other configurations\n        self.view_depth = view_depth\n        self.dpt_cm = dpt_cm\n        del self.shade_flat\n        del self.point_radius\n        del self.render_normal\n\n    # Disabling initialization\n    def load_from_file(self, *args, **kwargs):\n        pass\n\n    def load_from_data(self, *args, **kwargs):\n        pass\n\n    def compile_shaders(self):\n        pass\n\n    def update_gl_buffers(self):\n        pass\n\n    def resize_textures(self, H: int, W: int):\n        self.quad.resize_textures(H, W)\n\n    # The actual rendering function\n    @torch.no_grad()\n    def render(self, camera: Camera):\n        # Perform actual gaussian rendering\n        batch = add_batch(to_cuda(camera.to_batch()))\n        rgb, acc, dpt = self.gaussian_model.render(batch)\n\n        if self.view_depth:\n            rgba = torch.cat([depth_curve_fn(dpt, cm=self.dpt_cm), acc], dim=-1)  # H, W, 4\n        else:\n            rgba = torch.cat([rgb, acc], dim=-1)  # H, W, 4\n\n        # Copy rendered tensor to screen\n        rgba = (rgba.clip(0, 1) * 255).type(torch.uint8).flip(0)  # transform\n        self.quad.copy_to_texture(rgba)\n        self.quad.render()\n\n    def render_imgui(mesh, viewer: 'VolumetricVideoViewer', batch: dotdict):\n        super().render_imgui(viewer, batch)\n\n        from imgui_bundle import imgui\n        from easyvolcap.utils.imgui_utils import push_button_color, pop_button_color\n\n        i = batch.i\n        imgui.same_line()\n        push_button_color(0x55cc33ff if not mesh.view_depth else 0x8855aaff)\n        if imgui.button(f'Color##{i}' if not mesh.view_depth else f' Depth ##{i}'):\n            mesh.view_depth = not mesh.view_depth\n        pop_button_color()\n\n\nclass PointSplat(Gaussian, nn.Module):\n    def __init__(self,\n                 filename: str = 'assets/meshes/zju3dv.ply',\n                 quad_cfg: dotdict = dotdict(),\n                 view_depth: bool = False,  # show depth or show color\n                 dpt_cm: str = 'linear',\n\n                 H: int = 1024,\n                 W: int = 1024,\n                 **kwargs,\n                 ):\n        # Import Gaussian Model\n        from easyvolcap.engine.registry import call_from_cfg\n        from easyvolcap.utils.data_utils import load_pts\n        from easyvolcap.utils.net_utils import make_buffer\n        from easyvolcap.models.samplers.gaussiant_sampler import GaussianTSampler\n\n        # Housekeeping\n        super(Gaussian, self).__init__(**kwargs)\n        self.name = split(filename)[-1]\n        self.render_radius = MethodType(GaussianTSampler.render_radius, self)  # override the method\n\n        # Init PointSplat related models, for now only the first gaussian model is supported\n        if filename.endswith('.ply'):\n            # Load raw GaussianModel\n            pts, rgb, norms, scalars = load_pts(filename)\n            occ, rad = scalars.alpha, scalars.radius\n            self.pts = make_buffer(torch.from_numpy(pts))  # N, 3\n            self.rgb = make_buffer(torch.from_numpy(rgb))  # N, 3\n            self.occ = make_buffer(torch.from_numpy(occ))  # N, 1\n            self.rad = make_buffer(torch.from_numpy(rad))  # N, 1\n        else:\n            raise NotImplementedError\n\n        # Init rendering quad\n        self.quad: Quad = call_from_cfg(Quad, quad_cfg, H=H, W=W)\n        self.cuda()  # move to cuda\n\n        # Other configurations\n        self.view_depth = view_depth\n        self.dpt_cm = dpt_cm\n        self.radius_mult = 1.0\n        self.alpha_mult = 1.0\n\n    # The actual rendering function\n    @torch.no_grad()\n    def render(self, camera: Camera):\n        # Perform actual gaussian rendering\n        batch = add_batch(to_cuda(camera.to_batch()))\n        sh0 = rgb2sh0(self.rgb[..., None])\n        xyz = self.pts\n        occ = (self.occ * self.alpha_mult).clip(0, 1)\n        rad = self.rad * self.radius_mult\n\n        rgb, acc, dpt = self.render_radius(*add_batch([xyz, sh0, rad, occ]), batch)\n        rgb, acc, dpt = rgb[0], acc[0], dpt[0]\n\n        if self.view_depth:\n            rgba = torch.cat([depth_curve_fn(dpt, cm=self.dpt_cm), acc], dim=-1)  # H, W, 4\n        else:\n            rgba = torch.cat([rgb, acc], dim=-1)  # H, W, 4\n\n        # Copy rendered tensor to screen\n        rgba = (rgba.clip(0, 1) * 255).type(torch.uint8).flip(0)  # transform\n        self.quad.copy_to_texture(rgba)\n        self.quad.render()\n\n    def render_imgui(mesh, viewer: 'VolumetricVideoViewer', batch: dotdict):\n        super().render_imgui(viewer, batch)\n\n        i = batch.i\n        from imgui_bundle import imgui\n        mesh.radius_mult = imgui.slider_float(f'Point radius multiplier##{i}', mesh.radius_mult, 0.1, 3.0)[1]  # 0.1mm\n        mesh.alpha_mult = imgui.slider_float(f'Point alpha multiplier##{i}', mesh.alpha_mult, 0.1, 3.0)[1]  # 0.1mm\n\n\nclass Splat(Mesh):  # FIXME: Not rendering, need to debug this\n    def __init__(self,\n                 *args,\n                 H: int = 512,\n                 W: int = 512,\n                 tex_dtype: str = torch.half,\n\n                 pts_per_pix: int = 24,  # render less for the static background since we're only doing a demo\n                 blit_last_ratio: float = 0.0,\n                 volume_rendering: bool = True,\n                 radii_mult_volume: float = 1.00,  # 2 / 3 is the right integration, but will leave holes, 1.0 will make it bloat, 0.85 looks visually better\n                 radii_mult_solid: float = 0.85,  # 2 / 3 is the right integration, but will leave holes, 1.0 will make it bloat, 0.85 looks visually better\n\n                 point_smooth: bool = True,\n                 alpha_blending: bool = True,\n                 **kwargs):\n        kwargs = dotdict(kwargs)\n        kwargs.vert_sizes = kwargs.get('vert_sizes', [3, 3, 1, 1])\n        self.tex_dtype = getattr(torch, tex_dtype) if isinstance(tex_dtype, str) else tex_dtype\n        self.gl_tex_dtype = gl.GL_RGBA16F if self.tex_dtype == torch.half else gl.GL_RGBA32F\n\n        super().__init__(*args, **kwargs)\n        self.use_gl_program(self.splat_program)\n\n        self.pts_per_pix = pts_per_pix\n        self.blit_last_ratio = blit_last_ratio\n        self.volume_rendering = volume_rendering\n        self.radii_mult_volume = radii_mult_volume\n        self.radii_mult_solid = radii_mult_solid\n\n        self.point_smooth = point_smooth\n        self.alpha_blending = alpha_blending\n\n        self.max_H, self.max_W = H, W\n        self.H, self.W = H, W\n        self.init_textures()\n\n        from easyvolcap.models.samplers.gaussiant_sampler import GaussianTSampler\n        self.render_radius = MethodType(GaussianTSampler.render_radius, self)  # override the method\n\n    @property\n    def verts_data(self):  # a heavy copy operation\n        verts = torch.cat([self.verts, self.colors, self.radius, self.alpha], dim=-1).ravel().numpy()  # MARK: Maybe sync\n        verts = np.asarray(verts, dtype=np.float32, order='C')  # this should only be invoked once\n        return verts\n\n    def use_gl_program(self, program: shaders.ShaderProgram):\n        super().use_gl_program(program)\n        # Special controlling variables\n        self.uniforms.alpha_blending = gl.glGetUniformLocation(program, f'alpha_blending')\n        self.uniforms.point_smooth = gl.glGetUniformLocation(program, f'point_smooth')\n        self.uniforms.radii_mult = gl.glGetUniformLocation(program, f'radii_mult')\n\n        # Special rendering variables\n        self.uniforms.pass_index = gl.glGetUniformLocation(program, f'pass_index')\n        self.uniforms.read_color = gl.glGetUniformLocation(program, f'read_color')\n        self.uniforms.read_upper = gl.glGetUniformLocation(program, f'read_upper')\n        self.uniforms.read_lower = gl.glGetUniformLocation(program, f'read_lower')\n        gl.glUniform1i(self.uniforms.read_color, 0)\n        gl.glUniform1i(self.uniforms.read_upper, 1)\n        gl.glUniform1i(self.uniforms.read_lower, 2)\n\n    def compile_shaders(self):\n        try:\n            self.splat_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('splat.vert'), gl.GL_VERTEX_SHADER),\n                shaders.compileShader(load_shader_source('splat.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n            self.usplat_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('usplat.vert'), gl.GL_VERTEX_SHADER),\n                shaders.compileShader(load_shader_source('usplat.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n        except Exception as e:\n            print(str(e).encode('utf-8').decode('unicode_escape'))\n            raise e\n\n    def rasterize(self, camera: Camera = None, length: int = None):\n        if self.volume_rendering:\n            return self.rasterize_volume(camera, length)\n        else:\n            return self.rasterize_solid(camera, length)\n\n    def rasterize_volume(self, camera: Camera = None, length: int = None):  # some implementation requires no uploading of camera\n        \"\"\"\n        Let's try to analyze what's happening here\n\n        We want to:\n            1. Render the front-most color to color buffer\n            2. UNUSED: Render the front-most depth + some large margin to a depth upper limit buffer \n            3. Render the front-most depth + some small margin to a depth lower limit buffer\n            4. Switch between the render target and sampling target\n            5. Use the previous rendered color, depth upper limit and lower limit as textures\n            6. When current depth is smaller than the lower limit, we've already rendered this in the first pass, discard\n            7. UNUSED: When current depth is larger than the upper limit, it will probabily not contribute much to final results, discard\n            8. UNUSED: When the accumulated opacity reaches almost 1, subsequent rendering would not have much effect, return directly\n            9. When the point coordinates falls out of bound of the current sphere, dicard (this could be optimized with finutining in rectangle)\n            10. Finally, try to render the final color using the volume rendering equation (by accumulating alpha values from front to back)\n\n        Required cleanup checklist:\n            1. Before rendering the first pass, we need to clear the color and depth texture, this is not done, need to check multi-frame accumulation on this\n            2. Before rendering next pass, it's also recommended to blit color and depth values from previous pass to avoid assign them in the shader\n        \"\"\"\n\n        front_fbo, front_color, front_upper, front_lower = self.read_fbo, self.read_color, self.read_upper, self.read_lower\n        back_fbo, back_color, back_upper, back_lower = self.write_fbo, self.write_color, self.write_upper, self.write_lower\n\n        # Only clear the output once\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, front_fbo)  # for offscreen rendering to textures\n        gl.glClearBufferfv(gl.GL_COLOR, 0, [0.0, 0.0, 0.0, 0.0])\n        # gl.glClearBufferfv(gl.GL_COLOR, 1, [1e9])\n        gl.glClearBufferfv(gl.GL_COLOR, 2, [0.0, 0.0, 0.0, 0.0])\n        gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n\n        # Only clear the output once\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, back_fbo)  # for offscreen rendering to textures\n        gl.glClearBufferfv(gl.GL_COLOR, 0, [0.0, 0.0, 0.0, 0.0])\n        # gl.glClearBufferfv(gl.GL_COLOR, 1, [1e9])\n        gl.glClearBufferfv(gl.GL_COLOR, 2, [0.0, 0.0, 0.0, 0.0])\n        gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        self.use_gl_program(self.splat_program)  # TODO: Implement this with a mapping and a lazy modification\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        # The actual multi pass rendering process happens here\n        for pass_index in range(self.pts_per_pix):\n            # Swap buffers to render the next pass\n            front_fbo, front_color, front_upper, front_lower, back_fbo, back_color, back_upper, back_lower = \\\n                back_fbo, back_color, back_upper, back_lower, front_fbo, front_color, front_upper, front_lower\n\n            # Bind the read texture and bind the write render frame buffer\n            gl.glBindTextures(0, 3, [front_color, front_upper, front_lower])\n\n            # Move content from write_fbo to screen fbo\n            if pass_index > self.pts_per_pix * self.blit_last_ratio:  # no blitting almost has no effect on the rendering\n                gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, front_fbo)\n                gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, back_fbo)\n                for i in range(3):\n                    gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0 + i)\n                    gl.glDrawBuffer(gl.GL_COLOR_ATTACHMENT0 + i)\n                    gl.glBlitFramebuffer(0, 0, self.W, self.H, 0, 0, self.W, self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n                gl.glDrawBuffers(3, [gl.GL_COLOR_ATTACHMENT0, gl.GL_COLOR_ATTACHMENT1, gl.GL_COLOR_ATTACHMENT2])\n\n            # Clear depth buffer for depth testing\n            gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, back_fbo)  # for offscreen rendering to textures\n            gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n            gl.glUniform1i(self.uniforms.pass_index, pass_index)  # pass index\n\n            # The actual drawing pass with render things out to the write_fbo\n            gl.glDrawArrays(gl.GL_POINTS, 0, length if length is not None else len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glBindVertexArray(0)\n        return back_fbo\n\n    def upload_gl_uniforms(self, camera: Camera):\n        super().upload_gl_uniforms(camera)\n        gl.glUniform1i(self.uniforms.point_smooth, self.point_smooth)\n        gl.glUniform1i(self.uniforms.alpha_blending, self.alpha_blending)\n\n        if self.volume_rendering:\n            gl.glUniform1f(self.uniforms.radii_mult, self.radii_mult_volume)  # radii mult\n        else:\n            gl.glUniform1f(self.uniforms.radii_mult, self.radii_mult_solid)  # radii mult\n\n    def rasterize_solid(self, camera: Camera = None, length: int = None):\n        # Only clear the output once\n        back_fbo = self.write_fbo\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, back_fbo)  # for offscreen rendering to textures\n        gl.glClearBufferfv(gl.GL_COLOR, 0, [0.0, 0.0, 0.0, 0.0])  # color\n        # gl.glClearBufferfv(gl.GL_COLOR, 1, [0.0]) # depth upper\n        gl.glClearBufferfv(gl.GL_COLOR, 2, [0.0, 0.0, 0.0, 0.0])  # depth lower\n        gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        self.use_gl_program(self.usplat_program)\n        self.upload_gl_uniforms(camera)\n        gl.glUniform1i(self.uniforms.pass_index, 0)  # pass index\n        gl.glBindVertexArray(self.vao)\n\n        # The actual drawing pass with render things out to the write_fbo\n        gl.glDrawArrays(gl.GL_POINTS, 0, length if length is not None else len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glBindVertexArray(0)\n        return back_fbo\n\n    def show(self, back_fbo: int):\n        # Move content from write_fbo to screen fbo\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, back_fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # render the final content onto screen\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, 0, 0, self.W, self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n\n    def render(self, camera):\n        if not self.visible: return\n        self.show(self.rasterize(camera))\n\n    def resize_textures(self, H: int, W: int):  # analogy to update_gl_buffers\n        self.H, self.W = H, W\n        if self.H > self.max_H or self.W > self.max_W:  # max got updated\n            self.max_H, self.max_W = max(int(self.H * 1.05), self.max_H), max(int(self.W * 1.05), self.max_W)\n            self.init_textures()\n\n    def init_textures(self):\n        if hasattr(self, 'write_fbo'):\n            gl.glDeleteFramebuffers(2, [self.write_fbo, self.read_fbo])\n            gl.glDeleteTextures(8, [self.write_color, self.write_upper, self.write_lower, self.write_attach, self.read_color, self.read_upper, self.read_lower, self.read_attach])\n        self.write_color, self.write_upper, self.write_lower, self.write_attach, self.write_fbo = hardware_rendering_framebuffer(self.max_H, self.max_W, self.gl_tex_dtype)\n        self.read_color, self.read_upper, self.read_lower, self.read_attach, self.read_fbo = hardware_rendering_framebuffer(self.max_H, self.max_W, self.gl_tex_dtype)\n        log(f'Created texture of h, w: {self.max_H}, {self.max_W}')\n\n\nclass HardwareRendering(Splat):\n    def __init__(self,\n                 dtype=torch.half,\n                 **kwargs,\n                 ):\n        self.dtype = getattr(torch, dtype) if isinstance(dtype, str) else dtype\n        self.gl_dtype = gl.GL_HALF_FLOAT if self.dtype == torch.half else gl.GL_FLOAT\n        kwargs = dotdict(kwargs)\n        kwargs.blit_last_ratio = kwargs.get('blit_last_ratio', 0.90)\n        kwargs.vert_sizes = kwargs.get('vert_sizes', [3, 3, 1, 1])\n        super().__init__(**kwargs)  # verts, color, radius, alpha\n\n    @property\n    def verts_data(self):  # a heavy copy operation\n        verts = torch.cat([self.verts, self.colors, self.radius, self.alpha], dim=-1).ravel().numpy()  # MARK: Maybe sync\n        verts = np.asarray(verts, dtype=torch_dtype_to_numpy_dtype(self.dtype), order='C')  # this should only be invoked once\n        return verts\n\n    def init_gl_buffers(self, v: int = 0, f: int = 0):\n        from cuda import cudart\n        if hasattr(self, 'cu_vbo'):\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_vbo))\n\n        super().init_gl_buffers(v, f)\n\n        # Register vertex buffer obejct\n        flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsWriteDiscard\n        try:\n            self.cu_vbo = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterBuffer(self.vbo, flags))\n        except RuntimeError as e:\n            log(red(f'Your system does not support CUDA-GL interop, please use pytorch3d\\'s implementation instead'))\n            log(red(f'This can be done by specifying {blue(\"model_cfg.sampler_cfg.use_cudagl=False model_cfg.sampler_cfg.use_diffgl=False\")} at the end of your command'))\n            log(red(f'Note that this implementation is extremely slow, we recommend running on a native system that support the interop'))\n            # raise RuntimeError(str(e) + \": This unrecoverable, please read the error message above\")\n            raise e\n\n    def init_textures(self):\n        from cuda import cudart\n        if hasattr(self, 'cu_read_color'):\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_read_color))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_write_color))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_read_lower))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_write_lower))\n\n        super().init_textures()\n\n        # Register image to read from\n        flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsReadOnly\n        self.cu_read_color = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.read_color, gl.GL_TEXTURE_2D, flags))\n        self.cu_write_color = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.write_color, gl.GL_TEXTURE_2D, flags))\n        self.cu_read_lower = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.read_lower, gl.GL_TEXTURE_2D, flags))\n        self.cu_write_lower = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.write_lower, gl.GL_TEXTURE_2D, flags))\n\n    def forward(self, xyz: torch.Tensor, rgb: torch.Tensor, rad: torch.Tensor, occ: torch.Tensor, batch: dotdict):\n        \"\"\"\n        Renders a 3D point cloud using OpenGL and returns the rendered RGB image, accumulated alpha image, and depth map.\n\n        Args:\n            xyz (torch.Tensor): A tensor of shape (B, N, 3) containing the 3D coordinates of the points.\n            rgb (torch.Tensor): A tensor of shape (B, N, 3) containing the RGB color values of the points.\n            rad (torch.Tensor): A tensor of shape (B, N, 1) containing the radii of the points.\n            batch (dotdict): A dictionary containing the camera parameters and other metadata for the batch.\n\n        Returns:\n            A tuple containing the rendered RGB image, accumulated alpha image, and depth map, all as torch.Tensors.\n            The RGB image has shape (1, H, W, 3), the alpha image has shape (1, H, W, 1), and the depth map has shape (1, H, W, 1).\n\n        The method first resizes the OpenGL texture to match the height and width of the output image. It then sets the OpenGL viewport and scissor to only render in the region of the viewport specified by the output image size.\n\n        It concatenates the `xyz`, `rgb`, and `rad` tensors along the last dimension and flattens the result into a 1D tensor.\n\n        The method then uploads the input data to OpenGL for rendering and performs depth peeling using OpenGL. The method uploads the camera parameters to OpenGL and renders the point cloud, saving the output buffer to the `back_fbo` attribute of the class.\n\n        Finally, the method copies the rendered image and depth back to the CPU as torch.Tensors and reshapes them to match the output image size. The RGB image is returned with shape (1, H, W, 3), the accumulated alpha image is returned with shape (1, H, W, 1), and the depth map is returned with shape (1, H, W, 1).\n        \"\"\"\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n\n        # !: BATCH\n        H, W = batch.meta.H[0].item(), batch.meta.W[0].item()\n        self.resize_textures(H, W)  # maybe resize the texture\n        self.resize_buffers(xyz.shape[1])  # maybe resize the buffer\n        _, _, old_W, old_H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)  # only render in this small region of the viewport\n\n        # Prepare for input data\n        data = torch.cat([xyz, rgb, rad, occ], dim=-1).type(self.dtype).ravel()\n\n        # Upload to opengl for rendering\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_vbo, torch.cuda.current_stream().cuda_stream))\n        cu_vbo_ptr, cu_vbo_size = CHECK_CUDART_ERROR(cudart.cudaGraphicsResourceGetMappedPointer(self.cu_vbo))\n        assert cu_vbo_size >= data.numel() * data.element_size(), f'PyTorch(CUDA) and OpenGL vertex buffer size mismatch ({data.numel() * data.element_size()} v.s. {cu_vbo_size}), CUDA side should be less than or equal to the OpenGL side'\n        CHECK_CUDART_ERROR(cudart.cudaMemcpyAsync(cu_vbo_ptr,\n                                                  data.data_ptr(),\n                                                  data.numel() * data.element_size(),\n                                                  kind,\n                                                  torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_vbo, torch.cuda.current_stream().cuda_stream))\n\n        # Perform rasterization (depth peeling using OpenGL)\n        if 'meta_stream' in batch.meta: batch.meta.meta_stream.synchronize()  # wait for gpu -> cpu copy to finish\n        back_fbo = self.rasterize(Camera(batch=batch.meta), xyz.shape[-2])  # will upload and render, save output buffer to back_fbo\n\n        # Copy rendered image and depth back as tensor\n        cu_tex = self.cu_write_color if back_fbo == self.write_fbo else self.cu_read_color  # double buffered depth peeling\n        cu_dpt = self.cu_write_lower if back_fbo == self.write_fbo else self.cu_read_lower  # double buffered depth peeling\n\n        # Prepare the output # !: BATCH\n        rgb_map = torch.empty((H, W, 4), dtype=self.tex_dtype, device='cuda')  # to hold the data from opengl\n        dpt_map = torch.empty((H, W, 1), dtype=torch.float, device='cuda')  # to hold the data from opengl\n\n        # The resources in resources may be accessed by CUDA until they are unmapped.\n        # The graphics API from which resources were registered should not access any resources while they are mapped by CUDA.\n        # If an application does so, the results are undefined.\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, cu_tex, torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, cu_dpt, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(cu_tex, 0, 0))\n        cu_dpt_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(cu_dpt, 0, 0))\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DFromArrayAsync(rgb_map.data_ptr(),  # dst\n                                                             W * 4 * rgb_map.element_size(),  # dpitch\n                                                             cu_tex_arr,  # src\n                                                             0,  # wOffset\n                                                             0,  # hOffset\n                                                             W * 4 * rgb_map.element_size(),  # width Width of matrix transfer (columns in bytes)\n                                                             H,  # height\n                                                             kind,  # kind\n                                                             torch.cuda.current_stream().cuda_stream))  # stream\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DFromArrayAsync(dpt_map.data_ptr(),\n                                                             W * 1 * dpt_map.element_size(),\n                                                             cu_dpt_arr,\n                                                             0,\n                                                             0,\n                                                             W * 1 * dpt_map.element_size(),\n                                                             H,\n                                                             kind,\n                                                             torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, cu_tex, torch.cuda.current_stream().cuda_stream))  # MARK: SYNC\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, cu_dpt, torch.cuda.current_stream().cuda_stream))  # MARK: SYNC\n\n        # Ouput reshaping\n        rgb_map, dpt_map = rgb_map[None].flip(1), dpt_map[None].flip(1)\n        rgb_map, acc_map = rgb_map[..., :3], rgb_map[..., 3:]\n        dpt_map = torch.where(dpt_map == 0, dpt_map.max(), dpt_map)\n\n        # Some house keepings\n        gl.glViewport(0, 0, old_W, old_H)\n        gl.glScissor(0, 0, old_W, old_H)\n        return rgb_map, acc_map, dpt_map\n\n\nclass HardwarePeeling(Splat):\n    def __init__(self,\n                 dtype=torch.float,\n                 **kwargs):\n        self.dtype = getattr(torch, dtype) if isinstance(dtype, str) else dtype\n        self.gl_dtype = gl.GL_HALF_FLOAT if self.dtype == torch.half else gl.GL_FLOAT\n        super().__init__(**kwargs,\n                         blit_last_ratio=-10.0,\n                         vert_sizes=[3, 1],\n                         )  # verts, radius, index\n        # from pytorch3d.renderer import AlphaCompositor\n        # self.compositor = AlphaCompositor()  # this the key to convergence, this is differentiable\n\n    @property\n    def verts_data(self):  # a heavy copy operation\n        verts = torch.cat([self.verts, self.radius], dim=-1).ravel().numpy()  # MARK: Maybe sync\n        verts = np.asarray(verts, dtype=torch_dtype_to_numpy_dtype(self.dtype), order='C')  # this should only be invoked once\n        return verts\n\n    def init_gl_buffers(self, v: int = 0, f: int = 0):\n        from cuda import cudart\n        if hasattr(self, 'cu_vbo'):\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_vbo))\n\n        super().init_gl_buffers(v, f)\n\n        # Register vertex buffer obejct\n        flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsWriteDiscard\n        self.cu_vbo = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterBuffer(self.vbo, flags))\\\n\n\n    def use_gl_program(self, program):\n        super().use_gl_program(program)\n\n        gl.glUseProgram(self.splat_program)  # use a different program\n        self.uniforms.read_index = gl.glGetUniformLocation(program, f'read_index')\n        self.uniforms.read_lower = gl.glGetUniformLocation(program, f'read_lower')\n        gl.glUniform1i(self.uniforms.read_index, 0)\n        gl.glUniform1i(self.uniforms.read_lower, 1)\n\n    def upload_gl_uniforms(self, camera: Camera):\n        super().upload_gl_uniforms(camera)\n\n    def compile_shaders(self):\n        try:\n            self.splat_program = shaders.compileProgram(\n                shaders.compileShader(load_shader_source('idx_splat.vert'), gl.GL_VERTEX_SHADER),  # use the pass through quad shader\n                shaders.compileShader(load_shader_source('idx_splat.frag'), gl.GL_FRAGMENT_SHADER)\n            )\n        except Exception as e:\n            print(str(e).encode('utf-8').decode('unicode_escape'))\n            raise e\n\n    def init_textures(self):\n        from cuda import cudart\n        if hasattr(self, 'cu_read_index'):\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_read_index))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_write_index))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_read_lower))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnregisterResource(self.cu_write_lower))\n\n        if hasattr(self, 'write_fbo'):\n            gl.glDeleteFramebuffers(2, [self.write_fbo, self.read_fbo])\n            gl.glDeleteTextures(6, [self.write_index, self.write_lower, self.write_attach, self.read_index, self.read_lower, self.read_attach])\n\n        self.write_index, self.write_lower, self.write_attach, self.write_fbo = hareward_peeling_framebuffer(self.max_H, self.max_W)\n        self.read_index, self.read_lower, self.read_attach, self.read_fbo = hareward_peeling_framebuffer(self.max_H, self.max_W)\n\n        # Register image to read from\n        flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsReadOnly\n        self.cu_read_index = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.read_index, gl.GL_TEXTURE_2D, flags))\n        self.cu_write_index = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.write_index, gl.GL_TEXTURE_2D, flags))\n        self.cu_read_lower = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.read_lower, gl.GL_TEXTURE_2D, flags))\n        self.cu_write_lower = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.write_lower, gl.GL_TEXTURE_2D, flags))\n\n        log(f'Created texture of h, w: {self.max_H}, {self.max_W}')\n\n    def rasterize_generator(self, camera: Camera = None, length: int = None):  # some implementation requires no uploading of camera\n        front_fbo, front_index, front_lower = self.read_fbo, self.read_index, self.read_lower\n        back_fbo, back_index, back_lower = self.write_fbo, self.write_index, self.write_lower\n\n        # Only clear the output once\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, front_fbo)  # for offscreen rendering to textures\n        gl.glClearBufferiv(gl.GL_COLOR, 0, [-1])\n        gl.glClearBufferfv(gl.GL_COLOR, 1, [0.0])\n        gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n\n        # Only clear the output once\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, back_fbo)  # for offscreen rendering to textures\n        gl.glClearBufferiv(gl.GL_COLOR, 0, [-1])\n        gl.glClearBufferfv(gl.GL_COLOR, 1, [0.0])\n        gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        self.use_gl_program(self.splat_program)\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        # The actual multi pass rendering process happens here\n        for pass_index in range(self.pts_per_pix):\n            # Swap buffers to render the next pass\n            front_fbo, front_index, front_lower, back_fbo, back_index, back_lower = \\\n                back_fbo, back_index, back_lower, front_fbo, front_index, front_lower\n\n            # Bind the read texture and bind the write render frame buffer\n            gl.glBindTextures(0, 2, [front_index, front_lower])\n\n            # Move content from write_fbo to screen fbo\n            if pass_index > self.pts_per_pix * self.blit_last_ratio:  # no blitting almost has no effect on the rendering\n                gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, front_fbo)\n                gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, back_fbo)\n                gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0 + 1)\n                gl.glDrawBuffer(gl.GL_COLOR_ATTACHMENT0 + 1)\n                gl.glBlitFramebuffer(0, 0, self.W, self.H, 0, 0, self.W, self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n                gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, back_fbo)  # for offscreen rendering to textures\n                gl.glDrawBuffers(2, [gl.GL_COLOR_ATTACHMENT0, gl.GL_COLOR_ATTACHMENT1])\n            else:\n                # Only clear the output once\n                gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, back_fbo)  # for offscreen rendering to textures\n\n            # Clear depth buffer for depth testing\n            gl.glClearBufferiv(gl.GL_COLOR, 0, [-1])  # clear the indices buffer for later rendering and retrieving\n            gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n\n            # The actual drawing pass with render things out to the write_fbo\n            gl.glDrawArrays(gl.GL_POINTS, 0, length if length is not None else len(self.verts))  # number of vertices\n            yield back_fbo  # give the CUDA end a chance to read from this frame buffer after rendering\n\n        # Restore states of things\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glBindVertexArray(0)\n        return\n\n    def forward(self,\n                xyz: torch.Tensor, rgb: torch.Tensor, rad: torch.Tensor, occ: torch.Tensor,\n                batch: dotdict,\n                return_frags: bool = False,\n                return_full: bool = False,\n                ):\n        \"\"\"\n        Get all indices from the depth peeling passes\n        Compute the vertex weight here in torch(cuda)\n        Use the indices to pass through a compositor\n        The backward pass should only be valid on the torch side, and it should've been enough\n\n        TODO: This function is too memory intensive\n        TODO: Performing IBR is too memory intensive\n        \"\"\"\n\n        # This the slow part, but not differentiable\n        idx, _, _ = self.forward_idx(xyz, rad, batch)  # B, H, W, K\n        msk = idx != -1  # B, H, W, K\n        idx = torch.where(msk, idx, 0).long()\n\n        # Sample things needed for computing screen space weight\n        H, W, K, R, T, C = get_opencv_camera_params(batch)\n        K, R, T, C = K.to(xyz.dtype), R.to(xyz.dtype), T.to(xyz.dtype), C.to(xyz.dtype)\n        pix_xyz = (xyz @ R.mT + T.mT) @ K.mT  # B, P, 3\n        pix_xyz_xy = pix_xyz[..., :-1] / (pix_xyz[..., -1:] + 1e-10)\n        pix_rad = abs(K[..., 1, 1][..., None] * rad[..., 0] / (pix_xyz[..., -1] + 1e-10))  # z: B, 1 * B, N, world space radius\n\n        mean_xy = multi_gather(pix_xyz_xy, idx.view(idx.shape[0], -1), dim=-2).view(*idx.shape, 2)  # B, HWK, 2 -> B, H, W, K, 2\n        xy = create_meshgrid(H, W, idx.device, dtype=xyz.dtype).flip(-1)[None].expand(idx.shape[0], H, W, 2)  # create screen space xy (opencv)\n        dists = (xy[..., None, :] - mean_xy).pow(2).sum(-1)  # B, H, W, K\n\n        # Point values\n        dpt = (xyz - C.mT).norm(dim=-1, keepdim=True)  # B, N, 1\n        pix_occ = multi_gather(occ, idx.view(idx.shape[0], -1), dim=-2).view(*idx.shape)\n        pix_rad = multi_gather(pix_rad, idx.view(idx.shape[0], -1), dim=-1).view(*idx.shape)  # -> B, H, W, K\n        pix_occ = pix_occ * (1 - dists / (pix_rad * pix_rad + 1e-10))  # B, H, W, K\n        pix_occ = pix_occ.clip(0, 1)\n        pix_occ = torch.where(msk, pix_occ, 0)\n\n        if return_frags:\n            return idx, pix_occ  # B, H, W, K\n\n        # The actual computation\n        rgb = torch.cat([rgb, occ, dpt], dim=-1)  # B, N, 3 + C\n        pix_rgb = multi_gather(rgb, idx.view(idx.shape[0], -1), dim=-2).view(*idx.shape, rgb.shape[-1])  # B, H, W, K, -1\n        _, rgb, _ = volume_rendering(pix_rgb, pix_occ[..., None])  # B, H, W, -1\n\n        rgb, acc, dpt = rgb[..., :-2], rgb[..., -2:-1], rgb[..., -1:]\n        dpt = dpt + (1 - acc) * dpt.max()  # only for the looks (rendered depth are already premultiplied)\n\n        if return_full:\n            return rgb, acc, dpt, idx, pix_occ\n        else:\n            return rgb, acc, dpt\n\n    def forward_idx(self, xyz: torch.Tensor, rad: torch.Tensor, batch: dotdict):\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n\n        # !: BATCH\n        H, W = batch.meta.H[0].item(), batch.meta.W[0].item()\n        self.resize_textures(H, W)  # maybe resize the texture\n        self.resize_buffers(xyz.shape[1])  # maybe resize the buffer\n        _, _, old_W, old_H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)  # only render in this small region of the viewport\n\n        # Prepare for input data\n        data = torch.cat([xyz, rad], dim=-1).type(self.dtype).ravel()\n\n        # Upload to opengl for rendering\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_vbo, torch.cuda.current_stream().cuda_stream))\n        cu_vbo_ptr, cu_vbo_size = CHECK_CUDART_ERROR(cudart.cudaGraphicsResourceGetMappedPointer(self.cu_vbo))\n        assert cu_vbo_size >= data.numel() * data.element_size(), f'PyTorch(CUDA) and OpenGL vertex buffer size mismatch ({data.numel() * data.element_size()} v.s. {cu_vbo_size}), CUDA side should be less than or equal to the OpenGL side'\n        CHECK_CUDART_ERROR(cudart.cudaMemcpyAsync(cu_vbo_ptr,\n                                                  data.data_ptr(),\n                                                  data.numel() * data.element_size(),\n                                                  kind,\n                                                  torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_vbo, torch.cuda.current_stream().cuda_stream))\n\n        # Perform rasterization (depth peeling using OpenGL)\n        if 'meta_stream' in batch.meta: batch.meta.meta_stream.synchronize()  # wait for gpu -> cpu copy to finish\n        # FIXME: Strange bug occurs if batch parameter is passed in directly for the construction of Camera(batch=batch.meta)\n        gen = self.rasterize_generator(Camera(batch=batch.meta), xyz.shape[-2])  # will upload and render, save output buffer to back_fbo\n\n        ind_maps = []\n        dpt_maps = []\n        acc_maps = []\n        for back_fbo in gen:\n            # Copy rendered image and depth back as tensor\n            cu_tex = self.cu_write_index if back_fbo == self.write_fbo else self.cu_read_index  # double buffered depth peeling\n            cu_dpt = self.cu_write_lower if back_fbo == self.write_fbo else self.cu_read_lower  # double buffered depth peeling\n\n            # Prepare the output # !: BATCH\n            ind_map = torch.empty((H, W, 1), dtype=torch.int, device='cuda')  # to hold the data from opengl\n            dpt_map = torch.empty((H, W, 1), dtype=torch.float, device='cuda')  # to hold the data from opengl\n\n            # The resources in resources may be accessed by CUDA until they are unmapped.\n            # The graphics API from which resources were registered should not access any resources while they are mapped by CUDA.\n            # If an application does so, the results are undefined.\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, cu_tex, torch.cuda.current_stream().cuda_stream))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, cu_dpt, torch.cuda.current_stream().cuda_stream))\n            cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(cu_tex, 0, 0))\n            cu_dpt_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(cu_dpt, 0, 0))\n            CHECK_CUDART_ERROR(cudart.cudaMemcpy2DFromArrayAsync(ind_map.data_ptr(),  # dst\n                                                                 W * ind_map.shape[-1] * ind_map.element_size(),  # dpitch\n                                                                 cu_tex_arr,  # src\n                                                                 0,  # wOffset\n                                                                 0,  # hOffset\n                                                                 W * ind_map.shape[-1] * ind_map.element_size(),  # width Width of matrix transfer (columns in bytes)\n                                                                 H,  # height\n                                                                 kind,  # kind\n                                                                 torch.cuda.current_stream().cuda_stream))  # stream\n            CHECK_CUDART_ERROR(cudart.cudaMemcpy2DFromArrayAsync(dpt_map.data_ptr(),\n                                                                 W * dpt_map.shape[-1] * dpt_map.element_size(),\n                                                                 cu_dpt_arr,\n                                                                 0,\n                                                                 0,\n                                                                 W * dpt_map.shape[-1] * dpt_map.element_size(),\n                                                                 H,\n                                                                 kind,\n                                                                 torch.cuda.current_stream().cuda_stream))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, cu_tex, torch.cuda.current_stream().cuda_stream))  # MARK: SYNC\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, cu_dpt, torch.cuda.current_stream().cuda_stream))  # MARK: SYNC\n\n            # Ouput reshaping\n            ind_map, dpt_map = ind_map[None].flip(1), dpt_map[None].flip(1)\n            acc_map = ind_map != -1\n            dpt_map = torch.where(dpt_map == 0, dpt_map.max(), dpt_map)\n\n            ind_maps.append(ind_map)\n            acc_maps.append(acc_map)\n            dpt_maps.append(dpt_map)\n\n        ind_map = torch.cat(ind_maps, dim=-1)  # B, H, W, K\n        acc_map = torch.cat(acc_maps, dim=-1)  # B, H, W, K\n        dpt_map = torch.cat(dpt_maps, dim=-1)  # B, H, W, K\n\n        # Some house keepings\n        gl.glViewport(0, 0, old_W, old_H)\n        gl.glScissor(0, 0, old_W, old_H)\n        return ind_map, acc_map, dpt_map\n", "input_code": "    def upload_to_texture(self, ptr: np.ndarray, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n\n        \"\"\"\n        This function uploads a portion or the entirety of a numpy array or a PyTorch tensor to a texture in OpenGL. It is designed to update the texture content starting from a specified position (x, y) and covering a specified width (w) and height (h). If width and height are not provided, it defaults to the object's width and height. The function handles the conversion from a PyTorch tensor to a numpy array before uploading.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class, which contains the texture to be updated and the default dimensions (W, H) for the texture update.\n        :param ptr: np.ndarray or torch.Tensor, the data source for the texture update. It is used as the pixel data to be uploaded to the texture.\n        :param x: int, optional, the x-coordinate of the lower left corner where the texture update will start. Defaults to 0.\n        :param y: int, optional, the y-coordinate of the lower left corner where the texture update will start. Defaults to 0.\n        :param w: int, optional, the width of the portion of the texture to be updated. If 0, it defaults to the object's width (self.W).\n        :param h: int, optional, the height of the portion of the texture to be updated. If 0, it defaults to the object's height (self.H).\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a function `upload_to_texture` that takes parameters for a numpy array `ptr` and optional parameters for coordinates and dimensions (`x`, `y`, `w`, `h`).\n2. Set the width (`w`) and height (`h`) to the object's width (`self.W`) and height (`self.H`) if they are not provided or are zero.\n3. Check if the input `ptr` is a PyTorch tensor.\n4. If `ptr` is a tensor, detach it from the current computation graph, move it to the CPU, and convert it to a numpy array.\n5. Bind the texture object (`self.tex`) to the current OpenGL context using `glBindTexture`.\n6. Update a portion of the texture with new data using `glTexSubImage2D`.\n7. Specify the target texture (`GL_TEXTURE_2D`) and mipmap level (`0`) for the update.\n8. Provide the x and y offsets (`x`, `y`) within the texture where the update will begin.\n9. Set the width (`w`) and height (`h`) of the texture region to be updated.\n10. Pass the pixel data (`ptr`) from the specified region of the numpy array to the texture, specifying the format (`GL_RGBA`) and type (`GL_UNSIGNED_BYTE`) of the pixel data.", "reference_code": "def upload_to_texture(self, ptr: np.ndarray, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n    w = w or self.W\n    h = h or self.H\n    if isinstance(ptr, torch.Tensor):\n        ptr = ptr.detach().cpu().numpy()  # slow sync and copy operation # MARK: SYNC\n\n    gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n    gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr[y:h, x:w])  # to gpu, might slow down?\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "type": "function", "class_name": null, "function_name": "get_pulsar_camera_params", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.fcds_utils.matrix_to_rotation_6d\n    def matrix_to_rotation_6d(matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Converts rotation matrices to 6D rotation representation by Zhou et al. [1]\n        by dropping the last row. Note that 6D representation is not unique.\n        Args:\n            matrix: batch of rotation matrices of size (*, 3, 3)\n\n        Returns:\n            6D rotation representation, of size (*, 6)\n\n        [1] Zhou, Y., Barnes, C., Lu, J., Yang, J., & Li, H.\n        On the Continuity of Rotation Representations in Neural Networks.\n        IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n        Retrieved from http://arxiv.org/abs/1812.07035\n        \"\"\"\n\neasyvolcap.utils.fcds_utils.warn_once_about_pulsar_fxfy\n    def warn_once_about_pulsar_fxfy():\n\n", "dependency_sampled": "# Intra-file Dependency:\neasyvolcap.utils.fcds_utils.warn_once_about_pulsar_fxfy\n    def warn_once_about_pulsar_fxfy():\n\n", "contexts_above": "# Feature Cloud Sequence utilities\n# This files builds the components for the feature cloud sequence sampler\n\nimport torch\nfrom typing import List, Dict, Union\n\nfrom easyvolcap.utils.console_utils import *\nfrom easyvolcap.utils.base_utils import dotdict\nfrom easyvolcap.utils.raster_utils import get_ndc_perspective_matrix\nfrom easyvolcap.utils.chunk_utils import multi_gather, multi_scatter\nfrom easyvolcap.utils.math_utils import normalize_sum, affine_inverse, affine_padding\nfrom easyvolcap.utils.net_utils import MLP\n\n\ndef estimate_occupancy_field(xyz: torch.Tensor, rad: torch.Tensor, occ: torch.Tensor):\n    # This method builds a function to evaluate the occupancy field of the point cloud density field\n    # We sample the point cloud with a ball query for the largest radius in the set\n    # The actual alpha is decreased as the distance to the closest points\n    # If multiple points fall into the region of interest, we compute for alpha on all of them and performs a add operation\n    from pytorch3d.ops import ball_query\n    max_rad = rad.max()\n    # B, N, 3\n    # B, N, 1\n    # B, N, 1\n\n    def field(pts: torch.Tensor, K=10):\n        # pts: B, P, 3\n        sh = pts.shape\n        pts = pts.view(pts.shape[0], -1, 3)\n        knn = ball_query(pts, xyz, K=K, radius=max_rad, return_nn=False)\n        idx, dists = knn.idx, knn.dists  # B, P, K\n        msk = idx != -1\n        idx = torch.where(msk, idx, 0).long()\n        pix_rad = multi_gather(rad[..., 0], idx.view(idx.shape[0], -1), dim=-1).view(idx.shape)  # B, P, K\n        pix_occ = multi_gather(occ[..., 0], idx.view(idx.shape[0], -1), dim=-1).view(idx.shape)  # B, P, K\n        pix_occ = pix_occ * (1 - dists / (pix_rad * pix_rad))  # B, P, K\n        pix_occ = torch.where(msk, pix_occ, 0)\n        pix_occ = pix_occ.clip(0, 1)\n        pix_occ = pix_occ.sum(dim=-1, keepdim=True)  # B, P, 1\n        return pix_occ.view(*sh[:-1], 1)\n\n    return field\n\n# @torch.jit.script\n\n\ndef prepare_feedback_transform(H: int, W: int, K: torch.Tensor, R: torch.Tensor, T: torch.Tensor,\n                               n: torch.Tensor,\n                               f: torch.Tensor,\n                               xyz: torch.Tensor,\n                               rgb: torch.Tensor,\n                               rad: torch.Tensor):\n    ixt = get_ndc_perspective_matrix(K, H, W, n[..., 0], f[..., 0]).to(xyz.dtype)  # to opengl, remove last dim of n and f\n    w2c = affine_padding(torch.cat([R, T], dim=-1)).to(xyz.dtype)\n    c2w = affine_inverse(w2c)\n    c2w[..., 0] *= 1  # flip x\n    c2w[..., 1] *= -1  # flip y\n    c2w[..., 2] *= -1  # flip z\n    ext = affine_inverse(c2w)\n    pix_xyz = torch.cat([xyz, torch.ones_like(xyz[..., :1])], dim=-1) @ ext.mT @ ixt.mT\n    pix_rad = abs(H * ixt[..., 1, 1][..., None, None] * rad / pix_xyz[..., -1:])  # z: B, 1 * B, N, world space radius -> ndc radius B, N, 1\n\n    # Prepare data to be rendered\n    data = torch.cat([pix_xyz, rgb, pix_rad], dim=-1).ravel()  # organize the data inside vbo\n    return data\n\n\ndef matrix_to_rotation_6d(matrix: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Converts rotation matrices to 6D rotation representation by Zhou et al. [1]\n    by dropping the last row. Note that 6D representation is not unique.\n    Args:\n        matrix: batch of rotation matrices of size (*, 3, 3)\n\n    Returns:\n        6D rotation representation, of size (*, 6)\n\n    [1] Zhou, Y., Barnes, C., Lu, J., Yang, J., & Li, H.\n    On the Continuity of Rotation Representations in Neural Networks.\n    IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n    Retrieved from http://arxiv.org/abs/1812.07035\n    \"\"\"\n    batch_dim = matrix.size()[:-2]\n    return matrix[..., :2, :].clone().reshape(batch_dim + (6,))\n\n\n@run_once\ndef warn_once_about_pulsar_fxfy():\n    log(yellow(\n        \"Pulsar only supports a single focal lengths. For converting OpenCV \"\n        \"focal lengths, we average them for x and y directions. \"\n        \"The focal lengths for x and y you provided differ by more than 1%, \"\n        \"which means this could introduce a noticeable error.\"\n    ))\n\n\n", "contexts_below": "\n\ndef get_opencv_camera_params(batch: dotdict):\n    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n\ndef get_pytorch3d_camera_params(batch: dotdict):\n    # Extract pytorc3d camera parameters from batch input\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    R = batch.R.clone()\n    R[..., 0, :] *= -1  # flip x row\n    R[..., 1, :] *= -1  # flip y row\n    T = (-R @ C)[..., 0]  # c2w back to w2c\n    R = R.mT  # applied left (left multiply to right multiply, god knows why...)\n\n    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = get_pytorch3d_ndc_K(batch.K, H, W)\n\n    return H, W, K, R, T, C\n\n# TODO: Remove pcd_t and with_t semantics, this is a legacy API\n\n\ndef voxel_surface_down_sample(pcd: torch.Tensor, pcd_t: torch.Tensor = None, voxel_size: float = 0.01, dist_th: float = 0.025, n_points: int = 65536):\n    # !: BATCH\n    # TODO: Use number of vertices for good estimation\n    import open3d as o3d\n    import numpy as np\n    import mcubes\n    from easyvolcap.utils.sample_utils import point_mesh_distance\n    from pytorch3d.ops import knn_points, ball_query, sample_farthest_points\n\n    # Convert torch tensor to Open3D PointCloud\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n\n    # Create VoxelGrid from PointCloud\n    o3d_vox = o3d.geometry.VoxelGrid.create_from_point_cloud(o3d_pcd, voxel_size=voxel_size)\n\n    # Extract dense grid from VoxelGrid using get_voxel\n    voxels = o3d_vox.get_voxels()\n    max_index = np.array([vox.grid_index for vox in voxels]).max(axis=0)  # !: for-loop\n    dense_grid = np.zeros((max_index[0] + 1, max_index[1] + 1, max_index[2] + 1))\n\n    for vox in voxels:  # !: for-loop\n        dense_grid[vox.grid_index[0], vox.grid_index[1], vox.grid_index[2]] = 1\n\n    # Use marching cubes to obtain mesh from dense grid\n    vertices, triangles = mcubes.marching_cubes(dense_grid, 0.5)\n    vertices = vertices * voxel_size + o3d_vox.origin  # resizing\n\n    # Convert mesh data to torch tensors\n    triangles_torch = torch.as_tensor(vertices[triangles], device=pcd.device, dtype=pcd.dtype).float()\n\n    # Calculate distances using point_mesh_distance\n    dists, _ = point_mesh_distance(pcd[0], triangles_torch)\n\n    # Select points based on distances\n    valid = (dists < dist_th).nonzero()[..., 0]\n    while (len(valid) - n_points) / n_points > 0.005:\n        # There are too many valid points, should control its number\n        ratio = len(valid) / len(pcd[0])  # the ratio of valid points\n        n_expected = int(n_points / ratio)  # the expected number of points before surface sampling\n        pcd = random(pcd, n_points=n_expected)\n\n        # Calculate distances using point_mesh_distance\n        dists, _ = point_mesh_distance(pcd[0], triangles_torch)\n\n        # Select points based on distances\n        valid = (dists < dist_th).nonzero()[..., 0]\n\n    _, valid = dists.topk(n_points, dim=-1, sorted=False, largest=False)\n    pcd_new = torch.index_select(pcd[0], 0, valid)[None]\n\n    return pcd_new\n\n\ndef filter_bounds(pcd: torch.Tensor, pcd_t: torch.Tensor = None, bounds: torch.Tensor = None):\n    valid = ((pcd - bounds[..., 0, :]) > 0).all(dim=-1) & ((pcd - bounds[..., 1, :]) < 0).all(dim=-1)  # mask: B, N\n    valid = valid[0].nonzero()[None]  # B, S -> B, V # MARK: SYNC\n    pcd = multi_gather(pcd, valid, dim=-2)\n    return pcd\n\n\ndef duplicate(pcd: torch.Tensor, pcd_t: torch.Tensor = None, std: float = 0.005 * 0.1):\n    # return pcd.repeat_interleave(2, dim=-2), ind.repeat_interleave(2, dim=-2)\n    pcd_new = torch.normal(pcd, std=std)\n    return torch.cat([pcd, pcd_new], dim=-2)\n\n\ndef farthest(pcd: torch.Tensor, pcd_t: torch.Tensor = None, lengths: torch.Tensor = None, n_points: int = 65536):\n    from pytorch3d.ops import knn_points, ball_query, sample_farthest_points\n    idx = sample_farthest_points(pcd, lengths, K=n_points)[1]  # N, K (padded)\n    return multi_gather(pcd, idx)\n\n\ndef random(pcd: torch.Tensor, pcd_t: torch.Tensor = None, n_points: int = 65536, std: float = 0.001):\n    inds = torch.stack([torch.randperm(pcd.shape[-2], device=pcd.device)[:n_points] for b in range(len(pcd))])  # B, S,\n    return multi_gather(pcd, inds)\n\n\ndef voxel_down_sample(pcd: torch.Tensor, pcd_t: torch.Tensor = None, voxel_size=0.005):\n    import open3d as o3d\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n    o3d_pcd = o3d_pcd.voxel_down_sample(voxel_size)\n    return torch.as_tensor(np.array(o3d_pcd.points)).to(pcd.device, pcd.dtype, non_blocking=True).view(pcd.shape[0], -1, 3)\n\n\ndef remove_outlier(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 20, std_ratio=2.0, return_inds=False):  # !: BATCH\n    import open3d as o3d\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n    cl, ind = o3d_pcd.remove_statistical_outlier(nb_neighbors=K, std_ratio=std_ratio)\n    if return_inds:\n        return torch.as_tensor(np.array(ind), device=pcd.device)[None]  # N,\n    return torch.as_tensor(np.array(o3d_pcd.points)[np.array(ind)]).to(pcd.device, pcd.dtype, non_blocking=True).view(pcd.shape[0], -1, 3)\n\n\ndef farthest_down_sample(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 65536):\n    import open3d as o3d\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n    o3d_pcd = o3d_pcd.farthest_point_down_sample(K)\n    return torch.as_tensor(np.array(o3d_pcd.points)).to(pcd.device, pcd.dtype, non_blocking=True).view(pcd.shape[0], -1, 3)\n\n\ndef sample_random_points(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 500):\n    bounds = torch.stack([pcd.min(dim=-2)[0] - 0.033, pcd.max(dim=-2)[0] + 0.033], dim=-2)  # B, 2, 3\n    pts = torch.rand(*pcd.shape[:-2], K, 3, device=pcd.device) * (bounds[..., 1:, :] - bounds[..., :1, :]) + bounds[..., :1, :]\n    return pts\n\n\ndef sample_filter_random_points(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 500, update_radius=0.05, filter_K=10):\n    pts = sample_random_points(pcd, pcd_t, K)  # ugly interface\n    pts = filter_points(pts, pcd, update_radius, filter_K)\n    return pts\n\n\ndef get_pytorch3d_ndc_K(K: torch.Tensor, H: int, W: int):\n    M = min(H, W)\n    K = torch.cat([K, torch.zeros_like(K[..., -1:, :])], dim=-2)\n    K = torch.cat([K, torch.zeros_like(K[..., :, -1:])], dim=-1)\n    K[..., 3, 2] = 1  # ...? # HACK: pytorch3d magic\n    K[..., 2, 2] = 0  # ...? # HACK: pytorch3d magic\n    K[..., 2, 3] = 1  # ...? # HACK: pytorch3d magic\n\n    K[..., 0, 1] = 0\n    K[..., 1, 0] = 0\n    K[..., 2, 0] = 0\n    K[..., 2, 1] = 0\n    # return K\n\n    K[..., 0, 0] = K[..., 0, 0] * 2.0 / M  # fx\n    K[..., 1, 1] = K[..., 1, 1] * 2.0 / M  # fy\n    K[..., 0, 2] = -(K[..., 0, 2] - W / 2.0) * 2.0 / M  # px\n    K[..., 1, 2] = -(K[..., 1, 2] - H / 2.0) * 2.0 / M  # py\n    return K\n\n\ndef expand_points_features(render_scale: Union[float, int], pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float):\n    # FIXME: Duplicated code for these\n    n_points = pcd_old.shape[-2]\n    if isinstance(render_scale, int):\n        target_n_points = render_scale\n        n_points = pcd_old.shape[-2]\n        render_scale = target_n_points / n_points\n    target_n_points = int(render_scale * n_points)\n    return generate_points_features(target_n_points, pcd_old, ind_old, radius)\n\n\ndef expand_points(render_scale: Union[float, int], pcd_old: torch.Tensor, radius: float):\n    n_points = pcd_old.shape[-2]\n    if isinstance(render_scale, int):\n        target_n_points = render_scale\n        n_points = pcd_old.shape[-2]\n        render_scale = target_n_points / n_points\n    target_n_points = int(render_scale * n_points)\n    return generate_points(target_n_points, pcd_old, radius)\n\n\ndef generate_points_features(n_points: int, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float):\n    pcd_new = sample_random_points(pcd_old, K=n_points)\n    pcd_new, ind_new = update_points_features(pcd_new, pcd_old, ind_old, radius)\n    return pcd_new, ind_new\n\n\ndef generate_points(n_points: int, pcd_old: torch.Tensor, radius: float):\n    pcd_new = sample_random_points(pcd_old, K=n_points)\n    pcd_new = update_points(pcd_new, pcd_old, radius)\n    return pcd_new\n\n\ndef surface_points(pcd: torch.Tensor, pcd_t: torch.Tensor = None, radius: float = 0.05, K: int = 500, n_points: float = 16384):\n    # Try to retain the surface points\n    from pytorch3d.ops import knn_points, ball_query\n\n    # 1. Perform a ball query (with a large upper limit number of points)\n    # 2. Sort all points based on the number of neighbors\n    close = ball_query(pcd, pcd, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    dists = torch.where(idx == -1, torch.inf, 0.1)  # B, S, K, equal weight, just for filtering\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    # Find mean points\n    B, S, C = pcd.shape\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = multi_gather(pcd, idx.view(B, S * K)).view(B, S, K, -1)\n    pcd_new = (pcd_new * weights).sum(dim=-2)  # B, S, 3\n\n    # Find mean deviation\n    dists = (pcd_new - pcd).norm(dim=-1)  # B, S,\n    valid = (dists).topk(n_points, dim=-1, sorted=False)[1]  # B, K\n    pcd_new = multi_gather(pcd, valid, dim=-2)\n\n    return pcd_new\n\n\ndef surface_points_features(pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 500, n_points: float = 16384):\n    # Try to retain the surface points\n    from pytorch3d.ops import knn_points, ball_query\n\n    # 1. Perform a ball query (with a large upper limit number of points)\n    # 2. Sort all points based on the number of neighbors\n    close = ball_query(pcd_old, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    dists = torch.where(idx == -1, torch.inf, 0.1)  # B, S, K, equal weight, just for filtering\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    # Find mean points\n    B, S, C = pcd_old.shape\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)\n    pcd_new = (pcd_new * weights).sum(dim=-2)  # B, S, 3\n\n    # Find mean deviation\n    dists = (pcd_new - pcd_old).norm(dim=-1)  # B, S,\n    valid = (dists).topk(n_points, dim=-1, sorted=False)[1]  # B, K\n    pcd_new = multi_gather(pcd_old, valid, dim=-2)\n    ind_new = multi_gather(ind_old, valid, dim=-2)\n\n    return pcd_new, ind_new\n\n\ndef filter_points(pcd_new: torch.Tensor, pcd_old: torch.Tensor, radius: float = 0.05, K: int = 10, fill_ratio: float = 0.1):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n    # !: BATCH\n    good = (idx != -1).sum(dim=-1) / K > fill_ratio\n    valid = good[0].nonzero()[None]  # B, S -> B, V # MARK: SYNC\n\n    idx = multi_gather(idx, valid, dim=-2)\n    dists = multi_gather(dists, valid, dim=-2)\n    pcd_new = multi_gather(pcd_new, valid, dim=-2)\n    dists = torch.where(idx == -1, torch.inf, dists)  # B, S, K\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    return pcd_new\n\n\ndef filter_points_features(pcd_new: torch.Tensor, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 10, fill_ratio: float = 0.1):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n    # !: BATCH\n    good = (idx != -1).sum(dim=-1) / K > fill_ratio\n    valid = good[0].nonzero()[None]  # B, S -> B, V # MARK: SYNC\n\n    idx = multi_gather(idx, valid, dim=-2)\n    dists = multi_gather(dists, valid, dim=-2)\n    pcd_new = multi_gather(pcd_new, valid, dim=-2)\n    dists = torch.where(idx == -1, torch.inf, dists)  # B, S, K\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    ind_new = multi_gather(ind_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, C\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    ind_new = (ind_new * weights).sum(dim=-2)\n    # pcd_new = pcd_new.mean(dim=-2)\n    # ind_new = ind_new.mean(dim=-2)\n    return pcd_new, ind_new\n\n\ndef update_points_features(pcd_new: torch.Tensor, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    # close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    close = knn_points(pcd_new, pcd_old, return_sorted=False, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    ind_new = multi_gather(ind_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, C\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    ind_new = (ind_new * weights).sum(dim=-2)\n    # pcd_new = pcd_new.mean(dim=-2)\n    # ind_new = ind_new.mean(dim=-2)\n    return pcd_new, ind_new\n\n\ndef update_points(pcd_new: torch.Tensor, pcd_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    # close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    close = knn_points(pcd_new, pcd_old, return_sorted=False, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    # pcd_new = pcd_new.mean(dim=-2)\n    return pcd_new\n\n\ndef update_features(pcd_new: torch.Tensor, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    # close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    close = knn_points(pcd_new, pcd_old, return_sorted=False, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    ind_new = multi_gather(ind_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, C\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    ind_new = (ind_new * weights).sum(dim=-2)\n    # ind_new = ind_new.mean(dim=-2)\n    return ind_new\n\n\ndef weight_function(d2: torch.Tensor, radius: float = 0.05, delta: float = 0.001):\n    # Radius weighted function from structured local radiance field\n    weights = (-d2 / (2 * radius ** 2)).exp().clip(0)  # B, S, K\n    weights = normalize_sum(weights)\n    return weights\n", "input_code": "def get_pulsar_camera_params(\n    R: torch.Tensor,\n    tvec: torch.Tensor,\n    camera_matrix: torch.Tensor,\n    image_size: torch.Tensor,\n    znear: float = 0.1,\n) -> torch.Tensor:\n\n    \"\"\"\n    This function calculates and returns the camera parameters for Pulsar, including camera position, rotation, and intrinsic parameters such as focal length and sensor width, based on the input rotation matrix, translation vector, camera intrinsic matrix, and image size. It ensures that all inputs are batched and validates their shapes and values before computing the parameters.\n\n    Input-Output Arguments\n    :param R: torch.Tensor. A batch of rotation matrices of the camera, used to compute the camera's rotation in a different representation.\n    :param tvec: torch.Tensor. A batch of translation vectors of the camera, used to compute the camera's position.\n    :param camera_matrix: torch.Tensor. A batch of camera intrinsic matrices, used to derive focal lengths, principal points, and sensor width.\n    :param image_size: torch.Tensor. A batch of image sizes, used to adjust the principal point offsets and normalize the focal length.\n    :param znear: float, optional (default=0.1). The near clipping plane distance, used to adjust the focal length calculation.\n    :return: torch.Tensor. A tensor containing the computed camera parameters for each instance in the batch, including camera position, rotation, and intrinsic parameters.\n\n    \"\"\"", "reference_steps": "1. Verify that the inputs `R` (rotation matrix), `tvec` (translation vector), and `camera_matrix` are batched (3D tensors) and that `image_size` is a 2D tensor.\n2. Ensure that the `camera_matrix` has the shape 3x3 for each batch, and that the rotation matrix `R` also has the shape 3x3 for each batch.\n3. If `tvec` is a 2D tensor, add a new dimension to make it 3D with the shape Nx3x1.\n4. Check that all elements in the batch have the same image size by comparing the width and height of the first element with the rest.\n5. Calculate the average focal length `f` by averaging the focal lengths `fx` and `fy` from the diagonal of the `camera_matrix`.\n6. Normalize the focal length `f` into normalized device coordinates by dividing by the image width.\n7. Calculate the `focal_length` and `sensor_width` for the camera parameters using the provided `znear` value.\n8. Compute the principal point offsets `cx` and `cy` from the `camera_matrix` and adjust them to be centered offsets.\n9. Concatenate `focal_length`, `sensor_width`, `cx`, and `cy` to form the camera intrinsic parameters.\n10. Calculate the camera position `cam_pos` and rotation `cam_rot` from the rotation matrix `R` and translation vector `tvec`, and concatenate them with the intrinsic parameters to form the final camera parameters tensor `cam_params`. Return `cam_params`.", "reference_code": "def get_pulsar_camera_params(\n    R: torch.Tensor,\n    tvec: torch.Tensor,\n    camera_matrix: torch.Tensor,\n    image_size: torch.Tensor,\n    znear: float = 0.1,\n) -> torch.Tensor:\n    assert len(camera_matrix.size()) == 3, \"This function requires batched inputs!\"\n    assert len(R.size()) == 3, \"This function requires batched inputs!\"\n    assert len(tvec.size()) in (2, 3), \"This function reuqires batched inputs!\"\n\n    # Validate parameters.\n    image_size_wh = image_size.to(R).flip(dims=(1,))\n    assert torch.all(\n        image_size_wh > 0\n    ), \"height and width must be positive but min is: %s\" % (\n        str(image_size_wh.min().item())\n    )\n    assert (\n        camera_matrix.size(1) == 3 and camera_matrix.size(2) == 3\n    ), \"Incorrect camera matrix shape: expected 3x3 but got %dx%d\" % (\n        camera_matrix.size(1),\n        camera_matrix.size(2),\n    )\n    assert (\n        R.size(1) == 3 and R.size(2) == 3\n    ), \"Incorrect R shape: expected 3x3 but got %dx%d\" % (\n        R.size(1),\n        R.size(2),\n    )\n    if len(tvec.size()) == 2:\n        tvec = tvec.unsqueeze(2)\n    assert (\n        tvec.size(1) == 3 and tvec.size(2) == 1\n    ), \"Incorrect tvec shape: expected 3x1 but got %dx%d\" % (\n        tvec.size(1),\n        tvec.size(2),\n    )\n    # Check batch size.\n    batch_size = camera_matrix.size(0)\n    assert R.size(0) == batch_size, \"Expected R to have batch size %d. Has size %d.\" % (\n        batch_size,\n        R.size(0),\n    )\n    assert (\n        tvec.size(0) == batch_size\n    ), \"Expected tvec to have batch size %d. Has size %d.\" % (\n        batch_size,\n        tvec.size(0),\n    )\n    # Check image sizes.\n    image_w = image_size_wh[0, 0]\n    image_h = image_size_wh[0, 1]\n    assert torch.all(\n        image_size_wh[:, 0] == image_w\n    ), \"All images in a batch must have the same width!\"\n    assert torch.all(\n        image_size_wh[:, 1] == image_h\n    ), \"All images in a batch must have the same height!\"\n    # Focal length.\n    fx = camera_matrix[:, 0, 0].unsqueeze(1)\n    fy = camera_matrix[:, 1, 1].unsqueeze(1)\n    # Check that we introduce less than 1% error by averaging the focal lengths.\n    fx_y = fx / fy\n    if torch.any(fx_y > 1.01) or torch.any(fx_y < 0.99):\n        warn_once_about_pulsar_fxfy()\n    f = (fx + fy) / 2\n    # Normalize f into normalized device coordinates.\n    focal_length_px = f / image_w\n    # Transfer into focal_length and sensor_width.\n    # NOTE: Using torch.tensor instead of torch.as_tensor will cause cpu gpu sync\n    focal_length = torch.as_tensor([znear - 1e-5], dtype=torch.float32, device=R.device)\n    focal_length = focal_length[None, :].repeat(batch_size, 1)\n    sensor_width = focal_length / focal_length_px\n    # Principal point.\n    cx = camera_matrix[:, 0, 2].unsqueeze(1)\n    cy = camera_matrix[:, 1, 2].unsqueeze(1)\n    # Transfer principal point offset into centered offset.\n    cx = -(cx - image_w / 2)\n    cy = cy - image_h / 2\n    # Concatenate to final vector.\n    param = torch.cat([focal_length, sensor_width, cx, cy], dim=1)\n    R_trans = R.permute(0, 2, 1)\n    cam_pos = -torch.bmm(R_trans, tvec).squeeze(2)\n    cam_rot = matrix_to_rotation_6d(R_trans)\n    cam_params = torch.cat([cam_pos, cam_rot, param], dim=1)\n    return cam_params\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "type": "function", "class_name": null, "function_name": "get_pytorch3d_camera_params", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.fcds_utils.get_pytorch3d_ndc_K\n    def get_pytorch3d_ndc_K(K: torch.Tensor, H: int, W: int):\n\n# Cross-file Dependency:\neasyvolcap.utils.base_utils.dotdict\n    class dotdict(dict, Dict[KT, VT]):\n        \"\"\"\n        This is the default data passing object used throughout the codebase\n        Main function: dot access for dict values & dict like merging and updates\n\n        a dictionary that supports dot notation \n        as well as dictionary access notation \n        usage: d = make_dotdict() or d = make_dotdict{'val1':'first'})\n        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n        get attributes: d.val2 or d['val2']\n        \"\"\"\n\neasyvolcap.utils.base_utils.dotdict.H\n\neasyvolcap.utils.base_utils.dotdict.K\n\neasyvolcap.utils.base_utils.dotdict.R\n\neasyvolcap.utils.base_utils.dotdict.T\n\neasyvolcap.utils.base_utils.dotdict.W\n\neasyvolcap.utils.base_utils.dotdict.meta\n    def meta(self) -> dotdict:\n\n", "dependency_sampled": "# Intra-file Dependency:\neasyvolcap.utils.fcds_utils.get_pytorch3d_ndc_K\n    def get_pytorch3d_ndc_K(K: torch.Tensor, H: int, W: int):\n\n# Cross-file Dependency:\neasyvolcap.utils.base_utils.dotdict.K\n\neasyvolcap.utils.base_utils.dotdict.R\n\neasyvolcap.utils.base_utils.dotdict.W\n\n", "contexts_above": "# Feature Cloud Sequence utilities\n# This files builds the components for the feature cloud sequence sampler\n\nimport torch\nfrom typing import List, Dict, Union\n\nfrom easyvolcap.utils.console_utils import *\nfrom easyvolcap.utils.base_utils import dotdict\nfrom easyvolcap.utils.raster_utils import get_ndc_perspective_matrix\nfrom easyvolcap.utils.chunk_utils import multi_gather, multi_scatter\nfrom easyvolcap.utils.math_utils import normalize_sum, affine_inverse, affine_padding\nfrom easyvolcap.utils.net_utils import MLP\n\n\ndef estimate_occupancy_field(xyz: torch.Tensor, rad: torch.Tensor, occ: torch.Tensor):\n    # This method builds a function to evaluate the occupancy field of the point cloud density field\n    # We sample the point cloud with a ball query for the largest radius in the set\n    # The actual alpha is decreased as the distance to the closest points\n    # If multiple points fall into the region of interest, we compute for alpha on all of them and performs a add operation\n    from pytorch3d.ops import ball_query\n    max_rad = rad.max()\n    # B, N, 3\n    # B, N, 1\n    # B, N, 1\n\n    def field(pts: torch.Tensor, K=10):\n        # pts: B, P, 3\n        sh = pts.shape\n        pts = pts.view(pts.shape[0], -1, 3)\n        knn = ball_query(pts, xyz, K=K, radius=max_rad, return_nn=False)\n        idx, dists = knn.idx, knn.dists  # B, P, K\n        msk = idx != -1\n        idx = torch.where(msk, idx, 0).long()\n        pix_rad = multi_gather(rad[..., 0], idx.view(idx.shape[0], -1), dim=-1).view(idx.shape)  # B, P, K\n        pix_occ = multi_gather(occ[..., 0], idx.view(idx.shape[0], -1), dim=-1).view(idx.shape)  # B, P, K\n        pix_occ = pix_occ * (1 - dists / (pix_rad * pix_rad))  # B, P, K\n        pix_occ = torch.where(msk, pix_occ, 0)\n        pix_occ = pix_occ.clip(0, 1)\n        pix_occ = pix_occ.sum(dim=-1, keepdim=True)  # B, P, 1\n        return pix_occ.view(*sh[:-1], 1)\n\n    return field\n\n# @torch.jit.script\n\n\ndef prepare_feedback_transform(H: int, W: int, K: torch.Tensor, R: torch.Tensor, T: torch.Tensor,\n                               n: torch.Tensor,\n                               f: torch.Tensor,\n                               xyz: torch.Tensor,\n                               rgb: torch.Tensor,\n                               rad: torch.Tensor):\n    ixt = get_ndc_perspective_matrix(K, H, W, n[..., 0], f[..., 0]).to(xyz.dtype)  # to opengl, remove last dim of n and f\n    w2c = affine_padding(torch.cat([R, T], dim=-1)).to(xyz.dtype)\n    c2w = affine_inverse(w2c)\n    c2w[..., 0] *= 1  # flip x\n    c2w[..., 1] *= -1  # flip y\n    c2w[..., 2] *= -1  # flip z\n    ext = affine_inverse(c2w)\n    pix_xyz = torch.cat([xyz, torch.ones_like(xyz[..., :1])], dim=-1) @ ext.mT @ ixt.mT\n    pix_rad = abs(H * ixt[..., 1, 1][..., None, None] * rad / pix_xyz[..., -1:])  # z: B, 1 * B, N, world space radius -> ndc radius B, N, 1\n\n    # Prepare data to be rendered\n    data = torch.cat([pix_xyz, rgb, pix_rad], dim=-1).ravel()  # organize the data inside vbo\n    return data\n\n\ndef matrix_to_rotation_6d(matrix: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Converts rotation matrices to 6D rotation representation by Zhou et al. [1]\n    by dropping the last row. Note that 6D representation is not unique.\n    Args:\n        matrix: batch of rotation matrices of size (*, 3, 3)\n\n    Returns:\n        6D rotation representation, of size (*, 6)\n\n    [1] Zhou, Y., Barnes, C., Lu, J., Yang, J., & Li, H.\n    On the Continuity of Rotation Representations in Neural Networks.\n    IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n    Retrieved from http://arxiv.org/abs/1812.07035\n    \"\"\"\n    batch_dim = matrix.size()[:-2]\n    return matrix[..., :2, :].clone().reshape(batch_dim + (6,))\n\n\n@run_once\ndef warn_once_about_pulsar_fxfy():\n    log(yellow(\n        \"Pulsar only supports a single focal lengths. For converting OpenCV \"\n        \"focal lengths, we average them for x and y directions. \"\n        \"The focal lengths for x and y you provided differ by more than 1%, \"\n        \"which means this could introduce a noticeable error.\"\n    ))\n\n\ndef get_pulsar_camera_params(\n    R: torch.Tensor,\n    tvec: torch.Tensor,\n    camera_matrix: torch.Tensor,\n    image_size: torch.Tensor,\n    znear: float = 0.1,\n) -> torch.Tensor:\n    assert len(camera_matrix.size()) == 3, \"This function requires batched inputs!\"\n    assert len(R.size()) == 3, \"This function requires batched inputs!\"\n    assert len(tvec.size()) in (2, 3), \"This function reuqires batched inputs!\"\n\n    # Validate parameters.\n    image_size_wh = image_size.to(R).flip(dims=(1,))\n    assert torch.all(\n        image_size_wh > 0\n    ), \"height and width must be positive but min is: %s\" % (\n        str(image_size_wh.min().item())\n    )\n    assert (\n        camera_matrix.size(1) == 3 and camera_matrix.size(2) == 3\n    ), \"Incorrect camera matrix shape: expected 3x3 but got %dx%d\" % (\n        camera_matrix.size(1),\n        camera_matrix.size(2),\n    )\n    assert (\n        R.size(1) == 3 and R.size(2) == 3\n    ), \"Incorrect R shape: expected 3x3 but got %dx%d\" % (\n        R.size(1),\n        R.size(2),\n    )\n    if len(tvec.size()) == 2:\n        tvec = tvec.unsqueeze(2)\n    assert (\n        tvec.size(1) == 3 and tvec.size(2) == 1\n    ), \"Incorrect tvec shape: expected 3x1 but got %dx%d\" % (\n        tvec.size(1),\n        tvec.size(2),\n    )\n    # Check batch size.\n    batch_size = camera_matrix.size(0)\n    assert R.size(0) == batch_size, \"Expected R to have batch size %d. Has size %d.\" % (\n        batch_size,\n        R.size(0),\n    )\n    assert (\n        tvec.size(0) == batch_size\n    ), \"Expected tvec to have batch size %d. Has size %d.\" % (\n        batch_size,\n        tvec.size(0),\n    )\n    # Check image sizes.\n    image_w = image_size_wh[0, 0]\n    image_h = image_size_wh[0, 1]\n    assert torch.all(\n        image_size_wh[:, 0] == image_w\n    ), \"All images in a batch must have the same width!\"\n    assert torch.all(\n        image_size_wh[:, 1] == image_h\n    ), \"All images in a batch must have the same height!\"\n    # Focal length.\n    fx = camera_matrix[:, 0, 0].unsqueeze(1)\n    fy = camera_matrix[:, 1, 1].unsqueeze(1)\n    # Check that we introduce less than 1% error by averaging the focal lengths.\n    fx_y = fx / fy\n    if torch.any(fx_y > 1.01) or torch.any(fx_y < 0.99):\n        warn_once_about_pulsar_fxfy()\n    f = (fx + fy) / 2\n    # Normalize f into normalized device coordinates.\n    focal_length_px = f / image_w\n    # Transfer into focal_length and sensor_width.\n    # NOTE: Using torch.tensor instead of torch.as_tensor will cause cpu gpu sync\n    focal_length = torch.as_tensor([znear - 1e-5], dtype=torch.float32, device=R.device)\n    focal_length = focal_length[None, :].repeat(batch_size, 1)\n    sensor_width = focal_length / focal_length_px\n    # Principal point.\n    cx = camera_matrix[:, 0, 2].unsqueeze(1)\n    cy = camera_matrix[:, 1, 2].unsqueeze(1)\n    # Transfer principal point offset into centered offset.\n    cx = -(cx - image_w / 2)\n    cy = cy - image_h / 2\n    # Concatenate to final vector.\n    param = torch.cat([focal_length, sensor_width, cx, cy], dim=1)\n    R_trans = R.permute(0, 2, 1)\n    cam_pos = -torch.bmm(R_trans, tvec).squeeze(2)\n    cam_rot = matrix_to_rotation_6d(R_trans)\n    cam_params = torch.cat([cam_pos, cam_rot, param], dim=1)\n    return cam_params\n\n\ndef get_opencv_camera_params(batch: dotdict):\n    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n\n", "contexts_below": "\n# TODO: Remove pcd_t and with_t semantics, this is a legacy API\n\n\ndef voxel_surface_down_sample(pcd: torch.Tensor, pcd_t: torch.Tensor = None, voxel_size: float = 0.01, dist_th: float = 0.025, n_points: int = 65536):\n    # !: BATCH\n    # TODO: Use number of vertices for good estimation\n    import open3d as o3d\n    import numpy as np\n    import mcubes\n    from easyvolcap.utils.sample_utils import point_mesh_distance\n    from pytorch3d.ops import knn_points, ball_query, sample_farthest_points\n\n    # Convert torch tensor to Open3D PointCloud\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n\n    # Create VoxelGrid from PointCloud\n    o3d_vox = o3d.geometry.VoxelGrid.create_from_point_cloud(o3d_pcd, voxel_size=voxel_size)\n\n    # Extract dense grid from VoxelGrid using get_voxel\n    voxels = o3d_vox.get_voxels()\n    max_index = np.array([vox.grid_index for vox in voxels]).max(axis=0)  # !: for-loop\n    dense_grid = np.zeros((max_index[0] + 1, max_index[1] + 1, max_index[2] + 1))\n\n    for vox in voxels:  # !: for-loop\n        dense_grid[vox.grid_index[0], vox.grid_index[1], vox.grid_index[2]] = 1\n\n    # Use marching cubes to obtain mesh from dense grid\n    vertices, triangles = mcubes.marching_cubes(dense_grid, 0.5)\n    vertices = vertices * voxel_size + o3d_vox.origin  # resizing\n\n    # Convert mesh data to torch tensors\n    triangles_torch = torch.as_tensor(vertices[triangles], device=pcd.device, dtype=pcd.dtype).float()\n\n    # Calculate distances using point_mesh_distance\n    dists, _ = point_mesh_distance(pcd[0], triangles_torch)\n\n    # Select points based on distances\n    valid = (dists < dist_th).nonzero()[..., 0]\n    while (len(valid) - n_points) / n_points > 0.005:\n        # There are too many valid points, should control its number\n        ratio = len(valid) / len(pcd[0])  # the ratio of valid points\n        n_expected = int(n_points / ratio)  # the expected number of points before surface sampling\n        pcd = random(pcd, n_points=n_expected)\n\n        # Calculate distances using point_mesh_distance\n        dists, _ = point_mesh_distance(pcd[0], triangles_torch)\n\n        # Select points based on distances\n        valid = (dists < dist_th).nonzero()[..., 0]\n\n    _, valid = dists.topk(n_points, dim=-1, sorted=False, largest=False)\n    pcd_new = torch.index_select(pcd[0], 0, valid)[None]\n\n    return pcd_new\n\n\ndef filter_bounds(pcd: torch.Tensor, pcd_t: torch.Tensor = None, bounds: torch.Tensor = None):\n    valid = ((pcd - bounds[..., 0, :]) > 0).all(dim=-1) & ((pcd - bounds[..., 1, :]) < 0).all(dim=-1)  # mask: B, N\n    valid = valid[0].nonzero()[None]  # B, S -> B, V # MARK: SYNC\n    pcd = multi_gather(pcd, valid, dim=-2)\n    return pcd\n\n\ndef duplicate(pcd: torch.Tensor, pcd_t: torch.Tensor = None, std: float = 0.005 * 0.1):\n    # return pcd.repeat_interleave(2, dim=-2), ind.repeat_interleave(2, dim=-2)\n    pcd_new = torch.normal(pcd, std=std)\n    return torch.cat([pcd, pcd_new], dim=-2)\n\n\ndef farthest(pcd: torch.Tensor, pcd_t: torch.Tensor = None, lengths: torch.Tensor = None, n_points: int = 65536):\n    from pytorch3d.ops import knn_points, ball_query, sample_farthest_points\n    idx = sample_farthest_points(pcd, lengths, K=n_points)[1]  # N, K (padded)\n    return multi_gather(pcd, idx)\n\n\ndef random(pcd: torch.Tensor, pcd_t: torch.Tensor = None, n_points: int = 65536, std: float = 0.001):\n    inds = torch.stack([torch.randperm(pcd.shape[-2], device=pcd.device)[:n_points] for b in range(len(pcd))])  # B, S,\n    return multi_gather(pcd, inds)\n\n\ndef voxel_down_sample(pcd: torch.Tensor, pcd_t: torch.Tensor = None, voxel_size=0.005):\n    import open3d as o3d\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n    o3d_pcd = o3d_pcd.voxel_down_sample(voxel_size)\n    return torch.as_tensor(np.array(o3d_pcd.points)).to(pcd.device, pcd.dtype, non_blocking=True).view(pcd.shape[0], -1, 3)\n\n\ndef remove_outlier(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 20, std_ratio=2.0, return_inds=False):  # !: BATCH\n    import open3d as o3d\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n    cl, ind = o3d_pcd.remove_statistical_outlier(nb_neighbors=K, std_ratio=std_ratio)\n    if return_inds:\n        return torch.as_tensor(np.array(ind), device=pcd.device)[None]  # N,\n    return torch.as_tensor(np.array(o3d_pcd.points)[np.array(ind)]).to(pcd.device, pcd.dtype, non_blocking=True).view(pcd.shape[0], -1, 3)\n\n\ndef farthest_down_sample(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 65536):\n    import open3d as o3d\n    o3d_pcd = o3d.geometry.PointCloud()\n    o3d_pcd.points = o3d.utility.Vector3dVector(pcd.view(-1, 3).detach().cpu().numpy())\n    o3d_pcd = o3d_pcd.farthest_point_down_sample(K)\n    return torch.as_tensor(np.array(o3d_pcd.points)).to(pcd.device, pcd.dtype, non_blocking=True).view(pcd.shape[0], -1, 3)\n\n\ndef sample_random_points(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 500):\n    bounds = torch.stack([pcd.min(dim=-2)[0] - 0.033, pcd.max(dim=-2)[0] + 0.033], dim=-2)  # B, 2, 3\n    pts = torch.rand(*pcd.shape[:-2], K, 3, device=pcd.device) * (bounds[..., 1:, :] - bounds[..., :1, :]) + bounds[..., :1, :]\n    return pts\n\n\ndef sample_filter_random_points(pcd: torch.Tensor, pcd_t: torch.Tensor = None, K: int = 500, update_radius=0.05, filter_K=10):\n    pts = sample_random_points(pcd, pcd_t, K)  # ugly interface\n    pts = filter_points(pts, pcd, update_radius, filter_K)\n    return pts\n\n\ndef get_pytorch3d_ndc_K(K: torch.Tensor, H: int, W: int):\n    M = min(H, W)\n    K = torch.cat([K, torch.zeros_like(K[..., -1:, :])], dim=-2)\n    K = torch.cat([K, torch.zeros_like(K[..., :, -1:])], dim=-1)\n    K[..., 3, 2] = 1  # ...? # HACK: pytorch3d magic\n    K[..., 2, 2] = 0  # ...? # HACK: pytorch3d magic\n    K[..., 2, 3] = 1  # ...? # HACK: pytorch3d magic\n\n    K[..., 0, 1] = 0\n    K[..., 1, 0] = 0\n    K[..., 2, 0] = 0\n    K[..., 2, 1] = 0\n    # return K\n\n    K[..., 0, 0] = K[..., 0, 0] * 2.0 / M  # fx\n    K[..., 1, 1] = K[..., 1, 1] * 2.0 / M  # fy\n    K[..., 0, 2] = -(K[..., 0, 2] - W / 2.0) * 2.0 / M  # px\n    K[..., 1, 2] = -(K[..., 1, 2] - H / 2.0) * 2.0 / M  # py\n    return K\n\n\ndef expand_points_features(render_scale: Union[float, int], pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float):\n    # FIXME: Duplicated code for these\n    n_points = pcd_old.shape[-2]\n    if isinstance(render_scale, int):\n        target_n_points = render_scale\n        n_points = pcd_old.shape[-2]\n        render_scale = target_n_points / n_points\n    target_n_points = int(render_scale * n_points)\n    return generate_points_features(target_n_points, pcd_old, ind_old, radius)\n\n\ndef expand_points(render_scale: Union[float, int], pcd_old: torch.Tensor, radius: float):\n    n_points = pcd_old.shape[-2]\n    if isinstance(render_scale, int):\n        target_n_points = render_scale\n        n_points = pcd_old.shape[-2]\n        render_scale = target_n_points / n_points\n    target_n_points = int(render_scale * n_points)\n    return generate_points(target_n_points, pcd_old, radius)\n\n\ndef generate_points_features(n_points: int, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float):\n    pcd_new = sample_random_points(pcd_old, K=n_points)\n    pcd_new, ind_new = update_points_features(pcd_new, pcd_old, ind_old, radius)\n    return pcd_new, ind_new\n\n\ndef generate_points(n_points: int, pcd_old: torch.Tensor, radius: float):\n    pcd_new = sample_random_points(pcd_old, K=n_points)\n    pcd_new = update_points(pcd_new, pcd_old, radius)\n    return pcd_new\n\n\ndef surface_points(pcd: torch.Tensor, pcd_t: torch.Tensor = None, radius: float = 0.05, K: int = 500, n_points: float = 16384):\n    # Try to retain the surface points\n    from pytorch3d.ops import knn_points, ball_query\n\n    # 1. Perform a ball query (with a large upper limit number of points)\n    # 2. Sort all points based on the number of neighbors\n    close = ball_query(pcd, pcd, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    dists = torch.where(idx == -1, torch.inf, 0.1)  # B, S, K, equal weight, just for filtering\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    # Find mean points\n    B, S, C = pcd.shape\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = multi_gather(pcd, idx.view(B, S * K)).view(B, S, K, -1)\n    pcd_new = (pcd_new * weights).sum(dim=-2)  # B, S, 3\n\n    # Find mean deviation\n    dists = (pcd_new - pcd).norm(dim=-1)  # B, S,\n    valid = (dists).topk(n_points, dim=-1, sorted=False)[1]  # B, K\n    pcd_new = multi_gather(pcd, valid, dim=-2)\n\n    return pcd_new\n\n\ndef surface_points_features(pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 500, n_points: float = 16384):\n    # Try to retain the surface points\n    from pytorch3d.ops import knn_points, ball_query\n\n    # 1. Perform a ball query (with a large upper limit number of points)\n    # 2. Sort all points based on the number of neighbors\n    close = ball_query(pcd_old, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    dists = torch.where(idx == -1, torch.inf, 0.1)  # B, S, K, equal weight, just for filtering\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    # Find mean points\n    B, S, C = pcd_old.shape\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)\n    pcd_new = (pcd_new * weights).sum(dim=-2)  # B, S, 3\n\n    # Find mean deviation\n    dists = (pcd_new - pcd_old).norm(dim=-1)  # B, S,\n    valid = (dists).topk(n_points, dim=-1, sorted=False)[1]  # B, K\n    pcd_new = multi_gather(pcd_old, valid, dim=-2)\n    ind_new = multi_gather(ind_old, valid, dim=-2)\n\n    return pcd_new, ind_new\n\n\ndef filter_points(pcd_new: torch.Tensor, pcd_old: torch.Tensor, radius: float = 0.05, K: int = 10, fill_ratio: float = 0.1):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n    # !: BATCH\n    good = (idx != -1).sum(dim=-1) / K > fill_ratio\n    valid = good[0].nonzero()[None]  # B, S -> B, V # MARK: SYNC\n\n    idx = multi_gather(idx, valid, dim=-2)\n    dists = multi_gather(dists, valid, dim=-2)\n    pcd_new = multi_gather(pcd_new, valid, dim=-2)\n    dists = torch.where(idx == -1, torch.inf, dists)  # B, S, K\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    return pcd_new\n\n\ndef filter_points_features(pcd_new: torch.Tensor, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 10, fill_ratio: float = 0.1):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n    # !: BATCH\n    good = (idx != -1).sum(dim=-1) / K > fill_ratio\n    valid = good[0].nonzero()[None]  # B, S -> B, V # MARK: SYNC\n\n    idx = multi_gather(idx, valid, dim=-2)\n    dists = multi_gather(dists, valid, dim=-2)\n    pcd_new = multi_gather(pcd_new, valid, dim=-2)\n    dists = torch.where(idx == -1, torch.inf, dists)  # B, S, K\n    idx = torch.where(idx == -1, 0, idx)  # B, S, K\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    ind_new = multi_gather(ind_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, C\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    ind_new = (ind_new * weights).sum(dim=-2)\n    # pcd_new = pcd_new.mean(dim=-2)\n    # ind_new = ind_new.mean(dim=-2)\n    return pcd_new, ind_new\n\n\ndef update_points_features(pcd_new: torch.Tensor, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    # close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    close = knn_points(pcd_new, pcd_old, return_sorted=False, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    ind_new = multi_gather(ind_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, C\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    ind_new = (ind_new * weights).sum(dim=-2)\n    # pcd_new = pcd_new.mean(dim=-2)\n    # ind_new = ind_new.mean(dim=-2)\n    return pcd_new, ind_new\n\n\ndef update_points(pcd_new: torch.Tensor, pcd_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    # close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    close = knn_points(pcd_new, pcd_old, return_sorted=False, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    # pcd_new = pcd_new.mean(dim=-2)\n    return pcd_new\n\n\ndef update_features(pcd_new: torch.Tensor, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # This will lead to shrinking\n    from pytorch3d.ops import knn_points, ball_query\n\n    # close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    close = knn_points(pcd_new, pcd_old, return_sorted=False, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    ind_new = multi_gather(ind_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, C\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    ind_new = (ind_new * weights).sum(dim=-2)\n    # ind_new = ind_new.mean(dim=-2)\n    return ind_new\n\n\ndef weight_function(d2: torch.Tensor, radius: float = 0.05, delta: float = 0.001):\n    # Radius weighted function from structured local radiance field\n    weights = (-d2 / (2 * radius ** 2)).exp().clip(0)  # B, S, K\n    weights = normalize_sum(weights)\n    return weights\n", "input_code": "def get_pytorch3d_camera_params(batch: dotdict):\n    # Extract pytorc3d camera parameters from batch input\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n\n    \"\"\"\n    Extracts and adjusts PyTorch3D camera parameters from a given batch input to align with PyTorch3D's coordinate system and conventions. It performs necessary transformations on rotation and translation matrices and computes the camera intrinsic matrix for normalized device coordinates (NDC).\n\n    Input-Output Arguments\n    :param batch: dotdict. A batch of data containing camera parameters (R, T, K) and metadata (H, W) in a specific format. The rotation (R) and translation (T) matrices are adjusted to match PyTorch3D's requirements, and the intrinsic matrix (K) is recalculated for NDC.\n    :return: Tuple containing the height (H) and width (W) of the images, the intrinsic matrix (K) for NDC, the adjusted rotation matrix (R), the adjusted translation vector (T), and the camera center (C) in the camera's coordinate system.\n    \"\"\"", "reference_steps": "1. Define a function `get_pytorch3d_camera_params` that takes a `batch` object of type `dotdict` as input, which contains camera parameters and metadata.\n\n2. Extract the rotation matrix `R` and translation vector `T` from the input `batch`, where `R` is expected to be in a transposed format from OpenCV camera format.\n\n3. Adjust the coordinate system to match PyTorch3D's convention (left, up, front) from OpenCV's convention (right, down, front) by applying corrections to both `T` and `R`.\n\n4. Compute the camera center `C` by negating and multiplying the transposed rotation matrix `R` with the translation vector `T`.\n\n5. Clone the rotation matrix `R` to avoid modifying the original data.\n\n6. Flip the x and y rows of the cloned rotation matrix `R` to correct the coordinate system orientation.\n\n7. Calculate the corrected translation vector `T` by negating the matrix multiplication of the corrected rotation matrix `R` and the camera center `C`, and then selecting the first column.\n\n8. Transpose the corrected rotation matrix `R` to switch from a left-multiplication convention to a right-multiplication convention.\n\n9. Extract the image height `H` and width `W` from the `batch.meta` object, assuming the batch contains identical dimensions for all images.\n\n10. Call a function `get_pytorch3d_ndc_K` with the intrinsic matrix `K` from the batch, along with `H` and `W`, to get the normalized device coordinates (NDC) intrinsic matrix suitable for PyTorch3D.\n\n11. Return the image height `H`, width `W`, NDC intrinsic matrix `K`, corrected rotation matrix `R`, corrected translation vector `T`, and camera center `C` as the output of the function.", "reference_code": "def get_pytorch3d_camera_params(batch: dotdict):\n    # Extract pytorc3d camera parameters from batch input\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    R = batch.R.clone()\n    R[..., 0, :] *= -1  # flip x row\n    R[..., 1, :] *= -1  # flip y row\n    T = (-R @ C)[..., 0]  # c2w back to w2c\n    R = R.mT  # applied left (left multiply to right multiply, god knows why...)\n\n    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = get_pytorch3d_ndc_K(batch.K, H, W)\n\n    return H, W, K, R, T, C\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "type": "function", "class_name": null, "function_name": "inner_outer", "dependency_all": "# Cross-file Dependency:\neasyvolcap.utils.prop_utils.searchsorted\n    def searchsorted(a: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Find indices where v should be inserted into a to maintain order.\n        This behaves like jnp.searchsorted (its second output is the same as\n        jnp.searchsorted's output if all elements of v are in [a[0], a[-1]]) but is\n        faster because it wastes memory to save some compute.\n        Args:\n          a: tensor, the sorted reference points that we are scanning to see where v\n            should lie.\n          v: tensor, the query points that we are pretending to insert into a. Does\n            not need to be sorted. All but the last dimensions should match or expand\n            to those of a, the last dimension can differ.\n        Returns:\n          (idx_lo, idx_hi), where a[idx_lo] <= v < a[idx_hi], unless v is out of the\n          range [a[0], a[-1]] in which case idx_lo and idx_hi are both the first or\n          last index of a.\n        \"\"\"\n\n", "dependency_sampled": "# Cross-file Dependency:\neasyvolcap.utils.prop_utils.searchsorted\n    def searchsorted(a: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Find indices where v should be inserted into a to maintain order.\n        This behaves like jnp.searchsorted (its second output is the same as\n        jnp.searchsorted's output if all elements of v are in [a[0], a[-1]]) but is\n        faster because it wastes memory to save some compute.\n        Args:\n          a: tensor, the sorted reference points that we are scanning to see where v\n            should lie.\n          v: tensor, the query points that we are pretending to insert into a. Does\n            not need to be sorted. All but the last dimensions should match or expand\n            to those of a, the last dimension can differ.\n        Returns:\n          (idx_lo, idx_hi), where a[idx_lo] <= v < a[idx_hi], unless v is out of the\n          range [a[0], a[-1]] in which case idx_lo and idx_hi are both the first or\n          last index of a.\n        \"\"\"\n\n", "contexts_above": "import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models.vgg as vgg\nfrom collections import namedtuple\n\nfrom easyvolcap.utils.prop_utils import searchsorted, matchup_channels\n\nfrom enum import Enum, auto\n\nclass ElasticLossReduceType(Enum):\n    WEIGHT = auto()\n    MEDIAN = auto()\n\n\nclass ImgLossType(Enum):\n    PERC = auto()  # lpips\n    CHARB = auto()\n    HUBER = auto()\n    L1 = auto()\n    L2 = auto()\n    SSIM = auto()\n\nclass DptLossType(Enum):\n    SMOOTHL1 = auto()\n    L1 = auto()\n    L2 = auto()\n    SSIMSE = auto()\n    SSIMAE = auto()\n    SILOG = auto()\n    CONTINUITY = auto()\n    RANKING = auto()\n\n# from mipnerf360\n\n\n", "contexts_below": "\n# from mipnerf360\n\n\ndef lossfun_outer(t: torch.Tensor, w: torch.Tensor, t_env: torch.Tensor, w_env: torch.Tensor, eps=torch.finfo(torch.float32).eps):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n    \"\"\"The proposal weight should be an upper envelope on the nerf weight.\"\"\"\n    _, w_outer = inner_outer(t, t_env, w_env)\n    # We assume w_inner <= w <= w_outer. We don't penalize w_inner because it's\n    # more effective to pull w_outer up than it is to push w_inner down.\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    return (w - w_outer).clip(0.).pow(2) / (w + eps)\n\n\ndef blur_stepfun(x, y, r):\n    xr, xr_idx = torch.sort(torch.cat([x - r, x + r], dim=-1))\n    y1 = (torch.cat([y, torch.zeros_like(y[..., :1])], dim=-1) -\n          torch.cat([torch.zeros_like(y[..., :1]), y], dim=-1)) / (2 * r)\n    y2 = torch.cat([y1, -y1], dim=-1).take_along_dim(xr_idx[..., :-1], dim=-1)\n    yr = torch.cumsum((xr[..., 1:] - xr[..., :-1]) *\n                      torch.cumsum(y2, dim=-1), dim=-1).clamp_min(0)\n    yr = torch.cat([torch.zeros_like(yr[..., :1]), yr], dim=-1)\n    return xr, yr\n\n\ndef sorted_interp_quad(x, xp, fpdf, fcdf):\n    \"\"\"interp in quadratic\"\"\"\n\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[..., None, :] >= xp[..., :, None]\n\n    def find_interval(x, return_idx=False):\n        # Grab the value where `mask` switches from True to False, and vice versa.\n        # This approach takes advantage of the fact that `x` is sorted.\n        x0, x0_idx = torch.max(torch.where(mask, x[..., None], x[..., :1, None]), -2)\n        x1, x1_idx = torch.min(torch.where(~mask, x[..., None], x[..., -1:, None]), -2)\n        if return_idx:\n            return x0, x1, x0_idx, x1_idx\n        return x0, x1\n\n    fcdf0, fcdf1, fcdf0_idx, fcdf1_idx = find_interval(fcdf, return_idx=True)\n    fpdf0 = fpdf.take_along_dim(fcdf0_idx, dim=-1)\n    fpdf1 = fpdf.take_along_dim(fcdf1_idx, dim=-1)\n    xp0, xp1 = find_interval(xp)\n\n    offset = torch.clip(torch.nan_to_num((x - xp0) / (xp1 - xp0), 0), 0, 1)\n    ret = fcdf0 + (x - xp0) * (fpdf0 + fpdf1 * offset + fpdf0 * (1 - offset)) / 2\n    return ret\n\n\ndef lossfun_zip_outer(t, w, t_env, w_env, pulse_width, eps=1e-6):\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, pulse_width)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n\ndef lossfun_distortion(t: torch.Tensor, w: torch.Tensor):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    \"\"\"Compute iint w[i] w[j] |t[i] - t[j]| di dj.\"\"\"\n    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    return loss_inter + loss_intra\n\n\ndef interval_distortion(t0_lo, t0_hi, t1_lo, t1_hi):\n    \"\"\"Compute mean(abs(x-y); x in [t0_lo, t0_hi], y in [t1_lo, t1_hi]).\"\"\"\n    # Distortion when the intervals do not overlap.\n    d_disjoint = torch.abs((t1_lo + t1_hi) / 2 - (t0_lo + t0_hi) / 2)\n\n    # Distortion when the intervals overlap.\n    d_overlap = (2 *\n                 (torch.minimum(t0_hi, t1_hi)**3 - torch.maximum(t0_lo, t1_lo)**3) +\n                 3 * (t1_hi * t0_hi * torch.abs(t1_hi - t0_hi) +\n                      t1_lo * t0_lo * torch.abs(t1_lo - t0_lo) + t1_hi * t0_lo *\n                      (t0_lo - t1_hi) + t1_lo * t0_hi *\n                      (t1_lo - t0_hi))) / (6 * (t0_hi - t0_lo) * (t1_hi - t1_lo))\n\n    # Are the two intervals not overlapping?\n    are_disjoint = (t0_lo > t1_hi) | (t1_lo > t0_hi)\n\n    return torch.where(are_disjoint, d_disjoint, d_overlap)\n\n\ndef anneal_loss_weight(weight: float, gamma: float, iter: int, mile: int):\n    # exponentially anneal the loss weight\n    return weight * gamma ** min(iter / mile, 1)\n\n\ndef gaussian_entropy_relighting4d(albedo_pred):\n    albedo_entropy = 0\n    for i in range(3):\n        channel = albedo_pred[..., i]\n        hist = GaussianHistogram(15, 0., 1., sigma=torch.var(channel))\n        h = hist(channel)\n        if h.sum() > 1e-6:\n            h = h.div(h.sum()) + 1e-6\n        else:\n            h = torch.ones_like(h)\n        albedo_entropy += torch.sum(-h * torch.log(h))\n    return albedo_entropy\n\n\nclass GaussianHistogram(nn.Module):\n    def __init__(self, bins, min, max, sigma):\n        super(GaussianHistogram, self).__init__()\n        self.bins = bins\n        self.min = min\n        self.max = max\n        self.sigma = sigma\n        self.delta = float(max - min) / float(bins)\n        self.centers = float(min) + self.delta * (torch.arange(bins, device=sigma.device).float() + 0.5)\n\n    def forward(self, x):\n        x = torch.unsqueeze(x, 0) - torch.unsqueeze(self.centers, 1)\n        x = torch.exp(-0.5 * (x / self.sigma)**2) / (self.sigma * np.sqrt(np.pi * 2)) * self.delta\n        x = x.sum(dim=1)\n        return x\n\n\ndef gaussian_entropy(x: torch.Tensor, *args, **kwargs):\n    eps = 1e-6\n    hps = 1e-9\n    h = gaussian_histogram(x, *args, **kwargs)\n    # h = (h / (h.sum(dim=0) + hps)).clip(eps)  # 3,\n    # entropy = (-h * h.log()).sum(dim=0).sum(dim=0)  # per channel entropy summed\n    entropy = 0\n    for i in range(3):\n        hi = h[..., i]\n        if hi.sum() > eps:\n            hi = hi / hi.sum() + eps\n        else:\n            hi = torch.ones_like(hi)\n        entropy += torch.sum(-hi * torch.log(hi))\n    return entropy\n\n\ndef gaussian_histogram(x: torch.Tensor, bins: int = 15, min: float = 0.0, max: float = 1.0):\n    x = x.view(-1, x.shape[-1])  # N, 3\n    sigma = x.var(dim=0)  # 3,\n    delta = (max - min) / bins\n    centers = min + delta * (torch.arange(bins, device=x.device, dtype=x.dtype) + 0.5)  # BIN\n    x = x[None] - centers[:, None, None]  # BIN, N, 3\n    x = (-0.5 * (x / sigma).pow(2)).exp() / (sigma * np.sqrt(np.pi * 2)) * delta  # BIN, N, 3\n    x = x.sum(dim=1)\n    return x  # BIN, 3\n\n\ndef reg_diff_crit(x: torch.Tensor, iter_step: int, max_weight: float = 1e-4, ann_iter: int = 100 * 500):\n    weight = min(iter_step, ann_iter) * max_weight / ann_iter\n    return reg(x), weight\n\n\ndef reg_raw_crit(x: torch.Tensor, iter_step: int, max_weight: float = 1e-4, ann_iter: int = 100 * 500):\n    weight = min(iter_step, ann_iter) * max_weight / ann_iter\n    n_batch, n_pts_x2, D = x.shape\n    n_pts = n_pts_x2 // 2\n    length = x.norm(dim=-1, keepdim=True)  # length\n    vector = x / (length + 1e-8)  # vector direction (normalized to unit sphere)\n    # loss_length = mse(length[:, n_pts:, :], length[:, :n_pts, :])\n    loss_vector = reg((vector[:, n_pts:, :] - vector[:, :n_pts, :]))\n    # loss = loss_length + loss_vector\n    loss = loss_vector\n    return loss, weight\n\n\nclass LossNetwork(torch.nn.Module):\n    \"\"\"Reference:\n        https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n    \"\"\"\n\n    def __init__(self):\n        super(LossNetwork, self).__init__()\n        try:\n            from torchvision.models import VGG19_Weights\n            self.vgg_layers = vgg.vgg19(weights=VGG19_Weights.DEFAULT).features\n        except ImportError:\n            self.vgg_layers = vgg.vgg19(pretrained=True).features\n\n        for param in self.vgg_layers.parameters():\n            param.requires_grad = False\n        '''\n        self.layer_name_mapping = {\n            '3': \"relu1\",\n            '8': \"relu2\",\n            '17': \"relu3\",\n            '26': \"relu4\",\n            '35': \"relu5\",\n        }\n        '''\n\n        self.layer_name_mapping = {'3': \"relu1\", '8': \"relu2\"}\n\n    def forward(self, x):\n        output = {}\n        for name, module in self.vgg_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n            if name == '8':\n                break\n        LossOutput = namedtuple(\"LossOutput\", [\"relu1\", \"relu2\"])\n        return LossOutput(**output)\n\n\nclass PerceptualLoss(torch.nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n\n        self.model = LossNetwork()\n        self.model.cuda()\n        self.model.eval()\n        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n        self.l1_loss = torch.nn.L1Loss(reduction='mean')\n\n    def forward(self, x, target):\n        x_feature = self.model(x[:, 0:3, :, :])\n        target_feature = self.model(target[:, 0:3, :, :])\n\n        feature_loss = (\n            self.l1_loss(x_feature.relu1, target_feature.relu1) +\n            self.l1_loss(x_feature.relu2, target_feature.relu2)) / 2.0\n\n        l1_loss = self.l1_loss(x, target)\n        l2_loss = self.mse_loss(x, target)\n\n        loss = feature_loss + l1_loss + l2_loss\n\n        return loss\n\n\nclass VGGPerceptualLoss(torch.nn.Module):\n    def __init__(self, resize=False):\n        super(VGGPerceptualLoss, self).__init__()\n        blocks = []\n        import torchvision\n        vgg16 = torchvision.models.vgg16(pretrained=True)\n        blocks.append(vgg16.features[:4].eval())\n        blocks.append(vgg16.features[4:9].eval())\n        blocks.append(vgg16.features[9:16].eval())\n        blocks.append(vgg16.features[16:23].eval())\n        for bl in blocks:\n            for p in bl.parameters():\n                p.requires_grad = False\n        self.blocks = nn.ModuleList(blocks)\n        self.transform = F.interpolate\n        self.resize = resize\n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n        if input.shape[1] != 3:\n            input = input.repeat(1, 3, 1, 1)\n            target = target.repeat(1, 3, 1, 1)\n        input = (input - self.mean) / self.std\n        target = (target - self.mean) / self.std\n        if self.resize:\n            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n        loss = 0.0\n        x = input\n        y = target\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            y = block(y)\n            if i in feature_layers:\n                loss += F.l1_loss(x, y)\n            if i in style_layers:\n                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n                gram_x = act_x @ act_x.permute(0, 2, 1)\n                gram_y = act_y @ act_y.permute(0, 2, 1)\n                loss += F.l1_loss(gram_x, gram_y)\n        return loss\n\n\ndef eikonal(x: torch.Tensor, th=1.0) -> torch.Tensor:\n    return ((x.norm(dim=-1) - th)**2).mean()\n\n\ndef sdf_mask_crit(ret, batch):\n    msk_sdf = ret['msk_sdf']\n    msk_label = ret['msk_label']\n\n    alpha = 50\n    alpha_factor = 2\n    alpha_milestones = [10000, 20000, 30000, 40000, 50000]\n    for milestone in alpha_milestones:\n        if batch['iter_step'] > milestone:\n            alpha = alpha * alpha_factor\n\n    msk_sdf = -alpha * msk_sdf\n    mask_loss = F.binary_cross_entropy_with_logits(msk_sdf, msk_label) / alpha\n\n    return mask_loss\n\n\ndef cross_entropy(x: torch.Tensor, y: torch.Tensor):\n    # x: unormalized input logits\n    # channel last cross entropy loss\n    x = x.view(-1, x.shape[-1])  # N, C\n    y = y.view(-1, y.shape[-1])  # N, C\n    return F.cross_entropy(x, y)\n\n\ndef huber(x: torch.Tensor, y: torch.Tensor):\n    return F.huber_loss(x, y, reduction='mean')\n\n\ndef smoothl1(x: torch.Tensor, y: torch.Tensor):\n    return F.smooth_l1_loss(x, y)\n\n\ndef mse(x: torch.Tensor, y: torch.Tensor):\n    return ((x.float() - y.float())**2).mean()\n\n\ndef dot(x: torch.Tensor, y: torch.Tensor):\n    return (x * y).sum(dim=-1)\n\n\ndef l1(x: torch.Tensor, y: torch.Tensor):\n    return l1_reg(x - y)\n\n\ndef l2(x: torch.Tensor, y: torch.Tensor):\n    return l2_reg(x - y)\n\n\ndef l1_reg(x: torch.Tensor):\n    return x.abs().sum(dim=-1).mean()\n\n\ndef l2_reg(x: torch.Tensor) -> torch.Tensor:\n    return (x**2).sum(dim=-1).mean()\n\n\ndef bce_loss(x: torch.Tensor, y: torch.Tensor):\n    return F.binary_cross_entropy(x, y)\n\n\ndef mIoU_loss(x: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Compute the mean intersection of union loss over masked regions\n    x, y: B, N, 1\n    \"\"\"\n    I = (x * y).sum(-1).sum(-1)\n    U = (x + y).sum(-1).sum(-1) - I\n    mIoU = (I / U.detach()).mean()\n    return 1 - mIoU\n\n\ndef reg(x: torch.Tensor) -> torch.Tensor:\n    return x.norm(dim=-1).mean()\n\n\ndef thresh(x: torch.Tensor, a: torch.Tensor, eps: float = 1e-8):\n    return 1 / (l2(x, a) + eps)\n\n\ndef elastic_crit(jac: torch.Tensor) -> torch.Tensor:\n    \"\"\"Compute the raw 'log_svals' type elastic energy, and\n    remap it using the Geman-McClure type of robust loss.\n    Args:\n        jac (torch.Tensor): (B, N, 3, 3), the gradient of warpped xyz with respect to the original xyz\n    Return:\n        elastic_loss (torch.Tensor): (B, N), \n    \"\"\"\n    # !: CUDA IMPLEMENTATION OF SVD IS EXTREMELY SLOW\n    # old_device = jac.device\n    # jac = jac.cpu()\n    # svd_backward: Setting compute_uv to false in torch.svd doesn't compute singular matrices, and hence we cannot compute backward. Please use torch.svd(compute_uv=True)\n    _, S, _ = torch.svd(jac, compute_uv=True)           # (B, N, 3)\n    # S = S.to(old_device)\n    log_svals = torch.log(torch.clamp(S, min=1e-6))     # (B, N, 3)\n    sq_residual = torch.sum(log_svals**2, dim=-1)       # (B, N)\n    # TODO: determine whether it is a good choice to compute the robust loss here\n    elastic_loss = general_loss_with_squared_residual(sq_residual, alpha=-2.0, scale=0.03)\n    return elastic_loss\n\n\ndef general_loss_with_squared_residual(squared_x, alpha, scale):\n    r\"\"\"The general loss that takes a squared residual.\n    This fuses the sqrt operation done to compute many residuals while preserving\n    the square in the loss formulation.\n    This implements the rho(x, \\alpha, c) function described in \"A General and\n    Adaptive Robust Loss Function\", Jonathan T. Barron,\n    https://arxiv.org/abs/1701.03077.\n    Args:\n        squared_x: The residual for which the loss is being computed. x can have\n        any shape, and alpha and scale will be broadcasted to match x's shape if\n        necessary.\n        alpha: The shape parameter of the loss (\\alpha in the paper), where more\n        negative values produce a loss with more robust behavior (outliers \"cost\"\n        less), and more positive values produce a loss with less robust behavior\n        (outliers are penalized more heavily). Alpha can be any value in\n        [-infinity, infinity], but the gradient of the loss with respect to alpha\n        is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n        interpolation between several discrete robust losses:\n            alpha=-Infinity: Welsch/Leclerc Loss.\n            alpha=-2: Geman-McClure loss.\n            alpha=0: Cauchy/Lortentzian loss.\n            alpha=1: Charbonnier/pseudo-Huber loss.\n            alpha=2: L2 loss.\n        scale: The scale parameter of the loss. When |x| < scale, the loss is an\n        L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n        different shape according to alpha.\n    Returns:\n        The losses for each element of x, in the same shape as x.\n    \"\"\"\n    # https://pytorch.org/docs/stable/type_info.html\n    eps = torch.tensor(torch.finfo(torch.float32).eps)\n\n    # convert the float to torch.tensor\n    alpha = torch.tensor(alpha).to(squared_x.device)\n    scale = torch.tensor(scale).to(squared_x.device)\n\n    # This will be used repeatedly.\n    squared_scaled_x = squared_x / (scale ** 2)\n\n    # The loss when alpha == 2.\n    loss_two = 0.5 * squared_scaled_x\n    # The loss when alpha == 0.\n    loss_zero = log1p_safe(0.5 * squared_scaled_x)\n    # The loss when alpha == -infinity.\n    loss_neginf = -torch.expm1(-0.5 * squared_scaled_x)\n    # The loss when alpha == +infinity.\n    loss_posinf = expm1_safe(0.5 * squared_scaled_x)\n\n    # The loss when not in one of the above special cases.\n    # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n    beta_safe = torch.maximum(eps, torch.abs(alpha - 2.))\n    # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n    alpha_safe = torch.where(\n        torch.greater_equal(alpha, torch.tensor(0.)), torch.ones_like(alpha),\n        -torch.ones_like(alpha)) * torch.maximum(eps, torch.abs(alpha))\n    loss_otherwise = (beta_safe / alpha_safe) * (\n        torch.pow(squared_scaled_x / beta_safe + 1., 0.5 * alpha) - 1.)\n\n    # Select which of the cases of the loss to return.\n    loss = torch.where(\n        alpha == -torch.inf, loss_neginf,\n        torch.where(\n            alpha == 0, loss_zero,\n            torch.where(\n                alpha == 2, loss_two,\n                torch.where(alpha == torch.inf, loss_posinf, loss_otherwise))))\n\n    return scale * loss\n\n\ndef log1p_safe(x):\n    \"\"\"The same as torch.log1p(x), but clamps the input to prevent NaNs.\"\"\"\n    return torch.log1p(torch.minimum(x, torch.tensor(3e37)))\n\n\ndef expm1_safe(x):\n    \"\"\"The same as torch.expm1(x), but clamps the input to prevent NaNs.\"\"\"\n    return torch.expm1(torch.minimum(x, torch.tensor(87.5)))\n\n\ndef compute_plane_tv(t):\n    batch_size, c, h, w = t.shape\n    count_h = batch_size * c * (h - 1) * w\n    count_w = batch_size * c * h * (w - 1)\n    h_tv = torch.square(t[..., 1:, :] - t[..., :h - 1, :]).sum()\n    w_tv = torch.square(t[..., :, 1:] - t[..., :, :w - 1]).sum()\n    return 2 * (h_tv / count_h + w_tv / count_w)  # This is summing over batch and c instead of avg\n\n\ndef compute_planes_tv(embedding):\n    tv_loss = 0\n    for emb in embedding:\n        tv_loss += compute_plane_tv(emb)\n    return tv_loss\n\n\ndef compute_plane_smoothness(t):\n    batch_size, c, h, w = t.shape\n    # Convolve with a second derivative filter, in the time dimension which is dimension 2\n    first_difference = t[..., 1:] - t[..., :w - 1]  # [batch, c, h-1, w]\n    second_difference = first_difference[..., 1:] - first_difference[..., :w - 2]  # [batch, c, h-2, w]\n    # Take the L2 norm of the result\n    return torch.square(second_difference).mean()\n\n\ndef compute_time_planes_smooth(embedding):\n    loss = 0.\n    for emb in embedding:\n        loss += compute_plane_smoothness(emb)\n    return loss\n\n\ndef compute_ssim(x: torch.Tensor, y: torch.Tensor):\n    from pytorch_msssim import ssim\n    return ssim(x, y, data_range=1.0, win_size=11, win_sigma=1.5, K=(0.01, 0.03))\n\n\n# from MonoSDF\ndef compute_scale_and_shift(prediction, target, mask):\n    # System matrix: A = [[a_00, a_01], [a_10, a_11]]\n    a_00 = torch.sum(mask * prediction * prediction, (1, 2))\n    a_01 = torch.sum(mask * prediction, (1, 2))\n    a_11 = torch.sum(mask, (1, 2))\n\n    # Right hand side: b = [b_0, b_1]\n    b_0 = torch.sum(mask * prediction * target, (1, 2))\n    b_1 = torch.sum(mask * target, (1, 2))\n\n    # Solution: x = A^-1 . b = [[a_11, -a_01], [-a_10, a_00]] / (a_00 * a_11 - a_01 * a_10) . b\n    x_0 = torch.zeros_like(b_0)\n    x_1 = torch.zeros_like(b_1)\n\n    det = a_00 * a_11 - a_01 * a_01\n    valid = det.nonzero()\n\n    x_0[valid] = ( a_11[valid] * b_0[valid] - a_01[valid] * b_1[valid]) / det[valid]\n    x_1[valid] = (-a_01[valid] * b_0[valid] + a_00[valid] * b_1[valid]) / det[valid]\n\n    return x_0, x_1\n\n\ndef reduction_batch_based(image_loss, M):\n    # Average of all valid pixels of the batch\n    # Avoid division by 0 (if sum(M) = sum(sum(mask)) = 0: sum(image_loss) = 0)\n    divisor = torch.sum(M)\n\n    if divisor == 0: return 0\n    else: return torch.sum(image_loss) / divisor\n\n\ndef reduction_image_based(image_loss, M):\n    # Mean of average of valid pixels of an image\n    # Avoid division by 0 (if M = sum(mask) = 0: image_loss = 0)\n    valid = M.nonzero()\n    image_loss[valid] = image_loss[valid] / M[valid]\n\n    return torch.mean(image_loss)\n\n\ndef mse_loss(prediction, target, mask, reduction=reduction_batch_based):\n    # Number of valid pixels\n    M = torch.sum(mask, (1, 2))  # (B,)\n\n    # L2 loss\n    res = prediction - target  # (B, H, W)\n    image_loss = torch.sum(mask * res * res, (1, 2))  # (B,)\n\n    return reduction(image_loss, 2 * M)\n\n\ndef gradient_loss(prediction, target, mask, reduction=reduction_batch_based):\n\n    M = torch.sum(mask, (1, 2))\n\n    diff = prediction - target\n    diff = torch.mul(mask, diff)\n\n    grad_x = torch.abs(diff[:, :, 1:] - diff[:, :, :-1])\n    mask_x = torch.mul(mask[:, :, 1:], mask[:, :, :-1])\n    grad_x = torch.mul(mask_x, grad_x)\n\n    grad_y = torch.abs(diff[:, 1:, :] - diff[:, :-1, :])\n    mask_y = torch.mul(mask[:, 1:, :], mask[:, :-1, :])\n    grad_y = torch.mul(mask_y, grad_y)\n\n    image_loss = torch.sum(grad_x, (1, 2)) + torch.sum(grad_y, (1, 2))\n\n    return reduction(image_loss, M)\n\n\nclass MSELoss(nn.Module):\n    def __init__(self, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n    def forward(self, prediction, target, mask):\n        return mse_loss(prediction, target, mask, reduction=self.__reduction)\n\n\nclass GradientLoss(nn.Module):\n    def __init__(self, scales=1, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n        self.__scales = scales\n\n    def forward(self, prediction, target, mask):\n        total = 0\n\n        for scale in range(self.__scales):\n            step = pow(2, scale)\n\n            total += gradient_loss(prediction[:, ::step, ::step], target[:, ::step, ::step],\n                                   mask[:, ::step, ::step], reduction=self.__reduction)\n\n        return total\n\n\nclass ScaleAndShiftInvariantMSELoss(nn.Module):\n    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        self.__data_loss = MSELoss(reduction=reduction)\n        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)\n        self.__alpha = alpha\n\n        self.__prediction_ssi = None\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        # Compute scale and shift\n        scale, shift = compute_scale_and_shift(prediction, target, mask)\n        self.__prediction_ssi = scale.view(-1, 1, 1) * prediction + shift.view(-1, 1, 1)\n        total = self.__data_loss(self.__prediction_ssi, target, mask)\n\n        # Add regularization if needed\n        if self.__alpha > 0:\n            total += self.__alpha * self.__regularization_loss(self.__prediction_ssi, target, mask)\n\n        return total\n\n    def __get_prediction_ssi(self):\n        return self.__prediction_ssi\n\n    prediction_ssi = property(__get_prediction_ssi)\n# from MonoSDF\n\n\ndef median_normalize(x, mask):\n    \"\"\" Median normalize a tensor for all valid pixels.\n        This operation is performed without batch dimension.\n    Args:\n        x (torch.Tensor): (H, W), original tensor\n        mask (torch.Tensor): (H, W), mask tensor\n    Return:\n        y (torch.Tensor): (H, W), median normalized tensor\n    \"\"\"\n    M = torch.sum(mask)\n\n    # Return original tensor if there is no valid pixel\n    if M == 0:\n        return x\n\n    # Compute median and scale\n    t = torch.quantile(x[mask == 1], q=0.5)  # scalar\n    s = torch.sum(x[mask == 1] - t) / M  # scalar\n\n    # Return median normalized tensor\n    return (x - t) / s\n    \n\ndef mae_loss(prediction, target, mask, reduction=reduction_batch_based):\n    # Number of valid pixels\n    M = torch.sum(mask, (1, 2))  # (B,)\n\n    # L1 loss\n    res = (prediction - target).abs()  # (B, H, W)\n    image_loss = torch.sum(mask * res, (1, 2))  # (B,)\n\n    return reduction(image_loss, 2 * M)\n\n\nclass MAELoss(nn.Module):\n    def __init__(self, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n    def forward(self, prediction, target, mask):\n        return mae_loss(prediction, target, mask, reduction=self.__reduction)\n\n\nclass ScaleAndShiftInvariantMAELoss(nn.Module):\n    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        self.__data_loss = MAELoss(reduction=reduction)\n        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)\n        self.__alpha = alpha\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        # TODO: Maybe there is a better way to do the batching\n        # But `torch.quantile` does not support multiple `dim` argument for now\n        for i in range(prediction.shape[0]):\n            prediction[i] = median_normalize(prediction[i], mask[i])  # (H, W)\n            target[i] = median_normalize(target[i], mask[i])  # (H, W)\n\n        # Compute the scale-and-shift invariant MAE loss\n        total = self.__data_loss(prediction, target, mask)\n\n       # Add regularization if needed\n        if self.__alpha > 0:\n            total += self.__alpha * self.__regularization_loss(self.prediction, target, mask)\n\n        return total\n\n\n# Modified version of Adabins repository\n# https://github.com/shariqfarooq123/AdaBins/blob/0952d91e9e762be310bb4cd055cbfe2448c0ce20/loss.py#L7\nclass ScaleInvariantLogLoss(nn.Module):\n    def __init__(self, alpha=10.0, beta=0.15, eps=0.0):\n        super(ScaleInvariantLogLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        # The eps is added to avoid log(0) and division by zero\n        # But it should be gauranteed that the network output is always non-negative\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        total = 0\n        # Maybe there is a better way to do the batching\n        for i in range(prediction.shape[0]):\n            g = torch.log(prediction[i][mask[i]] + self.eps) - torch.log(target[i][mask[i]] + self.eps)  # (N,)\n            Dg = torch.var(g) + self.beta * torch.pow(torch.mean(g), 2)  # scalar\n            total += self.alpha * torch.sqrt(Dg)\n\n        return total\n", "input_code": "def inner_outer(t0, t1, y1):\n\n    \"\"\"\n    Constructs inner and outer measures based on cumulative sums for a given target time (t0) using the information from a source time (t1) and its corresponding values (y1). This is typically used in numerical methods or simulations where interpolation or approximation between time steps is required.\n\n    Input-Output Arguments\n    :param t0: Tensor. The target times for which the measures are to be constructed.\n    :param t1: Tensor. The source times that correspond to the given values (y1).\n    :param y1: Tensor. The values associated with the source times (t1) that are used to construct the measures.\n    :return: Tuple of Tensors. The first tensor is the inner measure, and the second tensor is the outer measure for the target time (t0).\n    \"\"\"", "reference_steps": "1. Define a function `inner_outer` that accepts three parameters: `t0`, `t1`, and `y1`. This function is designed to construct inner and outer measures based on the input parameters.\n\n2. Concatenate a tensor of zeros with the same shape as the first slice of `y1` to the cumulative sum of `y1` along the last dimension. This creates a new tensor `cy1` that represents the cumulative sum with an initial zero.\n\n3. Use the `searchsorted` function to find indices `idx_lo` and `idx_hi` in `t1` where the values of `t0` would be inserted to maintain order. These indices represent the lower and upper bounds for the interpolation.\n\n4. Use `torch.take_along_dim` to gather elements from `cy1` at the indices `idx_lo` and `idx_hi` along the last dimension. This results in tensors `cy1_lo` and `cy1_hi`, which are the cumulative sums at the lower and upper bounds.\n\n5. Calculate the outer measure `y0_outer` by subtracting the cumulative sum at the lower bounds (`cy1_lo`) from the cumulative sum at the upper bounds (`cy1_hi`), excluding the first element of `cy1_lo` and the last element of `cy1_hi`.\n\n6. Calculate the inner measure `y0_inner` by subtracting the cumulative sum at the upper bounds (`cy1_hi`) from the cumulative sum at the lower bounds (`cy1_lo`), excluding the last element of `cy1_hi` and the first element of `cy1_lo`. If the upper bound index is less than or equal to the lower bound index, set the inner measure to zero.\n\n7. Use `torch.where` to conditionally select values for `y0_inner` based on whether the upper bound indices are less than or equal to the corresponding lower bound indices.\n\n8. Return the tensors `y0_inner` and `y0_outer` as the result of the function. These tensors represent the inner and outer measures for the input `t0` with respect to the cumulative sums of `y1` at the bounds determined by `t1`.\n\n9. The function assumes that `t0`, `t1`, and `y1` are tensors compatible with PyTorch operations and that `t1` is sorted.\n\n10. The function utilizes PyTorch's tensor operations to efficiently compute the measures without explicit loops, making it suitable for batch processing and parallel computation.", "reference_code": "def inner_outer(t0, t1, y1):\n    \"\"\"Construct inner and outer measures on (t1, y1) for t0.\"\"\"\n    cy1 = torch.cat([torch.zeros_like(y1[..., :1]), torch.cumsum(y1, dim=-1)], dim=-1)  # 129\n    idx_lo, idx_hi = searchsorted(t1, t0)\n\n    cy1_lo = torch.take_along_dim(cy1, idx_lo, dim=-1)  # 128\n    cy1_hi = torch.take_along_dim(cy1, idx_hi, dim=-1)\n\n    y0_outer = cy1_hi[..., 1:] - cy1_lo[..., :-1]  # 127\n    y0_inner = torch.where(idx_hi[..., :-1] <= idx_lo[..., 1:], cy1_lo[..., 1:] - cy1_hi[..., :-1], 0)\n    return y0_inner, y0_outer\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "type": "function", "class_name": null, "function_name": "lossfun_outer", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.loss_utils.inner_outer\n    def inner_outer(t0, t1, y1):\n        \"\"\"Construct inner and outer measures on (t1, y1) for t0.\"\"\"\n\n# Cross-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "dependency_sampled": "# Cross-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "contexts_above": "import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models.vgg as vgg\nfrom collections import namedtuple\n\nfrom easyvolcap.utils.prop_utils import searchsorted, matchup_channels\n\nfrom enum import Enum, auto\n\nclass ElasticLossReduceType(Enum):\n    WEIGHT = auto()\n    MEDIAN = auto()\n\n\nclass ImgLossType(Enum):\n    PERC = auto()  # lpips\n    CHARB = auto()\n    HUBER = auto()\n    L1 = auto()\n    L2 = auto()\n    SSIM = auto()\n\nclass DptLossType(Enum):\n    SMOOTHL1 = auto()\n    L1 = auto()\n    L2 = auto()\n    SSIMSE = auto()\n    SSIMAE = auto()\n    SILOG = auto()\n    CONTINUITY = auto()\n    RANKING = auto()\n\n# from mipnerf360\n\n\ndef inner_outer(t0, t1, y1):\n    \"\"\"Construct inner and outer measures on (t1, y1) for t0.\"\"\"\n    cy1 = torch.cat([torch.zeros_like(y1[..., :1]), torch.cumsum(y1, dim=-1)], dim=-1)  # 129\n    idx_lo, idx_hi = searchsorted(t1, t0)\n\n    cy1_lo = torch.take_along_dim(cy1, idx_lo, dim=-1)  # 128\n    cy1_hi = torch.take_along_dim(cy1, idx_hi, dim=-1)\n\n    y0_outer = cy1_hi[..., 1:] - cy1_lo[..., :-1]  # 127\n    y0_inner = torch.where(idx_hi[..., :-1] <= idx_lo[..., 1:], cy1_lo[..., 1:] - cy1_hi[..., :-1], 0)\n    return y0_inner, y0_outer\n\n# from mipnerf360\n\n\n", "contexts_below": "\n\ndef blur_stepfun(x, y, r):\n    xr, xr_idx = torch.sort(torch.cat([x - r, x + r], dim=-1))\n    y1 = (torch.cat([y, torch.zeros_like(y[..., :1])], dim=-1) -\n          torch.cat([torch.zeros_like(y[..., :1]), y], dim=-1)) / (2 * r)\n    y2 = torch.cat([y1, -y1], dim=-1).take_along_dim(xr_idx[..., :-1], dim=-1)\n    yr = torch.cumsum((xr[..., 1:] - xr[..., :-1]) *\n                      torch.cumsum(y2, dim=-1), dim=-1).clamp_min(0)\n    yr = torch.cat([torch.zeros_like(yr[..., :1]), yr], dim=-1)\n    return xr, yr\n\n\ndef sorted_interp_quad(x, xp, fpdf, fcdf):\n    \"\"\"interp in quadratic\"\"\"\n\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[..., None, :] >= xp[..., :, None]\n\n    def find_interval(x, return_idx=False):\n        # Grab the value where `mask` switches from True to False, and vice versa.\n        # This approach takes advantage of the fact that `x` is sorted.\n        x0, x0_idx = torch.max(torch.where(mask, x[..., None], x[..., :1, None]), -2)\n        x1, x1_idx = torch.min(torch.where(~mask, x[..., None], x[..., -1:, None]), -2)\n        if return_idx:\n            return x0, x1, x0_idx, x1_idx\n        return x0, x1\n\n    fcdf0, fcdf1, fcdf0_idx, fcdf1_idx = find_interval(fcdf, return_idx=True)\n    fpdf0 = fpdf.take_along_dim(fcdf0_idx, dim=-1)\n    fpdf1 = fpdf.take_along_dim(fcdf1_idx, dim=-1)\n    xp0, xp1 = find_interval(xp)\n\n    offset = torch.clip(torch.nan_to_num((x - xp0) / (xp1 - xp0), 0), 0, 1)\n    ret = fcdf0 + (x - xp0) * (fpdf0 + fpdf1 * offset + fpdf0 * (1 - offset)) / 2\n    return ret\n\n\ndef lossfun_zip_outer(t, w, t_env, w_env, pulse_width, eps=1e-6):\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, pulse_width)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n\ndef lossfun_distortion(t: torch.Tensor, w: torch.Tensor):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    \"\"\"Compute iint w[i] w[j] |t[i] - t[j]| di dj.\"\"\"\n    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    return loss_inter + loss_intra\n\n\ndef interval_distortion(t0_lo, t0_hi, t1_lo, t1_hi):\n    \"\"\"Compute mean(abs(x-y); x in [t0_lo, t0_hi], y in [t1_lo, t1_hi]).\"\"\"\n    # Distortion when the intervals do not overlap.\n    d_disjoint = torch.abs((t1_lo + t1_hi) / 2 - (t0_lo + t0_hi) / 2)\n\n    # Distortion when the intervals overlap.\n    d_overlap = (2 *\n                 (torch.minimum(t0_hi, t1_hi)**3 - torch.maximum(t0_lo, t1_lo)**3) +\n                 3 * (t1_hi * t0_hi * torch.abs(t1_hi - t0_hi) +\n                      t1_lo * t0_lo * torch.abs(t1_lo - t0_lo) + t1_hi * t0_lo *\n                      (t0_lo - t1_hi) + t1_lo * t0_hi *\n                      (t1_lo - t0_hi))) / (6 * (t0_hi - t0_lo) * (t1_hi - t1_lo))\n\n    # Are the two intervals not overlapping?\n    are_disjoint = (t0_lo > t1_hi) | (t1_lo > t0_hi)\n\n    return torch.where(are_disjoint, d_disjoint, d_overlap)\n\n\ndef anneal_loss_weight(weight: float, gamma: float, iter: int, mile: int):\n    # exponentially anneal the loss weight\n    return weight * gamma ** min(iter / mile, 1)\n\n\ndef gaussian_entropy_relighting4d(albedo_pred):\n    albedo_entropy = 0\n    for i in range(3):\n        channel = albedo_pred[..., i]\n        hist = GaussianHistogram(15, 0., 1., sigma=torch.var(channel))\n        h = hist(channel)\n        if h.sum() > 1e-6:\n            h = h.div(h.sum()) + 1e-6\n        else:\n            h = torch.ones_like(h)\n        albedo_entropy += torch.sum(-h * torch.log(h))\n    return albedo_entropy\n\n\nclass GaussianHistogram(nn.Module):\n    def __init__(self, bins, min, max, sigma):\n        super(GaussianHistogram, self).__init__()\n        self.bins = bins\n        self.min = min\n        self.max = max\n        self.sigma = sigma\n        self.delta = float(max - min) / float(bins)\n        self.centers = float(min) + self.delta * (torch.arange(bins, device=sigma.device).float() + 0.5)\n\n    def forward(self, x):\n        x = torch.unsqueeze(x, 0) - torch.unsqueeze(self.centers, 1)\n        x = torch.exp(-0.5 * (x / self.sigma)**2) / (self.sigma * np.sqrt(np.pi * 2)) * self.delta\n        x = x.sum(dim=1)\n        return x\n\n\ndef gaussian_entropy(x: torch.Tensor, *args, **kwargs):\n    eps = 1e-6\n    hps = 1e-9\n    h = gaussian_histogram(x, *args, **kwargs)\n    # h = (h / (h.sum(dim=0) + hps)).clip(eps)  # 3,\n    # entropy = (-h * h.log()).sum(dim=0).sum(dim=0)  # per channel entropy summed\n    entropy = 0\n    for i in range(3):\n        hi = h[..., i]\n        if hi.sum() > eps:\n            hi = hi / hi.sum() + eps\n        else:\n            hi = torch.ones_like(hi)\n        entropy += torch.sum(-hi * torch.log(hi))\n    return entropy\n\n\ndef gaussian_histogram(x: torch.Tensor, bins: int = 15, min: float = 0.0, max: float = 1.0):\n    x = x.view(-1, x.shape[-1])  # N, 3\n    sigma = x.var(dim=0)  # 3,\n    delta = (max - min) / bins\n    centers = min + delta * (torch.arange(bins, device=x.device, dtype=x.dtype) + 0.5)  # BIN\n    x = x[None] - centers[:, None, None]  # BIN, N, 3\n    x = (-0.5 * (x / sigma).pow(2)).exp() / (sigma * np.sqrt(np.pi * 2)) * delta  # BIN, N, 3\n    x = x.sum(dim=1)\n    return x  # BIN, 3\n\n\ndef reg_diff_crit(x: torch.Tensor, iter_step: int, max_weight: float = 1e-4, ann_iter: int = 100 * 500):\n    weight = min(iter_step, ann_iter) * max_weight / ann_iter\n    return reg(x), weight\n\n\ndef reg_raw_crit(x: torch.Tensor, iter_step: int, max_weight: float = 1e-4, ann_iter: int = 100 * 500):\n    weight = min(iter_step, ann_iter) * max_weight / ann_iter\n    n_batch, n_pts_x2, D = x.shape\n    n_pts = n_pts_x2 // 2\n    length = x.norm(dim=-1, keepdim=True)  # length\n    vector = x / (length + 1e-8)  # vector direction (normalized to unit sphere)\n    # loss_length = mse(length[:, n_pts:, :], length[:, :n_pts, :])\n    loss_vector = reg((vector[:, n_pts:, :] - vector[:, :n_pts, :]))\n    # loss = loss_length + loss_vector\n    loss = loss_vector\n    return loss, weight\n\n\nclass LossNetwork(torch.nn.Module):\n    \"\"\"Reference:\n        https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n    \"\"\"\n\n    def __init__(self):\n        super(LossNetwork, self).__init__()\n        try:\n            from torchvision.models import VGG19_Weights\n            self.vgg_layers = vgg.vgg19(weights=VGG19_Weights.DEFAULT).features\n        except ImportError:\n            self.vgg_layers = vgg.vgg19(pretrained=True).features\n\n        for param in self.vgg_layers.parameters():\n            param.requires_grad = False\n        '''\n        self.layer_name_mapping = {\n            '3': \"relu1\",\n            '8': \"relu2\",\n            '17': \"relu3\",\n            '26': \"relu4\",\n            '35': \"relu5\",\n        }\n        '''\n\n        self.layer_name_mapping = {'3': \"relu1\", '8': \"relu2\"}\n\n    def forward(self, x):\n        output = {}\n        for name, module in self.vgg_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n            if name == '8':\n                break\n        LossOutput = namedtuple(\"LossOutput\", [\"relu1\", \"relu2\"])\n        return LossOutput(**output)\n\n\nclass PerceptualLoss(torch.nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n\n        self.model = LossNetwork()\n        self.model.cuda()\n        self.model.eval()\n        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n        self.l1_loss = torch.nn.L1Loss(reduction='mean')\n\n    def forward(self, x, target):\n        x_feature = self.model(x[:, 0:3, :, :])\n        target_feature = self.model(target[:, 0:3, :, :])\n\n        feature_loss = (\n            self.l1_loss(x_feature.relu1, target_feature.relu1) +\n            self.l1_loss(x_feature.relu2, target_feature.relu2)) / 2.0\n\n        l1_loss = self.l1_loss(x, target)\n        l2_loss = self.mse_loss(x, target)\n\n        loss = feature_loss + l1_loss + l2_loss\n\n        return loss\n\n\nclass VGGPerceptualLoss(torch.nn.Module):\n    def __init__(self, resize=False):\n        super(VGGPerceptualLoss, self).__init__()\n        blocks = []\n        import torchvision\n        vgg16 = torchvision.models.vgg16(pretrained=True)\n        blocks.append(vgg16.features[:4].eval())\n        blocks.append(vgg16.features[4:9].eval())\n        blocks.append(vgg16.features[9:16].eval())\n        blocks.append(vgg16.features[16:23].eval())\n        for bl in blocks:\n            for p in bl.parameters():\n                p.requires_grad = False\n        self.blocks = nn.ModuleList(blocks)\n        self.transform = F.interpolate\n        self.resize = resize\n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n        if input.shape[1] != 3:\n            input = input.repeat(1, 3, 1, 1)\n            target = target.repeat(1, 3, 1, 1)\n        input = (input - self.mean) / self.std\n        target = (target - self.mean) / self.std\n        if self.resize:\n            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n        loss = 0.0\n        x = input\n        y = target\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            y = block(y)\n            if i in feature_layers:\n                loss += F.l1_loss(x, y)\n            if i in style_layers:\n                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n                gram_x = act_x @ act_x.permute(0, 2, 1)\n                gram_y = act_y @ act_y.permute(0, 2, 1)\n                loss += F.l1_loss(gram_x, gram_y)\n        return loss\n\n\ndef eikonal(x: torch.Tensor, th=1.0) -> torch.Tensor:\n    return ((x.norm(dim=-1) - th)**2).mean()\n\n\ndef sdf_mask_crit(ret, batch):\n    msk_sdf = ret['msk_sdf']\n    msk_label = ret['msk_label']\n\n    alpha = 50\n    alpha_factor = 2\n    alpha_milestones = [10000, 20000, 30000, 40000, 50000]\n    for milestone in alpha_milestones:\n        if batch['iter_step'] > milestone:\n            alpha = alpha * alpha_factor\n\n    msk_sdf = -alpha * msk_sdf\n    mask_loss = F.binary_cross_entropy_with_logits(msk_sdf, msk_label) / alpha\n\n    return mask_loss\n\n\ndef cross_entropy(x: torch.Tensor, y: torch.Tensor):\n    # x: unormalized input logits\n    # channel last cross entropy loss\n    x = x.view(-1, x.shape[-1])  # N, C\n    y = y.view(-1, y.shape[-1])  # N, C\n    return F.cross_entropy(x, y)\n\n\ndef huber(x: torch.Tensor, y: torch.Tensor):\n    return F.huber_loss(x, y, reduction='mean')\n\n\ndef smoothl1(x: torch.Tensor, y: torch.Tensor):\n    return F.smooth_l1_loss(x, y)\n\n\ndef mse(x: torch.Tensor, y: torch.Tensor):\n    return ((x.float() - y.float())**2).mean()\n\n\ndef dot(x: torch.Tensor, y: torch.Tensor):\n    return (x * y).sum(dim=-1)\n\n\ndef l1(x: torch.Tensor, y: torch.Tensor):\n    return l1_reg(x - y)\n\n\ndef l2(x: torch.Tensor, y: torch.Tensor):\n    return l2_reg(x - y)\n\n\ndef l1_reg(x: torch.Tensor):\n    return x.abs().sum(dim=-1).mean()\n\n\ndef l2_reg(x: torch.Tensor) -> torch.Tensor:\n    return (x**2).sum(dim=-1).mean()\n\n\ndef bce_loss(x: torch.Tensor, y: torch.Tensor):\n    return F.binary_cross_entropy(x, y)\n\n\ndef mIoU_loss(x: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Compute the mean intersection of union loss over masked regions\n    x, y: B, N, 1\n    \"\"\"\n    I = (x * y).sum(-1).sum(-1)\n    U = (x + y).sum(-1).sum(-1) - I\n    mIoU = (I / U.detach()).mean()\n    return 1 - mIoU\n\n\ndef reg(x: torch.Tensor) -> torch.Tensor:\n    return x.norm(dim=-1).mean()\n\n\ndef thresh(x: torch.Tensor, a: torch.Tensor, eps: float = 1e-8):\n    return 1 / (l2(x, a) + eps)\n\n\ndef elastic_crit(jac: torch.Tensor) -> torch.Tensor:\n    \"\"\"Compute the raw 'log_svals' type elastic energy, and\n    remap it using the Geman-McClure type of robust loss.\n    Args:\n        jac (torch.Tensor): (B, N, 3, 3), the gradient of warpped xyz with respect to the original xyz\n    Return:\n        elastic_loss (torch.Tensor): (B, N), \n    \"\"\"\n    # !: CUDA IMPLEMENTATION OF SVD IS EXTREMELY SLOW\n    # old_device = jac.device\n    # jac = jac.cpu()\n    # svd_backward: Setting compute_uv to false in torch.svd doesn't compute singular matrices, and hence we cannot compute backward. Please use torch.svd(compute_uv=True)\n    _, S, _ = torch.svd(jac, compute_uv=True)           # (B, N, 3)\n    # S = S.to(old_device)\n    log_svals = torch.log(torch.clamp(S, min=1e-6))     # (B, N, 3)\n    sq_residual = torch.sum(log_svals**2, dim=-1)       # (B, N)\n    # TODO: determine whether it is a good choice to compute the robust loss here\n    elastic_loss = general_loss_with_squared_residual(sq_residual, alpha=-2.0, scale=0.03)\n    return elastic_loss\n\n\ndef general_loss_with_squared_residual(squared_x, alpha, scale):\n    r\"\"\"The general loss that takes a squared residual.\n    This fuses the sqrt operation done to compute many residuals while preserving\n    the square in the loss formulation.\n    This implements the rho(x, \\alpha, c) function described in \"A General and\n    Adaptive Robust Loss Function\", Jonathan T. Barron,\n    https://arxiv.org/abs/1701.03077.\n    Args:\n        squared_x: The residual for which the loss is being computed. x can have\n        any shape, and alpha and scale will be broadcasted to match x's shape if\n        necessary.\n        alpha: The shape parameter of the loss (\\alpha in the paper), where more\n        negative values produce a loss with more robust behavior (outliers \"cost\"\n        less), and more positive values produce a loss with less robust behavior\n        (outliers are penalized more heavily). Alpha can be any value in\n        [-infinity, infinity], but the gradient of the loss with respect to alpha\n        is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n        interpolation between several discrete robust losses:\n            alpha=-Infinity: Welsch/Leclerc Loss.\n            alpha=-2: Geman-McClure loss.\n            alpha=0: Cauchy/Lortentzian loss.\n            alpha=1: Charbonnier/pseudo-Huber loss.\n            alpha=2: L2 loss.\n        scale: The scale parameter of the loss. When |x| < scale, the loss is an\n        L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n        different shape according to alpha.\n    Returns:\n        The losses for each element of x, in the same shape as x.\n    \"\"\"\n    # https://pytorch.org/docs/stable/type_info.html\n    eps = torch.tensor(torch.finfo(torch.float32).eps)\n\n    # convert the float to torch.tensor\n    alpha = torch.tensor(alpha).to(squared_x.device)\n    scale = torch.tensor(scale).to(squared_x.device)\n\n    # This will be used repeatedly.\n    squared_scaled_x = squared_x / (scale ** 2)\n\n    # The loss when alpha == 2.\n    loss_two = 0.5 * squared_scaled_x\n    # The loss when alpha == 0.\n    loss_zero = log1p_safe(0.5 * squared_scaled_x)\n    # The loss when alpha == -infinity.\n    loss_neginf = -torch.expm1(-0.5 * squared_scaled_x)\n    # The loss when alpha == +infinity.\n    loss_posinf = expm1_safe(0.5 * squared_scaled_x)\n\n    # The loss when not in one of the above special cases.\n    # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n    beta_safe = torch.maximum(eps, torch.abs(alpha - 2.))\n    # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n    alpha_safe = torch.where(\n        torch.greater_equal(alpha, torch.tensor(0.)), torch.ones_like(alpha),\n        -torch.ones_like(alpha)) * torch.maximum(eps, torch.abs(alpha))\n    loss_otherwise = (beta_safe / alpha_safe) * (\n        torch.pow(squared_scaled_x / beta_safe + 1., 0.5 * alpha) - 1.)\n\n    # Select which of the cases of the loss to return.\n    loss = torch.where(\n        alpha == -torch.inf, loss_neginf,\n        torch.where(\n            alpha == 0, loss_zero,\n            torch.where(\n                alpha == 2, loss_two,\n                torch.where(alpha == torch.inf, loss_posinf, loss_otherwise))))\n\n    return scale * loss\n\n\ndef log1p_safe(x):\n    \"\"\"The same as torch.log1p(x), but clamps the input to prevent NaNs.\"\"\"\n    return torch.log1p(torch.minimum(x, torch.tensor(3e37)))\n\n\ndef expm1_safe(x):\n    \"\"\"The same as torch.expm1(x), but clamps the input to prevent NaNs.\"\"\"\n    return torch.expm1(torch.minimum(x, torch.tensor(87.5)))\n\n\ndef compute_plane_tv(t):\n    batch_size, c, h, w = t.shape\n    count_h = batch_size * c * (h - 1) * w\n    count_w = batch_size * c * h * (w - 1)\n    h_tv = torch.square(t[..., 1:, :] - t[..., :h - 1, :]).sum()\n    w_tv = torch.square(t[..., :, 1:] - t[..., :, :w - 1]).sum()\n    return 2 * (h_tv / count_h + w_tv / count_w)  # This is summing over batch and c instead of avg\n\n\ndef compute_planes_tv(embedding):\n    tv_loss = 0\n    for emb in embedding:\n        tv_loss += compute_plane_tv(emb)\n    return tv_loss\n\n\ndef compute_plane_smoothness(t):\n    batch_size, c, h, w = t.shape\n    # Convolve with a second derivative filter, in the time dimension which is dimension 2\n    first_difference = t[..., 1:] - t[..., :w - 1]  # [batch, c, h-1, w]\n    second_difference = first_difference[..., 1:] - first_difference[..., :w - 2]  # [batch, c, h-2, w]\n    # Take the L2 norm of the result\n    return torch.square(second_difference).mean()\n\n\ndef compute_time_planes_smooth(embedding):\n    loss = 0.\n    for emb in embedding:\n        loss += compute_plane_smoothness(emb)\n    return loss\n\n\ndef compute_ssim(x: torch.Tensor, y: torch.Tensor):\n    from pytorch_msssim import ssim\n    return ssim(x, y, data_range=1.0, win_size=11, win_sigma=1.5, K=(0.01, 0.03))\n\n\n# from MonoSDF\ndef compute_scale_and_shift(prediction, target, mask):\n    # System matrix: A = [[a_00, a_01], [a_10, a_11]]\n    a_00 = torch.sum(mask * prediction * prediction, (1, 2))\n    a_01 = torch.sum(mask * prediction, (1, 2))\n    a_11 = torch.sum(mask, (1, 2))\n\n    # Right hand side: b = [b_0, b_1]\n    b_0 = torch.sum(mask * prediction * target, (1, 2))\n    b_1 = torch.sum(mask * target, (1, 2))\n\n    # Solution: x = A^-1 . b = [[a_11, -a_01], [-a_10, a_00]] / (a_00 * a_11 - a_01 * a_10) . b\n    x_0 = torch.zeros_like(b_0)\n    x_1 = torch.zeros_like(b_1)\n\n    det = a_00 * a_11 - a_01 * a_01\n    valid = det.nonzero()\n\n    x_0[valid] = ( a_11[valid] * b_0[valid] - a_01[valid] * b_1[valid]) / det[valid]\n    x_1[valid] = (-a_01[valid] * b_0[valid] + a_00[valid] * b_1[valid]) / det[valid]\n\n    return x_0, x_1\n\n\ndef reduction_batch_based(image_loss, M):\n    # Average of all valid pixels of the batch\n    # Avoid division by 0 (if sum(M) = sum(sum(mask)) = 0: sum(image_loss) = 0)\n    divisor = torch.sum(M)\n\n    if divisor == 0: return 0\n    else: return torch.sum(image_loss) / divisor\n\n\ndef reduction_image_based(image_loss, M):\n    # Mean of average of valid pixels of an image\n    # Avoid division by 0 (if M = sum(mask) = 0: image_loss = 0)\n    valid = M.nonzero()\n    image_loss[valid] = image_loss[valid] / M[valid]\n\n    return torch.mean(image_loss)\n\n\ndef mse_loss(prediction, target, mask, reduction=reduction_batch_based):\n    # Number of valid pixels\n    M = torch.sum(mask, (1, 2))  # (B,)\n\n    # L2 loss\n    res = prediction - target  # (B, H, W)\n    image_loss = torch.sum(mask * res * res, (1, 2))  # (B,)\n\n    return reduction(image_loss, 2 * M)\n\n\ndef gradient_loss(prediction, target, mask, reduction=reduction_batch_based):\n\n    M = torch.sum(mask, (1, 2))\n\n    diff = prediction - target\n    diff = torch.mul(mask, diff)\n\n    grad_x = torch.abs(diff[:, :, 1:] - diff[:, :, :-1])\n    mask_x = torch.mul(mask[:, :, 1:], mask[:, :, :-1])\n    grad_x = torch.mul(mask_x, grad_x)\n\n    grad_y = torch.abs(diff[:, 1:, :] - diff[:, :-1, :])\n    mask_y = torch.mul(mask[:, 1:, :], mask[:, :-1, :])\n    grad_y = torch.mul(mask_y, grad_y)\n\n    image_loss = torch.sum(grad_x, (1, 2)) + torch.sum(grad_y, (1, 2))\n\n    return reduction(image_loss, M)\n\n\nclass MSELoss(nn.Module):\n    def __init__(self, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n    def forward(self, prediction, target, mask):\n        return mse_loss(prediction, target, mask, reduction=self.__reduction)\n\n\nclass GradientLoss(nn.Module):\n    def __init__(self, scales=1, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n        self.__scales = scales\n\n    def forward(self, prediction, target, mask):\n        total = 0\n\n        for scale in range(self.__scales):\n            step = pow(2, scale)\n\n            total += gradient_loss(prediction[:, ::step, ::step], target[:, ::step, ::step],\n                                   mask[:, ::step, ::step], reduction=self.__reduction)\n\n        return total\n\n\nclass ScaleAndShiftInvariantMSELoss(nn.Module):\n    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        self.__data_loss = MSELoss(reduction=reduction)\n        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)\n        self.__alpha = alpha\n\n        self.__prediction_ssi = None\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        # Compute scale and shift\n        scale, shift = compute_scale_and_shift(prediction, target, mask)\n        self.__prediction_ssi = scale.view(-1, 1, 1) * prediction + shift.view(-1, 1, 1)\n        total = self.__data_loss(self.__prediction_ssi, target, mask)\n\n        # Add regularization if needed\n        if self.__alpha > 0:\n            total += self.__alpha * self.__regularization_loss(self.__prediction_ssi, target, mask)\n\n        return total\n\n    def __get_prediction_ssi(self):\n        return self.__prediction_ssi\n\n    prediction_ssi = property(__get_prediction_ssi)\n# from MonoSDF\n\n\ndef median_normalize(x, mask):\n    \"\"\" Median normalize a tensor for all valid pixels.\n        This operation is performed without batch dimension.\n    Args:\n        x (torch.Tensor): (H, W), original tensor\n        mask (torch.Tensor): (H, W), mask tensor\n    Return:\n        y (torch.Tensor): (H, W), median normalized tensor\n    \"\"\"\n    M = torch.sum(mask)\n\n    # Return original tensor if there is no valid pixel\n    if M == 0:\n        return x\n\n    # Compute median and scale\n    t = torch.quantile(x[mask == 1], q=0.5)  # scalar\n    s = torch.sum(x[mask == 1] - t) / M  # scalar\n\n    # Return median normalized tensor\n    return (x - t) / s\n    \n\ndef mae_loss(prediction, target, mask, reduction=reduction_batch_based):\n    # Number of valid pixels\n    M = torch.sum(mask, (1, 2))  # (B,)\n\n    # L1 loss\n    res = (prediction - target).abs()  # (B, H, W)\n    image_loss = torch.sum(mask * res, (1, 2))  # (B,)\n\n    return reduction(image_loss, 2 * M)\n\n\nclass MAELoss(nn.Module):\n    def __init__(self, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n    def forward(self, prediction, target, mask):\n        return mae_loss(prediction, target, mask, reduction=self.__reduction)\n\n\nclass ScaleAndShiftInvariantMAELoss(nn.Module):\n    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        self.__data_loss = MAELoss(reduction=reduction)\n        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)\n        self.__alpha = alpha\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        # TODO: Maybe there is a better way to do the batching\n        # But `torch.quantile` does not support multiple `dim` argument for now\n        for i in range(prediction.shape[0]):\n            prediction[i] = median_normalize(prediction[i], mask[i])  # (H, W)\n            target[i] = median_normalize(target[i], mask[i])  # (H, W)\n\n        # Compute the scale-and-shift invariant MAE loss\n        total = self.__data_loss(prediction, target, mask)\n\n       # Add regularization if needed\n        if self.__alpha > 0:\n            total += self.__alpha * self.__regularization_loss(self.prediction, target, mask)\n\n        return total\n\n\n# Modified version of Adabins repository\n# https://github.com/shariqfarooq123/AdaBins/blob/0952d91e9e762be310bb4cd055cbfe2448c0ce20/loss.py#L7\nclass ScaleInvariantLogLoss(nn.Module):\n    def __init__(self, alpha=10.0, beta=0.15, eps=0.0):\n        super(ScaleInvariantLogLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        # The eps is added to avoid log(0) and division by zero\n        # But it should be gauranteed that the network output is always non-negative\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        total = 0\n        # Maybe there is a better way to do the batching\n        for i in range(prediction.shape[0]):\n            g = torch.log(prediction[i][mask[i]] + self.eps) - torch.log(target[i][mask[i]] + self.eps)  # (N,)\n            Dg = torch.var(g) + self.beta * torch.pow(torch.mean(g), 2)  # scalar\n            total += self.alpha * torch.sqrt(Dg)\n\n        return total\n", "input_code": "def lossfun_outer(t: torch.Tensor, w: torch.Tensor, t_env: torch.Tensor, w_env: torch.Tensor, eps=torch.finfo(torch.float32).eps):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n\n    \"\"\"\n    This function calculates a scaled half-quadratic loss based on the input tensors representing target and environment weights and positions. It ensures that the proposal weight acts as an upper envelope over the nerf weight by penalizing weights outside this envelope.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The target positions tensor, used along with weights to calculate the loss.\n    :param w: torch.Tensor. The target weights tensor, representing the weights of the target positions.\n    :param t_env: torch.Tensor. The environment positions tensor, used to calculate the upper envelope weights.\n    :param w_env: torch.Tensor. The environment weights tensor, representing the weights of the environment positions.\n    :param eps: torch.Tensor, optional. A small epsilon value to prevent division by zero, defaulting to the smallest positive number representable by torch.float32.\n    :return: torch.Tensor. The calculated loss based on the difference between target weights and the upper envelope, scaled by a half-quadratic loss function.\n    \"\"\"", "reference_steps": "1. Define a function `lossfun_outer` that calculates a loss based on the difference between a proposed weight `w` and an \"outer\" weight `w_outer` derived from the environment.\n2. Ensure that the function accepts `torch.Tensor` objects for `t`, `w`, `t_env`, and `w_env`, and an optional `eps` parameter to prevent division by zero.\n3. Use the `matchup_channels` function to align the channels of `t` with `w` and `t_env` with `w_env`, ensuring they have the same number of channels for further calculations.\n4. Call the `inner_outer` function to compute the `w_outer` tensor based on `t`, `t_env`, and `w_env`.\n5. Assume that the actual weight `w` should be between an \"inner\" weight and the computed `w_outer` weight, but only penalize the model when `w` exceeds `w_outer`.\n6. Calculate the loss by taking the positive part of the difference between `w` and `w_outer` using the `clip(0.)` method to zero out negative values.\n7. Square the clipped differences to emphasize larger deviations and to create a quadratic loss landscape.\n8. Normalize the squared differences by adding a small epsilon value `eps` to `w` to avoid division by zero when `w` is very small or zero.\n9. Return the final computed loss value, which is designed to give a constant gradient when `w_outer` is zero, encouraging `w` to not exceed `w_outer`.\n10. The loss function is designed to pull `w_outer` up rather than push an \"inner\" weight down, thus focusing on ensuring that the proposed weight `w` does not surpass the environmental upper bound `w_outer`.", "reference_code": "def lossfun_outer(t: torch.Tensor, w: torch.Tensor, t_env: torch.Tensor, w_env: torch.Tensor, eps=torch.finfo(torch.float32).eps):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n    \"\"\"The proposal weight should be an upper envelope on the nerf weight.\"\"\"\n    _, w_outer = inner_outer(t, t_env, w_env)\n    # We assume w_inner <= w <= w_outer. We don't penalize w_inner because it's\n    # more effective to pull w_outer up than it is to push w_inner down.\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    return (w - w_outer).clip(0.).pow(2) / (w + eps)\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "type": "function", "class_name": null, "function_name": "lossfun_distortion", "dependency_all": "# Cross-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "dependency_sampled": "# Cross-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "contexts_above": "import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models.vgg as vgg\nfrom collections import namedtuple\n\nfrom easyvolcap.utils.prop_utils import searchsorted, matchup_channels\n\nfrom enum import Enum, auto\n\nclass ElasticLossReduceType(Enum):\n    WEIGHT = auto()\n    MEDIAN = auto()\n\n\nclass ImgLossType(Enum):\n    PERC = auto()  # lpips\n    CHARB = auto()\n    HUBER = auto()\n    L1 = auto()\n    L2 = auto()\n    SSIM = auto()\n\nclass DptLossType(Enum):\n    SMOOTHL1 = auto()\n    L1 = auto()\n    L2 = auto()\n    SSIMSE = auto()\n    SSIMAE = auto()\n    SILOG = auto()\n    CONTINUITY = auto()\n    RANKING = auto()\n\n# from mipnerf360\n\n\ndef inner_outer(t0, t1, y1):\n    \"\"\"Construct inner and outer measures on (t1, y1) for t0.\"\"\"\n    cy1 = torch.cat([torch.zeros_like(y1[..., :1]), torch.cumsum(y1, dim=-1)], dim=-1)  # 129\n    idx_lo, idx_hi = searchsorted(t1, t0)\n\n    cy1_lo = torch.take_along_dim(cy1, idx_lo, dim=-1)  # 128\n    cy1_hi = torch.take_along_dim(cy1, idx_hi, dim=-1)\n\n    y0_outer = cy1_hi[..., 1:] - cy1_lo[..., :-1]  # 127\n    y0_inner = torch.where(idx_hi[..., :-1] <= idx_lo[..., 1:], cy1_lo[..., 1:] - cy1_hi[..., :-1], 0)\n    return y0_inner, y0_outer\n\n# from mipnerf360\n\n\ndef lossfun_outer(t: torch.Tensor, w: torch.Tensor, t_env: torch.Tensor, w_env: torch.Tensor, eps=torch.finfo(torch.float32).eps):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n    \"\"\"The proposal weight should be an upper envelope on the nerf weight.\"\"\"\n    _, w_outer = inner_outer(t, t_env, w_env)\n    # We assume w_inner <= w <= w_outer. We don't penalize w_inner because it's\n    # more effective to pull w_outer up than it is to push w_inner down.\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    return (w - w_outer).clip(0.).pow(2) / (w + eps)\n\n\ndef blur_stepfun(x, y, r):\n    xr, xr_idx = torch.sort(torch.cat([x - r, x + r], dim=-1))\n    y1 = (torch.cat([y, torch.zeros_like(y[..., :1])], dim=-1) -\n          torch.cat([torch.zeros_like(y[..., :1]), y], dim=-1)) / (2 * r)\n    y2 = torch.cat([y1, -y1], dim=-1).take_along_dim(xr_idx[..., :-1], dim=-1)\n    yr = torch.cumsum((xr[..., 1:] - xr[..., :-1]) *\n                      torch.cumsum(y2, dim=-1), dim=-1).clamp_min(0)\n    yr = torch.cat([torch.zeros_like(yr[..., :1]), yr], dim=-1)\n    return xr, yr\n\n\ndef sorted_interp_quad(x, xp, fpdf, fcdf):\n    \"\"\"interp in quadratic\"\"\"\n\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[..., None, :] >= xp[..., :, None]\n\n    def find_interval(x, return_idx=False):\n        # Grab the value where `mask` switches from True to False, and vice versa.\n        # This approach takes advantage of the fact that `x` is sorted.\n        x0, x0_idx = torch.max(torch.where(mask, x[..., None], x[..., :1, None]), -2)\n        x1, x1_idx = torch.min(torch.where(~mask, x[..., None], x[..., -1:, None]), -2)\n        if return_idx:\n            return x0, x1, x0_idx, x1_idx\n        return x0, x1\n\n    fcdf0, fcdf1, fcdf0_idx, fcdf1_idx = find_interval(fcdf, return_idx=True)\n    fpdf0 = fpdf.take_along_dim(fcdf0_idx, dim=-1)\n    fpdf1 = fpdf.take_along_dim(fcdf1_idx, dim=-1)\n    xp0, xp1 = find_interval(xp)\n\n    offset = torch.clip(torch.nan_to_num((x - xp0) / (xp1 - xp0), 0), 0, 1)\n    ret = fcdf0 + (x - xp0) * (fpdf0 + fpdf1 * offset + fpdf0 * (1 - offset)) / 2\n    return ret\n\n\ndef lossfun_zip_outer(t, w, t_env, w_env, pulse_width, eps=1e-6):\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, pulse_width)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n\n", "contexts_below": "\n\ndef interval_distortion(t0_lo, t0_hi, t1_lo, t1_hi):\n    \"\"\"Compute mean(abs(x-y); x in [t0_lo, t0_hi], y in [t1_lo, t1_hi]).\"\"\"\n    # Distortion when the intervals do not overlap.\n    d_disjoint = torch.abs((t1_lo + t1_hi) / 2 - (t0_lo + t0_hi) / 2)\n\n    # Distortion when the intervals overlap.\n    d_overlap = (2 *\n                 (torch.minimum(t0_hi, t1_hi)**3 - torch.maximum(t0_lo, t1_lo)**3) +\n                 3 * (t1_hi * t0_hi * torch.abs(t1_hi - t0_hi) +\n                      t1_lo * t0_lo * torch.abs(t1_lo - t0_lo) + t1_hi * t0_lo *\n                      (t0_lo - t1_hi) + t1_lo * t0_hi *\n                      (t1_lo - t0_hi))) / (6 * (t0_hi - t0_lo) * (t1_hi - t1_lo))\n\n    # Are the two intervals not overlapping?\n    are_disjoint = (t0_lo > t1_hi) | (t1_lo > t0_hi)\n\n    return torch.where(are_disjoint, d_disjoint, d_overlap)\n\n\ndef anneal_loss_weight(weight: float, gamma: float, iter: int, mile: int):\n    # exponentially anneal the loss weight\n    return weight * gamma ** min(iter / mile, 1)\n\n\ndef gaussian_entropy_relighting4d(albedo_pred):\n    albedo_entropy = 0\n    for i in range(3):\n        channel = albedo_pred[..., i]\n        hist = GaussianHistogram(15, 0., 1., sigma=torch.var(channel))\n        h = hist(channel)\n        if h.sum() > 1e-6:\n            h = h.div(h.sum()) + 1e-6\n        else:\n            h = torch.ones_like(h)\n        albedo_entropy += torch.sum(-h * torch.log(h))\n    return albedo_entropy\n\n\nclass GaussianHistogram(nn.Module):\n    def __init__(self, bins, min, max, sigma):\n        super(GaussianHistogram, self).__init__()\n        self.bins = bins\n        self.min = min\n        self.max = max\n        self.sigma = sigma\n        self.delta = float(max - min) / float(bins)\n        self.centers = float(min) + self.delta * (torch.arange(bins, device=sigma.device).float() + 0.5)\n\n    def forward(self, x):\n        x = torch.unsqueeze(x, 0) - torch.unsqueeze(self.centers, 1)\n        x = torch.exp(-0.5 * (x / self.sigma)**2) / (self.sigma * np.sqrt(np.pi * 2)) * self.delta\n        x = x.sum(dim=1)\n        return x\n\n\ndef gaussian_entropy(x: torch.Tensor, *args, **kwargs):\n    eps = 1e-6\n    hps = 1e-9\n    h = gaussian_histogram(x, *args, **kwargs)\n    # h = (h / (h.sum(dim=0) + hps)).clip(eps)  # 3,\n    # entropy = (-h * h.log()).sum(dim=0).sum(dim=0)  # per channel entropy summed\n    entropy = 0\n    for i in range(3):\n        hi = h[..., i]\n        if hi.sum() > eps:\n            hi = hi / hi.sum() + eps\n        else:\n            hi = torch.ones_like(hi)\n        entropy += torch.sum(-hi * torch.log(hi))\n    return entropy\n\n\ndef gaussian_histogram(x: torch.Tensor, bins: int = 15, min: float = 0.0, max: float = 1.0):\n    x = x.view(-1, x.shape[-1])  # N, 3\n    sigma = x.var(dim=0)  # 3,\n    delta = (max - min) / bins\n    centers = min + delta * (torch.arange(bins, device=x.device, dtype=x.dtype) + 0.5)  # BIN\n    x = x[None] - centers[:, None, None]  # BIN, N, 3\n    x = (-0.5 * (x / sigma).pow(2)).exp() / (sigma * np.sqrt(np.pi * 2)) * delta  # BIN, N, 3\n    x = x.sum(dim=1)\n    return x  # BIN, 3\n\n\ndef reg_diff_crit(x: torch.Tensor, iter_step: int, max_weight: float = 1e-4, ann_iter: int = 100 * 500):\n    weight = min(iter_step, ann_iter) * max_weight / ann_iter\n    return reg(x), weight\n\n\ndef reg_raw_crit(x: torch.Tensor, iter_step: int, max_weight: float = 1e-4, ann_iter: int = 100 * 500):\n    weight = min(iter_step, ann_iter) * max_weight / ann_iter\n    n_batch, n_pts_x2, D = x.shape\n    n_pts = n_pts_x2 // 2\n    length = x.norm(dim=-1, keepdim=True)  # length\n    vector = x / (length + 1e-8)  # vector direction (normalized to unit sphere)\n    # loss_length = mse(length[:, n_pts:, :], length[:, :n_pts, :])\n    loss_vector = reg((vector[:, n_pts:, :] - vector[:, :n_pts, :]))\n    # loss = loss_length + loss_vector\n    loss = loss_vector\n    return loss, weight\n\n\nclass LossNetwork(torch.nn.Module):\n    \"\"\"Reference:\n        https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n    \"\"\"\n\n    def __init__(self):\n        super(LossNetwork, self).__init__()\n        try:\n            from torchvision.models import VGG19_Weights\n            self.vgg_layers = vgg.vgg19(weights=VGG19_Weights.DEFAULT).features\n        except ImportError:\n            self.vgg_layers = vgg.vgg19(pretrained=True).features\n\n        for param in self.vgg_layers.parameters():\n            param.requires_grad = False\n        '''\n        self.layer_name_mapping = {\n            '3': \"relu1\",\n            '8': \"relu2\",\n            '17': \"relu3\",\n            '26': \"relu4\",\n            '35': \"relu5\",\n        }\n        '''\n\n        self.layer_name_mapping = {'3': \"relu1\", '8': \"relu2\"}\n\n    def forward(self, x):\n        output = {}\n        for name, module in self.vgg_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n            if name == '8':\n                break\n        LossOutput = namedtuple(\"LossOutput\", [\"relu1\", \"relu2\"])\n        return LossOutput(**output)\n\n\nclass PerceptualLoss(torch.nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n\n        self.model = LossNetwork()\n        self.model.cuda()\n        self.model.eval()\n        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n        self.l1_loss = torch.nn.L1Loss(reduction='mean')\n\n    def forward(self, x, target):\n        x_feature = self.model(x[:, 0:3, :, :])\n        target_feature = self.model(target[:, 0:3, :, :])\n\n        feature_loss = (\n            self.l1_loss(x_feature.relu1, target_feature.relu1) +\n            self.l1_loss(x_feature.relu2, target_feature.relu2)) / 2.0\n\n        l1_loss = self.l1_loss(x, target)\n        l2_loss = self.mse_loss(x, target)\n\n        loss = feature_loss + l1_loss + l2_loss\n\n        return loss\n\n\nclass VGGPerceptualLoss(torch.nn.Module):\n    def __init__(self, resize=False):\n        super(VGGPerceptualLoss, self).__init__()\n        blocks = []\n        import torchvision\n        vgg16 = torchvision.models.vgg16(pretrained=True)\n        blocks.append(vgg16.features[:4].eval())\n        blocks.append(vgg16.features[4:9].eval())\n        blocks.append(vgg16.features[9:16].eval())\n        blocks.append(vgg16.features[16:23].eval())\n        for bl in blocks:\n            for p in bl.parameters():\n                p.requires_grad = False\n        self.blocks = nn.ModuleList(blocks)\n        self.transform = F.interpolate\n        self.resize = resize\n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n        if input.shape[1] != 3:\n            input = input.repeat(1, 3, 1, 1)\n            target = target.repeat(1, 3, 1, 1)\n        input = (input - self.mean) / self.std\n        target = (target - self.mean) / self.std\n        if self.resize:\n            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n        loss = 0.0\n        x = input\n        y = target\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            y = block(y)\n            if i in feature_layers:\n                loss += F.l1_loss(x, y)\n            if i in style_layers:\n                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n                gram_x = act_x @ act_x.permute(0, 2, 1)\n                gram_y = act_y @ act_y.permute(0, 2, 1)\n                loss += F.l1_loss(gram_x, gram_y)\n        return loss\n\n\ndef eikonal(x: torch.Tensor, th=1.0) -> torch.Tensor:\n    return ((x.norm(dim=-1) - th)**2).mean()\n\n\ndef sdf_mask_crit(ret, batch):\n    msk_sdf = ret['msk_sdf']\n    msk_label = ret['msk_label']\n\n    alpha = 50\n    alpha_factor = 2\n    alpha_milestones = [10000, 20000, 30000, 40000, 50000]\n    for milestone in alpha_milestones:\n        if batch['iter_step'] > milestone:\n            alpha = alpha * alpha_factor\n\n    msk_sdf = -alpha * msk_sdf\n    mask_loss = F.binary_cross_entropy_with_logits(msk_sdf, msk_label) / alpha\n\n    return mask_loss\n\n\ndef cross_entropy(x: torch.Tensor, y: torch.Tensor):\n    # x: unormalized input logits\n    # channel last cross entropy loss\n    x = x.view(-1, x.shape[-1])  # N, C\n    y = y.view(-1, y.shape[-1])  # N, C\n    return F.cross_entropy(x, y)\n\n\ndef huber(x: torch.Tensor, y: torch.Tensor):\n    return F.huber_loss(x, y, reduction='mean')\n\n\ndef smoothl1(x: torch.Tensor, y: torch.Tensor):\n    return F.smooth_l1_loss(x, y)\n\n\ndef mse(x: torch.Tensor, y: torch.Tensor):\n    return ((x.float() - y.float())**2).mean()\n\n\ndef dot(x: torch.Tensor, y: torch.Tensor):\n    return (x * y).sum(dim=-1)\n\n\ndef l1(x: torch.Tensor, y: torch.Tensor):\n    return l1_reg(x - y)\n\n\ndef l2(x: torch.Tensor, y: torch.Tensor):\n    return l2_reg(x - y)\n\n\ndef l1_reg(x: torch.Tensor):\n    return x.abs().sum(dim=-1).mean()\n\n\ndef l2_reg(x: torch.Tensor) -> torch.Tensor:\n    return (x**2).sum(dim=-1).mean()\n\n\ndef bce_loss(x: torch.Tensor, y: torch.Tensor):\n    return F.binary_cross_entropy(x, y)\n\n\ndef mIoU_loss(x: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Compute the mean intersection of union loss over masked regions\n    x, y: B, N, 1\n    \"\"\"\n    I = (x * y).sum(-1).sum(-1)\n    U = (x + y).sum(-1).sum(-1) - I\n    mIoU = (I / U.detach()).mean()\n    return 1 - mIoU\n\n\ndef reg(x: torch.Tensor) -> torch.Tensor:\n    return x.norm(dim=-1).mean()\n\n\ndef thresh(x: torch.Tensor, a: torch.Tensor, eps: float = 1e-8):\n    return 1 / (l2(x, a) + eps)\n\n\ndef elastic_crit(jac: torch.Tensor) -> torch.Tensor:\n    \"\"\"Compute the raw 'log_svals' type elastic energy, and\n    remap it using the Geman-McClure type of robust loss.\n    Args:\n        jac (torch.Tensor): (B, N, 3, 3), the gradient of warpped xyz with respect to the original xyz\n    Return:\n        elastic_loss (torch.Tensor): (B, N), \n    \"\"\"\n    # !: CUDA IMPLEMENTATION OF SVD IS EXTREMELY SLOW\n    # old_device = jac.device\n    # jac = jac.cpu()\n    # svd_backward: Setting compute_uv to false in torch.svd doesn't compute singular matrices, and hence we cannot compute backward. Please use torch.svd(compute_uv=True)\n    _, S, _ = torch.svd(jac, compute_uv=True)           # (B, N, 3)\n    # S = S.to(old_device)\n    log_svals = torch.log(torch.clamp(S, min=1e-6))     # (B, N, 3)\n    sq_residual = torch.sum(log_svals**2, dim=-1)       # (B, N)\n    # TODO: determine whether it is a good choice to compute the robust loss here\n    elastic_loss = general_loss_with_squared_residual(sq_residual, alpha=-2.0, scale=0.03)\n    return elastic_loss\n\n\ndef general_loss_with_squared_residual(squared_x, alpha, scale):\n    r\"\"\"The general loss that takes a squared residual.\n    This fuses the sqrt operation done to compute many residuals while preserving\n    the square in the loss formulation.\n    This implements the rho(x, \\alpha, c) function described in \"A General and\n    Adaptive Robust Loss Function\", Jonathan T. Barron,\n    https://arxiv.org/abs/1701.03077.\n    Args:\n        squared_x: The residual for which the loss is being computed. x can have\n        any shape, and alpha and scale will be broadcasted to match x's shape if\n        necessary.\n        alpha: The shape parameter of the loss (\\alpha in the paper), where more\n        negative values produce a loss with more robust behavior (outliers \"cost\"\n        less), and more positive values produce a loss with less robust behavior\n        (outliers are penalized more heavily). Alpha can be any value in\n        [-infinity, infinity], but the gradient of the loss with respect to alpha\n        is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n        interpolation between several discrete robust losses:\n            alpha=-Infinity: Welsch/Leclerc Loss.\n            alpha=-2: Geman-McClure loss.\n            alpha=0: Cauchy/Lortentzian loss.\n            alpha=1: Charbonnier/pseudo-Huber loss.\n            alpha=2: L2 loss.\n        scale: The scale parameter of the loss. When |x| < scale, the loss is an\n        L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n        different shape according to alpha.\n    Returns:\n        The losses for each element of x, in the same shape as x.\n    \"\"\"\n    # https://pytorch.org/docs/stable/type_info.html\n    eps = torch.tensor(torch.finfo(torch.float32).eps)\n\n    # convert the float to torch.tensor\n    alpha = torch.tensor(alpha).to(squared_x.device)\n    scale = torch.tensor(scale).to(squared_x.device)\n\n    # This will be used repeatedly.\n    squared_scaled_x = squared_x / (scale ** 2)\n\n    # The loss when alpha == 2.\n    loss_two = 0.5 * squared_scaled_x\n    # The loss when alpha == 0.\n    loss_zero = log1p_safe(0.5 * squared_scaled_x)\n    # The loss when alpha == -infinity.\n    loss_neginf = -torch.expm1(-0.5 * squared_scaled_x)\n    # The loss when alpha == +infinity.\n    loss_posinf = expm1_safe(0.5 * squared_scaled_x)\n\n    # The loss when not in one of the above special cases.\n    # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n    beta_safe = torch.maximum(eps, torch.abs(alpha - 2.))\n    # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n    alpha_safe = torch.where(\n        torch.greater_equal(alpha, torch.tensor(0.)), torch.ones_like(alpha),\n        -torch.ones_like(alpha)) * torch.maximum(eps, torch.abs(alpha))\n    loss_otherwise = (beta_safe / alpha_safe) * (\n        torch.pow(squared_scaled_x / beta_safe + 1., 0.5 * alpha) - 1.)\n\n    # Select which of the cases of the loss to return.\n    loss = torch.where(\n        alpha == -torch.inf, loss_neginf,\n        torch.where(\n            alpha == 0, loss_zero,\n            torch.where(\n                alpha == 2, loss_two,\n                torch.where(alpha == torch.inf, loss_posinf, loss_otherwise))))\n\n    return scale * loss\n\n\ndef log1p_safe(x):\n    \"\"\"The same as torch.log1p(x), but clamps the input to prevent NaNs.\"\"\"\n    return torch.log1p(torch.minimum(x, torch.tensor(3e37)))\n\n\ndef expm1_safe(x):\n    \"\"\"The same as torch.expm1(x), but clamps the input to prevent NaNs.\"\"\"\n    return torch.expm1(torch.minimum(x, torch.tensor(87.5)))\n\n\ndef compute_plane_tv(t):\n    batch_size, c, h, w = t.shape\n    count_h = batch_size * c * (h - 1) * w\n    count_w = batch_size * c * h * (w - 1)\n    h_tv = torch.square(t[..., 1:, :] - t[..., :h - 1, :]).sum()\n    w_tv = torch.square(t[..., :, 1:] - t[..., :, :w - 1]).sum()\n    return 2 * (h_tv / count_h + w_tv / count_w)  # This is summing over batch and c instead of avg\n\n\ndef compute_planes_tv(embedding):\n    tv_loss = 0\n    for emb in embedding:\n        tv_loss += compute_plane_tv(emb)\n    return tv_loss\n\n\ndef compute_plane_smoothness(t):\n    batch_size, c, h, w = t.shape\n    # Convolve with a second derivative filter, in the time dimension which is dimension 2\n    first_difference = t[..., 1:] - t[..., :w - 1]  # [batch, c, h-1, w]\n    second_difference = first_difference[..., 1:] - first_difference[..., :w - 2]  # [batch, c, h-2, w]\n    # Take the L2 norm of the result\n    return torch.square(second_difference).mean()\n\n\ndef compute_time_planes_smooth(embedding):\n    loss = 0.\n    for emb in embedding:\n        loss += compute_plane_smoothness(emb)\n    return loss\n\n\ndef compute_ssim(x: torch.Tensor, y: torch.Tensor):\n    from pytorch_msssim import ssim\n    return ssim(x, y, data_range=1.0, win_size=11, win_sigma=1.5, K=(0.01, 0.03))\n\n\n# from MonoSDF\ndef compute_scale_and_shift(prediction, target, mask):\n    # System matrix: A = [[a_00, a_01], [a_10, a_11]]\n    a_00 = torch.sum(mask * prediction * prediction, (1, 2))\n    a_01 = torch.sum(mask * prediction, (1, 2))\n    a_11 = torch.sum(mask, (1, 2))\n\n    # Right hand side: b = [b_0, b_1]\n    b_0 = torch.sum(mask * prediction * target, (1, 2))\n    b_1 = torch.sum(mask * target, (1, 2))\n\n    # Solution: x = A^-1 . b = [[a_11, -a_01], [-a_10, a_00]] / (a_00 * a_11 - a_01 * a_10) . b\n    x_0 = torch.zeros_like(b_0)\n    x_1 = torch.zeros_like(b_1)\n\n    det = a_00 * a_11 - a_01 * a_01\n    valid = det.nonzero()\n\n    x_0[valid] = ( a_11[valid] * b_0[valid] - a_01[valid] * b_1[valid]) / det[valid]\n    x_1[valid] = (-a_01[valid] * b_0[valid] + a_00[valid] * b_1[valid]) / det[valid]\n\n    return x_0, x_1\n\n\ndef reduction_batch_based(image_loss, M):\n    # Average of all valid pixels of the batch\n    # Avoid division by 0 (if sum(M) = sum(sum(mask)) = 0: sum(image_loss) = 0)\n    divisor = torch.sum(M)\n\n    if divisor == 0: return 0\n    else: return torch.sum(image_loss) / divisor\n\n\ndef reduction_image_based(image_loss, M):\n    # Mean of average of valid pixels of an image\n    # Avoid division by 0 (if M = sum(mask) = 0: image_loss = 0)\n    valid = M.nonzero()\n    image_loss[valid] = image_loss[valid] / M[valid]\n\n    return torch.mean(image_loss)\n\n\ndef mse_loss(prediction, target, mask, reduction=reduction_batch_based):\n    # Number of valid pixels\n    M = torch.sum(mask, (1, 2))  # (B,)\n\n    # L2 loss\n    res = prediction - target  # (B, H, W)\n    image_loss = torch.sum(mask * res * res, (1, 2))  # (B,)\n\n    return reduction(image_loss, 2 * M)\n\n\ndef gradient_loss(prediction, target, mask, reduction=reduction_batch_based):\n\n    M = torch.sum(mask, (1, 2))\n\n    diff = prediction - target\n    diff = torch.mul(mask, diff)\n\n    grad_x = torch.abs(diff[:, :, 1:] - diff[:, :, :-1])\n    mask_x = torch.mul(mask[:, :, 1:], mask[:, :, :-1])\n    grad_x = torch.mul(mask_x, grad_x)\n\n    grad_y = torch.abs(diff[:, 1:, :] - diff[:, :-1, :])\n    mask_y = torch.mul(mask[:, 1:, :], mask[:, :-1, :])\n    grad_y = torch.mul(mask_y, grad_y)\n\n    image_loss = torch.sum(grad_x, (1, 2)) + torch.sum(grad_y, (1, 2))\n\n    return reduction(image_loss, M)\n\n\nclass MSELoss(nn.Module):\n    def __init__(self, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n    def forward(self, prediction, target, mask):\n        return mse_loss(prediction, target, mask, reduction=self.__reduction)\n\n\nclass GradientLoss(nn.Module):\n    def __init__(self, scales=1, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n        self.__scales = scales\n\n    def forward(self, prediction, target, mask):\n        total = 0\n\n        for scale in range(self.__scales):\n            step = pow(2, scale)\n\n            total += gradient_loss(prediction[:, ::step, ::step], target[:, ::step, ::step],\n                                   mask[:, ::step, ::step], reduction=self.__reduction)\n\n        return total\n\n\nclass ScaleAndShiftInvariantMSELoss(nn.Module):\n    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        self.__data_loss = MSELoss(reduction=reduction)\n        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)\n        self.__alpha = alpha\n\n        self.__prediction_ssi = None\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        # Compute scale and shift\n        scale, shift = compute_scale_and_shift(prediction, target, mask)\n        self.__prediction_ssi = scale.view(-1, 1, 1) * prediction + shift.view(-1, 1, 1)\n        total = self.__data_loss(self.__prediction_ssi, target, mask)\n\n        # Add regularization if needed\n        if self.__alpha > 0:\n            total += self.__alpha * self.__regularization_loss(self.__prediction_ssi, target, mask)\n\n        return total\n\n    def __get_prediction_ssi(self):\n        return self.__prediction_ssi\n\n    prediction_ssi = property(__get_prediction_ssi)\n# from MonoSDF\n\n\ndef median_normalize(x, mask):\n    \"\"\" Median normalize a tensor for all valid pixels.\n        This operation is performed without batch dimension.\n    Args:\n        x (torch.Tensor): (H, W), original tensor\n        mask (torch.Tensor): (H, W), mask tensor\n    Return:\n        y (torch.Tensor): (H, W), median normalized tensor\n    \"\"\"\n    M = torch.sum(mask)\n\n    # Return original tensor if there is no valid pixel\n    if M == 0:\n        return x\n\n    # Compute median and scale\n    t = torch.quantile(x[mask == 1], q=0.5)  # scalar\n    s = torch.sum(x[mask == 1] - t) / M  # scalar\n\n    # Return median normalized tensor\n    return (x - t) / s\n    \n\ndef mae_loss(prediction, target, mask, reduction=reduction_batch_based):\n    # Number of valid pixels\n    M = torch.sum(mask, (1, 2))  # (B,)\n\n    # L1 loss\n    res = (prediction - target).abs()  # (B, H, W)\n    image_loss = torch.sum(mask * res, (1, 2))  # (B,)\n\n    return reduction(image_loss, 2 * M)\n\n\nclass MAELoss(nn.Module):\n    def __init__(self, reduction='batch-based'):\n        super().__init__()\n\n        if reduction == 'batch-based':\n            self.__reduction = reduction_batch_based\n        else:\n            self.__reduction = reduction_image_based\n\n    def forward(self, prediction, target, mask):\n        return mae_loss(prediction, target, mask, reduction=self.__reduction)\n\n\nclass ScaleAndShiftInvariantMAELoss(nn.Module):\n    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):\n        super().__init__()\n\n        self.__data_loss = MAELoss(reduction=reduction)\n        self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)\n        self.__alpha = alpha\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        # TODO: Maybe there is a better way to do the batching\n        # But `torch.quantile` does not support multiple `dim` argument for now\n        for i in range(prediction.shape[0]):\n            prediction[i] = median_normalize(prediction[i], mask[i])  # (H, W)\n            target[i] = median_normalize(target[i], mask[i])  # (H, W)\n\n        # Compute the scale-and-shift invariant MAE loss\n        total = self.__data_loss(prediction, target, mask)\n\n       # Add regularization if needed\n        if self.__alpha > 0:\n            total += self.__alpha * self.__regularization_loss(self.prediction, target, mask)\n\n        return total\n\n\n# Modified version of Adabins repository\n# https://github.com/shariqfarooq123/AdaBins/blob/0952d91e9e762be310bb4cd055cbfe2448c0ce20/loss.py#L7\nclass ScaleInvariantLogLoss(nn.Module):\n    def __init__(self, alpha=10.0, beta=0.15, eps=0.0):\n        super(ScaleInvariantLogLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        # The eps is added to avoid log(0) and division by zero\n        # But it should be gauranteed that the network output is always non-negative\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        total = 0\n        # Maybe there is a better way to do the batching\n        for i in range(prediction.shape[0]):\n            g = torch.log(prediction[i][mask[i]] + self.eps) - torch.log(target[i][mask[i]] + self.eps)  # (N,)\n            Dg = torch.var(g) + self.beta * torch.pow(torch.mean(g), 2)  # scalar\n            total += self.alpha * torch.sqrt(Dg)\n\n        return total\n", "input_code": "def lossfun_distortion(t: torch.Tensor, w: torch.Tensor):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n\n    \"\"\"\n    Computes the distortion loss function for a given tensor of targets and weights. The function calculates both the inter-interval and intra-interval losses based on the provided tensors and combines them to produce the total distortion loss.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The target tensor for which the distortion loss is to be calculated. It is expected that the last dimension of 't' is one more than that of 'w'.\n    :param w: torch.Tensor. The weights tensor, used to weight the distortion loss calculation. The last dimension of 'w' should be one less than that of 't'.\n    :return: torch.Tensor. The calculated distortion loss as a tensor. This combines both inter-interval and intra-interval losses.\n    \"\"\"", "reference_steps": "1. Define a function `lossfun_distortion` that takes two tensors `t` and `w` as input, where `t` has one more element in its last dimension than `w`.\n2. Ensure `t` and `w` have the same number of channels by using a function `matchup_channels` (not provided in the reference code).\n3. Compute the midpoints `ut` of the intervals defined by `t` by averaging adjacent elements along the last dimension.\n4. Calculate the pairwise absolute differences `dut` between the midpoints `ut`.\n5. Compute the inter-interval loss `loss_inter` by weighting the pairwise differences `dut` with the weights `w`, summing over one dimension, and then applying the weights again before summing over the last dimension.\n6. Calculate the intra-interval loss `loss_intra` by squaring the weights `w`, multiplying by the interval lengths (`t[..., 1:] - t[..., :-1]`), and summing over the last dimension, then dividing by 3.\n7. Add the inter-interval loss `loss_inter` and the intra-interval loss `loss_intra` to obtain the total loss.\n8. Return the total loss as the output of the function.", "reference_code": "def lossfun_distortion(t: torch.Tensor, w: torch.Tensor):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    \"\"\"Compute iint w[i] w[j] |t[i] - t[j]| di dj.\"\"\"\n    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    return loss_inter + loss_intra\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "type": "function", "class_name": null, "function_name": "max_dilate", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "dependency_sampled": "# Intra-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "contexts_above": "import torch\nfrom typing import Tuple, Callable, List\n\n\ndef matchup_channels(t: torch.Tensor, w: torch.Tensor):\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dimension\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)  # 65\n    return t, w\n\n\n@torch.jit.script\ndef interpolate(x: torch.Tensor, xp: torch.Tensor, fp: torch.Tensor) -> torch.Tensor:\n    \"\"\"One-dimensional linear interpolation for monotonically increasing sample\n    points.\n\n    Returns the one-dimensional piecewise linear interpolant to a function with\n    given discrete data points :math:`(xp, fp)`, evaluated at :math:`x`.\n\n    Args:\n        x: the :math:`x`-coordinates at which to evaluate the interpolated\n            values.\n        xp: the :math:`x`-coordinates of the data points, must be increasing.\n        fp: the :math:`y`-coordinates of the data points, same length as `xp`.\n\n    Returns:\n        the interpolated values, same size as `x`.\n    \"\"\"\n    if x.ndim == xp.ndim - 1:\n        x = x[None]\n\n    m = (fp[..., 1:] - fp[..., :-1]) / (xp[..., 1:] - xp[..., :-1] + 1e-8)  # slope\n    b = fp[..., :-1] - (m * xp[..., :-1])\n\n    indices = torch.sum(torch.ge(x[..., :, None], xp[..., None, :]), -1) - 1  # torch.ge:  x[i] >= xp[i] ? true: false\n    indices = torch.clamp(indices, 0, m.shape[-1] - 1)\n\n    return m.gather(dim=-1, index=indices) * x + b.gather(dim=-1, index=indices)\n\n\n@torch.jit.script\ndef integrate_weights(w: torch.Tensor):\n    \"\"\"Compute the cumulative sum of w, assuming all weight vectors sum to 1.\n    The output's size on the last dimension is one greater than that of the input,\n    because we're computing the integral corresponding to the endpoints of a step\n    function, not the integral of the interior/bin values.\n    Args:\n      w: Tensor, which will be integrated along the last axis. This is assumed to\n        sum to 1 along the last axis, and this function will (silently) break if\n        that is not the case.\n    Returns:\n      cw0: Tensor, the integral of w, where cw0[..., 0] = 0 and cw0[..., -1] = 1\n    \"\"\"\n    cw = torch.cumsum(w[..., :-1], dim=-1).clip(max=1.0)\n    shape = cw.shape[:-1] + (1,)\n    # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n    cw0 = torch.cat([cw.new_zeros(shape), cw, cw.new_ones(shape)], dim=-1)\n    return cw0\n\n\n@torch.jit.script\ndef weighted_percentile(t: torch.Tensor, w: torch.Tensor, ps: List[float]):\n    \"\"\"Compute the weighted percentiles of a step function. w's must sum to 1.\"\"\"\n    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    # We want to interpolate into the integrated weights according to `ps`.\n    # Vmap fn to an arbitrary number of leading dimensions.\n    cw_mat = cw.reshape([-1, cw.shape[-1]])\n    t_mat = t.reshape([-1, t.shape[-1]])\n    wprctile_mat = interpolate(torch.as_tensor(ps).to(t, non_blocking=True),\n                               cw_mat,\n                               t_mat)\n    wprctile = wprctile_mat.reshape(cw.shape[:-1] + (len(ps),))\n    return wprctile\n\n\ndef s_vals_to_z_vals(s: torch.Tensor,\n                     tn: torch.Tensor,\n                     tf: torch.Tensor,\n                     g: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ig: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ):\n    # transfer ray depth from s space to t space (with inverse of g)\n    return ig(s * g(tf) + (1 - s) * g(tn))\n\n\ndef z_vals_to_s_vals(t: torch.Tensor,\n                     tn: torch.Tensor,\n                     tf: torch.Tensor,\n                     g: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ):\n    # transfer ray depth from t space back to s space (with function g)\n    return (g(t) - g(tn)) / (g(tf) - g(tn) + 1e-8)\n\n# Hierarchical sampling (section 5.2)\n\n\ndef searchsorted(a: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find indices where v should be inserted into a to maintain order.\n    This behaves like jnp.searchsorted (its second output is the same as\n    jnp.searchsorted's output if all elements of v are in [a[0], a[-1]]) but is\n    faster because it wastes memory to save some compute.\n    Args:\n      a: tensor, the sorted reference points that we are scanning to see where v\n        should lie.\n      v: tensor, the query points that we are pretending to insert into a. Does\n        not need to be sorted. All but the last dimensions should match or expand\n        to those of a, the last dimension can differ.\n    Returns:\n      (idx_lo, idx_hi), where a[idx_lo] <= v < a[idx_hi], unless v is out of the\n      range [a[0], a[-1]] in which case idx_lo and idx_hi are both the first or\n      last index of a.\n    \"\"\"\n    i = torch.arange(a.shape[-1], device=a.device)  # 128\n    v_ge_a = v[..., None, :] >= a[..., :, None]\n    idx_lo = torch.max(torch.where(v_ge_a, i[..., :, None], i[..., :1, None]), -2)[0]  # 128\n    idx_hi = torch.min(torch.where(~v_ge_a, i[..., :, None], i[..., -1:, None]), -2)[0]\n    return idx_lo, idx_hi\n\n\ndef invert_cdf(u, t, w):\n    \"\"\"Invert the CDF defined by (t, w) at the points specified by u in [0, 1).\"\"\"\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(u, cw, t)\n    return t_new\n\n\ndef importance_sampling(t: torch.Tensor,\n                        w: torch.Tensor,\n                        num_samples: int,\n                        perturb=True,\n                        single_jitter=False,\n                        ):\n    \"\"\"Piecewise-Constant PDF sampling from a step function.\n\n    Args:\n        rng: random number generator (or None for `linspace` sampling).\n        t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n        w_logits: [..., num_bins], logits corresponding to bin weights\n        num_samples: int, the number of samples.\n        single_jitter: bool, if True, jitter every sample along each ray by the same\n        amount in the inverse CDF. Otherwise, jitter each sample independently.\n        deterministic_center: bool, if False, when `rng` is None return samples that\n        linspace the entire PDF. If True, skip the front and back of the linspace\n        so that the centers of each PDF interval are returned.\n        use_gpu_resampling: bool, If True this resamples the rays based on a\n        \"gather\" instruction, which is fast on GPUs but slow on TPUs. If False,\n        this resamples the rays based on brute-force searches, which is fast on\n        TPUs, but slow on GPUs.\n\n    Returns:\n        t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n    \"\"\"\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dim\n\n    # preparing for size change\n    sh = *t.shape[:-1], num_samples  # B, P, I\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    # assuming sampling in s space\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # eps = torch.finfo(torch.float32).eps\n    eps = 1e-8\n\n    # Draw uniform samples.\n\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps if perturb else 0\n    d = 1 if single_jitter else num_samples\n    u = (\n        torch.linspace(0, 1 - u_max, num_samples, device=t.device, dtype=t.dtype) +\n        torch.rand(t.shape[:-1] + (d,), device=t.device, dtype=t.dtype) * max_jitter\n    )\n\n    u = invert_cdf(u, t, w)\n\n    # preparing for size change\n    u = u.reshape(sh)\n    return u\n\n\ndef weight_to_pdf(t: torch.Tensor, w: torch.Tensor, eps=torch.finfo(torch.float32).eps**2):\n    t, w = matchup_channels(t, w)\n    \"\"\"Turn a vector of weights that sums to 1 into a PDF that integrates to 1.\"\"\"\n    return w / (t[..., 1:] - t[..., :-1]).clip(eps)\n\n\ndef pdf_to_weight(t: torch.Tensor, p: torch.Tensor):\n    t, p = matchup_channels(t, p)\n    \"\"\"Turn a PDF that integrates to 1 into a vector of weights that sums to 1.\"\"\"\n    return p * (t[..., 1:] - t[..., :-1])\n\n\n", "contexts_below": "\n\ndef max_dilate_weights(t: torch.Tensor,\n                       w: torch.Tensor,\n                       dilation: float,\n                       domain=(-torch.inf, torch.inf),\n                       renormalize=False,\n                       eps=torch.finfo(torch.float32).eps**2):\n    \"\"\"Dilate (via max-pooling) a set of weights.\"\"\"\n    p = weight_to_pdf(t, w)\n    t_dilate, p_dilate = max_dilate(t, p, dilation, domain=domain)\n    w_dilate = pdf_to_weight(t_dilate, p_dilate)\n    if renormalize:\n        w_dilate /= torch.sum(w_dilate, dim=-1, keepdim=True).clip(eps)\n    return t_dilate, w_dilate\n\n\ndef anneal_weights(t: torch.Tensor,\n                   w: torch.Tensor,\n                   train_frac: float,\n                   anneal_slope: float = 10.0,\n                   eps=torch.finfo(torch.float32).eps ** 2):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n\n    # Optionally anneal the weights as a function of training iteration.\n    if anneal_slope > 0:\n        # Schlick's bias function, see https://arxiv.org/abs/2010.09714\n        def bias(x, s): return (s * x) / ((s - 1) * x + 1)\n        anneal = bias(train_frac, anneal_slope)\n    else:\n        anneal = 1.\n\n    # A slightly more stable way to compute weights**anneal. If the distance\n    # between adjacent intervals is zero then its weight is fixed to 0.\n    logits_resample = torch.where(\n        t[..., 1:] > t[..., :-1],\n        anneal * torch.log(w.clip(eps)), -torch.inf)  # MARK: prone to nan\n\n    # If all samples are -inf, softmax will produce a nan (all -torch.inf)\n    w = torch.softmax(logits_resample, dim=-1)\n    return w\n\n\ndef query(tq, t, y, outside_value=0):\n    \"\"\"Look up the values of the step function (t, y) at locations tq.\"\"\"\n    idx_lo, idx_hi = searchsorted(t, tq)\n    yq = torch.where(idx_lo == idx_hi, outside_value,\n                     torch.take_along_dim(torch.cat([y, torch.full_like(y[..., :1], outside_value)], dim=-1), idx_lo, dim=-1))  # ?\n    return yq\n", "input_code": "def max_dilate(t, w, dilation, domain=(-torch.inf, torch.inf)):\n\n    \"\"\"\n    Performs dilation (via max-pooling) on a non-negative step function. It dilates the time steps based on the specified dilation parameter and clips the dilated time steps within a given domain. The weights are also adjusted accordingly to match the dilated time steps.\n\n    Input-Output Arguments\n    :param t: Tensor, the time steps of the step function, used as the basis for dilation.\n    :param w: Tensor, the weights associated with each time step of the step function.\n    :param dilation: Float, the amount by which to dilate the time steps.\n    :param domain: Tuple of two floats, the minimum and maximum values to clip the dilated time steps to. Defaults to negative and positive infinity, allowing all dilated time steps.\n    :return: A tuple of two Tensors. The first tensor is the dilated and clipped time steps, and the second tensor is the adjusted weights corresponding to the dilated time steps.\n    \"\"\"", "reference_steps": "1. Define a function `max_dilate` that takes a time series `t`, corresponding weights `w`, a dilation factor `dilation`, and an optional `domain` parameter with default values for unrestricted domain.\n2. Ensure that the `t` and `w` have the same number of channels by calling `matchup_channels(t, w)`.\n3. Calculate the start (`t0`) and end (`t1`) times for the dilation by subtracting and adding the dilation factor to the original time series `t`, respectively.\n4. Concatenate the original time series `t` with the dilated start times `t0` and end times `t1`.\n5. Sort the concatenated times to create the dilated time series `t_dilate`.\n6. Clip `t_dilate` to ensure that all values fall within the specified `domain`.\n7. Use a boolean mask to determine which dilated weights `w_dilate` to consider by comparing `t0` and `t1` with `t_dilate`.\n8. Apply the boolean mask to the weights `w` and replace values outside the mask with zeros.\n9. Perform a max-pooling operation along the last dimension to obtain the dilated weights `w_dilate`.\n10. Return the dilated time series `t_dilate` and the corresponding dilated weights `w_dilate`, excluding the last time point.", "reference_code": "def max_dilate(t, w, dilation, domain=(-torch.inf, torch.inf)):\n    t, w = matchup_channels(t, w)\n    \"\"\"Dilate (via max-pooling) a non-negative step function.\"\"\"\n    t0 = t[..., :-1] - dilation\n    t1 = t[..., 1:] + dilation\n    t_dilate = torch.sort(torch.cat([t, t0, t1], dim=-1), dim=-1)[0]\n    t_dilate = t_dilate.clip(*domain)\n    w_dilate = torch.max(\n        torch.where(\n            (t0[..., None, :] <= t_dilate[..., None])\n            & (t1[..., None, :] > t_dilate[..., None]),\n            w[..., None, :],\n            0,\n        ),\n        dim=-1)[0][..., :-1]\n    return t_dilate, w_dilate\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "type": "function", "class_name": null, "function_name": "query", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.prop_utils.searchsorted\n    def searchsorted(a: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Find indices where v should be inserted into a to maintain order.\n        This behaves like jnp.searchsorted (its second output is the same as\n        jnp.searchsorted's output if all elements of v are in [a[0], a[-1]]) but is\n        faster because it wastes memory to save some compute.\n        Args:\n          a: tensor, the sorted reference points that we are scanning to see where v\n            should lie.\n          v: tensor, the query points that we are pretending to insert into a. Does\n            not need to be sorted. All but the last dimensions should match or expand\n            to those of a, the last dimension can differ.\n        Returns:\n          (idx_lo, idx_hi), where a[idx_lo] <= v < a[idx_hi], unless v is out of the\n          range [a[0], a[-1]] in which case idx_lo and idx_hi are both the first or\n          last index of a.\n        \"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\neasyvolcap.utils.prop_utils.searchsorted\n    def searchsorted(a: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Find indices where v should be inserted into a to maintain order.\n        This behaves like jnp.searchsorted (its second output is the same as\n        jnp.searchsorted's output if all elements of v are in [a[0], a[-1]]) but is\n        faster because it wastes memory to save some compute.\n        Args:\n          a: tensor, the sorted reference points that we are scanning to see where v\n            should lie.\n          v: tensor, the query points that we are pretending to insert into a. Does\n            not need to be sorted. All but the last dimensions should match or expand\n            to those of a, the last dimension can differ.\n        Returns:\n          (idx_lo, idx_hi), where a[idx_lo] <= v < a[idx_hi], unless v is out of the\n          range [a[0], a[-1]] in which case idx_lo and idx_hi are both the first or\n          last index of a.\n        \"\"\"\n\n", "contexts_above": "import torch\nfrom typing import Tuple, Callable, List\n\n\ndef matchup_channels(t: torch.Tensor, w: torch.Tensor):\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dimension\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)  # 65\n    return t, w\n\n\n@torch.jit.script\ndef interpolate(x: torch.Tensor, xp: torch.Tensor, fp: torch.Tensor) -> torch.Tensor:\n    \"\"\"One-dimensional linear interpolation for monotonically increasing sample\n    points.\n\n    Returns the one-dimensional piecewise linear interpolant to a function with\n    given discrete data points :math:`(xp, fp)`, evaluated at :math:`x`.\n\n    Args:\n        x: the :math:`x`-coordinates at which to evaluate the interpolated\n            values.\n        xp: the :math:`x`-coordinates of the data points, must be increasing.\n        fp: the :math:`y`-coordinates of the data points, same length as `xp`.\n\n    Returns:\n        the interpolated values, same size as `x`.\n    \"\"\"\n    if x.ndim == xp.ndim - 1:\n        x = x[None]\n\n    m = (fp[..., 1:] - fp[..., :-1]) / (xp[..., 1:] - xp[..., :-1] + 1e-8)  # slope\n    b = fp[..., :-1] - (m * xp[..., :-1])\n\n    indices = torch.sum(torch.ge(x[..., :, None], xp[..., None, :]), -1) - 1  # torch.ge:  x[i] >= xp[i] ? true: false\n    indices = torch.clamp(indices, 0, m.shape[-1] - 1)\n\n    return m.gather(dim=-1, index=indices) * x + b.gather(dim=-1, index=indices)\n\n\n@torch.jit.script\ndef integrate_weights(w: torch.Tensor):\n    \"\"\"Compute the cumulative sum of w, assuming all weight vectors sum to 1.\n    The output's size on the last dimension is one greater than that of the input,\n    because we're computing the integral corresponding to the endpoints of a step\n    function, not the integral of the interior/bin values.\n    Args:\n      w: Tensor, which will be integrated along the last axis. This is assumed to\n        sum to 1 along the last axis, and this function will (silently) break if\n        that is not the case.\n    Returns:\n      cw0: Tensor, the integral of w, where cw0[..., 0] = 0 and cw0[..., -1] = 1\n    \"\"\"\n    cw = torch.cumsum(w[..., :-1], dim=-1).clip(max=1.0)\n    shape = cw.shape[:-1] + (1,)\n    # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n    cw0 = torch.cat([cw.new_zeros(shape), cw, cw.new_ones(shape)], dim=-1)\n    return cw0\n\n\n@torch.jit.script\ndef weighted_percentile(t: torch.Tensor, w: torch.Tensor, ps: List[float]):\n    \"\"\"Compute the weighted percentiles of a step function. w's must sum to 1.\"\"\"\n    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    # We want to interpolate into the integrated weights according to `ps`.\n    # Vmap fn to an arbitrary number of leading dimensions.\n    cw_mat = cw.reshape([-1, cw.shape[-1]])\n    t_mat = t.reshape([-1, t.shape[-1]])\n    wprctile_mat = interpolate(torch.as_tensor(ps).to(t, non_blocking=True),\n                               cw_mat,\n                               t_mat)\n    wprctile = wprctile_mat.reshape(cw.shape[:-1] + (len(ps),))\n    return wprctile\n\n\ndef s_vals_to_z_vals(s: torch.Tensor,\n                     tn: torch.Tensor,\n                     tf: torch.Tensor,\n                     g: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ig: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ):\n    # transfer ray depth from s space to t space (with inverse of g)\n    return ig(s * g(tf) + (1 - s) * g(tn))\n\n\ndef z_vals_to_s_vals(t: torch.Tensor,\n                     tn: torch.Tensor,\n                     tf: torch.Tensor,\n                     g: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ):\n    # transfer ray depth from t space back to s space (with function g)\n    return (g(t) - g(tn)) / (g(tf) - g(tn) + 1e-8)\n\n# Hierarchical sampling (section 5.2)\n\n\ndef searchsorted(a: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find indices where v should be inserted into a to maintain order.\n    This behaves like jnp.searchsorted (its second output is the same as\n    jnp.searchsorted's output if all elements of v are in [a[0], a[-1]]) but is\n    faster because it wastes memory to save some compute.\n    Args:\n      a: tensor, the sorted reference points that we are scanning to see where v\n        should lie.\n      v: tensor, the query points that we are pretending to insert into a. Does\n        not need to be sorted. All but the last dimensions should match or expand\n        to those of a, the last dimension can differ.\n    Returns:\n      (idx_lo, idx_hi), where a[idx_lo] <= v < a[idx_hi], unless v is out of the\n      range [a[0], a[-1]] in which case idx_lo and idx_hi are both the first or\n      last index of a.\n    \"\"\"\n    i = torch.arange(a.shape[-1], device=a.device)  # 128\n    v_ge_a = v[..., None, :] >= a[..., :, None]\n    idx_lo = torch.max(torch.where(v_ge_a, i[..., :, None], i[..., :1, None]), -2)[0]  # 128\n    idx_hi = torch.min(torch.where(~v_ge_a, i[..., :, None], i[..., -1:, None]), -2)[0]\n    return idx_lo, idx_hi\n\n\ndef invert_cdf(u, t, w):\n    \"\"\"Invert the CDF defined by (t, w) at the points specified by u in [0, 1).\"\"\"\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(u, cw, t)\n    return t_new\n\n\ndef importance_sampling(t: torch.Tensor,\n                        w: torch.Tensor,\n                        num_samples: int,\n                        perturb=True,\n                        single_jitter=False,\n                        ):\n    \"\"\"Piecewise-Constant PDF sampling from a step function.\n\n    Args:\n        rng: random number generator (or None for `linspace` sampling).\n        t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n        w_logits: [..., num_bins], logits corresponding to bin weights\n        num_samples: int, the number of samples.\n        single_jitter: bool, if True, jitter every sample along each ray by the same\n        amount in the inverse CDF. Otherwise, jitter each sample independently.\n        deterministic_center: bool, if False, when `rng` is None return samples that\n        linspace the entire PDF. If True, skip the front and back of the linspace\n        so that the centers of each PDF interval are returned.\n        use_gpu_resampling: bool, If True this resamples the rays based on a\n        \"gather\" instruction, which is fast on GPUs but slow on TPUs. If False,\n        this resamples the rays based on brute-force searches, which is fast on\n        TPUs, but slow on GPUs.\n\n    Returns:\n        t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n    \"\"\"\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dim\n\n    # preparing for size change\n    sh = *t.shape[:-1], num_samples  # B, P, I\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    # assuming sampling in s space\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # eps = torch.finfo(torch.float32).eps\n    eps = 1e-8\n\n    # Draw uniform samples.\n\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps if perturb else 0\n    d = 1 if single_jitter else num_samples\n    u = (\n        torch.linspace(0, 1 - u_max, num_samples, device=t.device, dtype=t.dtype) +\n        torch.rand(t.shape[:-1] + (d,), device=t.device, dtype=t.dtype) * max_jitter\n    )\n\n    u = invert_cdf(u, t, w)\n\n    # preparing for size change\n    u = u.reshape(sh)\n    return u\n\n\ndef weight_to_pdf(t: torch.Tensor, w: torch.Tensor, eps=torch.finfo(torch.float32).eps**2):\n    t, w = matchup_channels(t, w)\n    \"\"\"Turn a vector of weights that sums to 1 into a PDF that integrates to 1.\"\"\"\n    return w / (t[..., 1:] - t[..., :-1]).clip(eps)\n\n\ndef pdf_to_weight(t: torch.Tensor, p: torch.Tensor):\n    t, p = matchup_channels(t, p)\n    \"\"\"Turn a PDF that integrates to 1 into a vector of weights that sums to 1.\"\"\"\n    return p * (t[..., 1:] - t[..., :-1])\n\n\ndef max_dilate(t, w, dilation, domain=(-torch.inf, torch.inf)):\n    t, w = matchup_channels(t, w)\n    \"\"\"Dilate (via max-pooling) a non-negative step function.\"\"\"\n    t0 = t[..., :-1] - dilation\n    t1 = t[..., 1:] + dilation\n    t_dilate = torch.sort(torch.cat([t, t0, t1], dim=-1), dim=-1)[0]\n    t_dilate = t_dilate.clip(*domain)\n    w_dilate = torch.max(\n        torch.where(\n            (t0[..., None, :] <= t_dilate[..., None])\n            & (t1[..., None, :] > t_dilate[..., None]),\n            w[..., None, :],\n            0,\n        ),\n        dim=-1)[0][..., :-1]\n    return t_dilate, w_dilate\n\n\ndef max_dilate_weights(t: torch.Tensor,\n                       w: torch.Tensor,\n                       dilation: float,\n                       domain=(-torch.inf, torch.inf),\n                       renormalize=False,\n                       eps=torch.finfo(torch.float32).eps**2):\n    \"\"\"Dilate (via max-pooling) a set of weights.\"\"\"\n    p = weight_to_pdf(t, w)\n    t_dilate, p_dilate = max_dilate(t, p, dilation, domain=domain)\n    w_dilate = pdf_to_weight(t_dilate, p_dilate)\n    if renormalize:\n        w_dilate /= torch.sum(w_dilate, dim=-1, keepdim=True).clip(eps)\n    return t_dilate, w_dilate\n\n\ndef anneal_weights(t: torch.Tensor,\n                   w: torch.Tensor,\n                   train_frac: float,\n                   anneal_slope: float = 10.0,\n                   eps=torch.finfo(torch.float32).eps ** 2):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n\n    # Optionally anneal the weights as a function of training iteration.\n    if anneal_slope > 0:\n        # Schlick's bias function, see https://arxiv.org/abs/2010.09714\n        def bias(x, s): return (s * x) / ((s - 1) * x + 1)\n        anneal = bias(train_frac, anneal_slope)\n    else:\n        anneal = 1.\n\n    # A slightly more stable way to compute weights**anneal. If the distance\n    # between adjacent intervals is zero then its weight is fixed to 0.\n    logits_resample = torch.where(\n        t[..., 1:] > t[..., :-1],\n        anneal * torch.log(w.clip(eps)), -torch.inf)  # MARK: prone to nan\n\n    # If all samples are -inf, softmax will produce a nan (all -torch.inf)\n    w = torch.softmax(logits_resample, dim=-1)\n    return w\n\n\n", "contexts_below": "", "input_code": "def query(tq, t, y, outside_value=0):\n\n    \"\"\"\n    This function looks up the values of a step function defined by time-value pairs (t, y) at specified query times (tq). If a query time matches a step change, the function returns an outside value; otherwise, it interpolates the value at the query time based on the step function.\n\n    Input-Output Arguments\n    :param tq: Tensor. The query times at which to evaluate the step function.\n    :param t: Tensor. The times at which the step function changes value.\n    :param y: Tensor. The values of the step function corresponding to the times in 't'.\n    :param outside_value: Numeric, optional. The value to return for query times that exactly match a step change time. Defaults to 0.\n    :return: Tensor. The interpolated or outside values of the step function at the query times.\n    \"\"\"", "reference_steps": "1. Define a function `query` that takes in a target query times `tq`, a tensor of times `t`, a tensor of corresponding values `y`, and an optional `outside_value` which defaults to 0.\n\n2. Use `searchsorted` to find the indices `idx_lo` and `idx_hi` in `t` where the elements of `tq` should be inserted to maintain order.\n\n3. Check if `idx_lo` is equal to `idx_hi` to determine if the query times fall exactly on the step changes.\n\n4. Create a tensor by concatenating `y` with a tensor of the same shape as `y[..., :1]` filled with `outside_value`. This ensures that queries beyond the range of `t` are assigned the `outside_value`.\n\n5. Use `torch.take_along_dim` to select the values from the concatenated tensor at the indices `idx_lo`. This effectively retrieves the step function values at the query times.\n\n6. Use `torch.where` to handle cases where the query times fall exactly on the step changes by assigning them the `outside_value`.\n\n7. Return the tensor `yq` containing the step function values at the query times `tq`.\n\n8. The function is designed to work with PyTorch tensors and leverages vectorized operations for efficiency.\n\n9. The function assumes that `t` is sorted in ascending order, as required by `searchsorted`.\n\n10. The function can be used to evaluate the value of a piecewise constant function (step function) at given query points.", "reference_code": "def query(tq, t, y, outside_value=0):\n    \"\"\"Look up the values of the step function (t, y) at locations tq.\"\"\"\n    idx_lo, idx_hi = searchsorted(t, tq)\n    yq = torch.where(idx_lo == idx_hi, outside_value,\n                     torch.take_along_dim(torch.cat([y, torch.full_like(y[..., :1], outside_value)], dim=-1), idx_lo, dim=-1))  # ?\n    return yq\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "type": "function", "class_name": null, "function_name": "anneal_weights", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "dependency_sampled": "# Intra-file Dependency:\neasyvolcap.utils.prop_utils.matchup_channels\n    def matchup_channels(t: torch.Tensor, w: torch.Tensor):\n\n", "contexts_above": "import torch\nfrom typing import Tuple, Callable, List\n\n\ndef matchup_channels(t: torch.Tensor, w: torch.Tensor):\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dimension\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)  # 65\n    return t, w\n\n\n@torch.jit.script\ndef interpolate(x: torch.Tensor, xp: torch.Tensor, fp: torch.Tensor) -> torch.Tensor:\n    \"\"\"One-dimensional linear interpolation for monotonically increasing sample\n    points.\n\n    Returns the one-dimensional piecewise linear interpolant to a function with\n    given discrete data points :math:`(xp, fp)`, evaluated at :math:`x`.\n\n    Args:\n        x: the :math:`x`-coordinates at which to evaluate the interpolated\n            values.\n        xp: the :math:`x`-coordinates of the data points, must be increasing.\n        fp: the :math:`y`-coordinates of the data points, same length as `xp`.\n\n    Returns:\n        the interpolated values, same size as `x`.\n    \"\"\"\n    if x.ndim == xp.ndim - 1:\n        x = x[None]\n\n    m = (fp[..., 1:] - fp[..., :-1]) / (xp[..., 1:] - xp[..., :-1] + 1e-8)  # slope\n    b = fp[..., :-1] - (m * xp[..., :-1])\n\n    indices = torch.sum(torch.ge(x[..., :, None], xp[..., None, :]), -1) - 1  # torch.ge:  x[i] >= xp[i] ? true: false\n    indices = torch.clamp(indices, 0, m.shape[-1] - 1)\n\n    return m.gather(dim=-1, index=indices) * x + b.gather(dim=-1, index=indices)\n\n\n@torch.jit.script\ndef integrate_weights(w: torch.Tensor):\n    \"\"\"Compute the cumulative sum of w, assuming all weight vectors sum to 1.\n    The output's size on the last dimension is one greater than that of the input,\n    because we're computing the integral corresponding to the endpoints of a step\n    function, not the integral of the interior/bin values.\n    Args:\n      w: Tensor, which will be integrated along the last axis. This is assumed to\n        sum to 1 along the last axis, and this function will (silently) break if\n        that is not the case.\n    Returns:\n      cw0: Tensor, the integral of w, where cw0[..., 0] = 0 and cw0[..., -1] = 1\n    \"\"\"\n    cw = torch.cumsum(w[..., :-1], dim=-1).clip(max=1.0)\n    shape = cw.shape[:-1] + (1,)\n    # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n    cw0 = torch.cat([cw.new_zeros(shape), cw, cw.new_ones(shape)], dim=-1)\n    return cw0\n\n\n@torch.jit.script\ndef weighted_percentile(t: torch.Tensor, w: torch.Tensor, ps: List[float]):\n    \"\"\"Compute the weighted percentiles of a step function. w's must sum to 1.\"\"\"\n    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    # We want to interpolate into the integrated weights according to `ps`.\n    # Vmap fn to an arbitrary number of leading dimensions.\n    cw_mat = cw.reshape([-1, cw.shape[-1]])\n    t_mat = t.reshape([-1, t.shape[-1]])\n    wprctile_mat = interpolate(torch.as_tensor(ps).to(t, non_blocking=True),\n                               cw_mat,\n                               t_mat)\n    wprctile = wprctile_mat.reshape(cw.shape[:-1] + (len(ps),))\n    return wprctile\n\n\ndef s_vals_to_z_vals(s: torch.Tensor,\n                     tn: torch.Tensor,\n                     tf: torch.Tensor,\n                     g: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ig: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ):\n    # transfer ray depth from s space to t space (with inverse of g)\n    return ig(s * g(tf) + (1 - s) * g(tn))\n\n\ndef z_vals_to_s_vals(t: torch.Tensor,\n                     tn: torch.Tensor,\n                     tf: torch.Tensor,\n                     g: Callable[[torch.Tensor], torch.Tensor] = lambda x: 1 / x,\n                     ):\n    # transfer ray depth from t space back to s space (with function g)\n    return (g(t) - g(tn)) / (g(tf) - g(tn) + 1e-8)\n\n# Hierarchical sampling (section 5.2)\n\n\ndef searchsorted(a: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find indices where v should be inserted into a to maintain order.\n    This behaves like jnp.searchsorted (its second output is the same as\n    jnp.searchsorted's output if all elements of v are in [a[0], a[-1]]) but is\n    faster because it wastes memory to save some compute.\n    Args:\n      a: tensor, the sorted reference points that we are scanning to see where v\n        should lie.\n      v: tensor, the query points that we are pretending to insert into a. Does\n        not need to be sorted. All but the last dimensions should match or expand\n        to those of a, the last dimension can differ.\n    Returns:\n      (idx_lo, idx_hi), where a[idx_lo] <= v < a[idx_hi], unless v is out of the\n      range [a[0], a[-1]] in which case idx_lo and idx_hi are both the first or\n      last index of a.\n    \"\"\"\n    i = torch.arange(a.shape[-1], device=a.device)  # 128\n    v_ge_a = v[..., None, :] >= a[..., :, None]\n    idx_lo = torch.max(torch.where(v_ge_a, i[..., :, None], i[..., :1, None]), -2)[0]  # 128\n    idx_hi = torch.min(torch.where(~v_ge_a, i[..., :, None], i[..., -1:, None]), -2)[0]\n    return idx_lo, idx_hi\n\n\ndef invert_cdf(u, t, w):\n    \"\"\"Invert the CDF defined by (t, w) at the points specified by u in [0, 1).\"\"\"\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(u, cw, t)\n    return t_new\n\n\ndef importance_sampling(t: torch.Tensor,\n                        w: torch.Tensor,\n                        num_samples: int,\n                        perturb=True,\n                        single_jitter=False,\n                        ):\n    \"\"\"Piecewise-Constant PDF sampling from a step function.\n\n    Args:\n        rng: random number generator (or None for `linspace` sampling).\n        t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n        w_logits: [..., num_bins], logits corresponding to bin weights\n        num_samples: int, the number of samples.\n        single_jitter: bool, if True, jitter every sample along each ray by the same\n        amount in the inverse CDF. Otherwise, jitter each sample independently.\n        deterministic_center: bool, if False, when `rng` is None return samples that\n        linspace the entire PDF. If True, skip the front and back of the linspace\n        so that the centers of each PDF interval are returned.\n        use_gpu_resampling: bool, If True this resamples the rays based on a\n        \"gather\" instruction, which is fast on GPUs but slow on TPUs. If False,\n        this resamples the rays based on brute-force searches, which is fast on\n        TPUs, but slow on GPUs.\n\n    Returns:\n        t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n    \"\"\"\n    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dim\n\n    # preparing for size change\n    sh = *t.shape[:-1], num_samples  # B, P, I\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    # assuming sampling in s space\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # eps = torch.finfo(torch.float32).eps\n    eps = 1e-8\n\n    # Draw uniform samples.\n\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps if perturb else 0\n    d = 1 if single_jitter else num_samples\n    u = (\n        torch.linspace(0, 1 - u_max, num_samples, device=t.device, dtype=t.dtype) +\n        torch.rand(t.shape[:-1] + (d,), device=t.device, dtype=t.dtype) * max_jitter\n    )\n\n    u = invert_cdf(u, t, w)\n\n    # preparing for size change\n    u = u.reshape(sh)\n    return u\n\n\ndef weight_to_pdf(t: torch.Tensor, w: torch.Tensor, eps=torch.finfo(torch.float32).eps**2):\n    t, w = matchup_channels(t, w)\n    \"\"\"Turn a vector of weights that sums to 1 into a PDF that integrates to 1.\"\"\"\n    return w / (t[..., 1:] - t[..., :-1]).clip(eps)\n\n\ndef pdf_to_weight(t: torch.Tensor, p: torch.Tensor):\n    t, p = matchup_channels(t, p)\n    \"\"\"Turn a PDF that integrates to 1 into a vector of weights that sums to 1.\"\"\"\n    return p * (t[..., 1:] - t[..., :-1])\n\n\ndef max_dilate(t, w, dilation, domain=(-torch.inf, torch.inf)):\n    t, w = matchup_channels(t, w)\n    \"\"\"Dilate (via max-pooling) a non-negative step function.\"\"\"\n    t0 = t[..., :-1] - dilation\n    t1 = t[..., 1:] + dilation\n    t_dilate = torch.sort(torch.cat([t, t0, t1], dim=-1), dim=-1)[0]\n    t_dilate = t_dilate.clip(*domain)\n    w_dilate = torch.max(\n        torch.where(\n            (t0[..., None, :] <= t_dilate[..., None])\n            & (t1[..., None, :] > t_dilate[..., None]),\n            w[..., None, :],\n            0,\n        ),\n        dim=-1)[0][..., :-1]\n    return t_dilate, w_dilate\n\n\ndef max_dilate_weights(t: torch.Tensor,\n                       w: torch.Tensor,\n                       dilation: float,\n                       domain=(-torch.inf, torch.inf),\n                       renormalize=False,\n                       eps=torch.finfo(torch.float32).eps**2):\n    \"\"\"Dilate (via max-pooling) a set of weights.\"\"\"\n    p = weight_to_pdf(t, w)\n    t_dilate, p_dilate = max_dilate(t, p, dilation, domain=domain)\n    w_dilate = pdf_to_weight(t_dilate, p_dilate)\n    if renormalize:\n        w_dilate /= torch.sum(w_dilate, dim=-1, keepdim=True).clip(eps)\n    return t_dilate, w_dilate\n\n\n", "contexts_below": "\n\ndef query(tq, t, y, outside_value=0):\n    \"\"\"Look up the values of the step function (t, y) at locations tq.\"\"\"\n    idx_lo, idx_hi = searchsorted(t, tq)\n    yq = torch.where(idx_lo == idx_hi, outside_value,\n                     torch.take_along_dim(torch.cat([y, torch.full_like(y[..., :1], outside_value)], dim=-1), idx_lo, dim=-1))  # ?\n    return yq\n", "input_code": "def anneal_weights(t: torch.Tensor,\n                   w: torch.Tensor,\n                   train_frac: float,\n                   anneal_slope: float = 10.0,\n                   eps=torch.finfo(torch.float32).eps ** 2):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n\n    \"\"\"\n    This function anneals the weights based on the training fraction and an annealing slope using Schlick's bias function. It adjusts the weights of a tensor based on the progression of training, making the weight adjustment more dynamic as training progresses. It ensures stability in the computation by handling cases where adjacent intervals have zero distance, setting their weight to zero, and preventing NaN values by using a softmax operation on the adjusted weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor representing time or another sequential dimension, used to align with the weights tensor.\n    :param w: torch.Tensor. The weights tensor that will be adjusted based on the training fraction and anneal slope.\n    :param train_frac: float. The fraction of training completed, used to calculate the annealing effect on weights.\n    :param anneal_slope: float, optional. The slope of the annealing function, determining how sharply weights are adjusted. Defaults to 10.0.\n    :param eps: torch.float32, optional. A very small number added to prevent division by zero and log of zero in computations. Defaults to a small epsilon value squared.\n    :return: torch.Tensor. The adjusted weights tensor after applying the annealing process.\n    \"\"\"", "reference_steps": "1. Define a function `anneal_weights` that takes a tensor `t`, a tensor `w`, a `train_frac` indicating the fraction of training completed, an `anneal_slope` to control the annealing process, and a small epsilon value `eps` to prevent numerical issues.\n\n2. Ensure that the last dimension of `t` is one more than the last dimension of `w` by calling a function `matchup_channels`.\n\n3. Check if `anneal_slope` is greater than 0 to determine if annealing should be applied.\n\n4. If annealing is required, use Schlick's bias function to compute the `anneal` factor based on the `train_frac` and `anneal_slope`.\n\n5. If annealing is not required, set the `anneal` factor to 1.\n\n6. Compute the `logits_resample` by taking the logarithm of `w` after clipping its values at `eps` and multiplying by the `anneal` factor, but only for intervals in `t` where the next value is greater than the previous one. If the interval is zero, set the corresponding weight to negative infinity.\n\n7. Handle potential numerical instability by using `torch.where` to avoid taking the logarithm of zero or negative values.\n\n8. Apply a softmax function to `logits_resample` along the last dimension to obtain the annealed weights `w`.\n\n9. Return the annealed weights `w` from the function.\n\n10. The function is designed to be used in a context where the weights are being adjusted as a function of the training progress to help with convergence or to focus on certain aspects of the data as training progresses.", "reference_code": "def anneal_weights(t: torch.Tensor,\n                   w: torch.Tensor,\n                   train_frac: float,\n                   anneal_slope: float = 10.0,\n                   eps=torch.finfo(torch.float32).eps ** 2):\n    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n\n    # Optionally anneal the weights as a function of training iteration.\n    if anneal_slope > 0:\n        # Schlick's bias function, see https://arxiv.org/abs/2010.09714\n        def bias(x, s): return (s * x) / ((s - 1) * x + 1)\n        anneal = bias(train_frac, anneal_slope)\n    else:\n        anneal = 1.\n\n    # A slightly more stable way to compute weights**anneal. If the distance\n    # between adjacent intervals is zero then its weight is fixed to 0.\n    logits_resample = torch.where(\n        t[..., 1:] > t[..., :-1],\n        anneal * torch.log(w.clip(eps)), -torch.inf)  # MARK: prone to nan\n\n    # If all samples are -inf, softmax will produce a nan (all -torch.inf)\n    w = torch.softmax(logits_resample, dim=-1)\n    return w\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "type": "function", "class_name": null, "function_name": "to_cuda", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.data_utils.to_cuda\n    def to_cuda(batch, device=\"cuda\", ignore_list: bool = False) -> torch.Tensor:\n\n# Cross-file Dependency:\neasyvolcap.utils.base_utils.dotdict\n    class dotdict(dict, Dict[KT, VT]):\n        \"\"\"\n        This is the default data passing object used throughout the codebase\n        Main function: dot access for dict values & dict like merging and updates\n\n        a dictionary that supports dot notation \n        as well as dictionary access notation \n        usage: d = make_dotdict() or d = make_dotdict{'val1':'first'})\n        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n        get attributes: d.val2 or d['val2']\n        \"\"\"\n\n", "dependency_sampled": "# Cross-file Dependency:\neasyvolcap.utils.base_utils.dotdict\n    class dotdict(dict, Dict[KT, VT]):\n        \"\"\"\n        This is the default data passing object used throughout the codebase\n        Main function: dot access for dict values & dict like merging and updates\n\n        a dictionary that supports dot notation \n        as well as dictionary access notation \n        usage: d = make_dotdict() or d = make_dotdict{'val1':'first'})\n        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n        get attributes: d.val2 or d['val2']\n        \"\"\"\n\n", "contexts_above": "import os\nimport re\nimport cv2\nimport h5py\nimport torch\nimport struct\nimport asyncio\nimport subprocess\nimport numpy as np\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom typing import overload\nfrom functools import lru_cache\n\n# from imgaug import augmenters as iaa\nfrom typing import Tuple, Union, List, Dict\n\nfrom torch.nn import functional as F\nfrom torch.utils.data._utils.pin_memory import pin_memory\nfrom torch.utils.data._utils.collate import default_collate, default_convert\n\nfrom easyvolcap.utils.parallel_utils import parallel_execution\nfrom easyvolcap.utils.base_utils import dotdict\nfrom easyvolcap.utils.console_utils import *\n\nfrom enum import Enum, auto\n\n# Copied from enerf (maybe was in turn copied from dtu)\n\n\ndef read_pickle(name):\n    import pickle\n    with open(name, 'rb') as f:\n        data = pickle.load(f, encoding='latin1')\n    return data\n\n\ndef read_cam_file(filename):\n    with open(filename) as f:\n        lines = [line.rstrip() for line in f.readlines()]\n    # extrinsics: line [1,5), 4x4 matrix\n    extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ')\n    extrinsics = extrinsics.reshape((4, 4))\n    # intrinsics: line [7-10), 3x3 matrix\n    intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ')\n    intrinsics = intrinsics.reshape((3, 3))\n    # depth_min & depth_interval: line 11\n    depth_min = float(lines[11].split()[0])\n    return intrinsics, extrinsics, depth_min\n\n\ndef read_pmn_cam_file(filename):\n    with open(filename) as f:\n        lines = [line.rstrip() for line in f.readlines()]\n    # extrinsics: line [1,5), 4x4 matrix\n    extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ')\n    extrinsics = extrinsics.reshape((4, 4))\n    # intrinsics: line [7-10), 3x3 matrix\n    intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ')\n    intrinsics = intrinsics.reshape((3, 3))\n    # depth_min & depth_interval: line 11\n    depth_min = float(lines[11].split()[0])\n    depth_max = float(lines[11].split()[1])\n    return intrinsics, extrinsics, depth_min, depth_max\n\n\ndef read_pfm(filename):\n    file = open(filename, 'rb')\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n\n    header = file.readline().decode('utf-8').rstrip()\n    if header == 'PF':\n        color = True\n    elif header == 'Pf':\n        color = False\n    else:\n        raise Exception('Not a PFM file.')\n\n    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode('utf-8'))\n    if dim_match:\n        width, height = map(int, dim_match.groups())\n    else:\n        raise Exception('Malformed PFM header.')\n\n    scale = float(file.readline().rstrip())\n    if scale < 0:  # little-endian\n        endian = '<'\n        scale = -scale\n    else:\n        endian = '>'  # big-endian\n\n    data = np.fromfile(file, endian + 'f')\n    shape = (height, width, 3) if color else (height, width)\n\n    data = np.reshape(data, shape)\n    data = np.flipud(data)\n    file.close()\n    return data, scale\n\n\ndef generate_video(result_str: str,\n                   output: str,\n                   fps: int = 30,\n                   crf: int = 17,\n                   cqv: int = 19,\n                   lookahead: int = 20,\n                   hwaccel: str = 'cuda',\n                   preset: str = 'p7',\n                   tag: str = 'hvc1',\n                   vcodec: str = 'hevc_nvenc',\n                   pix_fmt: str = 'yuv420p',  # chrome friendly\n                   ):\n    cmd = [\n        'ffmpeg',\n        '-hwaccel', hwaccel,\n        '-hide_banner',\n        '-loglevel', 'error',\n        '-framerate', fps,\n        '-f', 'image2',\n        '-pattern_type', 'glob',\n        '-nostdin',  # otherwise you cannot chain commands together\n        '-y',\n        '-r', fps,\n        '-i', result_str,\n        '-c:v', vcodec,\n        '-preset', preset,\n        '-cq:v', cqv,\n        '-rc:v', 'vbr',\n        '-tag:v', tag,\n        '-crf', crf,\n        '-pix_fmt', pix_fmt,\n        '-rc-lookahead', lookahead,\n        '-vf', '\"pad=ceil(iw/2)*2:ceil(ih/2)*2\"',  # avoid yuv420p odd number bug\n        output,\n    ]\n    run(cmd)\n    return output\n\n\ndef numpy_to_video(numpy_array: np.ndarray,\n                   output_filename: str,\n                   fps: float = 30.0,\n                   crf: int = 18,\n                   cqv: int = 19,\n                   lookahead: int = 20,\n                   preset='veryslow',\n                   vcodec='libx265',\n                   ):\n    \"\"\"\n    Convert a numpy array (T, H, W, C) to a video using ffmpeg.\n\n    Parameters:\n    - numpy_array: Numpy array to be converted.\n    - output_filename: The filename of the output video.\n    - framerate: Frame rate for the video.\n    \"\"\"\n    if isinstance(numpy_array, np.ndarray):\n        T, H, W, C = numpy_array.shape\n    else:\n        T = len(numpy_array)\n        H, W, C = numpy_array[0].shape\n    assert C == 3, \"Expected 3 channels!\"\n\n    cmd = [\n        'ffmpeg',\n        '-hwaccel', 'cuda',\n        '-v', 'quiet', '-stats',\n        '-y',  # Overwrite output file if it exists\n        '-f', 'rawvideo',\n        '-vcodec', 'rawvideo',\n        '-s', f'{W}x{H}',  # Size of one frame\n        '-pix_fmt', 'rgb24',\n        '-r', fps,  # Frame rate\n        '-i', '-',  # Read from pipe\n        '-an',  # No audio\n        '-vcodec', vcodec,\n        '-preset', preset,\n        '-cq:v', cqv,\n        '-crf', crf,\n        '-rc-lookahead', lookahead,\n        '-rc:v', 'vbr',\n        '-tag:v', 'hvc1',\n        output_filename\n    ]\n    os.makedirs(dirname(output_filename), exist_ok=True)\n    process = subprocess.Popen(map(str, cmd), stdin=subprocess.PIPE)\n    # process.communicate(input=numpy_array.tobytes())\n    for frame in numpy_array:\n        process.stdin.write(frame.tobytes())\n        # process.stdin.flush()\n    process.stdin.close()\n    process.communicate()\n\n\ndef get_video_dimensions(input_filename):\n    \"\"\"\n    Extract the width and height of a video using ffprobe.\n\n    Parameters:\n    - input_filename: The filename of the input video.\n\n    Returns:\n    - width and height of the video.\n    \"\"\"\n    cmd = [\n        'ffprobe',\n        '-v', 'error',\n        '-select_streams', 'v:0',\n        '-show_entries', 'stream=width,height',\n        '-of', 'csv=s=x:p=0',\n        input_filename\n    ]\n\n    pipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, _ = pipe.communicate()\n    width, height = map(int, out.decode('utf-8').strip().split('x'))\n    return width, height\n\n\ndef video_to_numpy(input_filename, hwaccel='cuda', vcodec='hevc_cuvid'):\n    \"\"\"\n    Convert a video file to a numpy array (T, H, W, C) using ffmpeg.\n\n    Parameters:\n    - input_filename: The filename of the input video.\n\n    Returns:\n    - Numpy array representing the video.\n    \"\"\"\n    W, H = get_video_dimensions(input_filename)\n\n    cmd = [\n        'ffmpeg',\n    ]\n    if hwaccel != 'none':\n        cmd += ['-hwaccel', hwaccel,]\n    cmd += [\n        '-v', 'quiet', '-stats',\n    ]\n    if vcodec != 'none':\n        cmd += ['-vcodec', vcodec,]\n    cmd += [\n        '-i', input_filename,\n        '-f', 'image2pipe',\n        '-pix_fmt', 'rgb24',\n        '-vcodec', 'rawvideo',\n        '-'\n    ]\n\n    pipe = subprocess.Popen(map(str, cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=10**8)\n    raw_data, _ = pipe.communicate()\n\n    # Convert the raw data to numpy array and reshape\n    video_np = np.frombuffer(raw_data, dtype=np.uint8)\n    H2, W2 = (H + 1) // 2 * 2, (W + 1) // 2 * 2\n    try:\n        video_np = video_np.reshape(-1, H2, W2, 3)[:, :H, :W, :]\n    except ValueError as e:\n        video_np = video_np.reshape(-1, H, W, 3)\n\n    return video_np\n\n\nclass Visualization(Enum):\n    # Universal visualization\n    RENDER = auto()  # plain rgb render output\n    SURFACE = auto()  # surface position (similar to depth)\n    DEFORM = auto()  # deformation magnitude (as in correspondence?)\n    DEPTH = auto()  # needs a little bit extra computation\n    ALPHA = auto()  # occupancy (rendered volume density)\n    NORMAL = auto()  # needs extra computation\n    FEATURE = auto()  # embedder results\n    SEMANTIC = auto()  # semantic nerf related\n    SRCINPS = auto()  # Souce input images for image based rendering\n\n    # jacobian related\n    JACOBIAN = auto()\n\n    # Relighting related\n    ENVMAP = auto()\n    ALBEDO = auto()\n    SHADING = auto()\n    ROUGHNESS = auto()\n\n    # Geometry related output\n    MESH = auto()\n    POINT = auto()\n    VOLUME = auto()\n\n\nclass DataSplit(Enum):\n    TRAIN = auto()\n    TEST = auto()\n    VAL = auto()\n\n\ndef variance_of_laplacian(image: np.ndarray):\n    if image.ndim == 3 and image.shape[-1] > 1:\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    if image.dtype == np.float32 or image.dtype == np.float64:\n        image = (image * 255).astype(np.uint8)\n    return cv2.Laplacian(image, cv2.CV_64F).var()\n\n\ndef alpha2sdf(alpha, beta, dists=0.005):\n    return beta * np.log(2 * beta * (-np.log(1 - alpha) / dists))\n\n\ndef h5_to_dotdict(h5: h5py.File) -> dotdict:\n    d = {key: h5_to_dotdict(h5[key]) if isinstance(h5[key], h5py.Group) else h5[key][:] for key in h5.keys()}  # loaded as numpy array\n    d = dotdict(d)\n    return d\n\n\ndef h5_to_list_of_dotdict(h5: h5py.File) -> list:\n    return [h5_to_dotdict(h5[key]) for key in tqdm(h5)]\n\n\ndef to_h5py(value, h5: h5py.File, key: str = None, compression: str = 'gzip'):\n    if isinstance(value, torch.Tensor):\n        value = value.detach().cpu().numpy()\n    if isinstance(value, np.ndarray):\n        h5.create_dataset(str(key), data=value, compression=compression)\n    elif isinstance(value, list):\n        if key is not None:\n            h5 = h5.create_group(str(key))\n        [to_h5py(v, h5, k) for k, v in enumerate(value)]\n    elif isinstance(value, dict):\n        if key is not None:\n            h5 = h5.create_group(str(key))\n        [to_h5py(v, h5, k) for k, v in value.items()]\n    else:\n        raise NotImplementedError(f'unsupported type to write to h5: {type(value)}')\n\n\ndef export_h5(batch: dotdict, filename):\n    with h5py.File(filename, 'w') as f:\n        to_h5py(batch, f)\n\n\ndef load_h5(filename):\n    with h5py.File(filename, 'r') as f:\n        return h5_to_dotdict(f)\n\n\ndef merge_faces(faces, *args):\n    # Copied from trimesh, this will select one uv coordinates for a particular vertex\n    \"\"\"\n    Textured meshes can come with faces referencing vertex\n    indices (`v`) and an array the same shape which references\n    vertex texture indices (`vt`) and sometimes even normal (`vn`).\n\n    Vertex locations with different values of any of these can't\n    be considered the \"same\" vertex, and for our simple data\n    model we need to not combine these vertices.\n\n    Parameters\n    -------------\n    faces : (n, d) int\n      References vertex indices\n    *args : (n, d) int\n      Various references of corresponding values\n      This is usually UV coordinates or normal indexes\n    maintain_faces : bool\n      Do not alter original faces and return no-op masks.\n\n    Returns\n    -------------\n    new_faces : (m, d) int\n      New faces for masked vertices\n    mask_v : (p,) int\n      A mask to apply to vertices\n    mask_* : (p,) int\n      A mask to apply to vt array to get matching UV coordinates\n      Returns as many of these as args were passed\n    \"\"\"\n\n    # start with not altering faces at all\n    result = [faces]\n    # find the maximum index referenced by faces\n    max_idx = faces.max()\n    # add a vertex mask which is just ordered\n    result.append(np.arange(max_idx + 1))\n\n    # now given the order is fixed do our best on the rest of the order\n    for arg in args:\n        # create a mask of the attribute-vertex mapping\n        # note that these might conflict since we're not unmerging\n        masks = np.zeros((3, max_idx + 1), dtype=np.int64)\n        # set the mask using the unmodified face indexes\n        for i, f, a in zip(range(3), faces.T, arg.T):\n            masks[i][f] = a\n        # find the most commonly occurring attribute (i.e. UV coordinate)\n        # and use that index note that this is doing a float conversion\n        # and then median before converting back to int: could also do this as\n        # a column diff and sort but this seemed easier and is fast enough\n        result.append(np.median(masks, axis=0).astype(np.int64))\n\n    return result\n\n\ndef get_mesh(verts: torch.Tensor, faces: torch.Tensor, uv: torch.Tensor = None, img: torch.Tensor = None, colors: torch.Tensor = None, normals: torch.Tensor = None, filename: str = \"default.ply\"):\n    from trimesh import Trimesh\n    from trimesh.visual import TextureVisuals\n    from trimesh.visual.material import PBRMaterial, SimpleMaterial\n    from easyvolcap.utils.mesh_utils import face_normals, loop_subdivision\n\n    verts, faces = to_numpy([verts, faces])\n    verts = verts.reshape(-1, 3)\n    faces = faces.reshape(-1, 3)\n    # MARK: used process=False here to preserve vertex order\n    mesh = Trimesh(verts, faces, process=False)\n    if colors is None:\n        # colors = verts\n        colors = face_normals(torch.from_numpy(verts), torch.from_numpy(faces).long()) * 0.5 + 0.5\n    colors = to_numpy(colors)\n    colors = colors.reshape(-1, 3)\n    colors = (np.concatenate([colors, np.ones([*colors.shape[:-1], 1])], axis=-1) * 255).astype(np.uint8)\n    if len(verts) == len(colors):\n        mesh.visual.vertex_colors = colors\n    elif len(faces) == len(colors):\n        mesh.visual.face_colors = colors\n\n    if normals is not None:\n        normals = to_numpy(normals)\n        mesh.vertex_normals = normals\n\n    if uv is not None:\n        uv = to_numpy(uv)\n        uv = uv.reshape(-1, 2)\n        img = to_numpy(img)\n        img = img.reshape(*img.shape[-3:])\n        img = Image.fromarray(np.uint8(img * 255))\n        mat = SimpleMaterial(\n            image=img,\n            diffuse=(0.8, 0.8, 0.8),\n            ambient=(1.0, 1.0, 1.0),\n        )\n        mat.name = os.path.splitext(os.path.split(filename)[1])[0]\n        texture = TextureVisuals(uv=uv, material=mat)\n        mesh.visual = texture\n\n    return mesh\n\n\ndef get_tensor_mesh_data(verts: torch.Tensor, faces: torch.Tensor, uv: torch.Tensor = None, img: torch.Tensor = None, uvfaces: torch.Tensor = None):\n\n    # pytorch3d wants a tensor\n    verts, faces, uv, img, uvfaces = to_tensor([verts, faces, uv, img, uvfaces])\n    verts = verts.reshape(-1, 3)\n    faces = faces.reshape(-1, 3)\n    uv = uv.reshape(-1, 2)\n    img = img.reshape(img.shape[-3:])\n    uvfaces = uvfaces.reshape(-1, 3)\n\n    # textures = TexturesUV(img, uvfaces, uv)\n    # meshes = Meshes(verts, faces, textures)\n    return verts, faces, uv, img, uvfaces\n\n\ndef export_npz(batch: dotdict, filename: struct):\n    export_dotdict(batch, filename)\n\n\ndef export_dotdict(batch: dotdict, filename: struct):\n    batch = to_numpy(batch)\n    np.savez_compressed(filename, **batch)\n\n\ndef load_mesh(filename: str, device='cuda', load_uv=False, load_aux=False, backend='pytorch3d'):\n    from pytorch3d.io import load_ply, load_obj\n    if backend == 'trimesh':\n        import trimesh\n        mesh: trimesh.Trimesh = trimesh.load(filename)\n        return mesh.vertices, mesh.faces\n\n    vm, fm = None, None\n    if filename.endswith('.npz'):\n        mesh = np.load(filename)\n        v = torch.from_numpy(mesh['verts'])\n        f = torch.from_numpy(mesh['faces'])\n\n        if load_uv:\n            vm = torch.from_numpy(mesh['uvs'])\n            fm = torch.from_numpy(mesh['uvfaces'])\n    else:\n        if filename.endswith('.ply'):\n            v, f = load_ply(filename)\n        elif filename.endswith('.obj'):\n            v, faces_attr, aux = load_obj(filename)\n            f = faces_attr.verts_idx\n\n            if load_uv:\n                vm = aux.verts_uvs\n                fm = faces_attr.textures_idx\n        else:\n            raise NotImplementedError(f'Unrecognized input format for: {filename}')\n\n    v = v.to(device, non_blocking=True).contiguous()\n    f = f.to(device, non_blocking=True).contiguous()\n\n    if load_uv:\n        vm = vm.to(device, non_blocking=True).contiguous()\n        fm = fm.to(device, non_blocking=True).contiguous()\n\n    if load_uv:\n        if load_aux:\n            return v, f, vm, fm, aux\n        else:\n            return v, f, vm, fm\n    else:\n        return v, f\n\n\ndef load_pts(filename: str):\n    from pyntcloud import PyntCloud\n    cloud = PyntCloud.from_file(filename)\n    verts = cloud.xyz\n    if 'red' in cloud.points and 'green' in cloud.points and 'blue' in cloud.points:\n        r = np.asarray(cloud.points['red'])\n        g = np.asarray(cloud.points['green'])\n        b = np.asarray(cloud.points['blue'])\n        colors = (np.stack([r, g, b], axis=-1) / 255).astype(np.float32)\n    elif 'r' in cloud.points and 'g' in cloud.points and 'b' in cloud.points:\n        r = np.asarray(cloud.points['r'])\n        g = np.asarray(cloud.points['g'])\n        b = np.asarray(cloud.points['b'])\n        colors = (np.stack([r, g, b], axis=-1) / 255).astype(np.float32)\n    else:\n        colors = None\n\n    if 'nx' in cloud.points and 'ny' in cloud.points and 'nz' in cloud.points:\n        nx = np.asarray(cloud.points['nx'])\n        ny = np.asarray(cloud.points['ny'])\n        nz = np.asarray(cloud.points['nz'])\n        norms = np.stack([nx, ny, nz], axis=-1)\n    else:\n        norms = None\n\n    # if 'alpha' in cloud.points:\n    #     cloud.points['alpha'] = cloud.points['alpha'] / 255\n\n    reserved = ['x', 'y', 'z', 'red', 'green', 'blue', 'r', 'g', 'b', 'nx', 'ny', 'nz']\n    scalars = dotdict({k: np.asarray(cloud.points[k])[..., None] for k in cloud.points if k not in reserved})  # one extra dimension at the back added\n    return verts, colors, norms, scalars\n\n\ndef export_pts(pts: torch.Tensor, color: torch.Tensor = None, normal: torch.Tensor = None, scalars: dotdict = dotdict(), filename: str = \"default.ply\"):\n    from pandas import DataFrame\n    from pyntcloud import PyntCloud\n\n    data = dotdict()\n    pts = to_numpy(pts)  # always blocking?\n    pts = pts.reshape(-1, 3)\n    data.x = pts[:, 0].astype(np.float32)\n    data.y = pts[:, 1].astype(np.float32)\n    data.z = pts[:, 2].astype(np.float32)\n\n    if color is not None:\n        color = to_numpy(color)\n        color = color.reshape(-1, 3)\n        data.red = (color[:, 0] * 255).astype(np.uint8)\n        data.green = (color[:, 1] * 255).astype(np.uint8)\n        data.blue = (color[:, 2] * 255).astype(np.uint8)\n    else:\n        data.red = (pts[:, 0] * 255).astype(np.uint8)\n        data.green = (pts[:, 1] * 255).astype(np.uint8)\n        data.blue = (pts[:, 2] * 255).astype(np.uint8)\n\n    # if 'alpha' in scalars:\n    #     data.alpha = (scalars.alpha * 255).astype(np.uint8)\n\n    if normal is not None:\n        normal = to_numpy(normal)\n        normal = normal / (np.linalg.norm(normal, axis=-1, keepdims=True) + 1e-13)\n        normal = normal.reshape(-1, 3)\n        data.nx = normal[:, 0].astype(np.float32)\n        data.ny = normal[:, 1].astype(np.float32)\n        data.nz = normal[:, 2].astype(np.float32)\n\n    if scalars is not None:\n        scalars = to_numpy(scalars)\n        for k, v in scalars.items():\n            v = v.reshape(-1, 1)\n            data[k] = v[:, 0]\n\n    df = DataFrame(data)\n    cloud = PyntCloud(df)  # construct the data\n    dir = dirname(filename)\n    if dir: os.makedirs(dir, exist_ok=True)\n    return cloud.to_file(filename)\n\n\ndef export_lines(verts: torch.Tensor, lines: torch.Tensor, color: torch.Tensor = None, filename: str = 'default.ply'):\n    if color is None:\n        color = verts\n    verts, lines, color = to_numpy([verts, lines, color])  # always blocking?\n    if color.dtype == np.float32:\n        color = (color * 255).astype(np.uint8)\n    verts = verts.reshape(-1, 3)\n    lines = lines.reshape(-1, 2)\n    color = color.reshape(-1, 3)\n\n    # Write to PLY\n    with open(filename, 'wb') as f:\n        # PLY header\n        f.write(b\"ply\\n\")\n        f.write(b\"format binary_little_endian 1.0\\n\")\n        f.write(f\"element vertex {len(verts)}\\n\".encode())\n        f.write(b\"property float x\\n\")\n        f.write(b\"property float y\\n\")\n        f.write(b\"property float z\\n\")\n        f.write(b\"property uchar red\\n\")\n        f.write(b\"property uchar green\\n\")\n        f.write(b\"property uchar blue\\n\")\n        f.write(f\"element edge {len(lines)}\\n\".encode())\n        f.write(b\"property int vertex1\\n\")\n        f.write(b\"property int vertex2\\n\")\n        f.write(b\"end_header\\n\")\n\n        # Write vertices and colors\n        for v, c in zip(verts, color):\n            f.write(struct.pack('fffBBB', v[0], v[1], v[2], c[0], c[1], c[2]))\n\n        # Write lines\n        for l in lines:\n            f.write(struct.pack('ii', l[0], l[1]))\n\n\ndef export_camera(c2w: torch.Tensor, ixt: torch.Tensor = None, col: torch.Tensor = torch.tensor([50, 50, 200]), axis_size=0.10, filename: str = 'default.ply'):\n    verts = []\n    lines = []\n    rgbs = []\n\n    def add_line(p0: torch.Tensor, p1: torch.Tensor, col: torch.Tensor):\n        # Add a and b vertices\n        verts.append(p0)  # N, M, 3\n        verts.append(p1)  # N, M, 3\n        sh = p0.shape[:-1]\n\n        # Add the vertex colors\n        col = torch.broadcast_to(col, sh + (3,))\n        rgbs.append(col)\n        rgbs.append(col)\n\n        # Add the faces\n        new = p0.numel() // 3  # number of new elements\n        curr = new * (len(verts) - 2)  # assume all previous elements are of the same size\n        start = torch.arange(curr, curr + new)\n        end = torch.arange(curr + new, curr + new * 2)\n        line = torch.stack([start, end], dim=-1)  # NM, 2\n        line = line.view(sh + (2,))\n        lines.append(line)\n\n    c2w = c2w[..., :3, :]\n    p = c2w[..., 3]  # third row (corresponding to 3rd column)\n\n    if ixt is None: aspect = 1.0\n    else: aspect = ixt[..., 0, 0][..., None] / ixt[..., 1, 1][..., None]\n    if ixt is None: focal = 1000\n    else: focal = (ixt[..., 0, 0][..., None] + ixt[..., 1, 1][..., None]) / 2\n\n    axis_size = focal * axis_size / 1000\n    xs = axis_size * aspect\n    ys = axis_size\n    zs = axis_size * aspect * 2\n\n    a = p + xs * c2w[..., 0] + ys * c2w[..., 1] + zs * c2w[..., 2]\n    b = p - xs * c2w[..., 0] + ys * c2w[..., 1] + zs * c2w[..., 2]\n    c = p - xs * c2w[..., 0] - ys * c2w[..., 1] + zs * c2w[..., 2]\n    d = p + xs * c2w[..., 0] - ys * c2w[..., 1] + zs * c2w[..., 2]\n\n    add_line(p, p + axis_size * c2w[..., 0], torch.tensor([255, 64, 64]))\n    add_line(p, p + axis_size * c2w[..., 1], torch.tensor([64, 255, 64]))\n    add_line(p, p + axis_size * c2w[..., 2], torch.tensor([64, 64, 255]))\n    add_line(p, a, col)\n    add_line(p, b, col)\n    add_line(p, c, col)\n    add_line(p, d, col)\n    add_line(a, b, col)\n    add_line(b, c, col)\n    add_line(c, d, col)\n    add_line(d, a, col)\n\n    verts = torch.stack(verts)\n    lines = torch.stack(lines)\n    rgbs = torch.stack(rgbs)\n\n    export_lines(verts, lines, rgbs, filename=filename)\n\n\ndef export_mesh(verts: torch.Tensor, faces: torch.Tensor, uv: torch.Tensor = None, img: torch.Tensor = None, uvfaces: torch.Tensor = None, colors: torch.Tensor = None, normals: torch.Tensor = None, filename: str = \"default.ply\", subdivision=0):\n    if dirname(filename): os.makedirs(dirname(filename), exist_ok=True)\n\n    if subdivision > 0:\n        from easyvolcap.utils.mesh_utils import face_normals, loop_subdivision\n        verts, faces = loop_subdivision(verts, faces, subdivision)\n\n    if filename.endswith('.npz'):\n        def collect_args(**kwargs): return kwargs\n        kwargs = collect_args(verts=verts, faces=faces, uv=uv, img=img, uvfaces=uvfaces, colors=colors, normals=normals)\n        ret = dotdict({k: v for k, v in kwargs.items() if v is not None})\n        export_dotdict(ret, filename)\n\n    elif filename.endswith('.ply') or filename.endswith('.obj'):\n        if uvfaces is None:\n            mesh = get_mesh(verts, faces, uv, img, colors, normals, filename)\n            mesh.export(filename)\n        else:\n            from pytorch3d.io import save_obj\n            verts, faces, uv, img, uvfaces = get_tensor_mesh_data(verts, faces, uv, img, uvfaces)\n            save_obj(filename, verts, faces, verts_uvs=uv, faces_uvs=uvfaces, texture_map=img)\n    else:\n        raise NotImplementedError(f'Unrecognized input format for: {filename}')\n\n\ndef export_pynt_pts_alone(pts, color=None, filename=\"default.ply\"):\n    import pandas as pd\n    from pyntcloud import PyntCloud\n    data = {}\n\n    pts = pts if isinstance(pts, np.ndarray) else pts.detach().cpu().numpy()\n    pts = pts.reshape(-1, 3)\n    data['x'] = pts[:, 0].astype(np.float32)\n    data['y'] = pts[:, 1].astype(np.float32)\n    data['z'] = pts[:, 2].astype(np.float32)\n\n    if color is not None:\n        color = color if isinstance(color, np.ndarray) else color.detach().cpu().numpy()\n        color = color.reshape(-1, 3)\n        data['red'] = color[:, 0].astype(np.uint8)\n        data['green'] = color[:, 1].astype(np.uint8)\n        data['blue'] = color[:, 2].astype(np.uint8)\n    else:\n        data['red'] = (pts[:, 0] * 255).astype(np.uint8)\n        data['green'] = (pts[:, 1] * 255).astype(np.uint8)\n        data['blue'] = (pts[:, 2] * 255).astype(np.uint8)\n\n    df = pd.DataFrame(data)\n    cloud = PyntCloud(df)  # construct the data\n    dirname = dirname(filename)\n    if dirname: os.makedirs(dirname, exist_ok=True)\n    return cloud.to_file(filename)\n\n\ndef export_o3d_pts(pts: torch.Tensor, filename: str = \"default.ply\"):\n    import open3d as o3d\n    pts = to_numpy(pts)\n    pts = pts.reshape(-1, 3)\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(pts)\n    return o3d.io.write_point_cloud(filename, pcd)\n\n\ndef export_o3d_pcd(pts: torch.Tensor, rgb: torch.Tensor, normal: torch.Tensor, filename=\"default.ply\"):\n    import open3d as o3d\n    pts, rgb, normal = to_numpy([pts, rgb, normal])\n    pts = pts.reshape(-1, 3)\n    rgb = rgb.reshape(-1, 3)\n    normal = normal.reshape(-1, 3)\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(pts)\n    pcd.colors = o3d.utility.Vector3dVector(rgb)\n    pcd.normals = o3d.utility.Vector3dVector(normal)\n    return o3d.io.write_point_cloud(filename, pcd)\n\n\ndef export_pcd(pts: torch.Tensor, rgb: torch.Tensor, occ: torch.Tensor, filename=\"default.ply\"):\n    import pandas as pd\n    from pyntcloud import PyntCloud\n    pts, rgb, occ = to_numpy([pts, rgb, occ])\n    pts = pts.reshape(-1, 3)\n    rgb = rgb.reshape(-1, 3)\n    occ = occ.reshape(-1, 1)\n    # MARK: CloudCompare bad, set first to 0, last to 1\n    for i in range(3):\n        rgb[0, i] = 0\n        rgb[-1, i] = 1\n    occ[0, 0] = 0\n    occ[-1, 0] = 1\n\n    data = dotdict()\n    data.x = pts[:, 0]\n    data.y = pts[:, 1]\n    data.z = pts[:, 2]\n    # TODO: maybe, for compability, save color as uint?\n    # currently saving as float number from [0, 1]\n    data.red = rgb[:, 0]\n    data.green = rgb[:, 1]\n    data.blue = rgb[:, 2]\n    data.alpha = occ[:, 0]\n\n    # MARK: We're saving extra scalars for loading in CloudCompare\n    # can't assign same property to multiple fields\n    data.r = rgb[:, 0]\n    data.g = rgb[:, 1]\n    data.b = rgb[:, 2]\n    data.a = occ[:, 0]\n    df = pd.DataFrame(data)\n\n    cloud = PyntCloud(df)  # construct the data\n    dirname = dirname(filename)\n    if dirname: os.makedirs(dirname, exist_ok=True)\n    return cloud.to_file(filename)\n\n\ndef load_rgb_image(img_path) -> np.ndarray:\n    # return cv2.imread(img_path, cv2.IMREAD_COLOR)[..., ::-1].copy()  # removing the stride (for conversion to tensor)\n    return cv2.imread(img_path, cv2.IMREAD_COLOR)[..., [2, 1, 0]]  # BGR to RGB\n\n\ndef load_unchanged_image(img_path) -> np.ndarray:\n    return cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n\n\ndef load_npz(index, folder):\n    path = os.path.join(folder, f\"{index}.npz\")\n    data = np.load(path)\n    return dotdict({**data})\n\n\ndef load_dotdict(path):\n    f = np.load(path)\n    f = dotdict({**f})\n    return f\n\n\ndef start_save_npz(index, dir, param: dict, remove_batch=True):\n    return asyncio.create_task(async_save_npz(index, dir, param, remove_batch))\n\n\nasync def async_save_npz(index, dir, param: dict, remove_batch=True):\n    log(f\"Trying to save: {index}\")\n    save_npz(index, dir, param, remove_batch)\n\n\ndef save_img(index, dir, img: torch.Tensor, remove_batch=True, remap=False, flip=False):\n\n    img = to_numpy(img)\n\n    if remap:\n        img *= 255\n        img = img.astype(np.uint8)\n    if flip:\n        img = img[..., ::-1]\n\n    if remove_batch:\n        n_batch = img.shape[0]\n        for b in range(n_batch):\n            file_path = os.path.join(dir, f\"{index*n_batch + b}.png\")\n            im = img[b]\n            cv2.imwrite(file_path, im)\n    else:\n        file_path = os.path.join(dir, f\"{index}.png\")\n        cv2.imwrite(file_path, img)\n\n\ndef save_npz(index, dir, param: dict, remove_batch=False):\n    param = to_numpy(param)\n    if remove_batch:\n        n_batch = param[next(iter(param))].shape[0]\n        for b in range(n_batch):\n            file_path = os.path.join(dir, f\"{index*n_batch + b}.npz\")\n            p = {k: v[b] for k, v in param.items()}\n            np.savez_compressed(file_path, **p)\n    else:\n        file_path = os.path.join(dir, f\"{index}.npz\")\n        np.savez_compressed(file_path, **param)\n\n\n", "contexts_below": "\n\ndef to_x_if(batch, x: str, cond):\n    if isinstance(batch, (tuple, list)):\n        batch = [to_x(b, x) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_x(v, x) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        if cond(x):\n            batch = batch.to(x, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        if cond(x):\n            batch = torch.as_tensor(batch).to(x, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n\ndef to_x(batch, x: str) -> Union[torch.Tensor, dotdict[str, torch.Tensor]]:\n    if isinstance(batch, (tuple, list)):\n        batch = [to_x(b, x) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_x(v, x) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(x, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(x, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n\ndef to_tensor(batch, ignore_list: bool = False) -> Union[torch.Tensor, dotdict[str, torch.Tensor]]:\n    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_tensor(b, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_tensor(v, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        pass\n    else:  # numpy and others\n        batch = torch.as_tensor(batch)\n    return batch\n\n\ndef to_list(batch, non_blocking=False) -> Union[List, Dict, np.ndarray]:  # almost always exporting, should block\n    if isinstance(batch, (tuple, list)):\n        batch = [to_list(b, non_blocking) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_list(v, non_blocking) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.detach().to('cpu', non_blocking=non_blocking).numpy().tolist()\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.tolist()\n    else:  # others, keep as is\n        pass\n    return batch\n\n\ndef to_cpu(batch, non_blocking=False, ignore_list: bool = False) -> torch.Tensor:\n    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cpu(b, non_blocking, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cpu(v, non_blocking, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.detach().to('cpu', non_blocking=non_blocking)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch, device=\"cpu\")\n    return batch\n\n\ndef to_numpy(batch, non_blocking=False, ignore_list: bool = False) -> Union[List, Dict, np.ndarray]:  # almost always exporting, should block\n    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_numpy(b, non_blocking, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_numpy(v, non_blocking, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.detach().to('cpu', non_blocking=non_blocking).numpy()\n    else:  # numpy and others\n        batch = np.asarray(batch)\n    return batch\n\n\ndef remove_batch(batch) -> Union[torch.Tensor, np.ndarray]:\n    if isinstance(batch, (tuple, list)):\n        batch = [remove_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: remove_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[0]\n    else:\n        batch = torch.as_tensor(batch)[0]\n    return batch\n\n\ndef add_batch(batch) -> Union[torch.Tensor, np.ndarray]:\n    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n\ndef add_iter(batch, iter, total) -> Union[torch.Tensor, np.ndarray]:\n    batch = add_scalar(batch, iter, name=\"iter\")\n    batch = add_scalar(batch, iter / total, name=\"frac\")\n    return batch  # training fraction and current iteration\n\n\ndef add_scalar(batch, value, name) -> Union[torch.Tensor, np.ndarray]:\n    if isinstance(batch, (tuple, list)):\n        for b in batch:\n            add_scalar(b, value, name)\n\n    if isinstance(batch, dict):\n        batch[name] = torch.tensor(value)\n        batch['meta'][name] = torch.tensor(value)\n    return batch\n\n\ndef get_voxel_grid_and_update_bounds(voxel_size: Union[List, np.ndarray], bounds: Union[List, np.ndarray]):\n    # now here's the problem\n    # 1. if you want the voxel size to be accurate, you bounds need to be changed along with this sampling process\n    #    since the F.grid_sample will treat the bounds based on align_corners=True or not\n    #    say we align corners, the actual bound on the sampled tpose blend weight should be determined by the actual sampling voxels\n    #    not the bound that we kind of used to produce the voxels, THEY DO NOT LINE UP UNLESS your bounds is divisible by the voxel size in every direction\n    # TODO: is it possible to somehow get rid of this book-keeping step\n    if isinstance(voxel_size, List):\n        voxel_size = np.array(voxel_size)\n        bounds = np.array(bounds)\n    # voxel_size: [0.005, 0.005, 0.005]\n    # bounds: n_batch, 2, 3, initial bounds\n    x = np.arange(bounds[0, 0], bounds[1, 0] + voxel_size[0] / 2, voxel_size[0])\n    y = np.arange(bounds[0, 1], bounds[1, 1] + voxel_size[1] / 2, voxel_size[1])\n    z = np.arange(bounds[0, 2], bounds[1, 2] + voxel_size[2] / 2, voxel_size[2])\n    pts = np.stack(np.meshgrid(x, y, z, indexing='ij'), axis=-1).astype(np.float32)\n    bounds = np.stack([pts[0, 0, 0], pts[-1, -1, -1]], axis=0).astype(np.float32)\n    return pts, bounds\n\n\ndef get_rigid_transform(pose: np.ndarray, joints: np.ndarray, parents: np.ndarray):\n    # pose: N, 3\n    # joints: N, 3\n    # parents: N\n    from easyvolcap.utils.blend_utils import get_rigid_transform_nobatch as net_get_rigid_transform\n    pose, joints, parents = default_convert([pose, joints, parents])\n    J, A = net_get_rigid_transform(pose, joints, parents)\n    J, A = to_numpy([J, A])\n\n    return J, A\n\n\ndef get_bounds(xyz, padding=0.05):\n    min_xyz = np.min(xyz, axis=0)\n    max_xyz = np.max(xyz, axis=0)\n    min_xyz -= padding\n    max_xyz += padding\n    bounds = np.stack([min_xyz, max_xyz], axis=0)\n    return bounds\n\n\ndef load_image_file(img_path: str, ratio=1.0):\n    if img_path.endswith('.jpg') or img_path.endswith('.JPG') or img_path.endswith('.jpeg') or img_path.endswith('.JPEG'):\n        im = Image.open(img_path)\n        w, h = im.width, im.height\n        draft = im.draft('RGB', (int(w * ratio), int(h * ratio)))\n        img = np.asarray(im)\n        if np.issubdtype(img.dtype, np.integer):\n            img = img.astype(np.float32) / np.iinfo(img.dtype).max  # normalize\n        if ratio != 1.0 and \\\n            draft is None or \\\n                draft is not None and \\\n            (draft[1][2] != int(w * ratio) or\n         draft[1][3] != int(h * ratio)):\n            img = cv2.resize(img, (int(w * ratio), int(h * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n    else:\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        if img.ndim >= 3 and img.shape[-1] >= 3:\n            img[..., :3] = img[..., [2, 1, 0]]  # BGR to RGB\n        if np.issubdtype(img.dtype, np.integer):\n            img = img.astype(np.float32) / np.iinfo(img.dtype).max  # normalize\n        if ratio != 1.0:\n            height, width = img.shape[:2]\n            img = cv2.resize(img, (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n\n\ndef load_depth(depth_file: str):\n    if depth_file.endswith('.npy'):\n        depth = np.load(depth_file)[..., None]  # H, W, 1\n    elif depth_file.endswith('.pfm'):\n        depth, scale = read_pfm(depth_file)\n        depth = depth / scale\n        if depth.ndim == 2:\n            depth = depth[..., None]  # H, W, 1\n        depth = depth[..., :1]\n    elif depth_file.endswith('.hdr') or depth_file.endswith('.exr'):\n        if depth_file.endswith('.exr'):\n            # ... https://github.com/opencv/opencv/issues/21326\n            os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n        depth = load_image(depth_file)\n        depth = depth[..., :1]\n    else:\n        raise NotImplementedError\n    return depth  # H, W, 1\n\n\ndef load_image(path: Union[str, np.ndarray], ratio: int = 1.0):\n    if isinstance(path, str):\n        return load_image_file(path, ratio)\n    elif isinstance(path, np.ndarray):\n        return load_image_from_bytes(path, ratio)\n    else:\n        raise NotImplementedError('Supported overloading')\n\n\ndef load_unchanged(img_path: str, ratio=1.0):\n    if img_path.endswith('.jpg') or img_path.endswith('.JPG') or img_path.endswith('.jpeg') or img_path.endswith('.JPEG'):\n        im = Image.open(img_path)\n        w, h = im.width, im.height\n        draft = im.draft('RGB', (int(w * ratio), int(h * ratio)))\n        img = np.asarray(im).copy()  # avoid writing error and already in RGB instead of BGR\n        if ratio != 1.0 and \\\n            draft is None or \\\n                draft is not None and \\\n            (draft[1][2] != int(w * ratio) or \\\n         draft[1][3] != int(h * ratio)):\n            img = cv2.resize(img, (int(w * ratio), int(h * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n    else:\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        if img.shape[-1] >= 3:\n            img[..., :3] = img[..., [2, 1, 0]]\n        if ratio != 1.0:\n            height, width = img.shape[:2]\n            img = cv2.resize(img, (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n\n\ndef load_mask(msk_path: str, ratio=1.0):\n    \"\"\"\n    Load single-channel binary mask\n    \"\"\"\n    if msk_path.endswith('.jpg') or msk_path.endswith('.JPG') or msk_path.endswith('.jpeg') or msk_path.endswith('.JPEG'):\n        msk = Image.open(msk_path)\n        w, h = msk.width, msk.height\n        draft = msk.draft('L', (int(w * ratio), int(h * ratio)))\n        msk = np.asarray(msk).astype(int)  # read the actual file content from drafted disk\n        msk = msk * 255 / msk.max()  # if max already 255, do nothing\n        msk = msk[..., None] > 128  # make it binary\n        msk = msk.astype(np.uint8)\n        if ratio != 1.0 and \\\n            draft is None or \\\n                draft is not None and \\\n            (draft[1][2] != int(w * ratio) or\n         draft[1][3] != int(h * ratio)):\n            msk = cv2.resize(msk.astype(np.uint8), (int(w * ratio), int(h * ratio)), interpolation=cv2.INTER_NEAREST)[..., None]\n        return msk\n    else:\n        msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE).astype(int)  # BGR to GRAY\n        msk = msk * 255 / msk.max()  # if max already 255, do nothing\n        msk = msk[..., None] > 128  # make it binary\n        msk = msk.astype(np.uint8)\n        if ratio != 1.0:\n            height, width = msk.shape[:2]\n            msk = cv2.resize(msk.astype(np.uint8), (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_NEAREST)[..., None]\n            # WTF: https://stackoverflow.com/questions/68502581/image-channel-missing-after-resizing-image-with-opencv\n        return msk\n\n\ndef save_unchanged(img_path: str, img: np.ndarray, quality=100, compression=6):\n    if img.shape[-1] >= 3:\n        img[..., :3] = img[..., [2, 1, 0]]\n    if img_path.endswith('.hdr'):\n        return cv2.imwrite(img_path, img)  # nothing to say about hdr\n    if dirname(img_path):\n        os.makedirs(dirname(img_path), exist_ok=True)\n    return cv2.imwrite(img_path, img, [cv2.IMWRITE_JPEG_QUALITY, quality, cv2.IMWRITE_PNG_COMPRESSION, compression])\n\n\ndef save_image(img_path: str, img: np.ndarray, jpeg_quality=75, png_compression=9, save_dtype=np.uint8):\n    if isinstance(img, torch.Tensor): img = img.detach().cpu().numpy()  # convert to numpy arrays\n    if img.ndim == 4: img = np.concatenate(img, axis=0)  # merge into one image along y axis\n    if img.ndim == 2: img = img[..., None]  # append last dim\n    if img.shape[0] < img.shape[-1] and (img.shape[0] == 3 or img.shape[0] == 4): img = np.transpose(img, (1, 2, 0))\n    if np.issubdtype(img.dtype, np.integer):\n        img = img / np.iinfo(img.dtype).max  # to float\n    if img.shape[-1] >= 3:\n        if not img.flags['WRITEABLE']:\n            img = img.copy()  # avoid assignment only inputs\n        img[..., :3] = img[..., [2, 1, 0]]\n    if dirname(img_path):\n        os.makedirs(dirname(img_path), exist_ok=True)\n    if img_path.endswith('.png'):\n        max = np.iinfo(save_dtype).max\n        img = (img * max).clip(0, max).astype(save_dtype)\n    elif img_path.endswith('.jpg'):\n        img = img[..., :3]  # only color\n        img = (img * 255).clip(0, 255).astype(np.uint8)\n    elif img_path.endswith('.hdr'):\n        img = img[..., :3]  # only color\n    elif img_path.endswith('.exr'):\n        # ... https://github.com/opencv/opencv/issues/21326\n        os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n    else:\n        # should we try to discard alpha channel here?\n        # exr could store alpha channel\n        pass  # no transformation for other unspecified file formats\n    # log(f'Writing image to: {img_path}')\n    # breakpoint()\n    return cv2.imwrite(img_path, img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality,\n                                       cv2.IMWRITE_PNG_COMPRESSION, png_compression,\n                                       cv2.IMWRITE_EXR_COMPRESSION, cv2.IMWRITE_EXR_COMPRESSION_PIZ])\n\n\ndef save_mask(msk_path: str, msk: np.ndarray, quality=75, compression=9):\n    if dirname(msk_path):\n        os.makedirs(dirname(msk_path), exist_ok=True)\n    if msk.ndim == 2:\n        msk = msk[..., None]\n    return cv2.imwrite(msk_path, msk[..., 0] * 255, [cv2.IMWRITE_JPEG_QUALITY, quality,\n                                                     cv2.IMWRITE_PNG_COMPRESSION, compression,\n                                                     cv2.IMWRITE_EXR_COMPRESSION, cv2.IMWRITE_EXR_COMPRESSION_PIZ])\n\n\ndef list_to_numpy(x: list): return np.stack(x).transpose(0, 3, 1, 2)\n\n\ndef numpy_to_list(x: np.ndarray): return [y for y in x.transpose(0, 2, 3, 1)]\n\n\ndef list_to_tensor(x: list, device='cuda'): return torch.from_numpy(list_to_numpy(x)).to(device, non_blocking=True)  # convert list of numpy arrays of HWC to BCHW\n\n\ndef tensor_to_list(x: torch.Tensor): return numpy_to_list(x.detach().cpu().numpy())  # convert tensor of BCHW to list of numpy arrays of HWC\n\n\ndef project(xyz, K, RT):\n    \"\"\"\n    xyz: [N, 3]\n    K: [3, 3]\n    RT: [3, 4]\n    \"\"\"\n    xyz = np.dot(xyz, RT[:, :3].T) + RT[:, 3:].T\n    xyz = np.dot(xyz, K.T)\n    xy = xyz[:, :2] / xyz[:, 2:]\n    return xy\n\n\ndef unproject(depth, K, R, T):\n    H, W = depth.shape\n    i, j = np.meshgrid(np.arange(W, dtype=np.float32),\n                       np.arange(H, dtype=np.float32),\n                       indexing='xy')\n    xy1 = np.stack([i, j, np.ones_like(i)], axis=2)\n    xyz = xy1 * depth[..., None]\n    pts3d = np.dot(xyz, np.linalg.inv(K).T)\n    pts3d = np.dot(pts3d - T.ravel(), R)\n    return pts3d\n\n\ndef read_mask_by_img_path(data_root: str, img_path: str, erode_dilate_edge: bool = False, mask: str = '') -> np.ndarray:\n    def read_mask_file(path):\n        msk = load_mask(path).astype(np.uint8)\n        if len(msk.shape) == 3:\n            msk = msk[..., 0]\n        return msk\n\n    if mask:\n        msk_path = os.path.join(data_root, img_path.replace('images', mask))\n        if not os.path.exists(msk_path):\n            msk_path = os.path.join(data_root, img_path.replace('images', mask)) + '.png'\n        if not os.path.exists(msk_path):\n            msk_path = os.path.join(data_root, img_path.replace('images', mask))[:-4] + '.png'\n        if not os.path.exists(msk_path):\n            log(f'warning: defined mask path {msk_path} does not exist', 'yellow')\n    else:\n        msk_path = os.path.join(data_root, 'mask', img_path)[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, 'mask', img_path)[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, 'mask_cihp', img_path)[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'merged_mask'))[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'rvm'))[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'rvm'))[:-4] + '.jpg'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'mask'))[:-4] + '.png'\n    if not os.path.exists(msk_path):  # background matte v2\n        msk_path = os.path.join(data_root, img_path.replace('images', 'bgmt'))[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'mask'))[:-4] + '.jpg'\n    if not os.path.exists(msk_path):\n        log(f'cannot find mask file: {msk_path}, using all ones', 'yellow')\n        img = load_unchanged_image(os.path.join(data_root, img_path))\n        msk = np.ones_like(img[:, :, 0]).astype(np.uint8)\n        return msk\n\n    msk = read_mask_file(msk_path)\n    # erode edge inconsistence when evaluating and training\n    if erode_dilate_edge:  # eroding edge on matte might erode the actual human\n        msk = fill_mask_edge_with(msk)\n\n    return msk\n\n\ndef fill_mask_edge_with(msk, border=5, value=100):\n    msk = msk.copy()\n    kernel = np.ones((border, border), np.uint8)\n    msk_erode = cv2.erode(msk.copy(), kernel)\n    msk_dilate = cv2.dilate(msk.copy(), kernel)\n    msk[(msk_dilate - msk_erode) == 1] = value\n    return msk\n\n\ndef get_rays_within_bounds_rendering(H, W, K, R, T, bounds):\n    ray_o, ray_d = get_rays(H, W, K, R, T)\n\n    ray_o = ray_o.reshape(-1, 3).astype(np.float32)\n    ray_d = ray_d.reshape(-1, 3).astype(np.float32)\n    near, far, mask_at_box = get_full_near_far(bounds, ray_o, ray_d)\n    near = near.reshape(H, W)\n    far = far.reshape(H, W)\n    ray_o = ray_o.reshape(H, W, 3)\n    ray_d = ray_d.reshape(H, W, 3)\n    mask_at_box = mask_at_box.reshape(H, W)\n\n    return ray_o, ray_d, near, far, mask_at_box\n\n\ndef get_rays(H, W, K, R, T):\n    # # calculate the camera origin\n    # ray_o = -np.dot(R.T, T).ravel()\n    # # calculate the world coodinates of pixels\n    # i, j = np.meshgrid(np.arange(H, dtype=np.float32),\n    #                    np.arange(W, dtype=np.float32),\n    #                    indexing='ij')  # 0.5 indicates pixel center\n    # i = i + 0.5\n    # j = j + 0.5\n    # # 0->H, 0->W\n    # xy1 = np.stack([j, i, np.ones_like(i)], axis=2)\n    # if subpixel:\n    #     rand = np.random.rand(H, W, 2) - 0.5\n    #     xy1[:, :, :2] += rand\n    # pixel_camera = np.dot(xy1, np.linalg.inv(K).T)\n    # pixel_world = np.dot(pixel_camera - T.ravel(), R)\n    # # calculate the ray direction\n    # ray_d = pixel_world - ray_o[None, None]\n    # ray_d = ray_d / np.linalg.norm(ray_d, axis=2, keepdims=True)\n    # ray_o = np.broadcast_to(ray_o, ray_d.shape)\n    # return ray_o, ray_d\n\n    from easyvolcap.utils.ray_utils import get_rays\n    K, R, T = to_tensor([K, R, T])\n    ray_o, ray_d = get_rays(H, W, K, R, T)\n    ray_o, ray_d = to_numpy([ray_o, ray_d])\n    return ray_o, ray_d\n\n\ndef get_near_far(bounds, ray_o, ray_d) -> Tuple[np.ndarray, np.ndarray]:\n    # \"\"\"\n    # calculate intersections with 3d bounding box\n    # return: near, far (indexed by mask_at_box (bounding box mask))\n    # \"\"\"\n    # near, far, mask_at_box = get_full_near_far(bounds, ray_o, ray_d)\n    # norm_d = np.linalg.norm(ray_d, axis=-1, keepdims=True)\n    # near = near[mask_at_box] / norm_d[mask_at_box, 0]\n    # far = far[mask_at_box] / norm_d[mask_at_box, 0]\n    # return near, far, mask_at_box\n    from easyvolcap.utils.ray_utils import get_near_far_aabb\n    bounds, ray_o, ray_d = to_tensor([bounds, ray_o, ray_d])  # no copy\n    near, far = get_near_far_aabb(bounds, ray_o, ray_d)\n    near, far = to_numpy([near, far])\n    return near, far\n\n\ndef get_full_near_far(bounds, ray_o, ray_d):\n    \"\"\"calculate intersections with 3d bounding box\"\"\"\n    norm_d = np.linalg.norm(ray_d, axis=-1, keepdims=True)\n    viewdir = ray_d / norm_d\n    viewdir[(viewdir < 1e-5) & (viewdir > -1e-10)] = 1e-5\n    viewdir[(viewdir > -1e-5) & (viewdir < 1e-10)] = -1e-5\n    tmin = (bounds[:1] - ray_o[:1]) / viewdir\n    tmax = (bounds[1:2] - ray_o[:1]) / viewdir\n    t1 = np.minimum(tmin, tmax)\n    t2 = np.maximum(tmin, tmax)\n    near = np.max(t1, axis=-1)\n    far = np.min(t2, axis=-1)\n    mask_at_box = near < far\n    near = near / norm_d[..., 0]\n    far = far / norm_d[..., 0]\n    return near, far, mask_at_box\n\n\ndef full_sample_ray(img, msk, K, R, T, bounds, split='train', subpixel=False):\n    H, W = img.shape[:2]\n    ray_o, ray_d = get_rays(H, W, K, R, T, subpixel)\n    near, far, mask_at_box = get_full_near_far(bounds, ray_o, ray_d)\n    msk = msk * mask_at_box\n    coords = np.argwhere(np.ones_like(mask_at_box))  # every pixel\n    ray_o = ray_o[coords[:, 0], coords[:, 1]].astype(np.float32)\n    ray_d = ray_d[coords[:, 0], coords[:, 1]].astype(np.float32)\n    near = near[coords[:, 0], coords[:, 1]].astype(np.float32)\n    far = far[coords[:, 0], coords[:, 1]].astype(np.float32)\n    rgb = img[coords[:, 0], coords[:, 1]].astype(np.float32)\n    return rgb, ray_o, ray_d, near, far, coords, mask_at_box\n\n\ndef affine_inverse(m: np.ndarray):\n    import torch\n    from easyvolcap.utils.math_utils import affine_inverse\n    return affine_inverse(torch.from_numpy(m)).numpy()\n\n\ndef load_image_from_bytes(buffer: np.ndarray, ratio=1.0, normalize=False, decode_flag=cv2.IMREAD_UNCHANGED):\n    # from nvjpeg import NvJpeg\n    # if not hasattr(load_image_from_bytes, 'nj'):\n    #     load_image_from_bytes.nj = NvJpeg()\n    # nj: NvJpeg = load_image_from_bytes.nj\n\n    def normalize_image(image):\n        image = torch.from_numpy(image)  # pytorch is significantly faster than np\n        if image.ndim >= 3 and image.shape[-1] >= 3:\n            image[..., :3] = image[..., [2, 1, 0]]\n        image = image / torch.iinfo(image.dtype).max\n        image = image.float()\n        return image.numpy()\n\n    if isinstance(buffer, BytesIO):\n        buffer = buffer.getvalue()  # slow? copy?\n    if isinstance(buffer, memoryview) or isinstance(buffer, bytes):\n        buffer = np.frombuffer(buffer, np.uint8)\n    if isinstance(buffer, torch.Tensor):\n        buffer = buffer.numpy()\n    buffer = buffer.astype(np.uint8)\n    image: np.ndarray = cv2.imdecode(buffer, decode_flag)  # MARK: 10-15ms\n    # image: np.ndarray = nj.decode(np.frombuffer(buffer, np.uint8))  # MARK: 10-15ms\n    # if decode_flag == cv2.IMREAD_GRAYSCALE:\n    # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    if image.ndim == 2:\n        image = image[..., None]\n\n    if normalize:\n        image = normalize_image(image)  # MARK: 3ms\n\n    height, width = image.shape[:2]\n    if ratio != 1.0:\n        image = cv2.resize(image, (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_AREA)\n    return image\n\n\ndef as_torch_func(func):\n    def wrapper(*args, **kwargs):\n        args = to_numpy(args)\n        kwargs = to_numpy(kwargs)\n        ret = func(*args, **kwargs)\n        return to_tensor(ret)\n    return wrapper\n\n\ndef as_numpy_func(func):\n    def wrapper(*args, **kwargs):\n        args = to_tensor(args)\n        kwargs = to_tensor(kwargs)\n        ret = func(*args, **kwargs)\n        return to_numpy(ret)\n    return wrapper\n\n\ndef load_image_bytes(im: str):\n    if im.endswith('.exr'):\n        os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n    with open(im, \"rb\") as fh:\n        buffer = fh.read()\n    return buffer\n\n\nclass UnstructuredTensors(torch.Tensor):\n    # https://github.com/pytorch/pytorch/issues/13246#issuecomment-617140519\n    # https://github.com/pytorch/pytorch/issues/69893\n    @staticmethod\n    def __new__(cls, bytes: Union[List[np.ndarray], List[torch.Tensor], np.ndarray], **kwargs):\n        \"\"\"\n        Creates a new UnstructuredTensors object from the given bytes.\n\n        Args:\n        - bytes (Union[List[np.ndarray], List[torch.Tensor], np.ndarray]): The bytes to create the object from.\n\n        Returns:\n        - self (UnstructuredTensors): The new UnstructuredTensors object.\n        \"\"\"\n        if isinstance(bytes, UnstructuredTensors):\n            return bytes\n        # Prepare the bytes array\n        if isinstance(bytes, np.ndarray):\n            bytes = [b for b in bytes]\n        if bytes[0].dtype == object:\n            bytes = [b.astype(np.uint8) for b in bytes]\n        bytes = to_tensor(bytes)  # now, every element is a list\n        dtype = torch.uint8\n        if len(bytes):\n            dtype = bytes[0].dtype\n\n        # Create an empty tensor\n        self = torch.Tensor.__new__(cls).to(dtype)\n\n        # Remember accessing related configs\n        self.set_(torch.cat(bytes))  # flatten # sum(N)\n        self.lengths = torch.as_tensor([len(b) for b in bytes], dtype=torch.int32)  # N,\n        self.cumsums = torch.cat([torch.as_tensor([0]), torch.cumsum(self.lengths, dim=0)[:-1]])\n        return self\n\n    @property\n    def is_unstructured(self): return hasattr(self, 'lengths')\n\n    def __getitem__(self, index: int):\n        \"\"\"\n        Returns a slice of the UnstructuredTensors object corresponding to the given index.\n\n        Args:\n            index (int): The index of the slice to return.\n\n        Returns:\n            torch.Tensor: A slice of the UnstructuredTensors object corresponding to the given index.\n\n        This function returns a slice of the UnstructuredTensors object corresponding to the given index. The slice is obtained by using the cumulative sums and lengths of the underlying bytes array to determine the start and end indices of the slice. If the index is out of range, the function returns the corresponding element of the underlying bytes array. This function is used to implement the indexing behavior of the UnstructuredTensors object, allowing it to be treated like a regular tensor.\n        \"\"\"\n        if self.is_unstructured:\n            return torch.Tensor.__getitem__(self, slice(self.cumsums[index], self.cumsums[index] + self.lengths[index]))\n        else:\n            return super().__getitem__(index)\n\n    def __len__(self):\n        if self.is_unstructured:\n            return len(self.lengths)\n        else:\n            return super().__len__()\n\n    def clone(self, *args, **kwargs):\n        if self.is_unstructured:\n            return UnstructuredTensors([self[i] for i in range(len(self.lengths))])  # manual cloning with copy and reconstruction\n        else:\n            return super().clone(*args, **kwargs)\n\n\ndef load_ims_bytes_from_disk(ims: np.ndarray, desc=\"Loading image bytes from disk\"):\n    sh = ims.shape\n    ims = ims.ravel()\n    ims_bytes = parallel_execution(list(ims), action=load_image_bytes, desc=desc, print_progress=True)\n    ims_bytes = np.asarray(ims_bytes).reshape(sh)  # reorganize shapes\n    return ims_bytes\n\n\ndef load_resize_undist_im_bytes(imp: str,\n                                K: np.ndarray,\n                                D: np.ndarray,\n                                ratio: Union[float, List[int]] = 1.0,\n                                center_crop_size: List[int] = [-1, -1],\n                                encode_ext='.jpg',\n                                decode_flag=cv2.IMREAD_UNCHANGED,\n                                dist_opt_K: bool = False,\n                                jpeg_quality: int = 100,\n                                png_compression: int = 6\n                                ):\n    # Load image -> resize -> undistort -> save to bytes (jpeg)\n    img = load_image_from_bytes(load_image_bytes(imp), decode_flag=decode_flag)[..., :3]  # cv2 decoding (fast)\n\n    oH, oW = img.shape[:2]\n\n    if dist_opt_K:\n        newCameraMatrix, _ = cv2.getOptimalNewCameraMatrix(K, D, (oW, oH), 0, (oW, oH))\n        img = cv2.undistort(img, K, D, newCameraMatrix=newCameraMatrix)\n        K = newCameraMatrix\n    else:\n        img = cv2.undistort(img, K, D)\n\n    # Maybe update image size\n    if not ((isinstance(ratio, float) and ratio == 1.0)):\n        if isinstance(ratio, float):\n            H, W = int(oH * ratio), int(oW * ratio)\n        else:\n            H, W = ratio  # ratio is actually the target image size\n        rH, rW = H / oH, W / oW\n        K = K.copy()\n        K[0:1] = K[0:1] * rW  # K[0, 0] *= rW\n        K[1:2] = K[1:2] * rH  # K[1, 1] *= rH\n\n        img = cv2.resize(img, (W, H), interpolation=cv2.INTER_AREA)  # H, W, 3, uint8\n\n    # Crop the image and intrinsic matrix if specified\n    if center_crop_size[0] > 0:\n        img, K, H, W = center_crop_img_ixt(img, K, H, W, center_crop_size)\n\n    is_success, buffer = cv2.imencode(encode_ext, img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])\n\n    if 'H' not in locals(): H, W = oH, oW\n    return buffer, K, H, W\n\n\ndef center_crop_img_ixt(img: np.ndarray, K: np.ndarray, H: int, W: int,\n                        center_crop_size: Union[int, List[int]]):\n    # Parse the original size and the target crop size\n    oH, oW = H, W\n    if isinstance(center_crop_size, int): cH, cW = center_crop_size, center_crop_size\n    else: cH, cW = center_crop_size\n\n    # Compute left and right crop size for height and width respectively\n    hlc, wlc = int((oH - cH) * 0.5), int((oW - cW) * 0.5)\n    hrc, wrc = oH - cH - hlc, oW - cW - wlc\n\n    # Crop the image\n    if hlc != 0: img = img[hlc:-hrc, :]\n    if wlc != 0: img = img[:, wlc:-wrc]\n\n    # Crop the intrinsic matrix\n    if hlc != 0: K[1, 2] -= hlc\n    if wlc != 0: K[0, 2] -= wlc\n\n    return img, K, cH, cW\n\n\ndef load_resize_undist_ims_bytes(ims: np.ndarray,\n                                 Ks: np.ndarray,\n                                 Ds: np.ndarray,\n                                 ratio: Union[float, List[int], List[float]] = 1.0,\n                                 center_crop_size: List[int] = [-1, -1],\n                                 desc=\"Loading image bytes from disk\",\n                                 **kwargs):\n    sh = ims.shape  # V, N\n    # Ks = np.broadcast_to(Ks[:, None], (sh + (3, 3)))\n    # Ds = np.broadcast_to(Ds[:, None], (sh + (1, 5)))\n\n    ims = ims.reshape((np.prod(sh)))\n    # from easyvolcap.utils.dist_utils import get_rank\n    # if not get_rank(): __import__('easyvolcap.utils.console_utils', fromlist=['debugger']).debugger()\n    # else:\n    #     while 1: pass\n    Ks = Ks.reshape((np.prod(sh), 3, 3))\n    Ds = Ds.reshape((np.prod(sh), 1, 5))\n\n    ims = list(ims)\n    Ks = list(Ks)\n    Ds = list(Ds)  # only convert outer most dim to list\n\n    if isinstance(ratio, list) and len(ratio) and isinstance(ratio[0], float):\n        ratio = np.broadcast_to(np.asarray(ratio)[:, None], sh)  # V, N\n        ratio = ratio.reshape((np.prod(sh)))\n        ratio = list(ratio)\n    elif isinstance(ratio, list):\n        ratio = np.asarray(ratio)  # avoid expansion in parallel execution\n\n    if isinstance(center_crop_size, list):\n        center_crop_size = np.asarray(center_crop_size)  # avoid expansion\n\n    # Should we batch these instead of loading?\n    out = parallel_execution(ims, Ks, Ds, ratio, center_crop_size,\n                             action=load_resize_undist_im_bytes,\n                             desc=desc, print_progress=True,\n                             **kwargs,\n                             )\n\n    ims_bytes, Ks, Hs, Ws = zip(*out)  # is this OK?\n    ims_bytes, Ks, Hs, Ws = np.asarray(ims_bytes, dtype=object), np.asarray(Ks), np.asarray(Hs), np.asarray(Ws)\n    # ims_bytes = ims_bytes.reshape(sh)  # numpy array of bytesio\n    Hs = Hs.reshape(sh)  # should all be the same?\n    Ws = Ws.reshape(sh)  # should all be the same?\n    Ks = Ks.reshape(sh + (3, 3))  # should all be the same?\n\n    return ims_bytes, Ks, Hs, Ws\n\n\ndef decode_crop_fill_im_bytes(im_bytes: BytesIO,\n                              mk_bytes: BytesIO,\n                              K: np.ndarray,\n                              R: np.ndarray,\n                              T: np.ndarray,\n                              bounds: np.ndarray,\n                              encode_ext=['.jpg', '.jpg'],\n                              decode_flag=cv2.IMREAD_UNCHANGED,\n                              jpeg_quality: int = 100,\n                              png_compression: int = 6,\n                              **kwargs):\n    # im_bytes: a series of jpeg bytes for the image\n    # mk_bytes: a series of jpeg bytes for the mask\n    # K: 3, 3 intrinsics matrix\n\n    # Use load_image_from_bytes to decode and update jpeg streams\n    img = load_image_from_bytes(im_bytes, decode_flag=decode_flag)  # H, W, 3\n    msk = load_image_from_bytes(mk_bytes, decode_flag=decode_flag)  # H, W, 3\n\n    # Crop both mask and the image using bbox's 2D projection\n    H, W, _ = img.shape\n    from easyvolcap.utils.bound_utils import get_bound_2d_bound\n    bx, by, bw, bh = as_numpy_func(get_bound_2d_bound)(bounds, K, R, T, H, W)\n    img = img[by:by + bh, bx:bx + bw]\n    msk = msk[by:by + bh, bx:bx + bw]\n\n    # Crop the image using the bounding rect of the mask\n    mx, my, mw, mh = cv2.boundingRect((msk > 128).astype(np.uint8))  # array data type = 0 is not supported\n    img = img[my:my + mh, mx:mx + mw]\n    msk = msk[my:my + mh, mx:mx + mw]\n\n    # Update the final size and intrinsics\n    x, y, w, h = bx + mx, by + my, mw, mh  # w and h will always be the smaller one, xy will be accumulated\n    K[0, 2] -= x\n    K[1, 2] -= y\n\n    # Fill the image with black (premultiply by mask)\n    img = (img * (msk / 255)).clip(0, 255).astype(np.uint8)  # fill with black, indexing starts at the front\n\n    # Reencode the videos and masks\n    if isinstance(encode_ext, str): encode_ext = [encode_ext] * 2  # '.jpg' -> ['.jpg', '.jpg']\n    im_bytes = cv2.imencode(encode_ext[0], img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])[1]  # is_sucess, bytes_array\n    mk_bytes = cv2.imencode(encode_ext[1], msk, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])[1]  # is_sucess, bytes_array\n    return im_bytes, mk_bytes, K, h, w, x, y\n\n\ndef decode_crop_fill_ims_bytes(ims_bytes: np.ndarray, mks_bytes: np.ndarray, Ks: np.ndarray, Rs: np.ndarray, Ts: np.ndarray, bounds: np.ndarray,\n                               desc=\"Cropping images using mask\", **kwargs):\n    sh = Ks.shape[:2]  # V, N\n    # ims_bytes = ims_bytes.reshape((np.prod(sh)))\n    # mks_bytes = mks_bytes.reshape((np.prod(sh)))\n    Ks = Ks.reshape((np.prod(sh), 3, 3))\n    Rs = Rs.reshape((np.prod(sh), 3, 3))\n    Ts = Ts.reshape((np.prod(sh), 3, 1))\n    bounds = bounds.reshape((np.prod(sh), 2, 3))\n\n    # Should we batch these instead of loading?\n    out = parallel_execution(list(ims_bytes), list(mks_bytes), list(Ks), list(Rs), list(Ts), list(bounds),\n                             action=decode_crop_fill_im_bytes,\n                             desc=desc, print_progress=True,\n                             **kwargs,\n                             )\n\n    ims_bytes, mks_bytes, Ks, Hs, Ws, xs, ys = zip(*out)  # is this OK?\n    ims_bytes, mks_bytes, Ks, Hs, Ws, xs, ys = np.asarray(ims_bytes, dtype=object), np.asarray(mks_bytes, dtype=object), np.asarray(Ks), np.asarray(Hs), np.asarray(Ws), np.asarray(xs), np.asarray(ys)\n    # ims_bytes = ims_bytes.reshape(sh)\n    # mks_bytes = mks_bytes.reshape(sh)\n    Hs = Hs.reshape(sh)  # should all be the same?\n    Ws = Ws.reshape(sh)  # should all be the same?\n    Ks = Ks.reshape(sh + (3, 3))  # should all be the same?\n    xs = xs.reshape(sh)  # should all be the same?\n    ys = ys.reshape(sh)  # should all be the same?\n\n    return ims_bytes, mks_bytes, Ks, Hs, Ws, xs, ys\n\n\ndef decode_fill_im_bytes(im_bytes: BytesIO,\n                         mk_bytes: BytesIO,\n                         encode_ext='.jpg',\n                         decode_flag=cv2.IMREAD_UNCHANGED,\n                         jpeg_quality: int = 100,\n                         png_compression: int = 6,\n                         **kwargs):\n    # im_bytes: a series of jpeg bytes for the image\n    # mk_bytes: a series of jpeg bytes for the mask\n    # K: 3, 3 intrinsics matrix\n\n    # Use load_image_from_bytes to decode and update jpeg streams\n    img = load_image_from_bytes(im_bytes, decode_flag=decode_flag)  # H, W, 3\n    msk = load_image_from_bytes(mk_bytes, decode_flag=decode_flag)  # H, W, 3\n\n    img = (img * (msk / 255)).clip(0, 255).astype(np.uint8)  # fill with black, indexing starts at the front\n\n    im_bytes = cv2.imencode(encode_ext, img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])[1]  # is_sucess, bytes_array\n    return im_bytes\n\n\ndef decode_fill_ims_bytes(ims_bytes: np.ndarray,\n                          mks_bytes: np.ndarray,\n                          desc=\"Filling images using mask\",\n                          **kwargs):\n    sh = ims_bytes.shape  # V, N\n    ims_bytes = ims_bytes.reshape((np.prod(sh)))\n    mks_bytes = mks_bytes.reshape((np.prod(sh)))\n\n    # Should we batch these instead of loading?\n    ims_bytes = parallel_execution(list(ims_bytes), list(mks_bytes),\n                                   action=decode_fill_im_bytes,\n                                   desc=desc, print_progress=True,\n                                   **kwargs,\n                                   )\n\n    ims_bytes = np.asarray(ims_bytes, dtype=object)\n    ims_bytes = ims_bytes.reshape(sh)\n    return ims_bytes\n\n\ndef batch_rodrigues(poses):\n    \"\"\" poses: N x 3\n    \"\"\"\n    batch_size = poses.shape[0]\n    angle = np.linalg.norm(poses + 1e-8, axis=1, keepdims=True)\n    rot_dir = poses / angle\n\n    cos = np.cos(angle)[:, None]\n    sin = np.sin(angle)[:, None]\n\n    rx, ry, rz = np.split(rot_dir, 3, axis=1)\n    zeros = np.zeros([batch_size, 1])\n    K = np.concatenate([zeros, -rz, ry, rz, zeros, -rx, -ry, rx, zeros],\n                       axis=1)\n    K = K.reshape([batch_size, 3, 3])\n\n    ident = np.eye(3)[None]\n    rot_mat = ident + sin * K + (1 - cos) * np.matmul(K, K)\n\n    return rot_mat.astype(np.float32)\n\n\ndef get_rigid_transformation_and_joints(poses, joints, parents):\n    \"\"\"\n    poses: n_bones x 3\n    joints: n_bones x 3\n    parents: n_bones\n    \"\"\"\n\n    n_bones = len(joints)\n    rot_mats = batch_rodrigues(poses)\n\n    # Obtain the relative joints\n    rel_joints = joints.copy()\n    rel_joints[1:] -= joints[parents[1:]]\n\n    # Create the transformation matrix\n    # First rotate then transform\n    transforms_mat = np.concatenate([rot_mats, rel_joints[..., None]], axis=2)\n    padding = np.zeros([n_bones, 1, 4])\n    padding[..., 3] = 1\n    transforms_mat = np.concatenate([transforms_mat, padding], axis=1)\n\n    # Rotate each part\n    # But this is a world transformation, with displacement...?\n    transform_chain = [transforms_mat[0]]\n    for i in range(1, parents.shape[0]):  # assuming parents are in topological order\n        curr_res = np.dot(transform_chain[parents[i]], transforms_mat[i])  # THEY'RE RIGHT, LEARN FORWARD KINEMATICS\n        transform_chain.append(curr_res)\n    transforms = np.stack(transform_chain, axis=0)\n\n    # Obtain the rigid transformation\n    # AND THIS WEIRD STUFF IS TRYING TO MOVE VERTEX FROM VERTEX COORDINATES TO JOINT COORDINATES\n    # AND THIS IS THE CORRECT IMPLEMENTATION...\n\n    # THIS IS JUST TOO CLEVER...\n    # These three lines is effectively doing: transforms = transforms * (negative trarslation matrix for all joints)\n    joints_vector = np.concatenate([joints, np.zeros([n_bones, 1])], axis=1)\n    rot_joints = np.sum(transforms * joints_vector[:, None], axis=2)  # This is effectively matmul\n    transforms[..., 3] = transforms[..., 3] - rot_joints  # add in the translation, we should translate first\n\n    joints_points = np.concatenate([joints, np.ones([n_bones, 1])], axis=1)\n    pose_joints = np.sum(transforms * joints_points[:, None], axis=2)  # This is effectively matmul\n\n    transforms = transforms.astype(np.float32)\n    return transforms, pose_joints[:, :3]\n\n\ndef get_rigid_transformation(poses, joints, parents):\n    \"\"\"\n    poses: n_bones x 3\n    joints: n_bones x 3\n    parents: n_bones\n    \"\"\"\n    transforms = get_rigid_transformation_and_joints(poses, joints, parents)[0]\n    return transforms\n\n\ndef padding_bbox_HW(bbox, h, w):\n    padding = 10\n    bbox[0] = bbox[0] - 10\n    bbox[1] = bbox[1] + 10\n\n    height = bbox[1, 1] - bbox[0, 1]\n    width = bbox[1, 0] - bbox[0, 0]\n    # a magic number of pytorch3d\n    ratio = 1.5\n\n    if height / width > ratio:\n        min_size = int(height / ratio)\n        if width < min_size:\n            padding = (min_size - width) // 2\n            bbox[0, 0] = bbox[0, 0] - padding\n            bbox[1, 0] = bbox[1, 0] + padding\n\n    if width / height > ratio:\n        min_size = int(width / ratio)\n        if height < min_size:\n            padding = (min_size - height) // 2\n            bbox[0, 1] = bbox[0, 1] - padding\n            bbox[1, 1] = bbox[1, 1] + padding\n\n    bbox[:, 0] = np.clip(bbox[:, 0], a_min=0, a_max=w - 1)\n    bbox[:, 1] = np.clip(bbox[:, 1], a_min=0, a_max=h - 1)\n\n    return bbox\n\n\ndef padding_bbox(bbox, img):\n    return padding_bbox_HW(bbox, *img.shape[:2])\n\n\ndef get_crop_box(H, W, K, ref_msk):\n    x, y, w, h = cv2.boundingRect(ref_msk)\n    bbox = np.array([[x, y], [x + w, y + h]])\n    bbox = padding_bbox_HW(bbox, H, W)\n\n    # revise the intrinsic camera matrix\n    K = K.copy()\n    K[0, 2] = K[0, 2] - bbox[0, 0]\n    K[1, 2] = K[1, 2] - bbox[0, 1]\n    K = K.astype(np.float32)\n\n    return K, bbox\n\n\ndef crop_image_msk(img, msk, K, ref_msk):\n    x, y, w, h = cv2.boundingRect(ref_msk)\n    bbox = np.array([[x, y], [x + w, y + h]])\n    bbox = padding_bbox(bbox, img)\n\n    crop = img[bbox[0, 1]:bbox[1, 1], bbox[0, 0]:bbox[1, 0]]\n    crop_msk = msk[bbox[0, 1]:bbox[1, 1], bbox[0, 0]:bbox[1, 0]]\n\n    # calculate the shape\n    shape = crop.shape\n    x = 8\n    height = (crop.shape[0] | (x - 1)) + 1\n    width = (crop.shape[1] | (x - 1)) + 1\n\n    # align image\n    aligned_image = np.zeros([height, width, 3])\n    aligned_image[:shape[0], :shape[1]] = crop\n    aligned_image = aligned_image.astype(np.float32)\n\n    # align mask\n    aligned_msk = np.zeros([height, width])\n    aligned_msk[:shape[0], :shape[1]] = crop_msk\n    aligned_msk = (aligned_msk == 1).astype(np.uint8)\n\n    # revise the intrinsic camera matrix\n    K = K.copy()\n    K[0, 2] = K[0, 2] - bbox[0, 0]\n    K[1, 2] = K[1, 2] - bbox[0, 1]\n    K = K.astype(np.float32)\n\n    return aligned_image, aligned_msk, K, bbox\n\n\ndef random_crop_image(img, msk, K, min_size, max_size):\n    # sometimes we sample regions with no valid pixel at all, this can be problematic for the training loop\n    # there's an assumption that the `msk` is always inside `mask_at_box`\n    # thus, if we're sampling inside the `msk`, we'll always be getting the correct results\n    H, W = img.shape[:2]\n    min_HW = min(H, W)\n    min_HW = min(min_HW, max_size)\n\n    max_size = min_HW\n    # min_size = int(min(min_size, 0.8 * min_HW))\n    if max_size < min_size:\n        H_size = np.random.randint(min_size, max_size)\n    else:\n        H_size = min_size\n\n    W_size = H_size\n    x = 8\n    H_size = (H_size | (x - 1)) + 1\n    W_size = (W_size | (x - 1)) + 1\n\n    # randomly select begin_x and begin_y\n    coords = np.argwhere(msk == 1)\n    center_xy = coords[np.random.randint(0, len(coords))][[1, 0]]\n    min_x, min_y = center_xy[0] - W_size // 2, center_xy[1] - H_size // 2\n    max_x, max_y = min_x + W_size, min_y + H_size\n    if min_x < 0:\n        min_x, max_x = 0, W_size\n    if max_x > W:\n        min_x, max_x = W - W_size, W\n    if min_y < 0:\n        min_y, max_y = 0, H_size\n    if max_y > H:\n        min_y, max_y = H - H_size, H\n\n    # crop image and mask\n    begin_x, begin_y = min_x, min_y\n    img = img[begin_y:begin_y + H_size, begin_x:begin_x + W_size]\n    msk = msk[begin_y:begin_y + H_size, begin_x:begin_x + W_size]\n\n    # revise the intrinsic camera matrix\n    K = K.copy()\n    K[0, 2] = K[0, 2] - begin_x\n    K[1, 2] = K[1, 2] - begin_y\n    K = K.astype(np.float32)\n\n    return img, msk, K\n\n\ndef get_bound_corners(bounds):\n    min_x, min_y, min_z = bounds[0]\n    max_x, max_y, max_z = bounds[1]\n    corners_3d = np.asarray([\n        [min_x, min_y, min_z],\n        [min_x, min_y, max_z],\n        [min_x, max_y, min_z],\n        [min_x, max_y, max_z],\n        [max_x, min_y, min_z],\n        [max_x, min_y, max_z],\n        [max_x, max_y, min_z],\n        [max_x, max_y, max_z],\n    ], dtype=np.float32)\n    return corners_3d\n\n\ndef get_bound_2d_mask(bounds, K, RT, H, W):\n    corners_3d = get_bound_corners(bounds)\n    corners_2d = project(corners_3d, K, RT)\n    corners_2d = np.round(corners_2d).astype(int)\n    mask = np.zeros((H, W), dtype=np.uint8)\n    cv2.fillPoly(mask, [corners_2d[[0, 1, 3, 2, 0]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[4, 5, 7, 6, 5]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[0, 1, 5, 4, 0]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[2, 3, 7, 6, 2]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[0, 2, 6, 4, 0]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[1, 3, 7, 5, 1]]], 1)\n    return mask\n\n\ndef get_bounds(xyz, box_padding=0.05):\n    min_xyz = np.min(xyz, axis=0)\n    max_xyz = np.max(xyz, axis=0)\n    min_xyz -= box_padding\n    max_xyz += box_padding\n    bounds = np.stack([min_xyz, max_xyz], axis=0)\n    bounds = bounds.astype(np.float32)\n    return bounds\n\n\ndef crop_mask_edge(msk):\n    msk = msk.copy()\n    border = 10\n    kernel = np.ones((border, border), np.uint8)\n    msk_erode = cv2.erode(msk.copy(), kernel)\n    msk_dilate = cv2.dilate(msk.copy(), kernel)\n    msk[(msk_dilate - msk_erode) == 1] = 100\n    return msk\n\n\ndef adjust_hsv(img, saturation, brightness, contrast):\n    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    hsv = hsv.astype(np.float32)\n    hsv[..., 1] = hsv[..., 1] * saturation\n    hsv[..., 1] = np.minimum(hsv[..., 1], 255)\n    hsv[..., 2] = hsv[..., 2] * brightness\n    hsv[..., 2] = np.minimum(hsv[..., 2], 255)\n    hsv = hsv.astype(np.uint8)\n    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n    img = img.astype(np.float32) * contrast\n    img = np.minimum(img, 255)\n    img = img.astype(np.uint8)\n    return img\n", "input_code": "def to_cuda(batch, device=\"cuda\", ignore_list: bool = False) -> torch.Tensor:\n\n    \"\"\"\n    Converts the input batch to a CUDA tensor, recursively handling nested structures like tuples, lists, and dictionaries, except for dictionary keys named \"meta\". It ensures that the data is moved to the specified CUDA device for GPU processing.\n    Input-Output Arguments\n    :param batch: tuple, list, dict, torch.Tensor, or other. The input data that needs to be converted to a CUDA tensor. It supports nested structures and handles them accordingly.\n    :param device: str, default \"cuda\". The target CUDA device to which the data will be moved.\n    :param ignore_list: bool, default False. A flag to indicate whether to ignore certain elements, though its usage is not directly apparent in the function's implementation.\n    :return: torch.Tensor. The input batch converted to a CUDA tensor, ready for GPU processing.\n    \"\"\"", "reference_steps": "1. Define a function `to_cuda` that takes a batch of data and a device (defaulting to \"cuda\") and an optional `ignore_list` parameter (defaulting to False) and returns a PyTorch tensor.\n\n2. Check if the input `batch` is an instance of a tuple or a list.\n\n3. If it is a tuple or a list, recursively apply the `to_cuda` function to each element in the batch.\n\n4. If the input `batch` is a dictionary, create a new `dotdict` (a dictionary with dot-accessible attributes) that applies `to_cuda` to each value unless the key is \"meta,\" in which case keep the value as is.\n\n5. If the input `batch` is a PyTorch tensor, use the `.to()` method to move the tensor to the specified device with `non_blocking=True` for potential asynchronous execution.\n\n6. If the input `batch` is neither a tuple, list, dictionary, nor a PyTorch tensor (e.g., a numpy array or other types), convert it to a PyTorch tensor using `torch.as_tensor()` and move it to the specified device.\n\n7. Return the modified batch, which is now a PyTorch tensor or a collection (tuple, list, dictionary) of PyTorch tensors on the specified device.\n\nNote: The `dotdict` mentioned in the code is not a standard Python or PyTorch object. It is assumed to be a custom class or function that creates a dictionary with attribute-style access. If it is not defined elsewhere in the codebase, you would need to define it or replace it with a regular dictionary.", "reference_code": "def to_cuda(batch, device=\"cuda\", ignore_list: bool = False) -> torch.Tensor:\n    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: (to_cuda(v, device, ignore_list) if k != \"meta\" else v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch, device=device)\n    return batch\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "type": "function", "class_name": null, "function_name": "add_batch", "dependency_all": "# Intra-file Dependency:\neasyvolcap.utils.data_utils.add_batch\n    def add_batch(batch) -> Union[torch.Tensor, np.ndarray]:\n\n# Cross-file Dependency:\neasyvolcap.utils.base_utils.dotdict\n    class dotdict(dict, Dict[KT, VT]):\n        \"\"\"\n        This is the default data passing object used throughout the codebase\n        Main function: dot access for dict values & dict like merging and updates\n\n        a dictionary that supports dot notation \n        as well as dictionary access notation \n        usage: d = make_dotdict() or d = make_dotdict{'val1':'first'})\n        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n        get attributes: d.val2 or d['val2']\n        \"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\neasyvolcap.utils.data_utils.add_batch\n    def add_batch(batch) -> Union[torch.Tensor, np.ndarray]:\n\n", "contexts_above": "import os\nimport re\nimport cv2\nimport h5py\nimport torch\nimport struct\nimport asyncio\nimport subprocess\nimport numpy as np\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom typing import overload\nfrom functools import lru_cache\n\n# from imgaug import augmenters as iaa\nfrom typing import Tuple, Union, List, Dict\n\nfrom torch.nn import functional as F\nfrom torch.utils.data._utils.pin_memory import pin_memory\nfrom torch.utils.data._utils.collate import default_collate, default_convert\n\nfrom easyvolcap.utils.parallel_utils import parallel_execution\nfrom easyvolcap.utils.base_utils import dotdict\nfrom easyvolcap.utils.console_utils import *\n\nfrom enum import Enum, auto\n\n# Copied from enerf (maybe was in turn copied from dtu)\n\n\ndef read_pickle(name):\n    import pickle\n    with open(name, 'rb') as f:\n        data = pickle.load(f, encoding='latin1')\n    return data\n\n\ndef read_cam_file(filename):\n    with open(filename) as f:\n        lines = [line.rstrip() for line in f.readlines()]\n    # extrinsics: line [1,5), 4x4 matrix\n    extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ')\n    extrinsics = extrinsics.reshape((4, 4))\n    # intrinsics: line [7-10), 3x3 matrix\n    intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ')\n    intrinsics = intrinsics.reshape((3, 3))\n    # depth_min & depth_interval: line 11\n    depth_min = float(lines[11].split()[0])\n    return intrinsics, extrinsics, depth_min\n\n\ndef read_pmn_cam_file(filename):\n    with open(filename) as f:\n        lines = [line.rstrip() for line in f.readlines()]\n    # extrinsics: line [1,5), 4x4 matrix\n    extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ')\n    extrinsics = extrinsics.reshape((4, 4))\n    # intrinsics: line [7-10), 3x3 matrix\n    intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ')\n    intrinsics = intrinsics.reshape((3, 3))\n    # depth_min & depth_interval: line 11\n    depth_min = float(lines[11].split()[0])\n    depth_max = float(lines[11].split()[1])\n    return intrinsics, extrinsics, depth_min, depth_max\n\n\ndef read_pfm(filename):\n    file = open(filename, 'rb')\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n\n    header = file.readline().decode('utf-8').rstrip()\n    if header == 'PF':\n        color = True\n    elif header == 'Pf':\n        color = False\n    else:\n        raise Exception('Not a PFM file.')\n\n    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode('utf-8'))\n    if dim_match:\n        width, height = map(int, dim_match.groups())\n    else:\n        raise Exception('Malformed PFM header.')\n\n    scale = float(file.readline().rstrip())\n    if scale < 0:  # little-endian\n        endian = '<'\n        scale = -scale\n    else:\n        endian = '>'  # big-endian\n\n    data = np.fromfile(file, endian + 'f')\n    shape = (height, width, 3) if color else (height, width)\n\n    data = np.reshape(data, shape)\n    data = np.flipud(data)\n    file.close()\n    return data, scale\n\n\ndef generate_video(result_str: str,\n                   output: str,\n                   fps: int = 30,\n                   crf: int = 17,\n                   cqv: int = 19,\n                   lookahead: int = 20,\n                   hwaccel: str = 'cuda',\n                   preset: str = 'p7',\n                   tag: str = 'hvc1',\n                   vcodec: str = 'hevc_nvenc',\n                   pix_fmt: str = 'yuv420p',  # chrome friendly\n                   ):\n    cmd = [\n        'ffmpeg',\n        '-hwaccel', hwaccel,\n        '-hide_banner',\n        '-loglevel', 'error',\n        '-framerate', fps,\n        '-f', 'image2',\n        '-pattern_type', 'glob',\n        '-nostdin',  # otherwise you cannot chain commands together\n        '-y',\n        '-r', fps,\n        '-i', result_str,\n        '-c:v', vcodec,\n        '-preset', preset,\n        '-cq:v', cqv,\n        '-rc:v', 'vbr',\n        '-tag:v', tag,\n        '-crf', crf,\n        '-pix_fmt', pix_fmt,\n        '-rc-lookahead', lookahead,\n        '-vf', '\"pad=ceil(iw/2)*2:ceil(ih/2)*2\"',  # avoid yuv420p odd number bug\n        output,\n    ]\n    run(cmd)\n    return output\n\n\ndef numpy_to_video(numpy_array: np.ndarray,\n                   output_filename: str,\n                   fps: float = 30.0,\n                   crf: int = 18,\n                   cqv: int = 19,\n                   lookahead: int = 20,\n                   preset='veryslow',\n                   vcodec='libx265',\n                   ):\n    \"\"\"\n    Convert a numpy array (T, H, W, C) to a video using ffmpeg.\n\n    Parameters:\n    - numpy_array: Numpy array to be converted.\n    - output_filename: The filename of the output video.\n    - framerate: Frame rate for the video.\n    \"\"\"\n    if isinstance(numpy_array, np.ndarray):\n        T, H, W, C = numpy_array.shape\n    else:\n        T = len(numpy_array)\n        H, W, C = numpy_array[0].shape\n    assert C == 3, \"Expected 3 channels!\"\n\n    cmd = [\n        'ffmpeg',\n        '-hwaccel', 'cuda',\n        '-v', 'quiet', '-stats',\n        '-y',  # Overwrite output file if it exists\n        '-f', 'rawvideo',\n        '-vcodec', 'rawvideo',\n        '-s', f'{W}x{H}',  # Size of one frame\n        '-pix_fmt', 'rgb24',\n        '-r', fps,  # Frame rate\n        '-i', '-',  # Read from pipe\n        '-an',  # No audio\n        '-vcodec', vcodec,\n        '-preset', preset,\n        '-cq:v', cqv,\n        '-crf', crf,\n        '-rc-lookahead', lookahead,\n        '-rc:v', 'vbr',\n        '-tag:v', 'hvc1',\n        output_filename\n    ]\n    os.makedirs(dirname(output_filename), exist_ok=True)\n    process = subprocess.Popen(map(str, cmd), stdin=subprocess.PIPE)\n    # process.communicate(input=numpy_array.tobytes())\n    for frame in numpy_array:\n        process.stdin.write(frame.tobytes())\n        # process.stdin.flush()\n    process.stdin.close()\n    process.communicate()\n\n\ndef get_video_dimensions(input_filename):\n    \"\"\"\n    Extract the width and height of a video using ffprobe.\n\n    Parameters:\n    - input_filename: The filename of the input video.\n\n    Returns:\n    - width and height of the video.\n    \"\"\"\n    cmd = [\n        'ffprobe',\n        '-v', 'error',\n        '-select_streams', 'v:0',\n        '-show_entries', 'stream=width,height',\n        '-of', 'csv=s=x:p=0',\n        input_filename\n    ]\n\n    pipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, _ = pipe.communicate()\n    width, height = map(int, out.decode('utf-8').strip().split('x'))\n    return width, height\n\n\ndef video_to_numpy(input_filename, hwaccel='cuda', vcodec='hevc_cuvid'):\n    \"\"\"\n    Convert a video file to a numpy array (T, H, W, C) using ffmpeg.\n\n    Parameters:\n    - input_filename: The filename of the input video.\n\n    Returns:\n    - Numpy array representing the video.\n    \"\"\"\n    W, H = get_video_dimensions(input_filename)\n\n    cmd = [\n        'ffmpeg',\n    ]\n    if hwaccel != 'none':\n        cmd += ['-hwaccel', hwaccel,]\n    cmd += [\n        '-v', 'quiet', '-stats',\n    ]\n    if vcodec != 'none':\n        cmd += ['-vcodec', vcodec,]\n    cmd += [\n        '-i', input_filename,\n        '-f', 'image2pipe',\n        '-pix_fmt', 'rgb24',\n        '-vcodec', 'rawvideo',\n        '-'\n    ]\n\n    pipe = subprocess.Popen(map(str, cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=10**8)\n    raw_data, _ = pipe.communicate()\n\n    # Convert the raw data to numpy array and reshape\n    video_np = np.frombuffer(raw_data, dtype=np.uint8)\n    H2, W2 = (H + 1) // 2 * 2, (W + 1) // 2 * 2\n    try:\n        video_np = video_np.reshape(-1, H2, W2, 3)[:, :H, :W, :]\n    except ValueError as e:\n        video_np = video_np.reshape(-1, H, W, 3)\n\n    return video_np\n\n\nclass Visualization(Enum):\n    # Universal visualization\n    RENDER = auto()  # plain rgb render output\n    SURFACE = auto()  # surface position (similar to depth)\n    DEFORM = auto()  # deformation magnitude (as in correspondence?)\n    DEPTH = auto()  # needs a little bit extra computation\n    ALPHA = auto()  # occupancy (rendered volume density)\n    NORMAL = auto()  # needs extra computation\n    FEATURE = auto()  # embedder results\n    SEMANTIC = auto()  # semantic nerf related\n    SRCINPS = auto()  # Souce input images for image based rendering\n\n    # jacobian related\n    JACOBIAN = auto()\n\n    # Relighting related\n    ENVMAP = auto()\n    ALBEDO = auto()\n    SHADING = auto()\n    ROUGHNESS = auto()\n\n    # Geometry related output\n    MESH = auto()\n    POINT = auto()\n    VOLUME = auto()\n\n\nclass DataSplit(Enum):\n    TRAIN = auto()\n    TEST = auto()\n    VAL = auto()\n\n\ndef variance_of_laplacian(image: np.ndarray):\n    if image.ndim == 3 and image.shape[-1] > 1:\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    if image.dtype == np.float32 or image.dtype == np.float64:\n        image = (image * 255).astype(np.uint8)\n    return cv2.Laplacian(image, cv2.CV_64F).var()\n\n\ndef alpha2sdf(alpha, beta, dists=0.005):\n    return beta * np.log(2 * beta * (-np.log(1 - alpha) / dists))\n\n\ndef h5_to_dotdict(h5: h5py.File) -> dotdict:\n    d = {key: h5_to_dotdict(h5[key]) if isinstance(h5[key], h5py.Group) else h5[key][:] for key in h5.keys()}  # loaded as numpy array\n    d = dotdict(d)\n    return d\n\n\ndef h5_to_list_of_dotdict(h5: h5py.File) -> list:\n    return [h5_to_dotdict(h5[key]) for key in tqdm(h5)]\n\n\ndef to_h5py(value, h5: h5py.File, key: str = None, compression: str = 'gzip'):\n    if isinstance(value, torch.Tensor):\n        value = value.detach().cpu().numpy()\n    if isinstance(value, np.ndarray):\n        h5.create_dataset(str(key), data=value, compression=compression)\n    elif isinstance(value, list):\n        if key is not None:\n            h5 = h5.create_group(str(key))\n        [to_h5py(v, h5, k) for k, v in enumerate(value)]\n    elif isinstance(value, dict):\n        if key is not None:\n            h5 = h5.create_group(str(key))\n        [to_h5py(v, h5, k) for k, v in value.items()]\n    else:\n        raise NotImplementedError(f'unsupported type to write to h5: {type(value)}')\n\n\ndef export_h5(batch: dotdict, filename):\n    with h5py.File(filename, 'w') as f:\n        to_h5py(batch, f)\n\n\ndef load_h5(filename):\n    with h5py.File(filename, 'r') as f:\n        return h5_to_dotdict(f)\n\n\ndef merge_faces(faces, *args):\n    # Copied from trimesh, this will select one uv coordinates for a particular vertex\n    \"\"\"\n    Textured meshes can come with faces referencing vertex\n    indices (`v`) and an array the same shape which references\n    vertex texture indices (`vt`) and sometimes even normal (`vn`).\n\n    Vertex locations with different values of any of these can't\n    be considered the \"same\" vertex, and for our simple data\n    model we need to not combine these vertices.\n\n    Parameters\n    -------------\n    faces : (n, d) int\n      References vertex indices\n    *args : (n, d) int\n      Various references of corresponding values\n      This is usually UV coordinates or normal indexes\n    maintain_faces : bool\n      Do not alter original faces and return no-op masks.\n\n    Returns\n    -------------\n    new_faces : (m, d) int\n      New faces for masked vertices\n    mask_v : (p,) int\n      A mask to apply to vertices\n    mask_* : (p,) int\n      A mask to apply to vt array to get matching UV coordinates\n      Returns as many of these as args were passed\n    \"\"\"\n\n    # start with not altering faces at all\n    result = [faces]\n    # find the maximum index referenced by faces\n    max_idx = faces.max()\n    # add a vertex mask which is just ordered\n    result.append(np.arange(max_idx + 1))\n\n    # now given the order is fixed do our best on the rest of the order\n    for arg in args:\n        # create a mask of the attribute-vertex mapping\n        # note that these might conflict since we're not unmerging\n        masks = np.zeros((3, max_idx + 1), dtype=np.int64)\n        # set the mask using the unmodified face indexes\n        for i, f, a in zip(range(3), faces.T, arg.T):\n            masks[i][f] = a\n        # find the most commonly occurring attribute (i.e. UV coordinate)\n        # and use that index note that this is doing a float conversion\n        # and then median before converting back to int: could also do this as\n        # a column diff and sort but this seemed easier and is fast enough\n        result.append(np.median(masks, axis=0).astype(np.int64))\n\n    return result\n\n\ndef get_mesh(verts: torch.Tensor, faces: torch.Tensor, uv: torch.Tensor = None, img: torch.Tensor = None, colors: torch.Tensor = None, normals: torch.Tensor = None, filename: str = \"default.ply\"):\n    from trimesh import Trimesh\n    from trimesh.visual import TextureVisuals\n    from trimesh.visual.material import PBRMaterial, SimpleMaterial\n    from easyvolcap.utils.mesh_utils import face_normals, loop_subdivision\n\n    verts, faces = to_numpy([verts, faces])\n    verts = verts.reshape(-1, 3)\n    faces = faces.reshape(-1, 3)\n    # MARK: used process=False here to preserve vertex order\n    mesh = Trimesh(verts, faces, process=False)\n    if colors is None:\n        # colors = verts\n        colors = face_normals(torch.from_numpy(verts), torch.from_numpy(faces).long()) * 0.5 + 0.5\n    colors = to_numpy(colors)\n    colors = colors.reshape(-1, 3)\n    colors = (np.concatenate([colors, np.ones([*colors.shape[:-1], 1])], axis=-1) * 255).astype(np.uint8)\n    if len(verts) == len(colors):\n        mesh.visual.vertex_colors = colors\n    elif len(faces) == len(colors):\n        mesh.visual.face_colors = colors\n\n    if normals is not None:\n        normals = to_numpy(normals)\n        mesh.vertex_normals = normals\n\n    if uv is not None:\n        uv = to_numpy(uv)\n        uv = uv.reshape(-1, 2)\n        img = to_numpy(img)\n        img = img.reshape(*img.shape[-3:])\n        img = Image.fromarray(np.uint8(img * 255))\n        mat = SimpleMaterial(\n            image=img,\n            diffuse=(0.8, 0.8, 0.8),\n            ambient=(1.0, 1.0, 1.0),\n        )\n        mat.name = os.path.splitext(os.path.split(filename)[1])[0]\n        texture = TextureVisuals(uv=uv, material=mat)\n        mesh.visual = texture\n\n    return mesh\n\n\ndef get_tensor_mesh_data(verts: torch.Tensor, faces: torch.Tensor, uv: torch.Tensor = None, img: torch.Tensor = None, uvfaces: torch.Tensor = None):\n\n    # pytorch3d wants a tensor\n    verts, faces, uv, img, uvfaces = to_tensor([verts, faces, uv, img, uvfaces])\n    verts = verts.reshape(-1, 3)\n    faces = faces.reshape(-1, 3)\n    uv = uv.reshape(-1, 2)\n    img = img.reshape(img.shape[-3:])\n    uvfaces = uvfaces.reshape(-1, 3)\n\n    # textures = TexturesUV(img, uvfaces, uv)\n    # meshes = Meshes(verts, faces, textures)\n    return verts, faces, uv, img, uvfaces\n\n\ndef export_npz(batch: dotdict, filename: struct):\n    export_dotdict(batch, filename)\n\n\ndef export_dotdict(batch: dotdict, filename: struct):\n    batch = to_numpy(batch)\n    np.savez_compressed(filename, **batch)\n\n\ndef load_mesh(filename: str, device='cuda', load_uv=False, load_aux=False, backend='pytorch3d'):\n    from pytorch3d.io import load_ply, load_obj\n    if backend == 'trimesh':\n        import trimesh\n        mesh: trimesh.Trimesh = trimesh.load(filename)\n        return mesh.vertices, mesh.faces\n\n    vm, fm = None, None\n    if filename.endswith('.npz'):\n        mesh = np.load(filename)\n        v = torch.from_numpy(mesh['verts'])\n        f = torch.from_numpy(mesh['faces'])\n\n        if load_uv:\n            vm = torch.from_numpy(mesh['uvs'])\n            fm = torch.from_numpy(mesh['uvfaces'])\n    else:\n        if filename.endswith('.ply'):\n            v, f = load_ply(filename)\n        elif filename.endswith('.obj'):\n            v, faces_attr, aux = load_obj(filename)\n            f = faces_attr.verts_idx\n\n            if load_uv:\n                vm = aux.verts_uvs\n                fm = faces_attr.textures_idx\n        else:\n            raise NotImplementedError(f'Unrecognized input format for: {filename}')\n\n    v = v.to(device, non_blocking=True).contiguous()\n    f = f.to(device, non_blocking=True).contiguous()\n\n    if load_uv:\n        vm = vm.to(device, non_blocking=True).contiguous()\n        fm = fm.to(device, non_blocking=True).contiguous()\n\n    if load_uv:\n        if load_aux:\n            return v, f, vm, fm, aux\n        else:\n            return v, f, vm, fm\n    else:\n        return v, f\n\n\ndef load_pts(filename: str):\n    from pyntcloud import PyntCloud\n    cloud = PyntCloud.from_file(filename)\n    verts = cloud.xyz\n    if 'red' in cloud.points and 'green' in cloud.points and 'blue' in cloud.points:\n        r = np.asarray(cloud.points['red'])\n        g = np.asarray(cloud.points['green'])\n        b = np.asarray(cloud.points['blue'])\n        colors = (np.stack([r, g, b], axis=-1) / 255).astype(np.float32)\n    elif 'r' in cloud.points and 'g' in cloud.points and 'b' in cloud.points:\n        r = np.asarray(cloud.points['r'])\n        g = np.asarray(cloud.points['g'])\n        b = np.asarray(cloud.points['b'])\n        colors = (np.stack([r, g, b], axis=-1) / 255).astype(np.float32)\n    else:\n        colors = None\n\n    if 'nx' in cloud.points and 'ny' in cloud.points and 'nz' in cloud.points:\n        nx = np.asarray(cloud.points['nx'])\n        ny = np.asarray(cloud.points['ny'])\n        nz = np.asarray(cloud.points['nz'])\n        norms = np.stack([nx, ny, nz], axis=-1)\n    else:\n        norms = None\n\n    # if 'alpha' in cloud.points:\n    #     cloud.points['alpha'] = cloud.points['alpha'] / 255\n\n    reserved = ['x', 'y', 'z', 'red', 'green', 'blue', 'r', 'g', 'b', 'nx', 'ny', 'nz']\n    scalars = dotdict({k: np.asarray(cloud.points[k])[..., None] for k in cloud.points if k not in reserved})  # one extra dimension at the back added\n    return verts, colors, norms, scalars\n\n\ndef export_pts(pts: torch.Tensor, color: torch.Tensor = None, normal: torch.Tensor = None, scalars: dotdict = dotdict(), filename: str = \"default.ply\"):\n    from pandas import DataFrame\n    from pyntcloud import PyntCloud\n\n    data = dotdict()\n    pts = to_numpy(pts)  # always blocking?\n    pts = pts.reshape(-1, 3)\n    data.x = pts[:, 0].astype(np.float32)\n    data.y = pts[:, 1].astype(np.float32)\n    data.z = pts[:, 2].astype(np.float32)\n\n    if color is not None:\n        color = to_numpy(color)\n        color = color.reshape(-1, 3)\n        data.red = (color[:, 0] * 255).astype(np.uint8)\n        data.green = (color[:, 1] * 255).astype(np.uint8)\n        data.blue = (color[:, 2] * 255).astype(np.uint8)\n    else:\n        data.red = (pts[:, 0] * 255).astype(np.uint8)\n        data.green = (pts[:, 1] * 255).astype(np.uint8)\n        data.blue = (pts[:, 2] * 255).astype(np.uint8)\n\n    # if 'alpha' in scalars:\n    #     data.alpha = (scalars.alpha * 255).astype(np.uint8)\n\n    if normal is not None:\n        normal = to_numpy(normal)\n        normal = normal / (np.linalg.norm(normal, axis=-1, keepdims=True) + 1e-13)\n        normal = normal.reshape(-1, 3)\n        data.nx = normal[:, 0].astype(np.float32)\n        data.ny = normal[:, 1].astype(np.float32)\n        data.nz = normal[:, 2].astype(np.float32)\n\n    if scalars is not None:\n        scalars = to_numpy(scalars)\n        for k, v in scalars.items():\n            v = v.reshape(-1, 1)\n            data[k] = v[:, 0]\n\n    df = DataFrame(data)\n    cloud = PyntCloud(df)  # construct the data\n    dir = dirname(filename)\n    if dir: os.makedirs(dir, exist_ok=True)\n    return cloud.to_file(filename)\n\n\ndef export_lines(verts: torch.Tensor, lines: torch.Tensor, color: torch.Tensor = None, filename: str = 'default.ply'):\n    if color is None:\n        color = verts\n    verts, lines, color = to_numpy([verts, lines, color])  # always blocking?\n    if color.dtype == np.float32:\n        color = (color * 255).astype(np.uint8)\n    verts = verts.reshape(-1, 3)\n    lines = lines.reshape(-1, 2)\n    color = color.reshape(-1, 3)\n\n    # Write to PLY\n    with open(filename, 'wb') as f:\n        # PLY header\n        f.write(b\"ply\\n\")\n        f.write(b\"format binary_little_endian 1.0\\n\")\n        f.write(f\"element vertex {len(verts)}\\n\".encode())\n        f.write(b\"property float x\\n\")\n        f.write(b\"property float y\\n\")\n        f.write(b\"property float z\\n\")\n        f.write(b\"property uchar red\\n\")\n        f.write(b\"property uchar green\\n\")\n        f.write(b\"property uchar blue\\n\")\n        f.write(f\"element edge {len(lines)}\\n\".encode())\n        f.write(b\"property int vertex1\\n\")\n        f.write(b\"property int vertex2\\n\")\n        f.write(b\"end_header\\n\")\n\n        # Write vertices and colors\n        for v, c in zip(verts, color):\n            f.write(struct.pack('fffBBB', v[0], v[1], v[2], c[0], c[1], c[2]))\n\n        # Write lines\n        for l in lines:\n            f.write(struct.pack('ii', l[0], l[1]))\n\n\ndef export_camera(c2w: torch.Tensor, ixt: torch.Tensor = None, col: torch.Tensor = torch.tensor([50, 50, 200]), axis_size=0.10, filename: str = 'default.ply'):\n    verts = []\n    lines = []\n    rgbs = []\n\n    def add_line(p0: torch.Tensor, p1: torch.Tensor, col: torch.Tensor):\n        # Add a and b vertices\n        verts.append(p0)  # N, M, 3\n        verts.append(p1)  # N, M, 3\n        sh = p0.shape[:-1]\n\n        # Add the vertex colors\n        col = torch.broadcast_to(col, sh + (3,))\n        rgbs.append(col)\n        rgbs.append(col)\n\n        # Add the faces\n        new = p0.numel() // 3  # number of new elements\n        curr = new * (len(verts) - 2)  # assume all previous elements are of the same size\n        start = torch.arange(curr, curr + new)\n        end = torch.arange(curr + new, curr + new * 2)\n        line = torch.stack([start, end], dim=-1)  # NM, 2\n        line = line.view(sh + (2,))\n        lines.append(line)\n\n    c2w = c2w[..., :3, :]\n    p = c2w[..., 3]  # third row (corresponding to 3rd column)\n\n    if ixt is None: aspect = 1.0\n    else: aspect = ixt[..., 0, 0][..., None] / ixt[..., 1, 1][..., None]\n    if ixt is None: focal = 1000\n    else: focal = (ixt[..., 0, 0][..., None] + ixt[..., 1, 1][..., None]) / 2\n\n    axis_size = focal * axis_size / 1000\n    xs = axis_size * aspect\n    ys = axis_size\n    zs = axis_size * aspect * 2\n\n    a = p + xs * c2w[..., 0] + ys * c2w[..., 1] + zs * c2w[..., 2]\n    b = p - xs * c2w[..., 0] + ys * c2w[..., 1] + zs * c2w[..., 2]\n    c = p - xs * c2w[..., 0] - ys * c2w[..., 1] + zs * c2w[..., 2]\n    d = p + xs * c2w[..., 0] - ys * c2w[..., 1] + zs * c2w[..., 2]\n\n    add_line(p, p + axis_size * c2w[..., 0], torch.tensor([255, 64, 64]))\n    add_line(p, p + axis_size * c2w[..., 1], torch.tensor([64, 255, 64]))\n    add_line(p, p + axis_size * c2w[..., 2], torch.tensor([64, 64, 255]))\n    add_line(p, a, col)\n    add_line(p, b, col)\n    add_line(p, c, col)\n    add_line(p, d, col)\n    add_line(a, b, col)\n    add_line(b, c, col)\n    add_line(c, d, col)\n    add_line(d, a, col)\n\n    verts = torch.stack(verts)\n    lines = torch.stack(lines)\n    rgbs = torch.stack(rgbs)\n\n    export_lines(verts, lines, rgbs, filename=filename)\n\n\ndef export_mesh(verts: torch.Tensor, faces: torch.Tensor, uv: torch.Tensor = None, img: torch.Tensor = None, uvfaces: torch.Tensor = None, colors: torch.Tensor = None, normals: torch.Tensor = None, filename: str = \"default.ply\", subdivision=0):\n    if dirname(filename): os.makedirs(dirname(filename), exist_ok=True)\n\n    if subdivision > 0:\n        from easyvolcap.utils.mesh_utils import face_normals, loop_subdivision\n        verts, faces = loop_subdivision(verts, faces, subdivision)\n\n    if filename.endswith('.npz'):\n        def collect_args(**kwargs): return kwargs\n        kwargs = collect_args(verts=verts, faces=faces, uv=uv, img=img, uvfaces=uvfaces, colors=colors, normals=normals)\n        ret = dotdict({k: v for k, v in kwargs.items() if v is not None})\n        export_dotdict(ret, filename)\n\n    elif filename.endswith('.ply') or filename.endswith('.obj'):\n        if uvfaces is None:\n            mesh = get_mesh(verts, faces, uv, img, colors, normals, filename)\n            mesh.export(filename)\n        else:\n            from pytorch3d.io import save_obj\n            verts, faces, uv, img, uvfaces = get_tensor_mesh_data(verts, faces, uv, img, uvfaces)\n            save_obj(filename, verts, faces, verts_uvs=uv, faces_uvs=uvfaces, texture_map=img)\n    else:\n        raise NotImplementedError(f'Unrecognized input format for: {filename}')\n\n\ndef export_pynt_pts_alone(pts, color=None, filename=\"default.ply\"):\n    import pandas as pd\n    from pyntcloud import PyntCloud\n    data = {}\n\n    pts = pts if isinstance(pts, np.ndarray) else pts.detach().cpu().numpy()\n    pts = pts.reshape(-1, 3)\n    data['x'] = pts[:, 0].astype(np.float32)\n    data['y'] = pts[:, 1].astype(np.float32)\n    data['z'] = pts[:, 2].astype(np.float32)\n\n    if color is not None:\n        color = color if isinstance(color, np.ndarray) else color.detach().cpu().numpy()\n        color = color.reshape(-1, 3)\n        data['red'] = color[:, 0].astype(np.uint8)\n        data['green'] = color[:, 1].astype(np.uint8)\n        data['blue'] = color[:, 2].astype(np.uint8)\n    else:\n        data['red'] = (pts[:, 0] * 255).astype(np.uint8)\n        data['green'] = (pts[:, 1] * 255).astype(np.uint8)\n        data['blue'] = (pts[:, 2] * 255).astype(np.uint8)\n\n    df = pd.DataFrame(data)\n    cloud = PyntCloud(df)  # construct the data\n    dirname = dirname(filename)\n    if dirname: os.makedirs(dirname, exist_ok=True)\n    return cloud.to_file(filename)\n\n\ndef export_o3d_pts(pts: torch.Tensor, filename: str = \"default.ply\"):\n    import open3d as o3d\n    pts = to_numpy(pts)\n    pts = pts.reshape(-1, 3)\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(pts)\n    return o3d.io.write_point_cloud(filename, pcd)\n\n\ndef export_o3d_pcd(pts: torch.Tensor, rgb: torch.Tensor, normal: torch.Tensor, filename=\"default.ply\"):\n    import open3d as o3d\n    pts, rgb, normal = to_numpy([pts, rgb, normal])\n    pts = pts.reshape(-1, 3)\n    rgb = rgb.reshape(-1, 3)\n    normal = normal.reshape(-1, 3)\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(pts)\n    pcd.colors = o3d.utility.Vector3dVector(rgb)\n    pcd.normals = o3d.utility.Vector3dVector(normal)\n    return o3d.io.write_point_cloud(filename, pcd)\n\n\ndef export_pcd(pts: torch.Tensor, rgb: torch.Tensor, occ: torch.Tensor, filename=\"default.ply\"):\n    import pandas as pd\n    from pyntcloud import PyntCloud\n    pts, rgb, occ = to_numpy([pts, rgb, occ])\n    pts = pts.reshape(-1, 3)\n    rgb = rgb.reshape(-1, 3)\n    occ = occ.reshape(-1, 1)\n    # MARK: CloudCompare bad, set first to 0, last to 1\n    for i in range(3):\n        rgb[0, i] = 0\n        rgb[-1, i] = 1\n    occ[0, 0] = 0\n    occ[-1, 0] = 1\n\n    data = dotdict()\n    data.x = pts[:, 0]\n    data.y = pts[:, 1]\n    data.z = pts[:, 2]\n    # TODO: maybe, for compability, save color as uint?\n    # currently saving as float number from [0, 1]\n    data.red = rgb[:, 0]\n    data.green = rgb[:, 1]\n    data.blue = rgb[:, 2]\n    data.alpha = occ[:, 0]\n\n    # MARK: We're saving extra scalars for loading in CloudCompare\n    # can't assign same property to multiple fields\n    data.r = rgb[:, 0]\n    data.g = rgb[:, 1]\n    data.b = rgb[:, 2]\n    data.a = occ[:, 0]\n    df = pd.DataFrame(data)\n\n    cloud = PyntCloud(df)  # construct the data\n    dirname = dirname(filename)\n    if dirname: os.makedirs(dirname, exist_ok=True)\n    return cloud.to_file(filename)\n\n\ndef load_rgb_image(img_path) -> np.ndarray:\n    # return cv2.imread(img_path, cv2.IMREAD_COLOR)[..., ::-1].copy()  # removing the stride (for conversion to tensor)\n    return cv2.imread(img_path, cv2.IMREAD_COLOR)[..., [2, 1, 0]]  # BGR to RGB\n\n\ndef load_unchanged_image(img_path) -> np.ndarray:\n    return cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n\n\ndef load_npz(index, folder):\n    path = os.path.join(folder, f\"{index}.npz\")\n    data = np.load(path)\n    return dotdict({**data})\n\n\ndef load_dotdict(path):\n    f = np.load(path)\n    f = dotdict({**f})\n    return f\n\n\ndef start_save_npz(index, dir, param: dict, remove_batch=True):\n    return asyncio.create_task(async_save_npz(index, dir, param, remove_batch))\n\n\nasync def async_save_npz(index, dir, param: dict, remove_batch=True):\n    log(f\"Trying to save: {index}\")\n    save_npz(index, dir, param, remove_batch)\n\n\ndef save_img(index, dir, img: torch.Tensor, remove_batch=True, remap=False, flip=False):\n\n    img = to_numpy(img)\n\n    if remap:\n        img *= 255\n        img = img.astype(np.uint8)\n    if flip:\n        img = img[..., ::-1]\n\n    if remove_batch:\n        n_batch = img.shape[0]\n        for b in range(n_batch):\n            file_path = os.path.join(dir, f\"{index*n_batch + b}.png\")\n            im = img[b]\n            cv2.imwrite(file_path, im)\n    else:\n        file_path = os.path.join(dir, f\"{index}.png\")\n        cv2.imwrite(file_path, img)\n\n\ndef save_npz(index, dir, param: dict, remove_batch=False):\n    param = to_numpy(param)\n    if remove_batch:\n        n_batch = param[next(iter(param))].shape[0]\n        for b in range(n_batch):\n            file_path = os.path.join(dir, f\"{index*n_batch + b}.npz\")\n            p = {k: v[b] for k, v in param.items()}\n            np.savez_compressed(file_path, **p)\n    else:\n        file_path = os.path.join(dir, f\"{index}.npz\")\n        np.savez_compressed(file_path, **param)\n\n\ndef to_cuda(batch, device=\"cuda\", ignore_list: bool = False) -> torch.Tensor:\n    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: (to_cuda(v, device, ignore_list) if k != \"meta\" else v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch, device=device)\n    return batch\n\n\ndef to_x_if(batch, x: str, cond):\n    if isinstance(batch, (tuple, list)):\n        batch = [to_x(b, x) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_x(v, x) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        if cond(x):\n            batch = batch.to(x, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        if cond(x):\n            batch = torch.as_tensor(batch).to(x, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n\ndef to_x(batch, x: str) -> Union[torch.Tensor, dotdict[str, torch.Tensor]]:\n    if isinstance(batch, (tuple, list)):\n        batch = [to_x(b, x) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_x(v, x) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(x, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(x, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n\ndef to_tensor(batch, ignore_list: bool = False) -> Union[torch.Tensor, dotdict[str, torch.Tensor]]:\n    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_tensor(b, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_tensor(v, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        pass\n    else:  # numpy and others\n        batch = torch.as_tensor(batch)\n    return batch\n\n\ndef to_list(batch, non_blocking=False) -> Union[List, Dict, np.ndarray]:  # almost always exporting, should block\n    if isinstance(batch, (tuple, list)):\n        batch = [to_list(b, non_blocking) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_list(v, non_blocking) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.detach().to('cpu', non_blocking=non_blocking).numpy().tolist()\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.tolist()\n    else:  # others, keep as is\n        pass\n    return batch\n\n\ndef to_cpu(batch, non_blocking=False, ignore_list: bool = False) -> torch.Tensor:\n    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cpu(b, non_blocking, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cpu(v, non_blocking, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.detach().to('cpu', non_blocking=non_blocking)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch, device=\"cpu\")\n    return batch\n\n\ndef to_numpy(batch, non_blocking=False, ignore_list: bool = False) -> Union[List, Dict, np.ndarray]:  # almost always exporting, should block\n    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_numpy(b, non_blocking, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_numpy(v, non_blocking, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.detach().to('cpu', non_blocking=non_blocking).numpy()\n    else:  # numpy and others\n        batch = np.asarray(batch)\n    return batch\n\n\ndef remove_batch(batch) -> Union[torch.Tensor, np.ndarray]:\n    if isinstance(batch, (tuple, list)):\n        batch = [remove_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: remove_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[0]\n    else:\n        batch = torch.as_tensor(batch)[0]\n    return batch\n\n\n", "contexts_below": "\n\ndef add_iter(batch, iter, total) -> Union[torch.Tensor, np.ndarray]:\n    batch = add_scalar(batch, iter, name=\"iter\")\n    batch = add_scalar(batch, iter / total, name=\"frac\")\n    return batch  # training fraction and current iteration\n\n\ndef add_scalar(batch, value, name) -> Union[torch.Tensor, np.ndarray]:\n    if isinstance(batch, (tuple, list)):\n        for b in batch:\n            add_scalar(b, value, name)\n\n    if isinstance(batch, dict):\n        batch[name] = torch.tensor(value)\n        batch['meta'][name] = torch.tensor(value)\n    return batch\n\n\ndef get_voxel_grid_and_update_bounds(voxel_size: Union[List, np.ndarray], bounds: Union[List, np.ndarray]):\n    # now here's the problem\n    # 1. if you want the voxel size to be accurate, you bounds need to be changed along with this sampling process\n    #    since the F.grid_sample will treat the bounds based on align_corners=True or not\n    #    say we align corners, the actual bound on the sampled tpose blend weight should be determined by the actual sampling voxels\n    #    not the bound that we kind of used to produce the voxels, THEY DO NOT LINE UP UNLESS your bounds is divisible by the voxel size in every direction\n    # TODO: is it possible to somehow get rid of this book-keeping step\n    if isinstance(voxel_size, List):\n        voxel_size = np.array(voxel_size)\n        bounds = np.array(bounds)\n    # voxel_size: [0.005, 0.005, 0.005]\n    # bounds: n_batch, 2, 3, initial bounds\n    x = np.arange(bounds[0, 0], bounds[1, 0] + voxel_size[0] / 2, voxel_size[0])\n    y = np.arange(bounds[0, 1], bounds[1, 1] + voxel_size[1] / 2, voxel_size[1])\n    z = np.arange(bounds[0, 2], bounds[1, 2] + voxel_size[2] / 2, voxel_size[2])\n    pts = np.stack(np.meshgrid(x, y, z, indexing='ij'), axis=-1).astype(np.float32)\n    bounds = np.stack([pts[0, 0, 0], pts[-1, -1, -1]], axis=0).astype(np.float32)\n    return pts, bounds\n\n\ndef get_rigid_transform(pose: np.ndarray, joints: np.ndarray, parents: np.ndarray):\n    # pose: N, 3\n    # joints: N, 3\n    # parents: N\n    from easyvolcap.utils.blend_utils import get_rigid_transform_nobatch as net_get_rigid_transform\n    pose, joints, parents = default_convert([pose, joints, parents])\n    J, A = net_get_rigid_transform(pose, joints, parents)\n    J, A = to_numpy([J, A])\n\n    return J, A\n\n\ndef get_bounds(xyz, padding=0.05):\n    min_xyz = np.min(xyz, axis=0)\n    max_xyz = np.max(xyz, axis=0)\n    min_xyz -= padding\n    max_xyz += padding\n    bounds = np.stack([min_xyz, max_xyz], axis=0)\n    return bounds\n\n\ndef load_image_file(img_path: str, ratio=1.0):\n    if img_path.endswith('.jpg') or img_path.endswith('.JPG') or img_path.endswith('.jpeg') or img_path.endswith('.JPEG'):\n        im = Image.open(img_path)\n        w, h = im.width, im.height\n        draft = im.draft('RGB', (int(w * ratio), int(h * ratio)))\n        img = np.asarray(im)\n        if np.issubdtype(img.dtype, np.integer):\n            img = img.astype(np.float32) / np.iinfo(img.dtype).max  # normalize\n        if ratio != 1.0 and \\\n            draft is None or \\\n                draft is not None and \\\n            (draft[1][2] != int(w * ratio) or\n         draft[1][3] != int(h * ratio)):\n            img = cv2.resize(img, (int(w * ratio), int(h * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n    else:\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        if img.ndim >= 3 and img.shape[-1] >= 3:\n            img[..., :3] = img[..., [2, 1, 0]]  # BGR to RGB\n        if np.issubdtype(img.dtype, np.integer):\n            img = img.astype(np.float32) / np.iinfo(img.dtype).max  # normalize\n        if ratio != 1.0:\n            height, width = img.shape[:2]\n            img = cv2.resize(img, (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n\n\ndef load_depth(depth_file: str):\n    if depth_file.endswith('.npy'):\n        depth = np.load(depth_file)[..., None]  # H, W, 1\n    elif depth_file.endswith('.pfm'):\n        depth, scale = read_pfm(depth_file)\n        depth = depth / scale\n        if depth.ndim == 2:\n            depth = depth[..., None]  # H, W, 1\n        depth = depth[..., :1]\n    elif depth_file.endswith('.hdr') or depth_file.endswith('.exr'):\n        if depth_file.endswith('.exr'):\n            # ... https://github.com/opencv/opencv/issues/21326\n            os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n        depth = load_image(depth_file)\n        depth = depth[..., :1]\n    else:\n        raise NotImplementedError\n    return depth  # H, W, 1\n\n\ndef load_image(path: Union[str, np.ndarray], ratio: int = 1.0):\n    if isinstance(path, str):\n        return load_image_file(path, ratio)\n    elif isinstance(path, np.ndarray):\n        return load_image_from_bytes(path, ratio)\n    else:\n        raise NotImplementedError('Supported overloading')\n\n\ndef load_unchanged(img_path: str, ratio=1.0):\n    if img_path.endswith('.jpg') or img_path.endswith('.JPG') or img_path.endswith('.jpeg') or img_path.endswith('.JPEG'):\n        im = Image.open(img_path)\n        w, h = im.width, im.height\n        draft = im.draft('RGB', (int(w * ratio), int(h * ratio)))\n        img = np.asarray(im).copy()  # avoid writing error and already in RGB instead of BGR\n        if ratio != 1.0 and \\\n            draft is None or \\\n                draft is not None and \\\n            (draft[1][2] != int(w * ratio) or \\\n         draft[1][3] != int(h * ratio)):\n            img = cv2.resize(img, (int(w * ratio), int(h * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n    else:\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        if img.shape[-1] >= 3:\n            img[..., :3] = img[..., [2, 1, 0]]\n        if ratio != 1.0:\n            height, width = img.shape[:2]\n            img = cv2.resize(img, (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_AREA)\n        if img.ndim == 2:  # MARK: cv.resize will discard the last dimension of mask images\n            img = img[..., None]\n        return img\n\n\ndef load_mask(msk_path: str, ratio=1.0):\n    \"\"\"\n    Load single-channel binary mask\n    \"\"\"\n    if msk_path.endswith('.jpg') or msk_path.endswith('.JPG') or msk_path.endswith('.jpeg') or msk_path.endswith('.JPEG'):\n        msk = Image.open(msk_path)\n        w, h = msk.width, msk.height\n        draft = msk.draft('L', (int(w * ratio), int(h * ratio)))\n        msk = np.asarray(msk).astype(int)  # read the actual file content from drafted disk\n        msk = msk * 255 / msk.max()  # if max already 255, do nothing\n        msk = msk[..., None] > 128  # make it binary\n        msk = msk.astype(np.uint8)\n        if ratio != 1.0 and \\\n            draft is None or \\\n                draft is not None and \\\n            (draft[1][2] != int(w * ratio) or\n         draft[1][3] != int(h * ratio)):\n            msk = cv2.resize(msk.astype(np.uint8), (int(w * ratio), int(h * ratio)), interpolation=cv2.INTER_NEAREST)[..., None]\n        return msk\n    else:\n        msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE).astype(int)  # BGR to GRAY\n        msk = msk * 255 / msk.max()  # if max already 255, do nothing\n        msk = msk[..., None] > 128  # make it binary\n        msk = msk.astype(np.uint8)\n        if ratio != 1.0:\n            height, width = msk.shape[:2]\n            msk = cv2.resize(msk.astype(np.uint8), (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_NEAREST)[..., None]\n            # WTF: https://stackoverflow.com/questions/68502581/image-channel-missing-after-resizing-image-with-opencv\n        return msk\n\n\ndef save_unchanged(img_path: str, img: np.ndarray, quality=100, compression=6):\n    if img.shape[-1] >= 3:\n        img[..., :3] = img[..., [2, 1, 0]]\n    if img_path.endswith('.hdr'):\n        return cv2.imwrite(img_path, img)  # nothing to say about hdr\n    if dirname(img_path):\n        os.makedirs(dirname(img_path), exist_ok=True)\n    return cv2.imwrite(img_path, img, [cv2.IMWRITE_JPEG_QUALITY, quality, cv2.IMWRITE_PNG_COMPRESSION, compression])\n\n\ndef save_image(img_path: str, img: np.ndarray, jpeg_quality=75, png_compression=9, save_dtype=np.uint8):\n    if isinstance(img, torch.Tensor): img = img.detach().cpu().numpy()  # convert to numpy arrays\n    if img.ndim == 4: img = np.concatenate(img, axis=0)  # merge into one image along y axis\n    if img.ndim == 2: img = img[..., None]  # append last dim\n    if img.shape[0] < img.shape[-1] and (img.shape[0] == 3 or img.shape[0] == 4): img = np.transpose(img, (1, 2, 0))\n    if np.issubdtype(img.dtype, np.integer):\n        img = img / np.iinfo(img.dtype).max  # to float\n    if img.shape[-1] >= 3:\n        if not img.flags['WRITEABLE']:\n            img = img.copy()  # avoid assignment only inputs\n        img[..., :3] = img[..., [2, 1, 0]]\n    if dirname(img_path):\n        os.makedirs(dirname(img_path), exist_ok=True)\n    if img_path.endswith('.png'):\n        max = np.iinfo(save_dtype).max\n        img = (img * max).clip(0, max).astype(save_dtype)\n    elif img_path.endswith('.jpg'):\n        img = img[..., :3]  # only color\n        img = (img * 255).clip(0, 255).astype(np.uint8)\n    elif img_path.endswith('.hdr'):\n        img = img[..., :3]  # only color\n    elif img_path.endswith('.exr'):\n        # ... https://github.com/opencv/opencv/issues/21326\n        os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n    else:\n        # should we try to discard alpha channel here?\n        # exr could store alpha channel\n        pass  # no transformation for other unspecified file formats\n    # log(f'Writing image to: {img_path}')\n    # breakpoint()\n    return cv2.imwrite(img_path, img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality,\n                                       cv2.IMWRITE_PNG_COMPRESSION, png_compression,\n                                       cv2.IMWRITE_EXR_COMPRESSION, cv2.IMWRITE_EXR_COMPRESSION_PIZ])\n\n\ndef save_mask(msk_path: str, msk: np.ndarray, quality=75, compression=9):\n    if dirname(msk_path):\n        os.makedirs(dirname(msk_path), exist_ok=True)\n    if msk.ndim == 2:\n        msk = msk[..., None]\n    return cv2.imwrite(msk_path, msk[..., 0] * 255, [cv2.IMWRITE_JPEG_QUALITY, quality,\n                                                     cv2.IMWRITE_PNG_COMPRESSION, compression,\n                                                     cv2.IMWRITE_EXR_COMPRESSION, cv2.IMWRITE_EXR_COMPRESSION_PIZ])\n\n\ndef list_to_numpy(x: list): return np.stack(x).transpose(0, 3, 1, 2)\n\n\ndef numpy_to_list(x: np.ndarray): return [y for y in x.transpose(0, 2, 3, 1)]\n\n\ndef list_to_tensor(x: list, device='cuda'): return torch.from_numpy(list_to_numpy(x)).to(device, non_blocking=True)  # convert list of numpy arrays of HWC to BCHW\n\n\ndef tensor_to_list(x: torch.Tensor): return numpy_to_list(x.detach().cpu().numpy())  # convert tensor of BCHW to list of numpy arrays of HWC\n\n\ndef project(xyz, K, RT):\n    \"\"\"\n    xyz: [N, 3]\n    K: [3, 3]\n    RT: [3, 4]\n    \"\"\"\n    xyz = np.dot(xyz, RT[:, :3].T) + RT[:, 3:].T\n    xyz = np.dot(xyz, K.T)\n    xy = xyz[:, :2] / xyz[:, 2:]\n    return xy\n\n\ndef unproject(depth, K, R, T):\n    H, W = depth.shape\n    i, j = np.meshgrid(np.arange(W, dtype=np.float32),\n                       np.arange(H, dtype=np.float32),\n                       indexing='xy')\n    xy1 = np.stack([i, j, np.ones_like(i)], axis=2)\n    xyz = xy1 * depth[..., None]\n    pts3d = np.dot(xyz, np.linalg.inv(K).T)\n    pts3d = np.dot(pts3d - T.ravel(), R)\n    return pts3d\n\n\ndef read_mask_by_img_path(data_root: str, img_path: str, erode_dilate_edge: bool = False, mask: str = '') -> np.ndarray:\n    def read_mask_file(path):\n        msk = load_mask(path).astype(np.uint8)\n        if len(msk.shape) == 3:\n            msk = msk[..., 0]\n        return msk\n\n    if mask:\n        msk_path = os.path.join(data_root, img_path.replace('images', mask))\n        if not os.path.exists(msk_path):\n            msk_path = os.path.join(data_root, img_path.replace('images', mask)) + '.png'\n        if not os.path.exists(msk_path):\n            msk_path = os.path.join(data_root, img_path.replace('images', mask))[:-4] + '.png'\n        if not os.path.exists(msk_path):\n            log(f'warning: defined mask path {msk_path} does not exist', 'yellow')\n    else:\n        msk_path = os.path.join(data_root, 'mask', img_path)[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, 'mask', img_path)[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, 'mask_cihp', img_path)[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'merged_mask'))[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'rvm'))[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'rvm'))[:-4] + '.jpg'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'mask'))[:-4] + '.png'\n    if not os.path.exists(msk_path):  # background matte v2\n        msk_path = os.path.join(data_root, img_path.replace('images', 'bgmt'))[:-4] + '.png'\n    if not os.path.exists(msk_path):\n        msk_path = os.path.join(data_root, img_path.replace('images', 'mask'))[:-4] + '.jpg'\n    if not os.path.exists(msk_path):\n        log(f'cannot find mask file: {msk_path}, using all ones', 'yellow')\n        img = load_unchanged_image(os.path.join(data_root, img_path))\n        msk = np.ones_like(img[:, :, 0]).astype(np.uint8)\n        return msk\n\n    msk = read_mask_file(msk_path)\n    # erode edge inconsistence when evaluating and training\n    if erode_dilate_edge:  # eroding edge on matte might erode the actual human\n        msk = fill_mask_edge_with(msk)\n\n    return msk\n\n\ndef fill_mask_edge_with(msk, border=5, value=100):\n    msk = msk.copy()\n    kernel = np.ones((border, border), np.uint8)\n    msk_erode = cv2.erode(msk.copy(), kernel)\n    msk_dilate = cv2.dilate(msk.copy(), kernel)\n    msk[(msk_dilate - msk_erode) == 1] = value\n    return msk\n\n\ndef get_rays_within_bounds_rendering(H, W, K, R, T, bounds):\n    ray_o, ray_d = get_rays(H, W, K, R, T)\n\n    ray_o = ray_o.reshape(-1, 3).astype(np.float32)\n    ray_d = ray_d.reshape(-1, 3).astype(np.float32)\n    near, far, mask_at_box = get_full_near_far(bounds, ray_o, ray_d)\n    near = near.reshape(H, W)\n    far = far.reshape(H, W)\n    ray_o = ray_o.reshape(H, W, 3)\n    ray_d = ray_d.reshape(H, W, 3)\n    mask_at_box = mask_at_box.reshape(H, W)\n\n    return ray_o, ray_d, near, far, mask_at_box\n\n\ndef get_rays(H, W, K, R, T):\n    # # calculate the camera origin\n    # ray_o = -np.dot(R.T, T).ravel()\n    # # calculate the world coodinates of pixels\n    # i, j = np.meshgrid(np.arange(H, dtype=np.float32),\n    #                    np.arange(W, dtype=np.float32),\n    #                    indexing='ij')  # 0.5 indicates pixel center\n    # i = i + 0.5\n    # j = j + 0.5\n    # # 0->H, 0->W\n    # xy1 = np.stack([j, i, np.ones_like(i)], axis=2)\n    # if subpixel:\n    #     rand = np.random.rand(H, W, 2) - 0.5\n    #     xy1[:, :, :2] += rand\n    # pixel_camera = np.dot(xy1, np.linalg.inv(K).T)\n    # pixel_world = np.dot(pixel_camera - T.ravel(), R)\n    # # calculate the ray direction\n    # ray_d = pixel_world - ray_o[None, None]\n    # ray_d = ray_d / np.linalg.norm(ray_d, axis=2, keepdims=True)\n    # ray_o = np.broadcast_to(ray_o, ray_d.shape)\n    # return ray_o, ray_d\n\n    from easyvolcap.utils.ray_utils import get_rays\n    K, R, T = to_tensor([K, R, T])\n    ray_o, ray_d = get_rays(H, W, K, R, T)\n    ray_o, ray_d = to_numpy([ray_o, ray_d])\n    return ray_o, ray_d\n\n\ndef get_near_far(bounds, ray_o, ray_d) -> Tuple[np.ndarray, np.ndarray]:\n    # \"\"\"\n    # calculate intersections with 3d bounding box\n    # return: near, far (indexed by mask_at_box (bounding box mask))\n    # \"\"\"\n    # near, far, mask_at_box = get_full_near_far(bounds, ray_o, ray_d)\n    # norm_d = np.linalg.norm(ray_d, axis=-1, keepdims=True)\n    # near = near[mask_at_box] / norm_d[mask_at_box, 0]\n    # far = far[mask_at_box] / norm_d[mask_at_box, 0]\n    # return near, far, mask_at_box\n    from easyvolcap.utils.ray_utils import get_near_far_aabb\n    bounds, ray_o, ray_d = to_tensor([bounds, ray_o, ray_d])  # no copy\n    near, far = get_near_far_aabb(bounds, ray_o, ray_d)\n    near, far = to_numpy([near, far])\n    return near, far\n\n\ndef get_full_near_far(bounds, ray_o, ray_d):\n    \"\"\"calculate intersections with 3d bounding box\"\"\"\n    norm_d = np.linalg.norm(ray_d, axis=-1, keepdims=True)\n    viewdir = ray_d / norm_d\n    viewdir[(viewdir < 1e-5) & (viewdir > -1e-10)] = 1e-5\n    viewdir[(viewdir > -1e-5) & (viewdir < 1e-10)] = -1e-5\n    tmin = (bounds[:1] - ray_o[:1]) / viewdir\n    tmax = (bounds[1:2] - ray_o[:1]) / viewdir\n    t1 = np.minimum(tmin, tmax)\n    t2 = np.maximum(tmin, tmax)\n    near = np.max(t1, axis=-1)\n    far = np.min(t2, axis=-1)\n    mask_at_box = near < far\n    near = near / norm_d[..., 0]\n    far = far / norm_d[..., 0]\n    return near, far, mask_at_box\n\n\ndef full_sample_ray(img, msk, K, R, T, bounds, split='train', subpixel=False):\n    H, W = img.shape[:2]\n    ray_o, ray_d = get_rays(H, W, K, R, T, subpixel)\n    near, far, mask_at_box = get_full_near_far(bounds, ray_o, ray_d)\n    msk = msk * mask_at_box\n    coords = np.argwhere(np.ones_like(mask_at_box))  # every pixel\n    ray_o = ray_o[coords[:, 0], coords[:, 1]].astype(np.float32)\n    ray_d = ray_d[coords[:, 0], coords[:, 1]].astype(np.float32)\n    near = near[coords[:, 0], coords[:, 1]].astype(np.float32)\n    far = far[coords[:, 0], coords[:, 1]].astype(np.float32)\n    rgb = img[coords[:, 0], coords[:, 1]].astype(np.float32)\n    return rgb, ray_o, ray_d, near, far, coords, mask_at_box\n\n\ndef affine_inverse(m: np.ndarray):\n    import torch\n    from easyvolcap.utils.math_utils import affine_inverse\n    return affine_inverse(torch.from_numpy(m)).numpy()\n\n\ndef load_image_from_bytes(buffer: np.ndarray, ratio=1.0, normalize=False, decode_flag=cv2.IMREAD_UNCHANGED):\n    # from nvjpeg import NvJpeg\n    # if not hasattr(load_image_from_bytes, 'nj'):\n    #     load_image_from_bytes.nj = NvJpeg()\n    # nj: NvJpeg = load_image_from_bytes.nj\n\n    def normalize_image(image):\n        image = torch.from_numpy(image)  # pytorch is significantly faster than np\n        if image.ndim >= 3 and image.shape[-1] >= 3:\n            image[..., :3] = image[..., [2, 1, 0]]\n        image = image / torch.iinfo(image.dtype).max\n        image = image.float()\n        return image.numpy()\n\n    if isinstance(buffer, BytesIO):\n        buffer = buffer.getvalue()  # slow? copy?\n    if isinstance(buffer, memoryview) or isinstance(buffer, bytes):\n        buffer = np.frombuffer(buffer, np.uint8)\n    if isinstance(buffer, torch.Tensor):\n        buffer = buffer.numpy()\n    buffer = buffer.astype(np.uint8)\n    image: np.ndarray = cv2.imdecode(buffer, decode_flag)  # MARK: 10-15ms\n    # image: np.ndarray = nj.decode(np.frombuffer(buffer, np.uint8))  # MARK: 10-15ms\n    # if decode_flag == cv2.IMREAD_GRAYSCALE:\n    # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    if image.ndim == 2:\n        image = image[..., None]\n\n    if normalize:\n        image = normalize_image(image)  # MARK: 3ms\n\n    height, width = image.shape[:2]\n    if ratio != 1.0:\n        image = cv2.resize(image, (int(width * ratio), int(height * ratio)), interpolation=cv2.INTER_AREA)\n    return image\n\n\ndef as_torch_func(func):\n    def wrapper(*args, **kwargs):\n        args = to_numpy(args)\n        kwargs = to_numpy(kwargs)\n        ret = func(*args, **kwargs)\n        return to_tensor(ret)\n    return wrapper\n\n\ndef as_numpy_func(func):\n    def wrapper(*args, **kwargs):\n        args = to_tensor(args)\n        kwargs = to_tensor(kwargs)\n        ret = func(*args, **kwargs)\n        return to_numpy(ret)\n    return wrapper\n\n\ndef load_image_bytes(im: str):\n    if im.endswith('.exr'):\n        os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n    with open(im, \"rb\") as fh:\n        buffer = fh.read()\n    return buffer\n\n\nclass UnstructuredTensors(torch.Tensor):\n    # https://github.com/pytorch/pytorch/issues/13246#issuecomment-617140519\n    # https://github.com/pytorch/pytorch/issues/69893\n    @staticmethod\n    def __new__(cls, bytes: Union[List[np.ndarray], List[torch.Tensor], np.ndarray], **kwargs):\n        \"\"\"\n        Creates a new UnstructuredTensors object from the given bytes.\n\n        Args:\n        - bytes (Union[List[np.ndarray], List[torch.Tensor], np.ndarray]): The bytes to create the object from.\n\n        Returns:\n        - self (UnstructuredTensors): The new UnstructuredTensors object.\n        \"\"\"\n        if isinstance(bytes, UnstructuredTensors):\n            return bytes\n        # Prepare the bytes array\n        if isinstance(bytes, np.ndarray):\n            bytes = [b for b in bytes]\n        if bytes[0].dtype == object:\n            bytes = [b.astype(np.uint8) for b in bytes]\n        bytes = to_tensor(bytes)  # now, every element is a list\n        dtype = torch.uint8\n        if len(bytes):\n            dtype = bytes[0].dtype\n\n        # Create an empty tensor\n        self = torch.Tensor.__new__(cls).to(dtype)\n\n        # Remember accessing related configs\n        self.set_(torch.cat(bytes))  # flatten # sum(N)\n        self.lengths = torch.as_tensor([len(b) for b in bytes], dtype=torch.int32)  # N,\n        self.cumsums = torch.cat([torch.as_tensor([0]), torch.cumsum(self.lengths, dim=0)[:-1]])\n        return self\n\n    @property\n    def is_unstructured(self): return hasattr(self, 'lengths')\n\n    def __getitem__(self, index: int):\n        \"\"\"\n        Returns a slice of the UnstructuredTensors object corresponding to the given index.\n\n        Args:\n            index (int): The index of the slice to return.\n\n        Returns:\n            torch.Tensor: A slice of the UnstructuredTensors object corresponding to the given index.\n\n        This function returns a slice of the UnstructuredTensors object corresponding to the given index. The slice is obtained by using the cumulative sums and lengths of the underlying bytes array to determine the start and end indices of the slice. If the index is out of range, the function returns the corresponding element of the underlying bytes array. This function is used to implement the indexing behavior of the UnstructuredTensors object, allowing it to be treated like a regular tensor.\n        \"\"\"\n        if self.is_unstructured:\n            return torch.Tensor.__getitem__(self, slice(self.cumsums[index], self.cumsums[index] + self.lengths[index]))\n        else:\n            return super().__getitem__(index)\n\n    def __len__(self):\n        if self.is_unstructured:\n            return len(self.lengths)\n        else:\n            return super().__len__()\n\n    def clone(self, *args, **kwargs):\n        if self.is_unstructured:\n            return UnstructuredTensors([self[i] for i in range(len(self.lengths))])  # manual cloning with copy and reconstruction\n        else:\n            return super().clone(*args, **kwargs)\n\n\ndef load_ims_bytes_from_disk(ims: np.ndarray, desc=\"Loading image bytes from disk\"):\n    sh = ims.shape\n    ims = ims.ravel()\n    ims_bytes = parallel_execution(list(ims), action=load_image_bytes, desc=desc, print_progress=True)\n    ims_bytes = np.asarray(ims_bytes).reshape(sh)  # reorganize shapes\n    return ims_bytes\n\n\ndef load_resize_undist_im_bytes(imp: str,\n                                K: np.ndarray,\n                                D: np.ndarray,\n                                ratio: Union[float, List[int]] = 1.0,\n                                center_crop_size: List[int] = [-1, -1],\n                                encode_ext='.jpg',\n                                decode_flag=cv2.IMREAD_UNCHANGED,\n                                dist_opt_K: bool = False,\n                                jpeg_quality: int = 100,\n                                png_compression: int = 6\n                                ):\n    # Load image -> resize -> undistort -> save to bytes (jpeg)\n    img = load_image_from_bytes(load_image_bytes(imp), decode_flag=decode_flag)[..., :3]  # cv2 decoding (fast)\n\n    oH, oW = img.shape[:2]\n\n    if dist_opt_K:\n        newCameraMatrix, _ = cv2.getOptimalNewCameraMatrix(K, D, (oW, oH), 0, (oW, oH))\n        img = cv2.undistort(img, K, D, newCameraMatrix=newCameraMatrix)\n        K = newCameraMatrix\n    else:\n        img = cv2.undistort(img, K, D)\n\n    # Maybe update image size\n    if not ((isinstance(ratio, float) and ratio == 1.0)):\n        if isinstance(ratio, float):\n            H, W = int(oH * ratio), int(oW * ratio)\n        else:\n            H, W = ratio  # ratio is actually the target image size\n        rH, rW = H / oH, W / oW\n        K = K.copy()\n        K[0:1] = K[0:1] * rW  # K[0, 0] *= rW\n        K[1:2] = K[1:2] * rH  # K[1, 1] *= rH\n\n        img = cv2.resize(img, (W, H), interpolation=cv2.INTER_AREA)  # H, W, 3, uint8\n\n    # Crop the image and intrinsic matrix if specified\n    if center_crop_size[0] > 0:\n        img, K, H, W = center_crop_img_ixt(img, K, H, W, center_crop_size)\n\n    is_success, buffer = cv2.imencode(encode_ext, img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])\n\n    if 'H' not in locals(): H, W = oH, oW\n    return buffer, K, H, W\n\n\ndef center_crop_img_ixt(img: np.ndarray, K: np.ndarray, H: int, W: int,\n                        center_crop_size: Union[int, List[int]]):\n    # Parse the original size and the target crop size\n    oH, oW = H, W\n    if isinstance(center_crop_size, int): cH, cW = center_crop_size, center_crop_size\n    else: cH, cW = center_crop_size\n\n    # Compute left and right crop size for height and width respectively\n    hlc, wlc = int((oH - cH) * 0.5), int((oW - cW) * 0.5)\n    hrc, wrc = oH - cH - hlc, oW - cW - wlc\n\n    # Crop the image\n    if hlc != 0: img = img[hlc:-hrc, :]\n    if wlc != 0: img = img[:, wlc:-wrc]\n\n    # Crop the intrinsic matrix\n    if hlc != 0: K[1, 2] -= hlc\n    if wlc != 0: K[0, 2] -= wlc\n\n    return img, K, cH, cW\n\n\ndef load_resize_undist_ims_bytes(ims: np.ndarray,\n                                 Ks: np.ndarray,\n                                 Ds: np.ndarray,\n                                 ratio: Union[float, List[int], List[float]] = 1.0,\n                                 center_crop_size: List[int] = [-1, -1],\n                                 desc=\"Loading image bytes from disk\",\n                                 **kwargs):\n    sh = ims.shape  # V, N\n    # Ks = np.broadcast_to(Ks[:, None], (sh + (3, 3)))\n    # Ds = np.broadcast_to(Ds[:, None], (sh + (1, 5)))\n\n    ims = ims.reshape((np.prod(sh)))\n    # from easyvolcap.utils.dist_utils import get_rank\n    # if not get_rank(): __import__('easyvolcap.utils.console_utils', fromlist=['debugger']).debugger()\n    # else:\n    #     while 1: pass\n    Ks = Ks.reshape((np.prod(sh), 3, 3))\n    Ds = Ds.reshape((np.prod(sh), 1, 5))\n\n    ims = list(ims)\n    Ks = list(Ks)\n    Ds = list(Ds)  # only convert outer most dim to list\n\n    if isinstance(ratio, list) and len(ratio) and isinstance(ratio[0], float):\n        ratio = np.broadcast_to(np.asarray(ratio)[:, None], sh)  # V, N\n        ratio = ratio.reshape((np.prod(sh)))\n        ratio = list(ratio)\n    elif isinstance(ratio, list):\n        ratio = np.asarray(ratio)  # avoid expansion in parallel execution\n\n    if isinstance(center_crop_size, list):\n        center_crop_size = np.asarray(center_crop_size)  # avoid expansion\n\n    # Should we batch these instead of loading?\n    out = parallel_execution(ims, Ks, Ds, ratio, center_crop_size,\n                             action=load_resize_undist_im_bytes,\n                             desc=desc, print_progress=True,\n                             **kwargs,\n                             )\n\n    ims_bytes, Ks, Hs, Ws = zip(*out)  # is this OK?\n    ims_bytes, Ks, Hs, Ws = np.asarray(ims_bytes, dtype=object), np.asarray(Ks), np.asarray(Hs), np.asarray(Ws)\n    # ims_bytes = ims_bytes.reshape(sh)  # numpy array of bytesio\n    Hs = Hs.reshape(sh)  # should all be the same?\n    Ws = Ws.reshape(sh)  # should all be the same?\n    Ks = Ks.reshape(sh + (3, 3))  # should all be the same?\n\n    return ims_bytes, Ks, Hs, Ws\n\n\ndef decode_crop_fill_im_bytes(im_bytes: BytesIO,\n                              mk_bytes: BytesIO,\n                              K: np.ndarray,\n                              R: np.ndarray,\n                              T: np.ndarray,\n                              bounds: np.ndarray,\n                              encode_ext=['.jpg', '.jpg'],\n                              decode_flag=cv2.IMREAD_UNCHANGED,\n                              jpeg_quality: int = 100,\n                              png_compression: int = 6,\n                              **kwargs):\n    # im_bytes: a series of jpeg bytes for the image\n    # mk_bytes: a series of jpeg bytes for the mask\n    # K: 3, 3 intrinsics matrix\n\n    # Use load_image_from_bytes to decode and update jpeg streams\n    img = load_image_from_bytes(im_bytes, decode_flag=decode_flag)  # H, W, 3\n    msk = load_image_from_bytes(mk_bytes, decode_flag=decode_flag)  # H, W, 3\n\n    # Crop both mask and the image using bbox's 2D projection\n    H, W, _ = img.shape\n    from easyvolcap.utils.bound_utils import get_bound_2d_bound\n    bx, by, bw, bh = as_numpy_func(get_bound_2d_bound)(bounds, K, R, T, H, W)\n    img = img[by:by + bh, bx:bx + bw]\n    msk = msk[by:by + bh, bx:bx + bw]\n\n    # Crop the image using the bounding rect of the mask\n    mx, my, mw, mh = cv2.boundingRect((msk > 128).astype(np.uint8))  # array data type = 0 is not supported\n    img = img[my:my + mh, mx:mx + mw]\n    msk = msk[my:my + mh, mx:mx + mw]\n\n    # Update the final size and intrinsics\n    x, y, w, h = bx + mx, by + my, mw, mh  # w and h will always be the smaller one, xy will be accumulated\n    K[0, 2] -= x\n    K[1, 2] -= y\n\n    # Fill the image with black (premultiply by mask)\n    img = (img * (msk / 255)).clip(0, 255).astype(np.uint8)  # fill with black, indexing starts at the front\n\n    # Reencode the videos and masks\n    if isinstance(encode_ext, str): encode_ext = [encode_ext] * 2  # '.jpg' -> ['.jpg', '.jpg']\n    im_bytes = cv2.imencode(encode_ext[0], img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])[1]  # is_sucess, bytes_array\n    mk_bytes = cv2.imencode(encode_ext[1], msk, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])[1]  # is_sucess, bytes_array\n    return im_bytes, mk_bytes, K, h, w, x, y\n\n\ndef decode_crop_fill_ims_bytes(ims_bytes: np.ndarray, mks_bytes: np.ndarray, Ks: np.ndarray, Rs: np.ndarray, Ts: np.ndarray, bounds: np.ndarray,\n                               desc=\"Cropping images using mask\", **kwargs):\n    sh = Ks.shape[:2]  # V, N\n    # ims_bytes = ims_bytes.reshape((np.prod(sh)))\n    # mks_bytes = mks_bytes.reshape((np.prod(sh)))\n    Ks = Ks.reshape((np.prod(sh), 3, 3))\n    Rs = Rs.reshape((np.prod(sh), 3, 3))\n    Ts = Ts.reshape((np.prod(sh), 3, 1))\n    bounds = bounds.reshape((np.prod(sh), 2, 3))\n\n    # Should we batch these instead of loading?\n    out = parallel_execution(list(ims_bytes), list(mks_bytes), list(Ks), list(Rs), list(Ts), list(bounds),\n                             action=decode_crop_fill_im_bytes,\n                             desc=desc, print_progress=True,\n                             **kwargs,\n                             )\n\n    ims_bytes, mks_bytes, Ks, Hs, Ws, xs, ys = zip(*out)  # is this OK?\n    ims_bytes, mks_bytes, Ks, Hs, Ws, xs, ys = np.asarray(ims_bytes, dtype=object), np.asarray(mks_bytes, dtype=object), np.asarray(Ks), np.asarray(Hs), np.asarray(Ws), np.asarray(xs), np.asarray(ys)\n    # ims_bytes = ims_bytes.reshape(sh)\n    # mks_bytes = mks_bytes.reshape(sh)\n    Hs = Hs.reshape(sh)  # should all be the same?\n    Ws = Ws.reshape(sh)  # should all be the same?\n    Ks = Ks.reshape(sh + (3, 3))  # should all be the same?\n    xs = xs.reshape(sh)  # should all be the same?\n    ys = ys.reshape(sh)  # should all be the same?\n\n    return ims_bytes, mks_bytes, Ks, Hs, Ws, xs, ys\n\n\ndef decode_fill_im_bytes(im_bytes: BytesIO,\n                         mk_bytes: BytesIO,\n                         encode_ext='.jpg',\n                         decode_flag=cv2.IMREAD_UNCHANGED,\n                         jpeg_quality: int = 100,\n                         png_compression: int = 6,\n                         **kwargs):\n    # im_bytes: a series of jpeg bytes for the image\n    # mk_bytes: a series of jpeg bytes for the mask\n    # K: 3, 3 intrinsics matrix\n\n    # Use load_image_from_bytes to decode and update jpeg streams\n    img = load_image_from_bytes(im_bytes, decode_flag=decode_flag)  # H, W, 3\n    msk = load_image_from_bytes(mk_bytes, decode_flag=decode_flag)  # H, W, 3\n\n    img = (img * (msk / 255)).clip(0, 255).astype(np.uint8)  # fill with black, indexing starts at the front\n\n    im_bytes = cv2.imencode(encode_ext, img, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality, cv2.IMWRITE_PNG_COMPRESSION, png_compression])[1]  # is_sucess, bytes_array\n    return im_bytes\n\n\ndef decode_fill_ims_bytes(ims_bytes: np.ndarray,\n                          mks_bytes: np.ndarray,\n                          desc=\"Filling images using mask\",\n                          **kwargs):\n    sh = ims_bytes.shape  # V, N\n    ims_bytes = ims_bytes.reshape((np.prod(sh)))\n    mks_bytes = mks_bytes.reshape((np.prod(sh)))\n\n    # Should we batch these instead of loading?\n    ims_bytes = parallel_execution(list(ims_bytes), list(mks_bytes),\n                                   action=decode_fill_im_bytes,\n                                   desc=desc, print_progress=True,\n                                   **kwargs,\n                                   )\n\n    ims_bytes = np.asarray(ims_bytes, dtype=object)\n    ims_bytes = ims_bytes.reshape(sh)\n    return ims_bytes\n\n\ndef batch_rodrigues(poses):\n    \"\"\" poses: N x 3\n    \"\"\"\n    batch_size = poses.shape[0]\n    angle = np.linalg.norm(poses + 1e-8, axis=1, keepdims=True)\n    rot_dir = poses / angle\n\n    cos = np.cos(angle)[:, None]\n    sin = np.sin(angle)[:, None]\n\n    rx, ry, rz = np.split(rot_dir, 3, axis=1)\n    zeros = np.zeros([batch_size, 1])\n    K = np.concatenate([zeros, -rz, ry, rz, zeros, -rx, -ry, rx, zeros],\n                       axis=1)\n    K = K.reshape([batch_size, 3, 3])\n\n    ident = np.eye(3)[None]\n    rot_mat = ident + sin * K + (1 - cos) * np.matmul(K, K)\n\n    return rot_mat.astype(np.float32)\n\n\ndef get_rigid_transformation_and_joints(poses, joints, parents):\n    \"\"\"\n    poses: n_bones x 3\n    joints: n_bones x 3\n    parents: n_bones\n    \"\"\"\n\n    n_bones = len(joints)\n    rot_mats = batch_rodrigues(poses)\n\n    # Obtain the relative joints\n    rel_joints = joints.copy()\n    rel_joints[1:] -= joints[parents[1:]]\n\n    # Create the transformation matrix\n    # First rotate then transform\n    transforms_mat = np.concatenate([rot_mats, rel_joints[..., None]], axis=2)\n    padding = np.zeros([n_bones, 1, 4])\n    padding[..., 3] = 1\n    transforms_mat = np.concatenate([transforms_mat, padding], axis=1)\n\n    # Rotate each part\n    # But this is a world transformation, with displacement...?\n    transform_chain = [transforms_mat[0]]\n    for i in range(1, parents.shape[0]):  # assuming parents are in topological order\n        curr_res = np.dot(transform_chain[parents[i]], transforms_mat[i])  # THEY'RE RIGHT, LEARN FORWARD KINEMATICS\n        transform_chain.append(curr_res)\n    transforms = np.stack(transform_chain, axis=0)\n\n    # Obtain the rigid transformation\n    # AND THIS WEIRD STUFF IS TRYING TO MOVE VERTEX FROM VERTEX COORDINATES TO JOINT COORDINATES\n    # AND THIS IS THE CORRECT IMPLEMENTATION...\n\n    # THIS IS JUST TOO CLEVER...\n    # These three lines is effectively doing: transforms = transforms * (negative trarslation matrix for all joints)\n    joints_vector = np.concatenate([joints, np.zeros([n_bones, 1])], axis=1)\n    rot_joints = np.sum(transforms * joints_vector[:, None], axis=2)  # This is effectively matmul\n    transforms[..., 3] = transforms[..., 3] - rot_joints  # add in the translation, we should translate first\n\n    joints_points = np.concatenate([joints, np.ones([n_bones, 1])], axis=1)\n    pose_joints = np.sum(transforms * joints_points[:, None], axis=2)  # This is effectively matmul\n\n    transforms = transforms.astype(np.float32)\n    return transforms, pose_joints[:, :3]\n\n\ndef get_rigid_transformation(poses, joints, parents):\n    \"\"\"\n    poses: n_bones x 3\n    joints: n_bones x 3\n    parents: n_bones\n    \"\"\"\n    transforms = get_rigid_transformation_and_joints(poses, joints, parents)[0]\n    return transforms\n\n\ndef padding_bbox_HW(bbox, h, w):\n    padding = 10\n    bbox[0] = bbox[0] - 10\n    bbox[1] = bbox[1] + 10\n\n    height = bbox[1, 1] - bbox[0, 1]\n    width = bbox[1, 0] - bbox[0, 0]\n    # a magic number of pytorch3d\n    ratio = 1.5\n\n    if height / width > ratio:\n        min_size = int(height / ratio)\n        if width < min_size:\n            padding = (min_size - width) // 2\n            bbox[0, 0] = bbox[0, 0] - padding\n            bbox[1, 0] = bbox[1, 0] + padding\n\n    if width / height > ratio:\n        min_size = int(width / ratio)\n        if height < min_size:\n            padding = (min_size - height) // 2\n            bbox[0, 1] = bbox[0, 1] - padding\n            bbox[1, 1] = bbox[1, 1] + padding\n\n    bbox[:, 0] = np.clip(bbox[:, 0], a_min=0, a_max=w - 1)\n    bbox[:, 1] = np.clip(bbox[:, 1], a_min=0, a_max=h - 1)\n\n    return bbox\n\n\ndef padding_bbox(bbox, img):\n    return padding_bbox_HW(bbox, *img.shape[:2])\n\n\ndef get_crop_box(H, W, K, ref_msk):\n    x, y, w, h = cv2.boundingRect(ref_msk)\n    bbox = np.array([[x, y], [x + w, y + h]])\n    bbox = padding_bbox_HW(bbox, H, W)\n\n    # revise the intrinsic camera matrix\n    K = K.copy()\n    K[0, 2] = K[0, 2] - bbox[0, 0]\n    K[1, 2] = K[1, 2] - bbox[0, 1]\n    K = K.astype(np.float32)\n\n    return K, bbox\n\n\ndef crop_image_msk(img, msk, K, ref_msk):\n    x, y, w, h = cv2.boundingRect(ref_msk)\n    bbox = np.array([[x, y], [x + w, y + h]])\n    bbox = padding_bbox(bbox, img)\n\n    crop = img[bbox[0, 1]:bbox[1, 1], bbox[0, 0]:bbox[1, 0]]\n    crop_msk = msk[bbox[0, 1]:bbox[1, 1], bbox[0, 0]:bbox[1, 0]]\n\n    # calculate the shape\n    shape = crop.shape\n    x = 8\n    height = (crop.shape[0] | (x - 1)) + 1\n    width = (crop.shape[1] | (x - 1)) + 1\n\n    # align image\n    aligned_image = np.zeros([height, width, 3])\n    aligned_image[:shape[0], :shape[1]] = crop\n    aligned_image = aligned_image.astype(np.float32)\n\n    # align mask\n    aligned_msk = np.zeros([height, width])\n    aligned_msk[:shape[0], :shape[1]] = crop_msk\n    aligned_msk = (aligned_msk == 1).astype(np.uint8)\n\n    # revise the intrinsic camera matrix\n    K = K.copy()\n    K[0, 2] = K[0, 2] - bbox[0, 0]\n    K[1, 2] = K[1, 2] - bbox[0, 1]\n    K = K.astype(np.float32)\n\n    return aligned_image, aligned_msk, K, bbox\n\n\ndef random_crop_image(img, msk, K, min_size, max_size):\n    # sometimes we sample regions with no valid pixel at all, this can be problematic for the training loop\n    # there's an assumption that the `msk` is always inside `mask_at_box`\n    # thus, if we're sampling inside the `msk`, we'll always be getting the correct results\n    H, W = img.shape[:2]\n    min_HW = min(H, W)\n    min_HW = min(min_HW, max_size)\n\n    max_size = min_HW\n    # min_size = int(min(min_size, 0.8 * min_HW))\n    if max_size < min_size:\n        H_size = np.random.randint(min_size, max_size)\n    else:\n        H_size = min_size\n\n    W_size = H_size\n    x = 8\n    H_size = (H_size | (x - 1)) + 1\n    W_size = (W_size | (x - 1)) + 1\n\n    # randomly select begin_x and begin_y\n    coords = np.argwhere(msk == 1)\n    center_xy = coords[np.random.randint(0, len(coords))][[1, 0]]\n    min_x, min_y = center_xy[0] - W_size // 2, center_xy[1] - H_size // 2\n    max_x, max_y = min_x + W_size, min_y + H_size\n    if min_x < 0:\n        min_x, max_x = 0, W_size\n    if max_x > W:\n        min_x, max_x = W - W_size, W\n    if min_y < 0:\n        min_y, max_y = 0, H_size\n    if max_y > H:\n        min_y, max_y = H - H_size, H\n\n    # crop image and mask\n    begin_x, begin_y = min_x, min_y\n    img = img[begin_y:begin_y + H_size, begin_x:begin_x + W_size]\n    msk = msk[begin_y:begin_y + H_size, begin_x:begin_x + W_size]\n\n    # revise the intrinsic camera matrix\n    K = K.copy()\n    K[0, 2] = K[0, 2] - begin_x\n    K[1, 2] = K[1, 2] - begin_y\n    K = K.astype(np.float32)\n\n    return img, msk, K\n\n\ndef get_bound_corners(bounds):\n    min_x, min_y, min_z = bounds[0]\n    max_x, max_y, max_z = bounds[1]\n    corners_3d = np.asarray([\n        [min_x, min_y, min_z],\n        [min_x, min_y, max_z],\n        [min_x, max_y, min_z],\n        [min_x, max_y, max_z],\n        [max_x, min_y, min_z],\n        [max_x, min_y, max_z],\n        [max_x, max_y, min_z],\n        [max_x, max_y, max_z],\n    ], dtype=np.float32)\n    return corners_3d\n\n\ndef get_bound_2d_mask(bounds, K, RT, H, W):\n    corners_3d = get_bound_corners(bounds)\n    corners_2d = project(corners_3d, K, RT)\n    corners_2d = np.round(corners_2d).astype(int)\n    mask = np.zeros((H, W), dtype=np.uint8)\n    cv2.fillPoly(mask, [corners_2d[[0, 1, 3, 2, 0]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[4, 5, 7, 6, 5]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[0, 1, 5, 4, 0]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[2, 3, 7, 6, 2]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[0, 2, 6, 4, 0]]], 1)\n    cv2.fillPoly(mask, [corners_2d[[1, 3, 7, 5, 1]]], 1)\n    return mask\n\n\ndef get_bounds(xyz, box_padding=0.05):\n    min_xyz = np.min(xyz, axis=0)\n    max_xyz = np.max(xyz, axis=0)\n    min_xyz -= box_padding\n    max_xyz += box_padding\n    bounds = np.stack([min_xyz, max_xyz], axis=0)\n    bounds = bounds.astype(np.float32)\n    return bounds\n\n\ndef crop_mask_edge(msk):\n    msk = msk.copy()\n    border = 10\n    kernel = np.ones((border, border), np.uint8)\n    msk_erode = cv2.erode(msk.copy(), kernel)\n    msk_dilate = cv2.dilate(msk.copy(), kernel)\n    msk[(msk_dilate - msk_erode) == 1] = 100\n    return msk\n\n\ndef adjust_hsv(img, saturation, brightness, contrast):\n    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    hsv = hsv.astype(np.float32)\n    hsv[..., 1] = hsv[..., 1] * saturation\n    hsv[..., 1] = np.minimum(hsv[..., 1], 255)\n    hsv[..., 2] = hsv[..., 2] * brightness\n    hsv[..., 2] = np.minimum(hsv[..., 2], 255)\n    hsv = hsv.astype(np.uint8)\n    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n    img = img.astype(np.float32) * contrast\n    img = np.minimum(img, 255)\n    img = img.astype(np.uint8)\n    return img\n", "input_code": "def add_batch(batch) -> Union[torch.Tensor, np.ndarray]:\n\n    \"\"\"\n    This function recursively adds a new dimension to the input data structure (which can be a batch of data in various formats such as tuple, list, dictionary, torch.Tensor, or numpy.ndarray) at the zeroth position. It is designed to handle nested data structures by applying the same operation to each element.\n\n    Input-Output Arguments\n    :param batch: Union[tuple, list, dict, torch.Tensor, np.ndarray]. The input data which can be a single item or a batch of items in various formats. The function processes this data by adding a new dimension to it or its elements.\n    :return: Union[torch.Tensor, np.ndarray]. The modified batch with an added dimension at the zeroth position. This return type ensures compatibility with PyTorch and NumPy data structures.\n    \"\"\"", "reference_steps": "1. Define a function `add_batch` that takes a single argument `batch`. The function should be able to return either a `torch.Tensor` or a `np.ndarray`.\n\n2. Check if the input `batch` is an instance of either a `tuple` or a `list`.\n\n3. If `batch` is a `tuple` or a `list`, recursively apply the `add_batch` function to each element in the `batch` and return the result as a list.\n\n4. If `batch` is a `dict`, create a new `dotdict` (a dictionary that supports dot notation) with the same keys.\n\n5. For each key-value pair in the `dict`, recursively apply the `add_batch` function to the value and assign it to the corresponding key in the new `dotdict`.\n\n6. If `batch` is an instance of `torch.Tensor` or `np.ndarray`, add a new dimension at index 0 using `None` (equivalent to `np.newaxis` or `unsqueeze(0)` in PyTorch).\n\n7. If `batch` is not a `tuple`, `list`, `dict`, `torch.Tensor`, or `np.ndarray`, convert it to a `torch.Tensor` and add a new dimension at index 0.\n\n8. Return the modified `batch` after applying the appropriate transformation based on its type.\n\n9. Ensure that the function can handle nested structures by applying the transformation recursively.\n\n10. The function should be flexible enough to handle various input types and structures, adding a batch dimension to tensors and arrays, and appropriately wrapping other data types.", "reference_code": "def add_batch(batch) -> Union[torch.Tensor, np.ndarray]:\n    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "type": "method", "class_name": "Camera", "function_name": "to_batch", "dependency_all": "# Intra-class Dependency:\neasyvolcap.utils.viewer_utils.Camera.H\n\neasyvolcap.utils.viewer_utils.Camera.K\n\neasyvolcap.utils.viewer_utils.Camera.R\n\neasyvolcap.utils.viewer_utils.Camera.T\n\neasyvolcap.utils.viewer_utils.Camera.W\n\neasyvolcap.utils.viewer_utils.Camera.bounds\n\neasyvolcap.utils.viewer_utils.Camera.f\n\neasyvolcap.utils.viewer_utils.Camera.mass\n\neasyvolcap.utils.viewer_utils.Camera.moment_of_inertia\n\neasyvolcap.utils.viewer_utils.Camera.movement_force\n\neasyvolcap.utils.viewer_utils.Camera.movement_speed\n\neasyvolcap.utils.viewer_utils.Camera.movement_torque\n\neasyvolcap.utils.viewer_utils.Camera.n\n\neasyvolcap.utils.viewer_utils.Camera.origin\n\neasyvolcap.utils.viewer_utils.Camera.t\n\neasyvolcap.utils.viewer_utils.Camera.v\n\neasyvolcap.utils.viewer_utils.Camera.world_up\n\n# Intra-file Dependency:\neasyvolcap.utils.base_utils.dotdict.mass\n\neasyvolcap.utils.base_utils.dotdict.moment_of_inertia\n\neasyvolcap.utils.base_utils.dotdict.movement_force\n\neasyvolcap.utils.base_utils.dotdict.movement_speed\n\neasyvolcap.utils.base_utils.dotdict.movement_torque\n\neasyvolcap.utils.base_utils.dotdict.origin\n\neasyvolcap.utils.base_utils.dotdict.world_up\n\n# Cross-file Dependency:\neasyvolcap.utils.base_utils.dotdict\n    class dotdict(dict, Dict[KT, VT]):\n        \"\"\"\n        This is the default data passing object used throughout the codebase\n        Main function: dot access for dict values & dict like merging and updates\n\n        a dictionary that supports dot notation \n        as well as dictionary access notation \n        usage: d = make_dotdict() or d = make_dotdict{'val1':'first'})\n        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n        get attributes: d.val2 or d['val2']\n        \"\"\"\n\neasyvolcap.utils.base_utils.dotdict.H\n\neasyvolcap.utils.base_utils.dotdict.K\n\neasyvolcap.utils.base_utils.dotdict.R\n\neasyvolcap.utils.base_utils.dotdict.T\n\neasyvolcap.utils.base_utils.dotdict.W\n\neasyvolcap.utils.base_utils.dotdict.bounds\n\neasyvolcap.utils.base_utils.dotdict.f\n\neasyvolcap.utils.base_utils.dotdict.meta\n    def meta(self) -> dotdict:\n\neasyvolcap.utils.base_utils.dotdict.n\n\neasyvolcap.utils.base_utils.dotdict.t\n\neasyvolcap.utils.base_utils.dotdict.update\n    def update(self, dct: Dict = None, **kwargs):\n\neasyvolcap.utils.base_utils.dotdict.v\n\n", "dependency_sampled": "# Intra-class Dependency:\neasyvolcap.utils.viewer_utils.Camera.origin\n\neasyvolcap.utils.viewer_utils.Camera.f\n\neasyvolcap.utils.viewer_utils.Camera.bounds\n\neasyvolcap.utils.viewer_utils.Camera.movement_torque\n\neasyvolcap.utils.viewer_utils.Camera.moment_of_inertia\n\neasyvolcap.utils.viewer_utils.Camera.K\n\neasyvolcap.utils.viewer_utils.Camera.t\n\neasyvolcap.utils.viewer_utils.Camera.T\n\neasyvolcap.utils.viewer_utils.Camera.n\n\n# Intra-file Dependency:\neasyvolcap.utils.base_utils.dotdict.movement_torque\n\neasyvolcap.utils.base_utils.dotdict.movement_force\n\neasyvolcap.utils.base_utils.dotdict.world_up\n\neasyvolcap.utils.base_utils.dotdict.mass\n\n# Cross-file Dependency:\neasyvolcap.utils.base_utils.dotdict\n    class dotdict(dict, Dict[KT, VT]):\n        \"\"\"\n        This is the default data passing object used throughout the codebase\n        Main function: dot access for dict values & dict like merging and updates\n\n        a dictionary that supports dot notation \n        as well as dictionary access notation \n        usage: d = make_dotdict() or d = make_dotdict{'val1':'first'})\n        set attributes: d.val2 = 'second' or d['val2'] = 'second'\n        get attributes: d.val2 or d['val2']\n        \"\"\"\n\neasyvolcap.utils.base_utils.dotdict.t\n\neasyvolcap.utils.base_utils.dotdict.R\n\neasyvolcap.utils.base_utils.dotdict.bounds\n\neasyvolcap.utils.base_utils.dotdict.H\n\n", "contexts_above": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from imgui_bundle import imgui\n    from easyvolcap.runners.volumetric_video_viewer import VolumetricVideoViewer\n\nimport glm\nimport torch\nimport numpy as np\n\nfrom os.path import join\nfrom scipy import interpolate\nfrom copy import copy, deepcopy\n\nfrom glm import vec2, vec3, vec4, mat3, mat4, mat4x3, mat2x3  # This is actually highly optimized\n\nfrom easyvolcap.utils.console_utils import *\nfrom easyvolcap.utils.base_utils import dotdict\nfrom easyvolcap.utils.easy_utils import read_camera, write_camera\nfrom easyvolcap.utils.math_utils import normalize, affine_inverse\nfrom easyvolcap.utils.data_utils import to_numpy, to_tensor, to_cuda, to_list\nfrom easyvolcap.utils.cam_utils import gen_cubic_spline_interp_func, gen_linear_interp_func\n\n\ndef debug_project(proj: mat4, a: vec3, aa: \"imgui.ImVec2\"):\n    # proj: 4, 4\n    # a: 3,\n    a: vec4 = proj @ vec4(a, 1.0)  # 4, 4 @ 4 = 4\n    if a.w <= 0.02:  # depth should be positive to save upfront\n        return False\n    else:\n        aa.x, aa.y = a.x / a.w, a.y / a.w\n        return True\n\n\ndef add_debug_line(proj: mat4, a: vec3, b: vec3, col: np.uint32 = 0xffffffff, thickness: float = 2.0):\n    # proj: world to screen transformation matrix: 4, 4\n    # a: 3,\n    # b: 3,\n    from imgui_bundle import imgui\n    from easyvolcap.utils.imgui_utils import col2imu32\n\n    draw_list: imgui.ImDrawList = imgui.get_background_draw_list()\n    aa, bb = imgui.ImVec2(), imgui.ImVec2()\n    if debug_project(proj, a, aa) and debug_project(proj, b, bb):\n        draw_list.add_line(aa, bb, col2imu32(col), thickness)\n\n\ndef add_debug_text(proj: mat4, a: vec3, text: str, col: np.uint32 = 0xffffffff):\n    # proj: world to screen transformation matrix: 4, 4\n    # a: 3,\n    # text: str\n    from imgui_bundle import imgui\n    from easyvolcap.utils.imgui_utils import col2imu32\n\n    draw_list: imgui.ImDrawList = imgui.get_background_draw_list()\n    aa = imgui.ImVec2()\n    if debug_project(proj, a, aa):\n        draw_list.add_text(aa, col2imu32(col), text)\n\n\ndef add_debug_text_2d(aa: \"imgui.ImVec2\", text: str, col: np.uint32 = 0xff4040ff):\n    from imgui_bundle import imgui\n    from easyvolcap.utils.imgui_utils import col2imu32\n\n    draw_list: imgui.ImDrawList = imgui.get_background_draw_list()\n    draw_list.add_text(aa, col2imu32(col), text)\n\n\ndef visualize_axes(proj: mat4, a: vec3, b: vec3, thickness=3.0, name: str = None):  # bounds in world coordinates\n    add_debug_text(proj, vec3(b.x + 0.025, a.y, a.z + 0.045), 'x', 0xccccccff)\n    add_debug_text(proj, vec3(a.x, b.y + 0.025, a.z + 0.045), 'y', 0xccccccff)\n    add_debug_text(proj, vec3(a.x, a.y, b.z + 0.025 + 0.045), 'z', 0xccccccff)\n    add_debug_line(proj, vec3(a.x, a.y, a.z), vec3(b.x, a.y, a.z), 0xff4040ff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, a.y, a.z), vec3(a.x, b.y, a.z), 0x40ff40ff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, a.y, a.z), vec3(a.x, a.y, b.z), 0x4040ffff, thickness=thickness)\n\n    if name is not None: add_debug_text(proj, a + vec3(0.045), str(name), 0xccccccff)  # maybe mark the cameras\n\n\ndef visualize_cube(proj: mat4, a: vec3, b: vec3, thickness=3.0, name: str = None):  # bounds in world coordinates\n    add_debug_line(proj, vec3(a.x, a.y, a.z), vec3(b.x, a.y, a.z), 0xff4040ff, thickness=thickness)  # X\n    add_debug_line(proj, vec3(a.x, b.y, a.z), vec3(b.x, b.y, a.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, a.y, b.z), vec3(b.x, a.y, b.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, b.y, b.z), vec3(b.x, b.y, b.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, a.y, a.z), vec3(a.x, b.y, a.z), 0x40ff40ff, thickness=thickness)  # Y\n    add_debug_line(proj, vec3(b.x, a.y, a.z), vec3(b.x, b.y, a.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, a.y, b.z), vec3(a.x, b.y, b.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(b.x, a.y, b.z), vec3(b.x, b.y, b.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, a.y, a.z), vec3(a.x, a.y, b.z), 0x4040ffff, thickness=thickness)  # Z\n    add_debug_line(proj, vec3(b.x, a.y, a.z), vec3(b.x, a.y, b.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(a.x, b.y, a.z), vec3(a.x, b.y, b.z), 0xffffffff, thickness=thickness)\n    add_debug_line(proj, vec3(b.x, b.y, a.z), vec3(b.x, b.y, b.z), 0xffffffff, thickness=thickness)\n\n    if name is not None: add_debug_text(proj, a + vec3(0.045), str(name), 0xffcccccc)  # maybe mark the cameras\n\n\ndef visualize_cameras(proj: mat4, ixt: mat3, c2w: mat4x3, axis_size: float = 0.10, col: np.uint32 = 0x80ffffff, thickness: float = 2.0, name: str = None):\n    p = c2w[3]  # third row (corresponding to 3rd column)\n    focal = (ixt[0, 0] + ixt[1, 1]) / 2\n    axis_size = focal * axis_size / 1000\n\n    aspect = ixt[0, 0] / ixt[1, 1]\n    xs = axis_size * aspect\n    ys = axis_size\n    zs = axis_size * aspect * 2\n\n    a = p + xs * c2w[0] + ys * c2w[1] + zs * c2w[2]\n    b = p - xs * c2w[0] + ys * c2w[1] + zs * c2w[2]\n    c = p - xs * c2w[0] - ys * c2w[1] + zs * c2w[2]\n    d = p + xs * c2w[0] - ys * c2w[1] + zs * c2w[2]\n\n    add_debug_line(proj, p, a, col, thickness)\n    add_debug_line(proj, p, b, col, thickness)\n    add_debug_line(proj, p, c, col, thickness)\n    add_debug_line(proj, p, d, col, thickness)\n    add_debug_line(proj, a, b, col, thickness)\n    add_debug_line(proj, b, c, col, thickness)\n    add_debug_line(proj, c, d, col, thickness)\n    add_debug_line(proj, d, a, col, thickness)\n\n    add_debug_line(proj, p, p + axis_size * c2w[0], 0xff4040ff, thickness)\n    add_debug_line(proj, p, p + axis_size * c2w[1], 0x40ff40ff, thickness)\n    add_debug_line(proj, p, p + axis_size * c2w[2], 0x4040ffff, thickness)\n\n    if name is not None: add_debug_text(proj, p, str(name), 0xccccccff)  # maybe mark the cameras\n\n\nclass CameraPath:\n    # This is the Model in the EVC gui designs\n\n    # Basic a list of cameras with interpolations\n    # Use the underlying Camera class as backbone\n    # Will export to a sequence of cameras extri.yml and intri.yml\n    # Will support keyframes based manipulations:\n    # 1. Adding current view as key frame\n    # 2. Jumping to previous keyframe (resize window as well?) (toggle edit state?) (or just has a replace button)\n    #    - Snap to current keyframe?\n    #    - Editing would be much better (just a button to replace the selected keyframe)\n    # 3. Toggle playing animation of this keyframe (supports some degress of control)\n    # 4. Export the animation as a pair of extri.yml and intri.yml\n    # 5. imguizmo control of the created camera in the list (translation, rotation etc)\n    def __init__(self,\n                 playing: bool = False,\n                 playing_time: float = 0.5,\n                 playing_speed: float = 0.0005,\n\n                 n_render_views: int = 100,\n                 render_plots: bool = True,\n\n                 # Visualization related\n                 visible: bool = True,\n                 name: str = 'camera_path',\n                 filename: str = '',\n                 plot_thickness: float = 8.0,\n                 camera_thickness: float = 6.0,\n                 plot_color: int = 0x80ff80ff,\n                 camera_color: int = 0x80ffffff,\n                 camera_axis_size: float = 0.10,\n\n                 **kwargs,\n                 ) -> None:\n        self.keyframes: List[Camera] = []  # orders matter\n        self.playing_time = playing_time  # range: 0-1\n\n        self.playing = playing  # is this playing? update cam if it is\n        self.playing_speed = playing_speed  # faster interpolation time\n        self.n_render_views = n_render_views\n        self.render_plots = render_plots\n\n        # Private\n        self.cursor_index = -1  # the camera to edit\n        self.periodic = True\n\n        # Visualization\n        self.name = name\n        self.visible = visible\n        self.plot_thickness = plot_thickness\n        self.camera_thickness = camera_thickness\n        self.plot_color = plot_color\n        self.camera_color = camera_color\n        self.camera_axis_size = camera_axis_size\n        if filename:\n            self.load_keyframes(filename)\n\n    def __len__(self):\n        return len(self.keyframes)\n\n    @property\n    def loop_interp(self):\n        return self.periodic\n\n    @loop_interp.setter\n    def loop_interp(self, v: bool):\n        changed = self.periodic != v\n        self.periodic = v\n        if changed: self.update()  # only perform heavy operation after change\n\n    @property\n    def selected(self):\n        return self.cursor_index\n\n    @selected.setter\n    def selected(self, v: int):\n        if v >= len(self): return\n        if not len(self): self.cursor_index = -1; return\n        self.cursor_index = range(len(self))[v]\n        denom = (len(self) - 1)\n        if denom: self.playing_time = self.cursor_index / denom  # 1 means last frame\n        else: self.playing_time = 0.5\n\n    def replace(self, camera: Camera):\n        self.keyframes[self.selected] = deepcopy(camera)\n        self.update()\n\n    def insert(self, camera: Camera):\n        self.keyframes = self.keyframes[:self.selected + 1] + [deepcopy(camera)] + self.keyframes[self.selected + 1:]\n        self.selected = self.selected + 1\n        self.update()\n\n    def delete(self, index: int):\n        del self.keyframes[index]\n        self.selected = self.selected - 1  # go back one\n        self.update()\n\n    def clear(self):\n        self.keyframes.clear()\n        self.selected = -1\n\n    def update(self):\n        # MARK: HEAVY\n        K = len(self.keyframes)\n        if K <= 3: return\n\n        # Prepare for linear and extrinsic parameters\n        ks = np.asarray([c.K.to_list() for c in self.keyframes]).transpose(0, 2, 1).reshape(K, -1)  # 9\n        hs = np.asarray([c.H for c in self.keyframes]).reshape(K, -1)\n        ws = np.asarray([c.W for c in self.keyframes]).reshape(K, -1)\n        ns = np.asarray([c.n for c in self.keyframes]).reshape(K, -1)\n        fs = np.asarray([c.f for c in self.keyframes]).reshape(K, -1)\n        ts = np.asarray([c.t for c in self.keyframes]).reshape(K, -1)\n        vs = np.asarray([c.v for c in self.keyframes]).reshape(K, -1)\n        bs = np.asarray([c.bounds.to_list() for c in self.keyframes]).reshape(K, -1)  # 6\n        lins = np.concatenate([ks, hs, ws, ns, fs, ts, vs, bs], axis=-1)  # K, D\n        c2ws = np.asarray([c.c2w.to_list() for c in self.keyframes]).transpose(0, 2, 1)  # K, 3, 4\n\n        # Recompute interpolation parameters\n        self.lin_func = gen_linear_interp_func(lins, smoothing_term=0.0 if self.periodic else 10.0)  # smoothness: 0 -> period, >0 -> non-period, -1 orbit (not here)\n        self.c2w_func = gen_cubic_spline_interp_func(c2ws, smoothing_term=0.0 if self.periodic else 10.0)\n\n    def interp(self, us: float, **kwargs):\n        K = len(self.keyframes)\n        if K <= 3: return\n\n        # MARK: HEAVY?\n        # Actual interpolation\n        lin = self.lin_func(us)\n        c2w = self.c2w_func(us)\n\n        # Extract linear parameters\n        K = torch.as_tensor(lin[:9]).view(3, 3)  # need a transpose\n        H = int(lin[9])\n        W = int(lin[10])\n        n = torch.as_tensor(lin[11], dtype=torch.float)\n        f = torch.as_tensor(lin[12], dtype=torch.float)\n        t = torch.as_tensor(lin[13], dtype=torch.float)\n        v = torch.as_tensor(lin[14], dtype=torch.float)\n        bounds = torch.as_tensor(lin[15:]).view(2, 3)  # no need for transpose\n\n        # Extract splined parameters\n        w2c = affine_inverse(torch.as_tensor(c2w))  # already float32\n        R = w2c[:3, :3]\n        T = w2c[:3, 3:]\n\n        return H, W, K, R, T, n, f, t, v, bounds\n\n    def export_keyframes(self, path: str):\n        # Store keyframes to path\n        cameras = {f'{i:06d}': k.to_easymocap() for i, k in enumerate(self.keyframes)}\n        write_camera(cameras, path)  # without extri.yml, only dirname\n        log(yellow(f'Keyframes saved to: {blue(path)}'))\n\n    def load_keyframes(self, path: str):\n        # Store keyframes to path\n        cameras = read_camera(join(path, 'intri.yml'), join(path, 'extri.yml'))\n        cameras = dotdict({k: cameras[k] for k in sorted(cameras.keys())})  # assuming dict is ordered (python 3.7+)\n        self.keyframes = [Camera().from_easymocap(cam) for cam in cameras.values()]\n        self.name = path\n        self.update()\n\n    def export_interps(self, path: str):\n        # Store interpolations (animation) to path\n        us = np.linspace(0, 1, self.n_render_views, dtype=np.float32)\n\n        cameras = dotdict()\n        for i, u in enumerate(tqdm(us, desc='Exporting interpolated cameras')):\n            cameras[f'{i:06d}'] = self.interp(u).to_easymocap()\n        write_camera(cameras, path)  # without extri.yml, only dirname\n        log(yellow(f'Interpolated cameras saved to: {blue(path)}'))\n\n    def render_imgui(self, viewer: 'VolumetricVideoViewer', batch: dotdict):\n        # from easyvolcap.utils.gl_utils import Mesh\n        # Mesh.render_imgui(self, viewer, batch)\n        from imgui_bundle import imgui\n        from easyvolcap.utils.imgui_utils import push_button_color, pop_button_color, col2rgba, col2vec4, vec42col, list2col, col2imu32\n\n        i = batch.i\n        will_delete = batch.will_delete\n        slider_width = batch.slider_width\n\n        imgui.push_item_width(slider_width * 0.5)\n        self.name = imgui.input_text(f'Mesh name##{i}', self.name)[1]\n        self.n_render_views = imgui.slider_int(f'Plot samples##{i}', self.n_render_views, 0, 3000)[1]\n        self.plot_thickness = imgui.slider_float(f'Plot thickness##{i}', self.plot_thickness, 0.01, 10.0)[1]\n        self.camera_thickness = imgui.slider_float(f'Camera thickness##{i}', self.camera_thickness, 0.01, 10.0)[1]\n        self.camera_axis_size = imgui.slider_float(f'Camera axis size##{i}', self.camera_axis_size, 0.01, 1.0)[1]\n\n        self.plot_color = list2col(imgui.color_edit4(f'Plot color##{i}', col2vec4(self.plot_color), flags=imgui.ColorEditFlags_.no_inputs.value)[1])\n        self.camera_color = list2col(imgui.color_edit4(f'Camera color##{i}', col2vec4(self.camera_color), flags=imgui.ColorEditFlags_.no_inputs.value)[1])\n\n        push_button_color(0x55cc33ff if not self.render_plots else 0x8855aaff)\n        if imgui.button(f'No Plot##{i}' if not self.render_plots else f' Plot ##{i}'):\n            self.render_plots = not self.render_plots\n        pop_button_color()\n\n        imgui.same_line()\n        push_button_color(0x55cc33ff if not self.visible else 0x8855aaff)\n        if imgui.button(f'Show##{i}' if not self.visible else f'Hide##{i}'):\n            self.visible = not self.visible\n        pop_button_color()\n\n        # Render the delete button\n        imgui.same_line()\n        push_button_color(0xff5533ff)\n        if imgui.button(f'Delete##{i}'):\n            will_delete.append(i)\n        pop_button_color()\n\n        # The actual rendering\n        self.draw(viewer.camera)\n\n    def draw(self, camera: Camera):\n\n        # The actual rendering starts here, the camera paths are considered GUI elements for eaiser management\n        # This rendering pattern is extremly slow and hard on the CPU, but whatever for now, just visualization\n        if not self.visible: return\n        if not len(self): return\n        proj = camera.w2p  # 3, 4\n\n        # Render cameras\n        for i, cam in enumerate(self.keyframes):\n            ixt = cam.ixt\n            c2w = cam.c2w\n            c2w = mat4x3(c2w)  # vis cam only supports this\n\n            # Add to imgui rendering list\n            visualize_cameras(proj, ixt, c2w, col=self.camera_color, thickness=self.camera_thickness, axis_size=self.camera_axis_size)\n\n        if self.render_plots and len(self) >= 4:\n            us = np.linspace(0, 1, self.n_render_views, dtype=np.float32)\n            c2ws = self.c2w_func(us)\n            cs = c2ws[..., :3, 3]  # N, 3\n            for i, c in enumerate(cs):\n                if i == 0:\n                    p = c  # previous\n                    continue\n                add_debug_line(proj, vec3(*p), vec3(*c), col=self.plot_color, thickness=self.plot_thickness)\n                p = c\n\n    def render(self, camera: Camera):\n        pass\n\n\nclass Camera:\n    # Helper class to manage camera parameters\n    def __init__(self,\n                 H: int = 512,\n                 W: int = 512,\n                 K: torch.Tensor = torch.tensor([[512.0, 0.0, 256], [0.0, 512.0, 256.0], [0.0, 0.0, 1.0]]),  # intrinsics\n                 R: torch.Tensor = torch.tensor([[-0.9977766275405884, 0.06664637476205826, 0.0], [0.004728599451482296, 0.07079283893108368, -0.9974799156188965], [-0.0664784237742424, -0.9952622056007385, -0.07095059007406235]]),  # extrinsics\n                 T: torch.Tensor = torch.tensor([[-2.059340476989746e-5], [2.5779008865356445e-6], [-3.000047445297241]]),  # extrinsics\n                 n: float = 0.002,  # bounds limit\n                 f: float = 100,  # bounds limit\n                 t: float = 0.0,  # temporal dimension (implemented as a float instead of int)\n                 v: float = 0.0,  # view dimension (implemented as a float instead of int)\n                 bounds: torch.Tensor = torch.tensor([[-1.0, -1.0, -1.0], [1.0, 1.0, 1.0]]),  # bounding box\n\n                 # camera update hyperparameters\n                 origin: torch.Tensor = torch.tensor([0.0, 0.0, 0.0]),\n                 world_up: torch.Tensor = torch.tensor([0.0, 0.0, 1.0]),\n                 movement_speed: float = 1.0,  # gui movement speed\n                 movement_force: float = 1.0,  # include some physiscs\n                 drag_coeff_mult: float = 1.0,  # include some physiscs\n                 constant_drag: float = 1.0,\n                 mass: float = 0.1,\n                 moment_of_inertia: float = 0.1,\n                 movement_torque: float = 1.0,\n                 angular_friction: float = 2.0,\n                 constant_torque: float = 1.0,\n\n                 min_interval: float = 0.0334,  # simulate at at least 30 fps\n                 pause_physics: bool = False,\n\n                 batch: dotdict = None,  # will ignore all other inputs\n                 string: str = None,  # will ignore all other inputs\n                 **kwargs,\n                 ) -> None:\n\n        # Batch (network input parameters)\n        if string is None:\n            if batch is None:\n                batch = dotdict()\n                batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = H, W, K, R, T, n, f, t, v, bounds\n            self.from_batch(batch)\n\n            # Other configurables\n            self.origin = vec3(*origin)\n            # self.origin = self.center  # rotate about center\n            self.world_up = vec3(*world_up)\n            self.movement_speed = movement_speed\n            # self.front = self.front  # will trigger an update\n        else:\n            self.from_string(string)\n\n        # Internal states to facilitate camera position change\n        self.is_dragging = False  # rotation\n        self.about_origin = False  # about origin rotation\n        self.is_panning = False  # translation\n        self.lock_fx_fy = True\n        self.drag_start = vec2(0.0)\n\n        # Internal states to facilitate moving with mass\n        self.mass = mass\n        self.force = vec3(0.0)\n        self.speed = vec3(0.0)  # no movement\n        self.acc = vec3(0.0)\n        self.drag_coeff_mult = drag_coeff_mult\n        self.movement_force = movement_force\n        self.constant_drag = constant_drag\n        self.pause_physics = pause_physics\n        self.min_interval = min_interval\n\n        self.torque = vec3(0.0)\n        self.moment_of_inertia = moment_of_inertia\n        self.angular_speed = vec3(0.0)  # relative angular speed on three euler angles\n        self.angular_acc = vec3(0.0)\n        self.angular_friction = angular_friction\n        self.constant_torque = constant_torque\n        self.movement_torque = movement_torque\n\n    def step(self, interval: float):\n        if self.pause_physics: return\n\n        # Limit interval to make the simulation more stable\n        interval = min(interval, self.min_interval)\n\n        # Compute the drag force\n        speed2 = glm.dot(self.speed, self.speed)\n        if speed2 > 1.0:\n            # Drag at opposite direction of movement\n            drag = -speed2 * (self.speed / speed2) * self.drag_coeff_mult\n        elif speed2 > 0:\n            # Constant drag if speed is blow a threshold to make it stop faster\n            drag = -self.constant_drag * self.speed\n        else:\n            drag = vec3(0.0)\n\n        # Compute acceleration and final speed\n        self.acc = (self.force + drag) / self.mass\n        self.speed += self.acc * interval\n\n        # Compute displacement in this interval\n        speed2 = glm.dot(self.speed, self.speed)\n        if speed2 > 0:\n            direction = mat3(self.right, -glm.normalize(glm.cross(self.right, self.world_up)), self.world_up)\n            movement = direction @ (self.speed - self.acc * interval / 2) * interval\n            self.center += movement\n\n        # Compute rotation change\n\n        # Compute the drag torque\n        speed2 = glm.dot(self.angular_speed, self.angular_speed)\n        if speed2 > 0.1:\n            # Drag at opposite direction of movement\n            drag = -speed2 * (self.angular_speed / speed2) * self.angular_friction\n        elif speed2 > 0.0:\n            # Constant drag if speed is blow a threshold to make it stop faster\n            drag = -self.constant_torque * self.angular_speed\n        else:\n            drag = vec3(0.0)\n\n        # Compute angular acceleration and final angular speed\n        self.angular_acc = (self.torque + drag) / self.moment_of_inertia\n        self.angular_speed += self.angular_acc * interval\n\n        # Angular movement direction\n        delta = self.angular_speed * interval  # about x, y and z axis (euler angle)\n\n        # Limit look up\n        dot = glm.dot(self.world_up, self.front)\n        self.drag_ymin = -np.arccos(-dot) + 0.01  # drag up, look down\n        self.drag_ymax = np.pi + self.drag_ymin - 0.02  # remove the 0.01 of drag_ymin\n\n        # Rotate about euler angle\n        EPS = 1e-7\n        if abs(delta.x) > EPS or abs(delta.y) > EPS or abs(delta.z) > EPS:\n            m = mat4(1.0)\n            m = glm.rotate(m, np.clip(delta.x, self.drag_ymin, self.drag_ymax), self.right)\n            m = glm.rotate(m, delta.y, -self.world_up)\n            m = glm.rotate(m, delta.z, self.front)\n            center = self.center\n            self.front = m @ self.front  # might overshoot and will update center\n            self.center = center\n\n    @property\n    def w2p(self):\n        ixt = mat4(self.ixt)\n        ixt[3, 3] = 0\n        ixt[2, 3] = 1\n        return ixt @ self.ext  # w2c -> c2p = w2p\n\n    @property\n    def V(self): return self.c2w\n\n    @property\n    def ixt(self): return self.K\n\n    @property\n    def gl_ext(self):\n        gl_c2w = self.c2w\n        gl_c2w[0] *= 1  # do notflip x\n        gl_c2w[1] *= -1  # flip y\n        gl_c2w[2] *= -1  # flip z\n        gl_ext = glm.affineInverse(gl_c2w)\n        return gl_ext  # use original opencv ext since we've taken care of the intrinsics in gl_ixt\n\n    @property\n    def gl_ixt(self):\n        # Construct opengl camera matrix with projection & clipping\n        # https://fruty.io/2019/08/29/augmented-reality-with-opencv-and-opengl-the-tricky-projection-matrix/\n        # https://gist.github.com/davegreenwood/3a32d779f81f08dce32f3bb423672191\n        # fmt: off\n        gl_ixt = mat4(\n                      2 * self.fx / self.W,                          0,                                       0,  0,\n                       2 * self.s / self.W,       2 * self.fy / self.H,                                       0,  0,\n                1 - 2 * (self.cx / self.W), 2 * (self.cy / self.H) - 1,   (self.f + self.n) / (self.n - self.f), -1,\n                                         0,                          0, 2 * self.f * self.n / (self.n - self.f),  0,\n        )\n        # fmt: on\n\n        return gl_ixt\n\n    @property\n    def ext(self): return self.w2c\n\n    @property\n    def w2c(self):\n        w2c = mat4(self.R)\n        w2c[3] = vec4(*self.T, 1.0)\n        return w2c\n\n    @property\n    def c2w(self):\n        return glm.affineInverse(self.w2c)\n\n    @property\n    def right(self) -> vec3: return vec3(self.R[0, 0], self.R[1, 0], self.R[2, 0])  # c2w R, 0 -> 3,\n\n    @property\n    def down(self) -> vec3: return vec3(self.R[0, 1], self.R[1, 1], self.R[2, 1])  # c2w R, 1 -> 3,\n\n    @property\n    def front(self) -> vec3: return vec3(self.R[0, 2], self.R[1, 2], self.R[2, 2])  # c2w R, 2 -> 3,\n\n    @front.setter\n    def front(self, v: vec3):\n        front = v  # the last row of R\n        self.R[0, 2], self.R[1, 2], self.R[2, 2] = front.x, front.y, front.z\n        right = glm.normalize(glm.cross(self.front, self.world_up))  # right\n        self.R[0, 0], self.R[1, 0], self.R[2, 0] = right.x, right.y, right.z\n        down = glm.cross(self.front, self.right)  # down\n        self.R[0, 1], self.R[1, 1], self.R[2, 1] = down.x, down.y, down.z\n\n    @property\n    def center(self):\n        return -glm.transpose(self.R) @ self.T  # 3,\n\n    @center.setter\n    def center(self, v: vec3):\n        self.T = -self.R @ v  # 3, 1\n\n    @property\n    def s(self): return self.K[1, 0]\n\n    @s.setter\n    def s(self, s): self.K[1, 0] = s\n\n    @property\n    def fx(self): return self.K[0, 0]\n\n    @fx.setter\n    def fx(self, v: float):\n        v = min(v, 1e5)\n        v = max(v, 1e-3)\n        if self.lock_fx_fy:\n            self.K[1, 1] = v / self.K[0, 0] * self.K[1, 1]\n        self.K[0, 0] = v\n\n    @property\n    def fy(self): return self.K[1, 1]\n\n    @fy.setter\n    def fy(self, v: float):\n        if self.lock_fx_fy:\n            self.K[0, 0] = v / self.K[1, 1] * self.K[0, 0]\n        self.K[1, 1] = v\n\n    @property\n    def cx(self): return self.K[2, 0]\n\n    @cx.setter\n    def cx(self, v: float):\n        self.K[2, 0] = v\n\n    @property\n    def cy(self): return self.K[2, 1]\n\n    @cy.setter\n    def cy(self, v: float):\n        self.K[2, 1] = v\n\n    def begin_dragging(self,\n                       x: float, y: float,\n                       is_panning: bool,\n                       about_origin: bool,\n                       ):\n        self.is_dragging = True\n        self.is_panning = is_panning\n        self.about_origin = about_origin\n        self.drag_start = vec2([x, y])\n\n    def end_dragging(self):\n        self.is_dragging = False\n\n    def update_dragging(self, x: float, y: float):\n        if not self.is_dragging:\n            return\n\n        current = vec2(x, y)\n        delta = current - self.drag_start\n        delta /= max(self.H, self.W)\n        delta *= -1\n\n        self.drag_start = vec2([x, y])\n        self.drag_start_front = self.front  # a recording\n        self.drag_start_down = self.down\n        self.drag_start_right = self.right\n        self.drag_start_center = self.center\n        self.drag_start_origin = self.origin\n        self.drag_start_world_up = self.world_up\n\n        # Need to find the max or min delta y to align with world_up\n        dot = glm.dot(self.world_up, self.front)\n        self.drag_ymin = -np.arccos(-dot) + 0.01  # drag up, look down\n        self.drag_ymax = np.pi + self.drag_ymin - 0.02  # remove the 0.01 of drag_ymin\n\n        if self.is_panning:\n            delta *= self.movement_speed\n            center_delta = delta[0] * self.drag_start_right + delta[1] * self.drag_start_down\n            self.center = self.drag_start_center + center_delta\n            if self.about_origin:\n                self.origin = self.drag_start_origin + center_delta\n        else:\n            m = mat4(1.0)\n            m = glm.rotate(m, delta.x % 2 * np.pi, self.world_up)\n            m = glm.rotate(m, np.clip(delta.y, self.drag_ymin, self.drag_ymax), self.drag_start_right)\n            self.front = m @ self.drag_start_front  # might overshoot\n\n            if self.about_origin:\n                self.center = -m @ (self.origin - self.drag_start_center) + self.origin\n\n    def move(self, x_offset: float, y_offset: float):\n        speed_factor = 1e-1\n        movement = y_offset * speed_factor\n        movement = movement * self.front * self.movement_speed\n        self.center += movement\n\n        if self.is_dragging:\n            self.drag_start_center += movement\n\n", "contexts_below": "\n    def to_easymocap(self):\n        batch = self.to_batch()\n        camera = to_numpy(batch)\n        return camera\n\n    def from_easymocap(self, camera: dict):\n        batch = to_tensor(camera)\n        self.from_batch(batch)\n        return self\n\n    def to_string(self) -> str:\n        batch = to_list(self.to_batch().meta)\n        return json.dumps(batch)\n\n    def from_string(self, string: str):\n        batch = to_tensor(dotdict(json.loads(string)), ignore_list=True)\n        self.from_batch(batch)\n\n    def from_batch(self, batch: dotdict):\n        H, W, K, R, T, n, f, t, v, bounds = batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds\n\n        # Batch (network input parameters)\n        self.H = int(H)\n        self.W = int(W)\n        self.K = mat3(*K.mT.ravel())\n        self.R = mat3(*R.mT.ravel())\n        self.T = vec3(*T.ravel())  # 3,\n        self.n = float(n)\n        self.f = float(f)\n        self.t = float(t)\n        self.v = float(v)\n        self.bounds = mat2x3(*bounds.ravel())  # 2, 3\n\n        if 'mass' in batch: self.mass = float(batch.mass)\n        if 'moment_of_inertia' in batch: self.moment_of_inertia = float(batch.moment_of_inertia)\n        if 'movement_force' in batch: self.movement_force = float(batch.movement_force)\n        if 'movement_torque' in batch: self.movement_torque = float(batch.movement_torque)\n        if 'movement_speed' in batch: self.movement_speed = float(batch.movement_speed)\n        if 'origin' in batch: self.origin = vec3(*batch.origin.ravel())  # 3,\n        if 'world_up' in batch: self.world_up = vec3(*batch.world_up.ravel())  # 3,\n        return self\n\n    def custom_pose(self, R: torch.Tensor, T: torch.Tensor, K: torch.Tensor):\n        # self.K = mat3(*K.mT.ravel())\n        self.R = mat3(*R.mT.ravel())\n        self.T = vec3(*T.ravel())\n", "input_code": "    def to_batch(self):\n\n        \"\"\"\n        The function converts camera parameters and GUI related elements into a batch format using tensors, suitable for processing with PyTorch. It organizes these parameters into a structured dictionary format for easy access and manipulation.\n\n        Input-Output Arguments\n        :param self: Camera. An instance of the Camera class, containing various camera parameters and GUI related elements.\n        :return: A dotdict instance containing all the camera parameters and GUI related elements converted into tensors. This structured dictionary includes both a direct mapping of parameters and a nested 'meta' dictionary with the same content.\n\n        Note: The function assumes the existence of a 'dotdict' class or function that behaves similarly to a dictionary but allows access to its items through attributes (dot notation). It also assumes that 'torch' refers to the PyTorch library, used here for tensor operations.\n        \"\"\"", "reference_steps": "1. Define a method `to_batch` that converts various attributes of an object into a batch of tensors.\n2. Create a `dotdict` object to store metadata as a dictionary with dot-accessible keys.\n3. Convert the height (`H`) and width (`W`) attributes to tensors using `torch.as_tensor`.\n4. Convert the intrinsic matrix (`K`), rotation matrix (`R`), and translation vector (`T`) to tensors, ensuring that `K` and `R` are transposed (`.mT`) and `T` is given an additional dimension (`[..., None]`).\n5. Convert additional attributes like `n`, `f`, `t`, and `v` to tensors with the specified data type `torch.float`.\n6. Convert the `bounds` attribute to a tensor without transposing it.\n7. Convert GUI-related attributes such as `mass`, `moment_of_inertia`, `movement_force`, `movement_torque`, `movement_speed`, `origin`, and `world_up` to tensors with the specified data type `torch.float`.\n8. Create another `dotdict` object called `batch` to store the batched data.\n9. Update the `batch` dictionary with the metadata from the `meta` dictionary.\n10. Return the `batch` object which now contains all the converted tensor data.", "reference_code": "def to_batch(self):\n    meta = dotdict()\n    meta.H = torch.as_tensor(self.H)\n    meta.W = torch.as_tensor(self.W)\n    meta.K = torch.as_tensor(self.K.to_list(), dtype=torch.float).mT\n    meta.R = torch.as_tensor(self.R.to_list(), dtype=torch.float).mT\n    meta.T = torch.as_tensor(self.T.to_list(), dtype=torch.float)[..., None]\n    meta.n = torch.as_tensor(self.n, dtype=torch.float)\n    meta.f = torch.as_tensor(self.f, dtype=torch.float)\n    meta.t = torch.as_tensor(self.t, dtype=torch.float)\n    meta.v = torch.as_tensor(self.v, dtype=torch.float)\n    meta.bounds = torch.as_tensor(self.bounds.to_list(), dtype=torch.float)  # no transpose for bounds\n\n    # GUI related elements\n    meta.mass = torch.as_tensor(self.mass, dtype=torch.float)\n    meta.moment_of_inertia = torch.as_tensor(self.moment_of_inertia, dtype=torch.float)\n    meta.movement_force = torch.as_tensor(self.movement_force, dtype=torch.float)\n    meta.movement_torque = torch.as_tensor(self.movement_torque, dtype=torch.float)\n    meta.movement_speed = torch.as_tensor(self.movement_speed, dtype=torch.float)\n    meta.origin = torch.as_tensor(self.origin.to_list(), dtype=torch.float)\n    meta.world_up = torch.as_tensor(self.world_up.to_list(), dtype=torch.float)\n\n    batch = dotdict()\n    batch.update(meta)\n    batch.meta.update(meta)\n    return batch\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "type": "method", "class_name": "AgentSimilarity", "function_name": "find_closest_agent", "dependency_all": "# Intra-class Dependency:\nagent_similarity.AgentSimilarity.agents\n\nagent_similarity.AgentSimilarity.get_embedding\n\n# Intra-file Dependency:\nagent_similarity.Agent\n\nagent_similarity.logger\n\n", "dependency_sampled": "# Intra-file Dependency:\nagent_similarity.Agent\n\nagent_similarity.logger\n\n", "contexts_above": "import logging\n\nimport numpy as np\nfrom typing import List, Tuple, Optional\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom integrations.openaiwrapper import OpenAIAPIWrapper\n\nlogger = logging.getLogger()\n\nclass Agent:\n    def __init__(self, purpose: str):\n        self.purpose = purpose\n        self.purpose_embedding=None\n\nclass AgentSimilarity:\n    def __init__(self, openai_wrapper: OpenAIAPIWrapper, agents: List[Agent]):\n        \"\"\"\n        Initializes the AgentSimilarity object.\n\n        :param openai_wrapper: Instance of OpenAIAPIWrapper to interact with OpenAI API.\n        :param agents: List of Agent objects.\n        \"\"\"\n        self.openai_wrapper = openai_wrapper\n        self.agents = agents\n\n    def get_embedding(self, text: str) -> np.ndarray:\n        \"\"\"\n        Retrieves the embedding for a given text.\n\n        :param text: Text to get embedding for.\n        :return: Embedding as a numpy array.\n        \"\"\"\n        try:\n            response = self.openai_wrapper.get_embedding(text)\n            if 'data' in response and len(response['data']) > 0 and 'embedding' in response['data'][0]:\n                return np.array(response['data'][0]['embedding'])\n            else:\n                logger.exception(\"Invalid response format\")\n                raise ValueError(\"Invalid response format\")\n        except Exception as e:\n            logger.exception(f\"Error retrieving embedding: {e}\")\n            raise ValueError(f\"Error retrieving embedding: {e}\")\n\n\n    def calculate_similarity_threshold(self) -> float:\n        \"\"\"\n        Calculates the 98th percentile of the similarity threshold across all agents.\n\n        :return: 98th percentile of similarity threshold.\n        \"\"\"\n        try:\n            embeddings=[]\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                   agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                embeddings.append(agent.purpose_embedding)\n\n            if len(embeddings) < 250:\n                return 0.999\n\n            similarities = [cosine_similarity([e1], [e2])[0][0] for i, e1 in enumerate(embeddings) for e2 in embeddings[i+1:]]\n            return np.percentile(similarities, 98) if similarities else 0.999\n        except Exception as e:\n            logger.exception(f\"Error calculating similarity threshold: {e}\")\n            raise ValueError(f\"Error calculating similarity threshold: {e}\")\n\n\n", "contexts_below": "", "input_code": "    def find_closest_agent(self, purpose_embedding: np.ndarray) -> Tuple[Optional[Agent], float]:\n\n        \"\"\"\n        Finds the closest agent to a given purpose embedding by calculating the cosine similarity between the purpose embedding of each agent and the given purpose embedding. It returns the agent with the highest similarity score to the given purpose embedding.\n\n        Input-Output Arguments\n        :param purpose_embedding: np.ndarray, The embedding vector of the purpose for which the closest agent is being searched. It is used to compare against the purpose embeddings of the agents to find the closest match.\n        :return: Tuple[Optional[Agent], float], A tuple containing the closest agent to the given purpose embedding and the highest similarity score. If no agents are found or an error occurs, it may return None for the agent and -inf for the similarity score.\n        \"\"\"", "reference_steps": "1. Define a function `find_closest_agent` that accepts a purpose embedding as a parameter and returns a tuple containing the closest agent and their similarity score.\n2. Initialize `closest_agent` to `None` and `highest_similarity` to negative infinity to track the best match.\n3. Iterate through each agent in the `self.agents` list.\n4. Check if the current agent's purpose embedding is `None`. If so, compute the embedding using a method `self.get_embedding` and assign it to the agent's `purpose_embedding`.\n5. Calculate the cosine similarity between the agent's purpose embedding and the given purpose embedding.\n6. Compare the calculated similarity with the current `highest_similarity`. If it's higher, update `highest_similarity` with the new value and set `closest_agent` to the current agent.\n7. Repeat steps 4 through 6 for all agents in the list.\n8. After iterating through all agents, return the `closest_agent` and the `highest_similarity` as a tuple.\n9. Handle any exceptions that occur during the process by logging the exception and raising a `ValueError` with the error message.\n10. Ensure that the function returns the closest agent and the highest similarity score even if an exception is not raised.", "reference_code": "def find_closest_agent(self, purpose_embedding: np.ndarray) -> Tuple[Optional[Agent], float]:\n    \"\"\"\n    Finds the closest agent based on the given purpose embedding.\n\n    :param purpose_embedding: The embedding of the purpose to find the closest agent for.\n    :return: Tuple of the closest agent and the highest similarity score.\n    \"\"\"\n    closest_agent: Optional[Agent] = None\n    highest_similarity: float = -np.inf\n\n    try:\n        for agent in self.agents:\n            if agent.purpose_embedding is None:\n               agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n            similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n\n            if similarity > highest_similarity:\n                highest_similarity = similarity\n                closest_agent = agent\n\n        return closest_agent, highest_similarity\n    except Exception as e:\n        logger.exception(f\"Error finding closest agent: {e}\")\n        raise ValueError(f\"Error finding closest agent: {e}\")\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "type": "method", "class_name": "AgentLifecycle", "function_name": "create_prime_agent", "dependency_all": "# Intra-class Dependency:\nagent_lifecycle.AgentLifecycle.agents\n\nagent_lifecycle.AgentLifecycle.openai_wrapper\n\n# Intra-file Dependency:\nagent_lifecycle.PRIME_AGENT_WEIGHT\n\n", "dependency_sampled": "# Intra-file Dependency:\nagent_lifecycle.PRIME_AGENT_WEIGHT\n\n", "contexts_above": "import logging\nfrom typing import List\nfrom agents.microagent import MicroAgent\nfrom integrations.openaiwrapper import OpenAIAPIWrapper\nfrom agents.agent_similarity import AgentSimilarity\nfrom agents.agent_persistence_manager import AgentPersistenceManager\nfrom numpy import ndarray\nfrom prompt_management.prompts import (\n    PRIME_PROMPT, PRIME_NAME, \n    PROMPT_ENGINEERING_SYSTEM_PROMPT, \n    PROMPT_ENGINEERING_TEMPLATE, EXAMPLES\n)\n\nlogger = logging.getLogger()\n\nDEFAULT_MAX_AGENTS = 2000\nPRIME_AGENT_WEIGHT = 25\n\nclass AgentLifecycle:\n    def __init__(self, openai_wrapper: OpenAIAPIWrapper, agent_persistence_manager: AgentPersistenceManager, max_agents: int = DEFAULT_MAX_AGENTS):\n        self.agents: List[MicroAgent] = []\n        self.openai_wrapper = openai_wrapper\n        self.agent_persistence = agent_persistence_manager\n        self.max_agents = max_agents\n\n    def stop_all_agents(self) -> None:\n        \"\"\"Stops all agents.\"\"\"\n        for agent in self.agents:\n            agent.stop()\n\n    def reset_all_agents(self) -> None:\n        \"\"\"Resets all agents.\"\"\"\n        for agent in self.agents:\n            agent.reset()\n\n    def cleanup_agents(self):\n        \"\"\"Remove all agents with status stopped = True in an efficient manner.\"\"\"\n        self.agents = [agent for agent in self.agents if not agent.stopped]\n\n", "contexts_below": "\n    def add_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Adds an agent to the list of agents.\"\"\"\n        self.agents.append(agent)\n\n\n\n    def get_available_agents_for_agent(self, agent) -> List[MicroAgent]:\n        \"\"\"Returns the list of available agents for the given purpose.\"\"\"\n        agent_id = agent.id \n        available_agents = [agent for agent in self.agents if agent.purpose != \"Bootstrap Agent\" and agent.working_agent]\n        for agent in available_agents:\n            if agent.parent_id != agent_id:\n                available_agents.remove(agent)\n\n        return available_agents\n\n    def get_or_create_agent(self, purpose: str, depth: int, sample_input: str, force_new: bool = False, parent_agent=None) -> MicroAgent:\n        \"\"\"\n        Retrieves or creates an agent based on the given purpose.\n        Optionally creates a new agent regardless of similarity if force_new is True.\n        \"\"\"\n        if not force_new:\n            agent_similarity = AgentSimilarity(self.openai_wrapper, self.agents)\n            purpose_embedding = agent_similarity.get_embedding(purpose)\n            closest_agent, highest_similarity = agent_similarity.find_closest_agent(purpose_embedding)\n            similarity_threshold = agent_similarity.calculate_similarity_threshold()\n\n            if highest_similarity >= similarity_threshold:\n                closest_agent.usage_count += 1\n                return closest_agent\n\n        return self._create_and_add_agent(purpose, depth, sample_input, parent_agent=parent_agent)\n\n    def _create_and_add_agent(self, purpose: str, depth: int, sample_input: str, parent_agent=None) -> MicroAgent:\n        \"\"\"Helper method to create and add a new agent.\"\"\"\n        if len(self.agents) >= self.max_agents:\n            self._remove_least_used_agent()\n\n        new_agent = MicroAgent(self._generate_llm_prompt(purpose, sample_input), purpose, depth, self, self.openai_wrapper, parent=parent_agent)\n        new_agent.usage_count = 1\n        self.agents.append(new_agent)\n        return new_agent\n\n    def _remove_least_used_agent(self):\n        \"\"\"Removes the least used agent.\"\"\"\n        least_used_agent = min(self.agents, key=lambda agent: agent.usage_count)\n        self.agents.remove(least_used_agent)\n\n    def save_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Saves the given agent with error handling.\"\"\"\n        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n    \n    \n    def remove_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Removes the given agent with error handling.\"\"\"\n        try:\n            self.agent_persistence.remove_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n\n    def _generate_llm_prompt(self, goal: str, sample_input: str) -> str:\n        \"\"\"\n        Generates a prompt for the LLM based on the given goal and sample input.\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": PROMPT_ENGINEERING_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)}\n        ]\n\n        try:\n            return self.openai_wrapper.chat_completion(messages=messages)\n        except Exception as e:\n            logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n", "input_code": "    def create_prime_agent(self) -> None:\n\n        \"\"\"\n        Creates the prime agent and adds it to the agent list. This prime agent is initialized with specific attributes such as prompt, name, weight, and flags indicating its prime status and another unspecified flag.\n        Input-Output Arguments\n        :param self: AgentLifecycle. An instance of the AgentLifecycle class. It is used to access the agent list where the prime agent will be added.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method called `create_prime_agent` in a class, which does not take any arguments besides `self`.\n2. Inside the method, instantiate a `MicroAgent` object with specific parameters: `PRIME_PROMPT`, `PRIME_NAME`, an initial score of `0`, a reference to `self`, an `openai_wrapper` attribute from `self`, `PRIME_AGENT_WEIGHT`, and two boolean values `True`.\n3. Add the newly created `prime_agent` object to the `agents` list attribute of the class instance (`self`).", "reference_code": "def create_prime_agent(self) -> None:\n    \"\"\"Creates the prime agent and adds it to the agent list.\"\"\"\n    prime_agent = MicroAgent(\n        PRIME_PROMPT, PRIME_NAME, 0, self, \n        self.openai_wrapper, PRIME_AGENT_WEIGHT, True, True\n    )\n    self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "type": "method", "class_name": "AgentLifecycle", "function_name": "save_agent", "dependency_all": "# Intra-class Dependency:\nagent_lifecycle.AgentLifecycle.agent_persistence\n\n# Intra-file Dependency:\nagent_lifecycle.logger\n\n", "dependency_sampled": "# Intra-class Dependency:\nagent_lifecycle.AgentLifecycle.agent_persistence\n\n", "contexts_above": "import logging\nfrom typing import List\nfrom agents.microagent import MicroAgent\nfrom integrations.openaiwrapper import OpenAIAPIWrapper\nfrom agents.agent_similarity import AgentSimilarity\nfrom agents.agent_persistence_manager import AgentPersistenceManager\nfrom numpy import ndarray\nfrom prompt_management.prompts import (\n    PRIME_PROMPT, PRIME_NAME, \n    PROMPT_ENGINEERING_SYSTEM_PROMPT, \n    PROMPT_ENGINEERING_TEMPLATE, EXAMPLES\n)\n\nlogger = logging.getLogger()\n\nDEFAULT_MAX_AGENTS = 2000\nPRIME_AGENT_WEIGHT = 25\n\nclass AgentLifecycle:\n    def __init__(self, openai_wrapper: OpenAIAPIWrapper, agent_persistence_manager: AgentPersistenceManager, max_agents: int = DEFAULT_MAX_AGENTS):\n        self.agents: List[MicroAgent] = []\n        self.openai_wrapper = openai_wrapper\n        self.agent_persistence = agent_persistence_manager\n        self.max_agents = max_agents\n\n    def stop_all_agents(self) -> None:\n        \"\"\"Stops all agents.\"\"\"\n        for agent in self.agents:\n            agent.stop()\n\n    def reset_all_agents(self) -> None:\n        \"\"\"Resets all agents.\"\"\"\n        for agent in self.agents:\n            agent.reset()\n\n    def cleanup_agents(self):\n        \"\"\"Remove all agents with status stopped = True in an efficient manner.\"\"\"\n        self.agents = [agent for agent in self.agents if not agent.stopped]\n\n    def create_prime_agent(self) -> None:\n        \"\"\"Creates the prime agent and adds it to the agent list.\"\"\"\n        prime_agent = MicroAgent(\n            PRIME_PROMPT, PRIME_NAME, 0, self, \n            self.openai_wrapper, PRIME_AGENT_WEIGHT, True, True\n        )\n        self.agents.append(prime_agent)\n\n    def add_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Adds an agent to the list of agents.\"\"\"\n        self.agents.append(agent)\n\n\n\n    def get_available_agents_for_agent(self, agent) -> List[MicroAgent]:\n        \"\"\"Returns the list of available agents for the given purpose.\"\"\"\n        agent_id = agent.id \n        available_agents = [agent for agent in self.agents if agent.purpose != \"Bootstrap Agent\" and agent.working_agent]\n        for agent in available_agents:\n            if agent.parent_id != agent_id:\n                available_agents.remove(agent)\n\n        return available_agents\n\n    def get_or_create_agent(self, purpose: str, depth: int, sample_input: str, force_new: bool = False, parent_agent=None) -> MicroAgent:\n        \"\"\"\n        Retrieves or creates an agent based on the given purpose.\n        Optionally creates a new agent regardless of similarity if force_new is True.\n        \"\"\"\n        if not force_new:\n            agent_similarity = AgentSimilarity(self.openai_wrapper, self.agents)\n            purpose_embedding = agent_similarity.get_embedding(purpose)\n            closest_agent, highest_similarity = agent_similarity.find_closest_agent(purpose_embedding)\n            similarity_threshold = agent_similarity.calculate_similarity_threshold()\n\n            if highest_similarity >= similarity_threshold:\n                closest_agent.usage_count += 1\n                return closest_agent\n\n        return self._create_and_add_agent(purpose, depth, sample_input, parent_agent=parent_agent)\n\n    def _create_and_add_agent(self, purpose: str, depth: int, sample_input: str, parent_agent=None) -> MicroAgent:\n        \"\"\"Helper method to create and add a new agent.\"\"\"\n        if len(self.agents) >= self.max_agents:\n            self._remove_least_used_agent()\n\n        new_agent = MicroAgent(self._generate_llm_prompt(purpose, sample_input), purpose, depth, self, self.openai_wrapper, parent=parent_agent)\n        new_agent.usage_count = 1\n        self.agents.append(new_agent)\n        return new_agent\n\n    def _remove_least_used_agent(self):\n        \"\"\"Removes the least used agent.\"\"\"\n        least_used_agent = min(self.agents, key=lambda agent: agent.usage_count)\n        self.agents.remove(least_used_agent)\n\n", "contexts_below": "    \n    \n    def remove_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Removes the given agent with error handling.\"\"\"\n        try:\n            self.agent_persistence.remove_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n\n    def _generate_llm_prompt(self, goal: str, sample_input: str) -> str:\n        \"\"\"\n        Generates a prompt for the LLM based on the given goal and sample input.\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": PROMPT_ENGINEERING_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)}\n        ]\n\n        try:\n            return self.openai_wrapper.chat_completion(messages=messages)\n        except Exception as e:\n            logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n", "input_code": "    def save_agent(self, agent: MicroAgent) -> None:\n\n        \"\"\"\n        Saves the given agent using the agent persistence mechanism. If an error occurs during the save operation, it logs the exception and re-raises the error.\n        Input-Output Arguments\n        :param self: AgentLifecycle. An instance of the AgentLifecycle class.\n        :param agent: MicroAgent, the agent to be saved. It is used by the agent persistence mechanism to save the agent.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method called `save_agent` that accepts an instance of `MicroAgent` as a parameter.\n2. Start a `try` block to attempt the saving operation.\n3. Call the `save_agent` method on the `agent_persistence` object, passing in the `MicroAgent` instance.\n4. If an exception occurs during the saving process, catch it in the `except` block.\n5. Log the exception with a message indicating an error occurred while saving the agent, including the exception details.\n6. Re-raise the caught exception to propagate it up the call stack.\n7. Ensure the method `save_agent` does not return any value (`None` is implied in Python when no return statement is present).\n8. The method `save_agent` is a member of a class, indicated by the `self` parameter.\n9. The method includes a docstring explaining its purpose, which is to save an agent with error handling.\n10. The code snippet is written in Python and follows its syntax and exception handling conventions.", "reference_code": "def save_agent(self, agent: MicroAgent) -> None:\n    \"\"\"Saves the given agent with error handling.\"\"\"\n    try:\n        self.agent_persistence.save_agent(agent)\n    except Exception as e:\n        logger.exception(f\"Error in saving agent: {e}\")\n        raise\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "type": "method", "class_name": "MicroAgentManager", "function_name": "get_agents", "dependency_all": "# Intra-class Dependency:\nmicroagent_manager.MicroAgentManager.agent_lifecycle\n\nmicroagent_manager.MicroAgentManager.cleanup_agents\n\n", "dependency_sampled": "# Intra-class Dependency:\nmicroagent_manager.MicroAgentManager.agent_lifecycle\n\n", "contexts_above": "import logging\n\nfrom typing import List, Optional, Any\nfrom agents.agent_lifecycle import AgentLifecycle \nfrom agents.agent_similarity import AgentSimilarity\nfrom agents.agent_persistence_manager import AgentPersistenceManager \nfrom integrations.openaiwrapper import OpenAIAPIWrapper\n\nlogger= logging.getLogger()\n\nclass MicroAgentManager:\n    \"\"\"\n    Manages the creation and retrieval of micro agents.\n    \"\"\"\n\n    def __init__(self, openai_wrapper: OpenAIAPIWrapper, max_agents: int = 20, db_filename : str = \"agents.db\"):\n        self.max_agents = max_agents\n        self.openai_wrapper = openai_wrapper\n        self.agent_persistence = AgentPersistenceManager(db_filename)\n        self.agent_lifecycle = AgentLifecycle(self.openai_wrapper, self.agent_persistence, max_agents)\n        self.load_agents()\n\n    def stop_all_agents(self) -> None:\n        \"\"\"Stops all agents.\"\"\"\n        self.agent_lifecycle.stop_all_agents()\n\n    def cleanup_agents(self):\n        \"\"\"Remove all agents with status stopped = True\"\"\"\n        self.agent_lifecycle.cleanup_agents()\n    \n    def load_agents(self):\n        \"\"\"Loads agents from the database.\"\"\"\n        loaded_agents = self.agent_persistence.load_all_agents(self.agent_lifecycle, self.openai_wrapper)\n        self.agent_lifecycle.agents.extend(loaded_agents)\n        logger.info(f\"Loaded {len(loaded_agents)} agents from the database.\")\n\n\n", "contexts_below": "\n    def create_agents(self) -> None:\n        \"\"\"Creates prime agents and logs the process.\"\"\"\n        logger.info(\"Creating agents...\")\n        try:\n            self.agent_lifecycle.create_prime_agent()\n            logger.info(\"Agents created successfully.\")\n        except Exception as e:\n            logger.exception(f\"Error in creating agents: {e}\")\n            raise\n    \n    def get_or_create_agent(self, purpose: str, depth: int, sample_input: str, parent_agent=None) -> Any:\n        \"\"\"\n        Retrieves an existing agent or creates a new one based on the given purpose.\n        \"\"\"\n        logger.info(f\"Getting or creating agent for purpose: {purpose}\")\n        try:\n            agent = self.agent_lifecycle.get_or_create_agent(purpose, depth, sample_input, parent_agent=parent_agent)\n            logger.info(f\"Agent for purpose '{purpose}' retrieved or created.\")\n            return agent\n        except Exception as e:\n            logging.exception(f\"Error in getting or creating agent: {e}\")\n            raise\n\n\n    def display_agent_status(self):\n        \"\"\"Displays the current status of all agents.\"\"\"\n        for agent in self.get_agents():\n            logger.info(f\"Agent {agent.purpose}: Status = {agent.current_status}, Evolve Count = {agent.evolve_count}\")\n\n    def display_active_agent_tree(self):\n        \"\"\"Displays a tree view of active agent relationships.\"\"\"\n        for agent in self.get_agents():\n            if agent.active_agents:\n                logger.info(f\"Agent {agent.purpose} is calling: {agent.active_agents}\")\n            else:\n                logger.info(f\"Agent {agent.purpose} is currently idle.\")", "input_code": "    def get_agents(self) -> List[Any]:\n\n        \"\"\"\n        This function cleans up the agents and then returns the current list of agents managed by the MicroAgentManager instance.\n\n        Input-Output Arguments\n        :param self: MicroAgentManager. An instance of the MicroAgentManager class, used to access the instance's methods and attributes.\n        :return: List[Any]. The current list of agents after cleanup.\n        \"\"\"", "reference_steps": "1. Define a method `get_agents` within a class.\n2. Inside the method, call the `cleanup_agents` method to perform any necessary cleanup operations on the agents.\n3. Retrieve the list of agents from the `agent_lifecycle` attribute of the class.\n4. Return the list of agents.", "reference_code": "def get_agents(self) -> List[Any]:\n    \"\"\"Returns the list of agents.\"\"\"\n    self.cleanup_agents()\n    return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "type": "method", "class_name": "AgentLifecycle", "function_name": "_generate_llm_prompt", "dependency_all": "# Intra-class Dependency:\nagent_lifecycle.AgentLifecycle.openai_wrapper\n\n# Intra-file Dependency:\nagent_lifecycle.logger\n\n", "dependency_sampled": "# Intra-class Dependency:\nagent_lifecycle.AgentLifecycle.openai_wrapper\n\n", "contexts_above": "import logging\nfrom typing import List\nfrom agents.microagent import MicroAgent\nfrom integrations.openaiwrapper import OpenAIAPIWrapper\nfrom agents.agent_similarity import AgentSimilarity\nfrom agents.agent_persistence_manager import AgentPersistenceManager\nfrom numpy import ndarray\nfrom prompt_management.prompts import (\n    PRIME_PROMPT, PRIME_NAME, \n    PROMPT_ENGINEERING_SYSTEM_PROMPT, \n    PROMPT_ENGINEERING_TEMPLATE, EXAMPLES\n)\n\nlogger = logging.getLogger()\n\nDEFAULT_MAX_AGENTS = 2000\nPRIME_AGENT_WEIGHT = 25\n\nclass AgentLifecycle:\n    def __init__(self, openai_wrapper: OpenAIAPIWrapper, agent_persistence_manager: AgentPersistenceManager, max_agents: int = DEFAULT_MAX_AGENTS):\n        self.agents: List[MicroAgent] = []\n        self.openai_wrapper = openai_wrapper\n        self.agent_persistence = agent_persistence_manager\n        self.max_agents = max_agents\n\n    def stop_all_agents(self) -> None:\n        \"\"\"Stops all agents.\"\"\"\n        for agent in self.agents:\n            agent.stop()\n\n    def reset_all_agents(self) -> None:\n        \"\"\"Resets all agents.\"\"\"\n        for agent in self.agents:\n            agent.reset()\n\n    def cleanup_agents(self):\n        \"\"\"Remove all agents with status stopped = True in an efficient manner.\"\"\"\n        self.agents = [agent for agent in self.agents if not agent.stopped]\n\n    def create_prime_agent(self) -> None:\n        \"\"\"Creates the prime agent and adds it to the agent list.\"\"\"\n        prime_agent = MicroAgent(\n            PRIME_PROMPT, PRIME_NAME, 0, self, \n            self.openai_wrapper, PRIME_AGENT_WEIGHT, True, True\n        )\n        self.agents.append(prime_agent)\n\n    def add_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Adds an agent to the list of agents.\"\"\"\n        self.agents.append(agent)\n\n\n\n    def get_available_agents_for_agent(self, agent) -> List[MicroAgent]:\n        \"\"\"Returns the list of available agents for the given purpose.\"\"\"\n        agent_id = agent.id \n        available_agents = [agent for agent in self.agents if agent.purpose != \"Bootstrap Agent\" and agent.working_agent]\n        for agent in available_agents:\n            if agent.parent_id != agent_id:\n                available_agents.remove(agent)\n\n        return available_agents\n\n    def get_or_create_agent(self, purpose: str, depth: int, sample_input: str, force_new: bool = False, parent_agent=None) -> MicroAgent:\n        \"\"\"\n        Retrieves or creates an agent based on the given purpose.\n        Optionally creates a new agent regardless of similarity if force_new is True.\n        \"\"\"\n        if not force_new:\n            agent_similarity = AgentSimilarity(self.openai_wrapper, self.agents)\n            purpose_embedding = agent_similarity.get_embedding(purpose)\n            closest_agent, highest_similarity = agent_similarity.find_closest_agent(purpose_embedding)\n            similarity_threshold = agent_similarity.calculate_similarity_threshold()\n\n            if highest_similarity >= similarity_threshold:\n                closest_agent.usage_count += 1\n                return closest_agent\n\n        return self._create_and_add_agent(purpose, depth, sample_input, parent_agent=parent_agent)\n\n    def _create_and_add_agent(self, purpose: str, depth: int, sample_input: str, parent_agent=None) -> MicroAgent:\n        \"\"\"Helper method to create and add a new agent.\"\"\"\n        if len(self.agents) >= self.max_agents:\n            self._remove_least_used_agent()\n\n        new_agent = MicroAgent(self._generate_llm_prompt(purpose, sample_input), purpose, depth, self, self.openai_wrapper, parent=parent_agent)\n        new_agent.usage_count = 1\n        self.agents.append(new_agent)\n        return new_agent\n\n    def _remove_least_used_agent(self):\n        \"\"\"Removes the least used agent.\"\"\"\n        least_used_agent = min(self.agents, key=lambda agent: agent.usage_count)\n        self.agents.remove(least_used_agent)\n\n    def save_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Saves the given agent with error handling.\"\"\"\n        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n    \n    \n    def remove_agent(self, agent: MicroAgent) -> None:\n        \"\"\"Removes the given agent with error handling.\"\"\"\n        try:\n            self.agent_persistence.remove_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n\n", "contexts_below": "", "input_code": "    def _generate_llm_prompt(self, goal: str, sample_input: str) -> str:\n\n        \"\"\"\n        Generates a prompt for the Language Learning Model (LLM) based on a specified goal and sample input, then attempts to get a chat completion from an OpenAI wrapper. If an error occurs during this process, it logs the exception and returns an empty string.\n\n        Input-Output Arguments\n        :param self: AgentLifecycle. An instance of the AgentLifecycle class.\n        :param goal: str, The goal or objective that the prompt is intended to achieve.\n        :param sample_input: str, A sample input to provide context or an example for the prompt generation.\n        :return: str, The generated prompt from the LLM if successful, or an empty string if an error occurs.\n        \"\"\"", "reference_steps": "1. Define a function `_generate_llm_prompt` that accepts a `goal` and `sample_input` as parameters.\n2. Create a list named `messages` containing dictionaries with keys `\"role\"` and `\"content\"`.\n3. The first dictionary in the `messages` list represents the system's role with a predefined prompt `PROMPT_ENGINEERING_SYSTEM_PROMPT`.\n4. The second dictionary in the `messages` list represents the user's role, with content formatted using `PROMPT_ENGINEERING_TEMPLATE`, incorporating the `goal`, `sample_input`, and predefined `EXAMPLES`.\n5. Inside a `try` block, call the `chat_completion` method of the `openai_wrapper` object, passing the `messages` list as an argument.\n6. Return the result of the `chat_completion` method call.\n7. Handle any exceptions that occur during the `chat_completion` method call using an `except` block.\n8. Log the exception using `logger.exception` with a message that includes the error details.\n9. In the case of an exception, return an empty string `\"\"`.\n10. Ensure that the function is a method of a class by using `self` as the first parameter.", "reference_code": "def _generate_llm_prompt(self, goal: str, sample_input: str) -> str:\n    \"\"\"\n    Generates a prompt for the LLM based on the given goal and sample input.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": PROMPT_ENGINEERING_SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)}\n    ]\n\n    try:\n        return self.openai_wrapper.chat_completion(messages=messages)\n    except Exception as e:\n        logger.exception(f\"Error generating LLM prompt: {e}\")\n        return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "type": "method", "class_name": "SQLiteAgentPersistence", "function_name": "save_agent", "dependency_all": "# Intra-class Dependency:\nsqlite_agent_persistence.SQLiteAgentPersistence.filename\n\n", "dependency_sampled": "# Intra-class Dependency:\nsqlite_agent_persistence.SQLiteAgentPersistence.filename\n\n", "contexts_above": "import sqlite3\nimport json\nfrom integrations.agent_persistence import AbstractAgentPersistence\n\nclass SQLiteAgentPersistence(AbstractAgentPersistence):\n    def __init__(self, filename=\"agents.db\"):\n        self.filename = filename\n        self._initialize_database()\n\n    def _initialize_database(self):\n        \"\"\"\n        Initialize the SQLite database with the required schema.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS agents (\n                    id TEXT PRIMARY KEY,\n                    purpose TEXT,\n                    data TEXT\n                )\n            \"\"\")\n    def remove_agent(self, purpose):\n        \"\"\"\n        Remove an agent from the SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"DELETE FROM agents WHERE id = ?\", (purpose,))\n\n", "contexts_below": "\n    def fetch_agent(self, purpose):\n        \"\"\"\n        Fetch a serialized agent based on its purpose from the SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            return json.loads(row[0]) if row else None\n\n    def load_all_purposes(self):\n        \"\"\"\n        Load all agent purposes from the SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]", "input_code": "    def save_agent(self, agent_dict):\n\n        \"\"\"\n        Saves the serialized agent information into an SQLite database by replacing the existing record with the same ID or inserting a new record if the ID does not exist.\n        Input-Output Arguments\n        :param self: SQLiteAgentPersistence. An instance of the SQLiteAgentPersistence class, which contains the filename of the SQLite database.\n        :param agent_dict: dict, The dictionary containing the agent's information, including its 'id', 'purpose', and other data, which is used to insert or update the agent's record in the database.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method `save_agent` that takes `self` and `agent_dict` as parameters.\n2. Establish a connection to an SQLite database using `sqlite3.connect(self.filename)`.\n3. Open a context manager to ensure the database connection is properly managed.\n4. Execute an SQL `REPLACE INTO` statement on the database connection.\n5. Specify the target table in the database as `agents`.\n6. Define the columns to insert or replace data into as `(id, purpose, data)`.\n7. Prepare the values to be inserted or replaced by extracting `id` and `purpose` from `agent_dict`.\n8. Serialize the entire `agent_dict` to a JSON string using `json.dumps(agent_dict)`.\n9. Pass the prepared values as parameters to the SQL statement.\n10. Close the database connection automatically when the context manager block is exited.", "reference_code": "def save_agent(self, agent_dict):\n    \"\"\"\n    Save the serialized agent to an SQLite database.\n    \"\"\"\n    with sqlite3.connect(self.filename) as conn:\n        conn.execute(\n            # add id field\n            \"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n            (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict))\n        )\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "type": "method", "class_name": "SQLiteAgentPersistence", "function_name": "fetch_agent", "dependency_all": "# Intra-class Dependency:\nsqlite_agent_persistence.SQLiteAgentPersistence.filename\n\n", "dependency_sampled": "# Intra-class Dependency:\nsqlite_agent_persistence.SQLiteAgentPersistence.filename\n\n", "contexts_above": "import sqlite3\nimport json\nfrom integrations.agent_persistence import AbstractAgentPersistence\n\nclass SQLiteAgentPersistence(AbstractAgentPersistence):\n    def __init__(self, filename=\"agents.db\"):\n        self.filename = filename\n        self._initialize_database()\n\n    def _initialize_database(self):\n        \"\"\"\n        Initialize the SQLite database with the required schema.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS agents (\n                    id TEXT PRIMARY KEY,\n                    purpose TEXT,\n                    data TEXT\n                )\n            \"\"\")\n    def remove_agent(self, purpose):\n        \"\"\"\n        Remove an agent from the SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"DELETE FROM agents WHERE id = ?\", (purpose,))\n\n    def save_agent(self, agent_dict):\n        \"\"\"\n        Save the serialized agent to an SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\n                # add id field\n                \"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict))\n            )\n\n", "contexts_below": "\n    def load_all_purposes(self):\n        \"\"\"\n        Load all agent purposes from the SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]", "input_code": "    def fetch_agent(self, purpose):\n\n        \"\"\"\n        Fetches a serialized agent from the SQLite database based on its purpose and deserializes it. If no agent with the given purpose is found, None is returned.\n\n        Input-Output Arguments\n        :param self: SQLiteAgentPersistence. An instance of the SQLiteAgentPersistence class, which contains the filename of the SQLite database.\n        :param purpose: str, The purpose of the agent to be fetched. It is used to query the database for the corresponding agent data.\n        :return: dict or None. The deserialized agent data as a dictionary if an agent with the given purpose is found; otherwise, None.\n        \"\"\"", "reference_steps": "1. Define a function `fetch_agent` with parameters `self` and `purpose`.\n2. Connect to an SQLite database using the `sqlite3.connect` method with the database filename (`self.filename`).\n3. Create a cursor object from the database connection to execute SQL commands.\n4. Execute an SQL query to select the `data` column from the `agents` table where the `purpose` column matches the given `purpose` argument.\n5. Fetch the first row of the result set from the query using `cursor.fetchone()`.\n6. Check if a row was returned from the query.\n7. If a row was found, deserialize the JSON data from the first column of the row using `json.loads`.\n8. If no row was found, return `None`.\n9. Return the deserialized JSON data if a row was found.\n10. Ensure that the database connection is automatically closed after the operation using the `with` statement context manager.", "reference_code": "def fetch_agent(self, purpose):\n    \"\"\"\n    Fetch a serialized agent based on its purpose from the SQLite database.\n    \"\"\"\n    with sqlite3.connect(self.filename) as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n        row = cursor.fetchone()\n        return json.loads(row[0]) if row else None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "type": "method", "class_name": "SQLiteAgentPersistence", "function_name": "load_all_purposes", "dependency_all": "# Intra-class Dependency:\nsqlite_agent_persistence.SQLiteAgentPersistence.filename\n\n", "dependency_sampled": "# Intra-class Dependency:\nsqlite_agent_persistence.SQLiteAgentPersistence.filename\n\n", "contexts_above": "import sqlite3\nimport json\nfrom integrations.agent_persistence import AbstractAgentPersistence\n\nclass SQLiteAgentPersistence(AbstractAgentPersistence):\n    def __init__(self, filename=\"agents.db\"):\n        self.filename = filename\n        self._initialize_database()\n\n    def _initialize_database(self):\n        \"\"\"\n        Initialize the SQLite database with the required schema.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS agents (\n                    id TEXT PRIMARY KEY,\n                    purpose TEXT,\n                    data TEXT\n                )\n            \"\"\")\n    def remove_agent(self, purpose):\n        \"\"\"\n        Remove an agent from the SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"DELETE FROM agents WHERE id = ?\", (purpose,))\n\n    def save_agent(self, agent_dict):\n        \"\"\"\n        Save the serialized agent to an SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\n                # add id field\n                \"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict))\n            )\n\n    def fetch_agent(self, purpose):\n        \"\"\"\n        Fetch a serialized agent based on its purpose from the SQLite database.\n        \"\"\"\n        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            return json.loads(row[0]) if row else None\n\n", "contexts_below": "", "input_code": "    def load_all_purposes(self):\n\n        \"\"\"\n        Loads all agent purposes from the SQLite database and returns them as a list. This function is used to retrieve a list of purposes for all agents stored in the database, which can be useful for various operations such as filtering or analysis.\n\n        Input-Output Arguments\n        :param self: SQLiteAgentPersistence. An instance of the SQLiteAgentPersistence class, which contains the filename of the SQLite database as an attribute.\n        :return: list. A list of purposes for all agents. Each element in the list is a string representing the purpose of an agent.\n\n        No additional input parameters are required for this function.\n        \"\"\"", "reference_steps": "1. Define a function `load_all_purposes` within a class to load agent purposes from a SQLite database.\n2. Establish a connection to the SQLite database using `sqlite3.connect` with `self.filename` as the database file.\n3. Open a context manager (`with` statement) to ensure the database connection is properly managed.\n4. Create a cursor object by calling `cursor()` on the database connection to execute SQL commands.\n5. Execute an SQL query to select the `purpose` column from the `agents` table using `cursor.execute`.\n6. Fetch all the results of the query with `cursor.fetchall()`.\n7. Iterate over each row of the fetched results, extracting the first element (purpose) of each tuple.\n8. Store the extracted purposes in a list comprehension.\n9. Return the list of purposes.\n10. Ensure the database connection is automatically closed after exiting the `with` block.", "reference_code": "def load_all_purposes(self):\n    \"\"\"\n    Load all agent purposes from the SQLite database.\n    \"\"\"\n    with sqlite3.connect(self.filename) as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT purpose FROM agents\")\n        return [row[0] for row in cursor.fetchall()]"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "type": "method", "class_name": "SQLiteMemoization", "function_name": "_fetch_from_cache", "dependency_all": "# Intra-class Dependency:\nmemoize.SQLiteMemoization.connection\n\n", "dependency_sampled": "# Intra-class Dependency:\nmemoize.SQLiteMemoization.connection\n\n", "contexts_above": "import sqlite3\nimport hashlib\nimport json\nimport functools\n\n## Originally from https://www.kevinkatz.io/posts/memoize-to-sqlite\n\ndef memoize_to_sqlite(func_name: str, filename: str = \"cache.db\"):\n    \"\"\"\n    Memoization decorator that caches the output of a method in a SQLite\n    database.\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            with SQLiteMemoization(filename) as memoizer:\n                return memoizer.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapped\n    return decorator\n\nclass SQLiteMemoization:\n    def __init__(self, filename):\n        self.filename = filename\n        self.connection = None\n\n    def __enter__(self):\n        self.connection = sqlite3.connect(self.filename)\n        self._initialize_database()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.connection.close()\n        self.connection = None\n\n    def _initialize_database(self):\n        self.connection.execute(\n            \"CREATE TABLE IF NOT EXISTS cache (hash TEXT PRIMARY KEY, result TEXT)\"\n        )\n        self.connection.execute(\n            \"CREATE INDEX IF NOT EXISTS cache_ndx ON cache(hash)\"\n        )\n\n    def fetch_or_compute(self, func, func_name, *args, **kwargs):\n        arg_hash = self._compute_hash(func_name, *args, **kwargs)\n\n        result = self._fetch_from_cache(arg_hash)\n        if result is not None:\n            return result\n\n        return self._compute_and_cache_result(func, arg_hash, *args, **kwargs)\n\n    def _compute_hash(self, func_name, *args, **kwargs):\n        data = f\"{func_name}:{repr(args)}:{repr(kwargs)}\".encode(\"utf-8\")\n        return hashlib.sha256(data).hexdigest()\n\n", "contexts_below": "\n    def _compute_and_cache_result(self, func, arg_hash, *args, **kwargs):\n        result = func(*args, **kwargs)\n        self._cache_result(arg_hash, result)\n        return result\n\n    def _cache_result(self, arg_hash, result):\n        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result))\n        )\n        self.connection.commit()", "input_code": "    def _fetch_from_cache(self, arg_hash):\n\n        \"\"\"\n        Fetches the cached result from an SQLite database using the hash of the argument. If the result is found, it is loaded from JSON format and returned; otherwise, None is returned.\n        Input-Output Arguments\n        :param self: SQLiteMemoization. An instance of the SQLiteMemoization class, which manages caching of function results in an SQLite database.\n        :param arg_hash: str, The hash of the argument for which the cached result is being fetched. It is used to query the database for the corresponding cached result.\n        :return: The result corresponding to the provided hash, loaded from JSON format if found; otherwise, None.\n        \"\"\"", "reference_steps": "1. Define a method `_fetch_from_cache` with parameters `self` and `arg_hash`.\n2. Create a cursor object from the database connection attribute `self.connection`.\n3. Execute an SQL query using the cursor to select the `result` column from the `cache` table where the `hash` column matches `arg_hash`.\n4. Fetch the first row of the result set from the cursor.\n5. Check if a row is returned (i.e., the result is not `None`).\n6. If a row is found, extract the first element of the row (which contains the cached result).\n7. Deserialize the JSON-formatted string (cached result) into a Python object using `json.loads`.\n8. Return the deserialized Python object.\n9. If no row is found (cache miss), return `None`.", "reference_code": "def _fetch_from_cache(self, arg_hash):\n    cursor = self.connection.cursor()\n    cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n    row = cursor.fetchone()\n    return json.loads(row[0]) if row else None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "type": "method", "class_name": "SQLiteMemoization", "function_name": "_cache_result", "dependency_all": "# Intra-class Dependency:\nmemoize.SQLiteMemoization.connection\n\n", "dependency_sampled": "# Intra-class Dependency:\nmemoize.SQLiteMemoization.connection\n\n", "contexts_above": "import sqlite3\nimport hashlib\nimport json\nimport functools\n\n## Originally from https://www.kevinkatz.io/posts/memoize-to-sqlite\n\ndef memoize_to_sqlite(func_name: str, filename: str = \"cache.db\"):\n    \"\"\"\n    Memoization decorator that caches the output of a method in a SQLite\n    database.\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            with SQLiteMemoization(filename) as memoizer:\n                return memoizer.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapped\n    return decorator\n\nclass SQLiteMemoization:\n    def __init__(self, filename):\n        self.filename = filename\n        self.connection = None\n\n    def __enter__(self):\n        self.connection = sqlite3.connect(self.filename)\n        self._initialize_database()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.connection.close()\n        self.connection = None\n\n    def _initialize_database(self):\n        self.connection.execute(\n            \"CREATE TABLE IF NOT EXISTS cache (hash TEXT PRIMARY KEY, result TEXT)\"\n        )\n        self.connection.execute(\n            \"CREATE INDEX IF NOT EXISTS cache_ndx ON cache(hash)\"\n        )\n\n    def fetch_or_compute(self, func, func_name, *args, **kwargs):\n        arg_hash = self._compute_hash(func_name, *args, **kwargs)\n\n        result = self._fetch_from_cache(arg_hash)\n        if result is not None:\n            return result\n\n        return self._compute_and_cache_result(func, arg_hash, *args, **kwargs)\n\n    def _compute_hash(self, func_name, *args, **kwargs):\n        data = f\"{func_name}:{repr(args)}:{repr(kwargs)}\".encode(\"utf-8\")\n        return hashlib.sha256(data).hexdigest()\n\n    def _fetch_from_cache(self, arg_hash):\n        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        return json.loads(row[0]) if row else None\n\n    def _compute_and_cache_result(self, func, arg_hash, *args, **kwargs):\n        result = func(*args, **kwargs)\n        self._cache_result(arg_hash, result)\n        return result\n\n", "contexts_below": "", "input_code": "    def _cache_result(self, arg_hash, result):\n\n        \"\"\"\n        This function caches the result of a computation in an SQLite database. It inserts a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value.\n\n        Input-Output Arguments\n        :param self: SQLiteMemoization. An instance of the SQLiteMemoization class, which manages caching of function results in an SQLite database.\n        :param arg_hash: str, The hash of the arguments to the function whose result is being cached. It uniquely identifies the function call.\n        :param result: Any, The result of the function call that is being cached. This result is serialized to JSON format before being stored in the database.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method named `_cache_result` with parameters `self`, `arg_hash`, and `result`.\n2. Obtain a cursor from the database connection associated with the current instance (`self.connection.cursor()`).\n3. Prepare an SQL `INSERT` statement to add a new record into the `cache` table with columns `hash` and `result`.\n4. Convert the `result` to a JSON string using `json.dumps(result)`.\n5. Execute the prepared SQL statement with the cursor, passing in `arg_hash` and the JSON string as parameters.\n6. Commit the transaction to the database to ensure the insertion is saved (`self.connection.commit()`).", "reference_code": "def _cache_result(self, arg_hash, result):\n    cursor = self.connection.cursor()\n    cursor.execute(\n        \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n        (arg_hash, json.dumps(result))\n    )\n    self.connection.commit()"}
{"namespace": "litdata.streaming.client.S3Client.client", "type": "method", "class_name": "S3Client", "function_name": "client", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.client.S3Client._client\n\nlitdata.streaming.client.S3Client._create_client\n    def _create_client(self) -> None:\n\nlitdata.streaming.client.S3Client._last_time\n\nlitdata.streaming.client.S3Client._refetch_interval\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.client.S3Client._last_time\n\nlitdata.streaming.client.S3Client._client\n\n", "contexts_above": "import os\nfrom time import time\nfrom typing import Any, Optional\n\nfrom litdata.constants import _BOTO3_AVAILABLE, _IS_IN_STUDIO\n\nif _BOTO3_AVAILABLE:\n    import boto3\n    import botocore\n    from botocore.credentials import InstanceMetadataProvider\n    from botocore.utils import InstanceMetadataFetcher\n\n\nclass S3Client:\n    # TODO: Generalize to support more cloud providers.\n\n    def __init__(self, refetch_interval: int = 3300) -> None:\n        self._refetch_interval = refetch_interval\n        self._last_time: Optional[float] = None\n        self._client: Optional[Any] = None\n\n    def _create_client(self) -> None:\n        has_shared_credentials_file = (\n            os.getenv(\"AWS_SHARED_CREDENTIALS_FILE\") == os.getenv(\"AWS_CONFIG_FILE\") == \"/.credentials/.aws_credentials\"\n        )\n\n        if has_shared_credentials_file or not _IS_IN_STUDIO:\n            self._client = boto3.client(\n                \"s3\", config=botocore.config.Config(retries={\"max_attempts\": 1000, \"mode\": \"adaptive\"})\n            )\n        else:\n            provider = InstanceMetadataProvider(iam_role_fetcher=InstanceMetadataFetcher(timeout=3600, num_attempts=5))\n            credentials = provider.load()\n            self._client = boto3.client(\n                \"s3\",\n                aws_access_key_id=credentials.access_key,\n                aws_secret_access_key=credentials.secret_key,\n                aws_session_token=credentials.token,\n                config=botocore.config.Config(retries={\"max_attempts\": 1000, \"mode\": \"adaptive\"}),\n            )\n\n    @property\n", "contexts_below": "", "input_code": "    def client(self) -> Any:\n\n        \"\"\"\n        This function returns an S3 client instance. It creates a new S3 client if one does not already exist or if the credentials have expired based on a predefined interval. This ensures that the client is always up-to-date and valid for use.\n\n        Input-Output Arguments\n        :param self: S3Client. An instance of the S3Client class. It uses the instance's attributes to manage the S3 client and its creation time.\n        :return: Any. The S3 client instance that is either retrieved from the existing instance or newly created if necessary.\n        \"\"\"", "reference_steps": "1. Define a method named `client` that checks if an existing client is set or needs to be created or refreshed.\n2. Check if the `_client` attribute is `None`, indicating that a client has not been created yet.\n3. If `_client` is `None`, call the `_create_client` method to create a new client.\n4. Set `_last_time` to the current time using the `time()` function after creating a new client.\n5. Check if `_last_time` is `None` or if the current time minus `_last_time` is greater than a predefined interval `_refetch_interval`.\n6. If the time condition is met, it indicates that the credentials are stale and need to be re-generated.\n7. Call the `_create_client` method again to refresh the client and its credentials.\n8. Update `_last_time` to the current time using the `time()` function after refreshing the client.\n9. Return the `_client` object.\n10. Ensure that the client is either created or refreshed based on the time interval before being returned.", "reference_code": "def client(self) -> Any:\n    if self._client is None:\n        self._create_client()\n        self._last_time = time()\n\n    # Re-generate credentials for EC2\n    if self._last_time is None or (time() - self._last_time) > self._refetch_interval:\n        self._create_client()\n        self._last_time = time()\n\n    return self._client\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "type": "method", "class_name": "StreamingDataset", "function_name": "state_dict", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.dataset.StreamingDataset._state_dict\n\nlitdata.streaming.dataset.StreamingDataset.current_epoch\n\nlitdata.streaming.dataset.StreamingDataset.distributed_env\n\nlitdata.streaming.dataset.StreamingDataset.drop_last\n\nlitdata.streaming.dataset.StreamingDataset.input_dir\n\nlitdata.streaming.dataset.StreamingDataset.item_loader\n\nlitdata.streaming.dataset.StreamingDataset.seed\n\nlitdata.streaming.dataset.StreamingDataset.shuffle\n\n# Cross-file Dependency:\nlitdata.streaming.item_loader.BaseItemLoader.state_dict\n    def state_dict(self) -> Dict:\n\nlitdata.streaming.resolver.Dir.path\n\nlitdata.streaming.resolver.Dir.url\n\nlitdata.utilities.env._DistributedEnv.world_size\n\nlitdata.utilities.env._is_in_dataloader_worker\n    def _is_in_dataloader_worker() -> bool:\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.dataset.StreamingDataset.item_loader\n\nlitdata.streaming.dataset.StreamingDataset.drop_last\n\nlitdata.streaming.dataset.StreamingDataset.input_dir\n\nlitdata.streaming.dataset.StreamingDataset.current_epoch\n\nlitdata.streaming.dataset.StreamingDataset.distributed_env\n\n# Cross-file Dependency:\nlitdata.utilities.env._is_in_dataloader_worker\n    def _is_in_dataloader_worker() -> bool:\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hashlib\nimport os\nfrom logging import Logger\nfrom time import time\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nfrom torch.utils.data import IterableDataset\n\nfrom litdata.constants import (\n    _DEFAULT_CACHE_DIR,\n    _INDEX_FILENAME,\n)\nfrom litdata.streaming import Cache\nfrom litdata.streaming.item_loader import BaseItemLoader\nfrom litdata.streaming.resolver import Dir, _resolve_dir\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer\nfrom litdata.streaming.shuffle import FullShuffle, NoShuffle, Shuffle\nfrom litdata.utilities.env import _DistributedEnv, _is_in_dataloader_worker, _WorkerEnv\n\nlogger = Logger(__name__)\n\n\nclass StreamingDataset(IterableDataset):\n    \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\"\"\"\n\n    def __init__(\n        self,\n        input_dir: Union[str, \"Dir\"],\n        item_loader: Optional[BaseItemLoader] = None,\n        shuffle: bool = False,\n        drop_last: Optional[bool] = None,\n        seed: int = 42,\n        serializers: Optional[Dict[str, Serializer]] = None,\n        max_cache_size: Union[int, str] = \"100GB\",\n    ) -> None:\n        \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\n\n        Arguments:\n            input_dir: Path to the folder where the input data is stored.\n            item_loader: The logic to load an item from a chunk.\n            shuffle: Whether to shuffle the data.\n            drop_last: If `True`, drops the last items to ensure that\n                all processes/workers return the same amount of data.\n                The argument `drop_last` is set to `True` in a distributed setting\n                and `False` otherwise.\n            seed: Random seed for shuffling.\n            serializers: The serializers used to serialize and deserialize the chunks.\n            max_cache_size: The maximum cache size used by the StreamingDataset.\n\n        \"\"\"\n        super().__init__()\n        if not isinstance(shuffle, bool):\n            raise ValueError(f\"Shuffle should be a boolean. Found {shuffle}\")\n\n        input_dir = _resolve_dir(input_dir)\n\n        self.input_dir = input_dir\n\n        self.item_loader = item_loader\n        self.shuffle: bool = shuffle\n        self.distributed_env = _DistributedEnv.detect()\n\n        if self.distributed_env.world_size > 1:\n            if drop_last is False:\n                logger.warn(\n                    \"You're operating within a distributed environment and have disabled the `drop_last` option. \"\n                    \"Please note that this configuration may lead to training interruptions if your system depends \"\n                    \"on distributed collectives.\"\n                )\n            else:\n                drop_last = True\n\n        self.drop_last = drop_last or False\n\n        self.seed = seed\n        self.max_cache_size = max_cache_size\n\n        self.cache: Optional[Cache] = None\n        self.worker_env: Optional[_WorkerEnv] = None\n        self.worker_chunks: List[int] = []\n        self.worker_intervals: List[List[int]] = []\n        self.current_indexes: List[int] = []\n        self.chunk_index = 0\n        self.num_chunks: Optional[int] = None\n        self.global_index = 0\n        self.index = 0\n        self.has_triggered_download = False\n        self.min_items_per_replica: Optional[int] = None\n        self.current_epoch = 1\n        self.random_state = None\n        self.shuffler: Optional[Shuffle] = None\n        self.serializers = serializers\n        self._state_dict: Optional[Dict[str, Any]] = None\n\n    def set_shuffle(self, shuffle: bool) -> None:\n        self.shuffle = shuffle\n\n    def set_epoch(self, current_epoch: int) -> None:\n        \"\"\"Set the current epoch to the dataset on epoch starts.\n\n        When using the StreamingDataLoader, this is done automatically\n\n        \"\"\"\n        # If the state dict has been reloaded, don't override the current epoch\n        # The StreamingDataloader would clean this out\n        if self._state_dict is None:\n            self.current_epoch = current_epoch\n\n    def _create_cache(self, worker_env: _WorkerEnv) -> Cache:\n        if _should_replace_path(self.input_dir.path):\n            cache_path = _try_create_cache_dir(\n                input_dir=self.input_dir.path if self.input_dir.path else self.input_dir.url\n            )\n            if cache_path is not None:\n                self.input_dir.path = cache_path\n\n        cache = Cache(\n            input_dir=self.input_dir,\n            item_loader=self.item_loader,\n            chunk_bytes=1,\n            serializers=self.serializers,\n            max_cache_size=self.max_cache_size,\n        )\n        cache._reader._try_load_config()\n\n        if not cache.filled:\n            raise ValueError(\n                f\"The provided dataset `{self.input_dir}` doesn't contain any {_INDEX_FILENAME} file.\"\n                \" HINT: Did you successfully optimize a dataset to the provided `input_dir`?\"\n            )\n\n        return cache\n\n    def _create_shuffler(self, cache: Cache) -> Shuffle:\n        seed = self.seed\n        drop_last = self.drop_last\n        if self._state_dict is not None:\n            state: Dict[str, Any] = self._state_dict\n            seed = state[\"seed\"]\n            drop_last = state[\"drop_last\"]\n        return FullShuffle(cache, seed, drop_last) if self.shuffle else NoShuffle(cache, seed, drop_last)\n\n    def __len__(self) -> int:\n        if self.shuffler is None:\n            cache = self._create_cache(worker_env=_WorkerEnv.detect())\n            self.shuffler = self._create_shuffler(cache)\n        return self.shuffler.get_len(self.distributed_env, self.current_epoch)\n\n    def __iter__(self) -> \"StreamingDataset\":\n        # When the StreamingDataset is used within map or optimize, let's refetch the distributed env.\n        if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n            self.distributed_env = _DistributedEnv.detect()\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        # Handle restart\n        if self._state_dict:\n            self._validate_state_dict()\n            state: Dict[str, Any] = self._state_dict\n            self.current_epoch = state[\"current_epoch\"]\n\n        chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n        chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n        intervals_replica = intervals_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(chunks_replica, intervals_replica)\n        else:\n            chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n                self.distributed_env, self.current_epoch\n            )\n            chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n            intervals_replica = intervals_per_replica[\n                self.distributed_env.global_rank % self.distributed_env.world_size\n            ]\n\n            self.worker_chunks = []\n            self.worker_intervals = []\n\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                if i % self.worker_env.world_size != self.worker_env.rank:\n                    continue\n                self.worker_chunks.append(chunk_index)\n                self.worker_intervals.append(chunk_interval)\n\n            self.num_chunks = len(self.worker_chunks)\n\n            self.current_indexes = []\n            self.chunk_index = 0\n            self.global_index = 0\n            self.index = 0\n\n        self.has_triggered_download = False\n        self.last_time = time()\n\n        return self\n\n    def _resume(self, chunks_replica: List[int], intervals_replica: List[Any]) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, chunks_replica, intervals_replica\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index = num_samples_yielded\n\n        # bump the chunk_index\n        self.chunk_index += 1\n\n    def __getitem__(self, index: Union[ChunkedIndex, int]) -> Any:\n        if self.cache is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n        if isinstance(index, int):\n            index = ChunkedIndex(index, self.cache._get_chunk_index_from_index(index))\n        return self.cache[index]\n\n    def __next__(self) -> Any:\n        # Prevent to create more batch on a given process\n        if self.global_index >= len(self):\n            self.current_epoch += 1\n            raise StopIteration\n\n        # Lazily re-populate the interval to reduce memory usage.\n        if len(self.current_indexes) == 0:\n            if self.chunk_index == self.num_chunks:\n                self.current_epoch += 1\n                raise StopIteration\n\n            # reset index\n            self.index = 0\n\n            interval = self.worker_intervals[self.chunk_index]\n            current_indexes = np.arange(interval[0], interval[1])\n\n            assert self.shuffler is not None\n            assert self.num_chunks is not None\n            self.current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n            self.chunk_index += 1\n\n        # Get the first index\n        index = self.current_indexes.pop(0)\n\n        # Call the `__getitem__` method.\n        data = self.__getitem__(\n            ChunkedIndex(\n                index=index,\n                chunk_index=self.worker_chunks[self.chunk_index - 1],\n                # We provide the chunks indexes only one the first\n                chunk_indexes=None if self.has_triggered_download else self.worker_chunks,\n                is_last_index=(self.chunk_index - 1) == len(self.worker_intervals) and len(self.current_indexes) == 1,\n            )\n        )\n\n        self.has_triggered_download = True\n        self.global_index += 1\n        self.index += 1\n\n        return data\n\n", "contexts_below": "\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        if state_dict:\n            # the state is restored within the workers\n            self._state_dict = state_dict\n\n    def _validate_state_dict(self) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                \"The provided `num_workers` state doesn't match the current one. \"\n                f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n            )\n\n        # Note: We need to check whether the path has been resolved to its associated cache.\n        # In this case, validate the cache folder is the same.\n        if _should_replace_path(state[\"input_dir_path\"]):\n            cache_path = _try_create_cache_dir(\n                input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n            )\n            if cache_path != self.input_dir.path:\n                raise ValueError(\n                    \"The provided `input_dir` path state doesn't match the current one. \"\n                    f\"Found `{self.input_dir.path}` instead of `{cache_path}`.\"\n                )\n        elif state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                \"The provided `input_dir` path state doesn't match the current one. \"\n                f\"Found `{self.input_dir.path}` instead of `{state['input_dir_path']}`.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                \"The provided `input_dir` URL state doesn't match the current one. \"\n                f\"Found `{self.input_dir.url}` instead of `{state['input_dir_url']}`.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                \"The provided `seed` state doesn't match the current one. \"\n                f\"Found `{self.seed}` instead of `{state['seed']}`.\"\n            )\n\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                \"The provided `item_loader` state doesn't match the current one. \"\n                f\"Found `{self.item_loader.state_dict()}` instead of `{state['item_loader']}`.\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                \"The provided `drop_last` state doesn't match the current one. \"\n                f\"Found `{self.drop_last}` instead of `{state['drop_last']}`.\"\n            )\n\n\ndef _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n    hash_object = hashlib.md5((input_dir or \"\").encode())\n    if \"LIGHTNING_CLUSTER_ID\" not in os.environ or \"LIGHTNING_CLOUD_PROJECT_ID\" not in os.environ:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    cache_dir = os.path.join(\"/cache\", \"chunks\", hash_object.hexdigest())\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n\n\ndef _should_replace_path(path: Optional[str]) -> bool:\n    \"\"\"Whether the input path is a special path to be replaced.\"\"\"\n    if path is None or path == \"\":\n        return True\n\n    return path.startswith(\"/teamspace/datasets/\") or path.startswith(\"/teamspace/s3_connections/\")\n\n\ndef is_integer(value: str) -> bool:\n    try:\n        int(value)\n        return True\n    except Exception:\n        return False\n\n\ndef _associate_chunks_to_workers(\n    num_workers: int, worker_env: _WorkerEnv, chunks_replica: List[int], intervals_replica: List[Any]\n) -> Any:\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_idx:\n                continue\n\n            worker_chunks.append(chunk_index)\n            worker_intervals.append(chunk_interval)\n\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    return workers_chunks, workers_intervals\n\n\ndef _replay_sampling(num_samples_yielded: int, batch_size: int, num_workers: int) -> Dict[int, int]:\n    \"\"\"This function replays the sampling from the dataloader.\"\"\"\n    divisible_num_batches_yielded = num_samples_yielded // (num_workers * batch_size)\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = divisible_num_batches_yielded * batch_size\n\n    num_samples_yielded = num_samples_yielded - (num_workers * divisible_num_batches_yielded * batch_size)\n\n    # take care of the reminder\n    worker_idx = 0  # reset the worker_idx\n    while True:\n        if num_samples_yielded >= batch_size:\n            indexes[worker_idx] += batch_size\n            worker_idx = (worker_idx + 1) % num_workers\n            num_samples_yielded -= batch_size\n        else:\n            indexes[worker_idx] += num_samples_yielded\n            break\n    return indexes\n\n\ndef _replay_chunks_sampling(\n    workers_intervals: Dict[int, List[Any]], indexes: Dict[int, int]\n) -> Tuple[Dict[int, int], Dict[int, int]]:\n    chunks_index = {}\n\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[-1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes\n", "input_code": "    def state_dict(self, num_samples_yielded: int, num_workers: int, batch_size: int) -> Dict[str, Any]:\n\n        \"\"\"\n        The `state_dict` method generates and returns a dictionary representing the current state of the StreamingDataset instance, including information about the number of samples yielded, the number of workers, batch size, and other relevant dataset parameters. It raises an error if called from a DataLoader worker process.\n\n        Input-Output Arguments\n        :param num_samples_yielded: int, the number of samples that have been yielded by the dataset.\n        :param num_workers: int, the number of worker processes used for loading the data.\n        :param batch_size: int, the size of the batches that data is divided into for processing.\n        :return: Dict[str, Any], a dictionary containing the current state of the StreamingDataset instance, including details such as the number of samples yielded, number of workers, batch size, current epoch, input directory path and URL, item loader state (if applicable), whether the last batch is dropped, seed, world size of the distributed environment, and shuffle status.\n        \"\"\"", "reference_steps": "1. Define a method `state_dict` that takes `num_samples_yielded`, `num_workers`, and `batch_size` as arguments and returns a dictionary representing the state of an object.\n\n2. Check if the method is being called within a DataLoader worker process using the `_is_in_dataloader_worker` function.\n\n3. Raise a `RuntimeError` if the method is called from within a DataLoader worker to ensure it is only used in the main process.\n\n4. If the object's `_state_dict` attribute is already set, update the `num_samples_yielded` key with the new value and return the `_state_dict`.\n\n5. If the `_state_dict` is not set, create a new dictionary `state` with keys and values for `num_samples_yielded`, `num_workers`, `batch_size`, `current_epoch`, `input_dir_path`, `input_dir_url`, `item_loader` state (if available), `drop_last`, `seed`, `world_size`, and `shuffle`.\n\n6. Extract `current_epoch` from the object's attribute.\n\n7. Get the `input_dir` path and URL from the object's `input_dir` attribute.\n\n8. If `item_loader` is not `None`, call its `state_dict` method and include the result in the `state` dictionary.\n\n9. Include the object's `drop_last`, `seed`, `world_size`, and `shuffle` attributes in the `state` dictionary.\n\n10. Return the constructed `state` dictionary.", "reference_code": "def state_dict(self, num_samples_yielded: int, num_workers: int, batch_size: int) -> Dict[str, Any]:\n    if _is_in_dataloader_worker():\n        raise RuntimeError(\"The method `state_dict` should only be called in the main process.\")\n\n    if self._state_dict is not None:\n        self._state_dict[\"num_samples_yielded\"] = num_samples_yielded\n        return self._state_dict\n\n    state = {\n        \"num_samples_yielded\": num_samples_yielded,\n        \"num_workers\": num_workers,\n        \"batch_size\": batch_size,\n        \"current_epoch\": self.current_epoch,\n        \"input_dir_path\": self.input_dir.path,\n        \"input_dir_url\": self.input_dir.url,\n        \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n        \"drop_last\": self.drop_last,\n        \"seed\": self.seed,\n        \"world_size\": self.distributed_env.world_size,\n        \"shuffle\": self.shuffle,\n    }\n\n    return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "type": "method", "class_name": "StreamingDataset", "function_name": "load_state_dict", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.dataset.StreamingDataset._state_dict\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.dataset.StreamingDataset._state_dict\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hashlib\nimport os\nfrom logging import Logger\nfrom time import time\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nfrom torch.utils.data import IterableDataset\n\nfrom litdata.constants import (\n    _DEFAULT_CACHE_DIR,\n    _INDEX_FILENAME,\n)\nfrom litdata.streaming import Cache\nfrom litdata.streaming.item_loader import BaseItemLoader\nfrom litdata.streaming.resolver import Dir, _resolve_dir\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer\nfrom litdata.streaming.shuffle import FullShuffle, NoShuffle, Shuffle\nfrom litdata.utilities.env import _DistributedEnv, _is_in_dataloader_worker, _WorkerEnv\n\nlogger = Logger(__name__)\n\n\nclass StreamingDataset(IterableDataset):\n    \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\"\"\"\n\n    def __init__(\n        self,\n        input_dir: Union[str, \"Dir\"],\n        item_loader: Optional[BaseItemLoader] = None,\n        shuffle: bool = False,\n        drop_last: Optional[bool] = None,\n        seed: int = 42,\n        serializers: Optional[Dict[str, Serializer]] = None,\n        max_cache_size: Union[int, str] = \"100GB\",\n    ) -> None:\n        \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\n\n        Arguments:\n            input_dir: Path to the folder where the input data is stored.\n            item_loader: The logic to load an item from a chunk.\n            shuffle: Whether to shuffle the data.\n            drop_last: If `True`, drops the last items to ensure that\n                all processes/workers return the same amount of data.\n                The argument `drop_last` is set to `True` in a distributed setting\n                and `False` otherwise.\n            seed: Random seed for shuffling.\n            serializers: The serializers used to serialize and deserialize the chunks.\n            max_cache_size: The maximum cache size used by the StreamingDataset.\n\n        \"\"\"\n        super().__init__()\n        if not isinstance(shuffle, bool):\n            raise ValueError(f\"Shuffle should be a boolean. Found {shuffle}\")\n\n        input_dir = _resolve_dir(input_dir)\n\n        self.input_dir = input_dir\n\n        self.item_loader = item_loader\n        self.shuffle: bool = shuffle\n        self.distributed_env = _DistributedEnv.detect()\n\n        if self.distributed_env.world_size > 1:\n            if drop_last is False:\n                logger.warn(\n                    \"You're operating within a distributed environment and have disabled the `drop_last` option. \"\n                    \"Please note that this configuration may lead to training interruptions if your system depends \"\n                    \"on distributed collectives.\"\n                )\n            else:\n                drop_last = True\n\n        self.drop_last = drop_last or False\n\n        self.seed = seed\n        self.max_cache_size = max_cache_size\n\n        self.cache: Optional[Cache] = None\n        self.worker_env: Optional[_WorkerEnv] = None\n        self.worker_chunks: List[int] = []\n        self.worker_intervals: List[List[int]] = []\n        self.current_indexes: List[int] = []\n        self.chunk_index = 0\n        self.num_chunks: Optional[int] = None\n        self.global_index = 0\n        self.index = 0\n        self.has_triggered_download = False\n        self.min_items_per_replica: Optional[int] = None\n        self.current_epoch = 1\n        self.random_state = None\n        self.shuffler: Optional[Shuffle] = None\n        self.serializers = serializers\n        self._state_dict: Optional[Dict[str, Any]] = None\n\n    def set_shuffle(self, shuffle: bool) -> None:\n        self.shuffle = shuffle\n\n    def set_epoch(self, current_epoch: int) -> None:\n        \"\"\"Set the current epoch to the dataset on epoch starts.\n\n        When using the StreamingDataLoader, this is done automatically\n\n        \"\"\"\n        # If the state dict has been reloaded, don't override the current epoch\n        # The StreamingDataloader would clean this out\n        if self._state_dict is None:\n            self.current_epoch = current_epoch\n\n    def _create_cache(self, worker_env: _WorkerEnv) -> Cache:\n        if _should_replace_path(self.input_dir.path):\n            cache_path = _try_create_cache_dir(\n                input_dir=self.input_dir.path if self.input_dir.path else self.input_dir.url\n            )\n            if cache_path is not None:\n                self.input_dir.path = cache_path\n\n        cache = Cache(\n            input_dir=self.input_dir,\n            item_loader=self.item_loader,\n            chunk_bytes=1,\n            serializers=self.serializers,\n            max_cache_size=self.max_cache_size,\n        )\n        cache._reader._try_load_config()\n\n        if not cache.filled:\n            raise ValueError(\n                f\"The provided dataset `{self.input_dir}` doesn't contain any {_INDEX_FILENAME} file.\"\n                \" HINT: Did you successfully optimize a dataset to the provided `input_dir`?\"\n            )\n\n        return cache\n\n    def _create_shuffler(self, cache: Cache) -> Shuffle:\n        seed = self.seed\n        drop_last = self.drop_last\n        if self._state_dict is not None:\n            state: Dict[str, Any] = self._state_dict\n            seed = state[\"seed\"]\n            drop_last = state[\"drop_last\"]\n        return FullShuffle(cache, seed, drop_last) if self.shuffle else NoShuffle(cache, seed, drop_last)\n\n    def __len__(self) -> int:\n        if self.shuffler is None:\n            cache = self._create_cache(worker_env=_WorkerEnv.detect())\n            self.shuffler = self._create_shuffler(cache)\n        return self.shuffler.get_len(self.distributed_env, self.current_epoch)\n\n    def __iter__(self) -> \"StreamingDataset\":\n        # When the StreamingDataset is used within map or optimize, let's refetch the distributed env.\n        if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n            self.distributed_env = _DistributedEnv.detect()\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        # Handle restart\n        if self._state_dict:\n            self._validate_state_dict()\n            state: Dict[str, Any] = self._state_dict\n            self.current_epoch = state[\"current_epoch\"]\n\n        chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n        chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n        intervals_replica = intervals_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(chunks_replica, intervals_replica)\n        else:\n            chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n                self.distributed_env, self.current_epoch\n            )\n            chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n            intervals_replica = intervals_per_replica[\n                self.distributed_env.global_rank % self.distributed_env.world_size\n            ]\n\n            self.worker_chunks = []\n            self.worker_intervals = []\n\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                if i % self.worker_env.world_size != self.worker_env.rank:\n                    continue\n                self.worker_chunks.append(chunk_index)\n                self.worker_intervals.append(chunk_interval)\n\n            self.num_chunks = len(self.worker_chunks)\n\n            self.current_indexes = []\n            self.chunk_index = 0\n            self.global_index = 0\n            self.index = 0\n\n        self.has_triggered_download = False\n        self.last_time = time()\n\n        return self\n\n    def _resume(self, chunks_replica: List[int], intervals_replica: List[Any]) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, chunks_replica, intervals_replica\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index = num_samples_yielded\n\n        # bump the chunk_index\n        self.chunk_index += 1\n\n    def __getitem__(self, index: Union[ChunkedIndex, int]) -> Any:\n        if self.cache is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n        if isinstance(index, int):\n            index = ChunkedIndex(index, self.cache._get_chunk_index_from_index(index))\n        return self.cache[index]\n\n    def __next__(self) -> Any:\n        # Prevent to create more batch on a given process\n        if self.global_index >= len(self):\n            self.current_epoch += 1\n            raise StopIteration\n\n        # Lazily re-populate the interval to reduce memory usage.\n        if len(self.current_indexes) == 0:\n            if self.chunk_index == self.num_chunks:\n                self.current_epoch += 1\n                raise StopIteration\n\n            # reset index\n            self.index = 0\n\n            interval = self.worker_intervals[self.chunk_index]\n            current_indexes = np.arange(interval[0], interval[1])\n\n            assert self.shuffler is not None\n            assert self.num_chunks is not None\n            self.current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n            self.chunk_index += 1\n\n        # Get the first index\n        index = self.current_indexes.pop(0)\n\n        # Call the `__getitem__` method.\n        data = self.__getitem__(\n            ChunkedIndex(\n                index=index,\n                chunk_index=self.worker_chunks[self.chunk_index - 1],\n                # We provide the chunks indexes only one the first\n                chunk_indexes=None if self.has_triggered_download else self.worker_chunks,\n                is_last_index=(self.chunk_index - 1) == len(self.worker_intervals) and len(self.current_indexes) == 1,\n            )\n        )\n\n        self.has_triggered_download = True\n        self.global_index += 1\n        self.index += 1\n\n        return data\n\n    def state_dict(self, num_samples_yielded: int, num_workers: int, batch_size: int) -> Dict[str, Any]:\n        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            self._state_dict[\"num_samples_yielded\"] = num_samples_yielded\n            return self._state_dict\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n\n", "contexts_below": "\n    def _validate_state_dict(self) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                \"The provided `num_workers` state doesn't match the current one. \"\n                f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n            )\n\n        # Note: We need to check whether the path has been resolved to its associated cache.\n        # In this case, validate the cache folder is the same.\n        if _should_replace_path(state[\"input_dir_path\"]):\n            cache_path = _try_create_cache_dir(\n                input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n            )\n            if cache_path != self.input_dir.path:\n                raise ValueError(\n                    \"The provided `input_dir` path state doesn't match the current one. \"\n                    f\"Found `{self.input_dir.path}` instead of `{cache_path}`.\"\n                )\n        elif state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                \"The provided `input_dir` path state doesn't match the current one. \"\n                f\"Found `{self.input_dir.path}` instead of `{state['input_dir_path']}`.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                \"The provided `input_dir` URL state doesn't match the current one. \"\n                f\"Found `{self.input_dir.url}` instead of `{state['input_dir_url']}`.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                \"The provided `seed` state doesn't match the current one. \"\n                f\"Found `{self.seed}` instead of `{state['seed']}`.\"\n            )\n\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                \"The provided `item_loader` state doesn't match the current one. \"\n                f\"Found `{self.item_loader.state_dict()}` instead of `{state['item_loader']}`.\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                \"The provided `drop_last` state doesn't match the current one. \"\n                f\"Found `{self.drop_last}` instead of `{state['drop_last']}`.\"\n            )\n\n\ndef _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n    hash_object = hashlib.md5((input_dir or \"\").encode())\n    if \"LIGHTNING_CLUSTER_ID\" not in os.environ or \"LIGHTNING_CLOUD_PROJECT_ID\" not in os.environ:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    cache_dir = os.path.join(\"/cache\", \"chunks\", hash_object.hexdigest())\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n\n\ndef _should_replace_path(path: Optional[str]) -> bool:\n    \"\"\"Whether the input path is a special path to be replaced.\"\"\"\n    if path is None or path == \"\":\n        return True\n\n    return path.startswith(\"/teamspace/datasets/\") or path.startswith(\"/teamspace/s3_connections/\")\n\n\ndef is_integer(value: str) -> bool:\n    try:\n        int(value)\n        return True\n    except Exception:\n        return False\n\n\ndef _associate_chunks_to_workers(\n    num_workers: int, worker_env: _WorkerEnv, chunks_replica: List[int], intervals_replica: List[Any]\n) -> Any:\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_idx:\n                continue\n\n            worker_chunks.append(chunk_index)\n            worker_intervals.append(chunk_interval)\n\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    return workers_chunks, workers_intervals\n\n\ndef _replay_sampling(num_samples_yielded: int, batch_size: int, num_workers: int) -> Dict[int, int]:\n    \"\"\"This function replays the sampling from the dataloader.\"\"\"\n    divisible_num_batches_yielded = num_samples_yielded // (num_workers * batch_size)\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = divisible_num_batches_yielded * batch_size\n\n    num_samples_yielded = num_samples_yielded - (num_workers * divisible_num_batches_yielded * batch_size)\n\n    # take care of the reminder\n    worker_idx = 0  # reset the worker_idx\n    while True:\n        if num_samples_yielded >= batch_size:\n            indexes[worker_idx] += batch_size\n            worker_idx = (worker_idx + 1) % num_workers\n            num_samples_yielded -= batch_size\n        else:\n            indexes[worker_idx] += num_samples_yielded\n            break\n    return indexes\n\n\ndef _replay_chunks_sampling(\n    workers_intervals: Dict[int, List[Any]], indexes: Dict[int, int]\n) -> Tuple[Dict[int, int], Dict[int, int]]:\n    chunks_index = {}\n\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[-1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes\n", "input_code": "    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n\n        \"\"\"\n        This function loads a given state dictionary into the StreamingDataset instance, effectively restoring its state based on the provided dictionary. This is particularly useful for resuming data streaming processes from a saved state.\n\n        Input-Output Arguments\n        :param self: StreamingDataset. An instance of the StreamingDataset class.\n        :param state_dict: Dict[str, Any]. A dictionary containing the state to be loaded into the StreamingDataset instance. It maps state keys to their corresponding values.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method `load_state_dict` within a class, which takes `self` and a dictionary `state_dict` as parameters.\n2. Check if `state_dict` is not empty or `None`.\n3. If `state_dict` has content, assign it to the instance variable `_state_dict` of the class.\n4. The method does not return any value (`None`).", "reference_code": "def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n    if state_dict:\n        # the state is restored within the workers\n        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "type": "method", "class_name": "StreamingDataset", "function_name": "_validate_state_dict", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.dataset.StreamingDataset._state_dict\n\nlitdata.streaming.dataset.StreamingDataset.cache\n\nlitdata.streaming.dataset.StreamingDataset.drop_last\n\nlitdata.streaming.dataset.StreamingDataset.input_dir\n\nlitdata.streaming.dataset.StreamingDataset.item_loader\n\nlitdata.streaming.dataset.StreamingDataset.seed\n\nlitdata.streaming.dataset.StreamingDataset.shuffle\n\nlitdata.streaming.dataset.StreamingDataset.worker_env\n\n# Intra-file Dependency:\nlitdata.streaming.dataset._should_replace_path\n    def _should_replace_path(path: Optional[str]) -> bool:\n        \"\"\"Whether the input path is a special path to be replaced.\"\"\"\n\nlitdata.streaming.dataset._try_create_cache_dir\n    def _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n\nlitdata.utilities.env._WorkerEnv.world_size\n\n# Cross-file Dependency:\nlitdata.streaming.item_loader.BaseItemLoader.state_dict\n    def state_dict(self) -> Dict:\n\nlitdata.streaming.resolver.Dir.path\n\nlitdata.streaming.resolver.Dir.url\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.dataset.StreamingDataset.drop_last\n\nlitdata.streaming.dataset.StreamingDataset.worker_env\n\nlitdata.streaming.dataset.StreamingDataset.shuffle\n\nlitdata.streaming.dataset.StreamingDataset.item_loader\n\nlitdata.streaming.dataset.StreamingDataset.input_dir\n\nlitdata.streaming.dataset.StreamingDataset.seed\n\n# Intra-file Dependency:\nlitdata.streaming.dataset._should_replace_path\n    def _should_replace_path(path: Optional[str]) -> bool:\n        \"\"\"Whether the input path is a special path to be replaced.\"\"\"\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hashlib\nimport os\nfrom logging import Logger\nfrom time import time\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nfrom torch.utils.data import IterableDataset\n\nfrom litdata.constants import (\n    _DEFAULT_CACHE_DIR,\n    _INDEX_FILENAME,\n)\nfrom litdata.streaming import Cache\nfrom litdata.streaming.item_loader import BaseItemLoader\nfrom litdata.streaming.resolver import Dir, _resolve_dir\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer\nfrom litdata.streaming.shuffle import FullShuffle, NoShuffle, Shuffle\nfrom litdata.utilities.env import _DistributedEnv, _is_in_dataloader_worker, _WorkerEnv\n\nlogger = Logger(__name__)\n\n\nclass StreamingDataset(IterableDataset):\n    \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\"\"\"\n\n    def __init__(\n        self,\n        input_dir: Union[str, \"Dir\"],\n        item_loader: Optional[BaseItemLoader] = None,\n        shuffle: bool = False,\n        drop_last: Optional[bool] = None,\n        seed: int = 42,\n        serializers: Optional[Dict[str, Serializer]] = None,\n        max_cache_size: Union[int, str] = \"100GB\",\n    ) -> None:\n        \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\n\n        Arguments:\n            input_dir: Path to the folder where the input data is stored.\n            item_loader: The logic to load an item from a chunk.\n            shuffle: Whether to shuffle the data.\n            drop_last: If `True`, drops the last items to ensure that\n                all processes/workers return the same amount of data.\n                The argument `drop_last` is set to `True` in a distributed setting\n                and `False` otherwise.\n            seed: Random seed for shuffling.\n            serializers: The serializers used to serialize and deserialize the chunks.\n            max_cache_size: The maximum cache size used by the StreamingDataset.\n\n        \"\"\"\n        super().__init__()\n        if not isinstance(shuffle, bool):\n            raise ValueError(f\"Shuffle should be a boolean. Found {shuffle}\")\n\n        input_dir = _resolve_dir(input_dir)\n\n        self.input_dir = input_dir\n\n        self.item_loader = item_loader\n        self.shuffle: bool = shuffle\n        self.distributed_env = _DistributedEnv.detect()\n\n        if self.distributed_env.world_size > 1:\n            if drop_last is False:\n                logger.warn(\n                    \"You're operating within a distributed environment and have disabled the `drop_last` option. \"\n                    \"Please note that this configuration may lead to training interruptions if your system depends \"\n                    \"on distributed collectives.\"\n                )\n            else:\n                drop_last = True\n\n        self.drop_last = drop_last or False\n\n        self.seed = seed\n        self.max_cache_size = max_cache_size\n\n        self.cache: Optional[Cache] = None\n        self.worker_env: Optional[_WorkerEnv] = None\n        self.worker_chunks: List[int] = []\n        self.worker_intervals: List[List[int]] = []\n        self.current_indexes: List[int] = []\n        self.chunk_index = 0\n        self.num_chunks: Optional[int] = None\n        self.global_index = 0\n        self.index = 0\n        self.has_triggered_download = False\n        self.min_items_per_replica: Optional[int] = None\n        self.current_epoch = 1\n        self.random_state = None\n        self.shuffler: Optional[Shuffle] = None\n        self.serializers = serializers\n        self._state_dict: Optional[Dict[str, Any]] = None\n\n    def set_shuffle(self, shuffle: bool) -> None:\n        self.shuffle = shuffle\n\n    def set_epoch(self, current_epoch: int) -> None:\n        \"\"\"Set the current epoch to the dataset on epoch starts.\n\n        When using the StreamingDataLoader, this is done automatically\n\n        \"\"\"\n        # If the state dict has been reloaded, don't override the current epoch\n        # The StreamingDataloader would clean this out\n        if self._state_dict is None:\n            self.current_epoch = current_epoch\n\n    def _create_cache(self, worker_env: _WorkerEnv) -> Cache:\n        if _should_replace_path(self.input_dir.path):\n            cache_path = _try_create_cache_dir(\n                input_dir=self.input_dir.path if self.input_dir.path else self.input_dir.url\n            )\n            if cache_path is not None:\n                self.input_dir.path = cache_path\n\n        cache = Cache(\n            input_dir=self.input_dir,\n            item_loader=self.item_loader,\n            chunk_bytes=1,\n            serializers=self.serializers,\n            max_cache_size=self.max_cache_size,\n        )\n        cache._reader._try_load_config()\n\n        if not cache.filled:\n            raise ValueError(\n                f\"The provided dataset `{self.input_dir}` doesn't contain any {_INDEX_FILENAME} file.\"\n                \" HINT: Did you successfully optimize a dataset to the provided `input_dir`?\"\n            )\n\n        return cache\n\n    def _create_shuffler(self, cache: Cache) -> Shuffle:\n        seed = self.seed\n        drop_last = self.drop_last\n        if self._state_dict is not None:\n            state: Dict[str, Any] = self._state_dict\n            seed = state[\"seed\"]\n            drop_last = state[\"drop_last\"]\n        return FullShuffle(cache, seed, drop_last) if self.shuffle else NoShuffle(cache, seed, drop_last)\n\n    def __len__(self) -> int:\n        if self.shuffler is None:\n            cache = self._create_cache(worker_env=_WorkerEnv.detect())\n            self.shuffler = self._create_shuffler(cache)\n        return self.shuffler.get_len(self.distributed_env, self.current_epoch)\n\n    def __iter__(self) -> \"StreamingDataset\":\n        # When the StreamingDataset is used within map or optimize, let's refetch the distributed env.\n        if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n            self.distributed_env = _DistributedEnv.detect()\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        # Handle restart\n        if self._state_dict:\n            self._validate_state_dict()\n            state: Dict[str, Any] = self._state_dict\n            self.current_epoch = state[\"current_epoch\"]\n\n        chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n        chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n        intervals_replica = intervals_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(chunks_replica, intervals_replica)\n        else:\n            chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n                self.distributed_env, self.current_epoch\n            )\n            chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n            intervals_replica = intervals_per_replica[\n                self.distributed_env.global_rank % self.distributed_env.world_size\n            ]\n\n            self.worker_chunks = []\n            self.worker_intervals = []\n\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                if i % self.worker_env.world_size != self.worker_env.rank:\n                    continue\n                self.worker_chunks.append(chunk_index)\n                self.worker_intervals.append(chunk_interval)\n\n            self.num_chunks = len(self.worker_chunks)\n\n            self.current_indexes = []\n            self.chunk_index = 0\n            self.global_index = 0\n            self.index = 0\n\n        self.has_triggered_download = False\n        self.last_time = time()\n\n        return self\n\n    def _resume(self, chunks_replica: List[int], intervals_replica: List[Any]) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, chunks_replica, intervals_replica\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index = num_samples_yielded\n\n        # bump the chunk_index\n        self.chunk_index += 1\n\n    def __getitem__(self, index: Union[ChunkedIndex, int]) -> Any:\n        if self.cache is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n        if isinstance(index, int):\n            index = ChunkedIndex(index, self.cache._get_chunk_index_from_index(index))\n        return self.cache[index]\n\n    def __next__(self) -> Any:\n        # Prevent to create more batch on a given process\n        if self.global_index >= len(self):\n            self.current_epoch += 1\n            raise StopIteration\n\n        # Lazily re-populate the interval to reduce memory usage.\n        if len(self.current_indexes) == 0:\n            if self.chunk_index == self.num_chunks:\n                self.current_epoch += 1\n                raise StopIteration\n\n            # reset index\n            self.index = 0\n\n            interval = self.worker_intervals[self.chunk_index]\n            current_indexes = np.arange(interval[0], interval[1])\n\n            assert self.shuffler is not None\n            assert self.num_chunks is not None\n            self.current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n            self.chunk_index += 1\n\n        # Get the first index\n        index = self.current_indexes.pop(0)\n\n        # Call the `__getitem__` method.\n        data = self.__getitem__(\n            ChunkedIndex(\n                index=index,\n                chunk_index=self.worker_chunks[self.chunk_index - 1],\n                # We provide the chunks indexes only one the first\n                chunk_indexes=None if self.has_triggered_download else self.worker_chunks,\n                is_last_index=(self.chunk_index - 1) == len(self.worker_intervals) and len(self.current_indexes) == 1,\n            )\n        )\n\n        self.has_triggered_download = True\n        self.global_index += 1\n        self.index += 1\n\n        return data\n\n    def state_dict(self, num_samples_yielded: int, num_workers: int, batch_size: int) -> Dict[str, Any]:\n        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            self._state_dict[\"num_samples_yielded\"] = num_samples_yielded\n            return self._state_dict\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        if state_dict:\n            # the state is restored within the workers\n            self._state_dict = state_dict\n\n", "contexts_below": "\n\ndef _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n    hash_object = hashlib.md5((input_dir or \"\").encode())\n    if \"LIGHTNING_CLUSTER_ID\" not in os.environ or \"LIGHTNING_CLOUD_PROJECT_ID\" not in os.environ:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    cache_dir = os.path.join(\"/cache\", \"chunks\", hash_object.hexdigest())\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n\n\ndef _should_replace_path(path: Optional[str]) -> bool:\n    \"\"\"Whether the input path is a special path to be replaced.\"\"\"\n    if path is None or path == \"\":\n        return True\n\n    return path.startswith(\"/teamspace/datasets/\") or path.startswith(\"/teamspace/s3_connections/\")\n\n\ndef is_integer(value: str) -> bool:\n    try:\n        int(value)\n        return True\n    except Exception:\n        return False\n\n\ndef _associate_chunks_to_workers(\n    num_workers: int, worker_env: _WorkerEnv, chunks_replica: List[int], intervals_replica: List[Any]\n) -> Any:\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_idx:\n                continue\n\n            worker_chunks.append(chunk_index)\n            worker_intervals.append(chunk_interval)\n\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    return workers_chunks, workers_intervals\n\n\ndef _replay_sampling(num_samples_yielded: int, batch_size: int, num_workers: int) -> Dict[int, int]:\n    \"\"\"This function replays the sampling from the dataloader.\"\"\"\n    divisible_num_batches_yielded = num_samples_yielded // (num_workers * batch_size)\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = divisible_num_batches_yielded * batch_size\n\n    num_samples_yielded = num_samples_yielded - (num_workers * divisible_num_batches_yielded * batch_size)\n\n    # take care of the reminder\n    worker_idx = 0  # reset the worker_idx\n    while True:\n        if num_samples_yielded >= batch_size:\n            indexes[worker_idx] += batch_size\n            worker_idx = (worker_idx + 1) % num_workers\n            num_samples_yielded -= batch_size\n        else:\n            indexes[worker_idx] += num_samples_yielded\n            break\n    return indexes\n\n\ndef _replay_chunks_sampling(\n    workers_intervals: Dict[int, List[Any]], indexes: Dict[int, int]\n) -> Tuple[Dict[int, int], Dict[int, int]]:\n    chunks_index = {}\n\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[-1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes\n", "input_code": "    def _validate_state_dict(self) -> None:\n\n        \"\"\"\n        This function validates the state dictionary of a StreamingDataset instance against its current state. It checks for consistency across various parameters such as shuffle, num_workers, input directory path and URL, seed, item_loader state, and drop_last flag. If any of the parameters in the state dictionary do not match the current state of the StreamingDataset instance, a ValueError is raised indicating the mismatch.\n\n        Input-Output Arguments\n        :param self: StreamingDataset. An instance of the StreamingDataset class. It uses various attributes of the instance such as _state_dict, worker_env, cache, shuffle, etc., to validate the state dictionary against the current state of the instance.\n        :return: No return values. This function raises a ValueError if there is a mismatch between the state dictionary and the current state of the StreamingDataset instance.\n        \"\"\"", "reference_steps": "1. Define a method `_validate_state_dict` within a class to validate the internal state dictionary against the current object's attributes.\n\n2. Ensure that the internal state dictionary (`_state_dict`), worker environment (`worker_env`), and cache are not `None` by using assertions.\n\n3. Retrieve the state dictionary and store it in a local variable `state`.\n\n4. Compare the `shuffle` value in the state dictionary with the object's `shuffle` attribute and raise a `ValueError` if they do not match.\n\n5. Check if the `num_workers` value in the state dictionary matches the `world_size` attribute of the `worker_env` and raise a `ValueError` if there is a mismatch.\n\n6. Determine if the path in the state dictionary should be replaced by checking if `_should_replace_path(state[\"input_dir_path\"])` is `True`.\n\n7. If the path should be replaced, attempt to create a cache directory with `_try_create_cache_dir` and compare the resulting path with the object's `input_dir.path`. Raise a `ValueError` if they differ.\n\n8. If the path should not be replaced, directly compare the `input_dir_path` from the state dictionary with the object's `input_dir.path` and raise a `ValueError` if they do not match.\n\n9. Compare the `input_dir_url` from the state dictionary with the object's `input_dir.url` and raise a `ValueError` if there is a discrepancy.\n\n10. Validate the rest of the state dictionary values (`seed`, `item_loader`, and `drop_last`) against their corresponding attributes in the object, raising a `ValueError` for any mismatches found.", "reference_code": "def _validate_state_dict(self) -> None:\n    assert self._state_dict\n    assert self.worker_env\n    assert self.cache\n\n    state: Dict[str, Any] = self._state_dict\n\n    if state[\"shuffle\"] != self.shuffle:\n        raise ValueError(\n            \"The provided `shuffle` state doesn't match the current one. \"\n            f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n        )\n\n    if state[\"num_workers\"] != self.worker_env.world_size:\n        raise ValueError(\n            \"The provided `num_workers` state doesn't match the current one. \"\n            f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n        )\n\n    # Note: We need to check whether the path has been resolved to its associated cache.\n    # In this case, validate the cache folder is the same.\n    if _should_replace_path(state[\"input_dir_path\"]):\n        cache_path = _try_create_cache_dir(\n            input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n        )\n        if cache_path != self.input_dir.path:\n            raise ValueError(\n                \"The provided `input_dir` path state doesn't match the current one. \"\n                f\"Found `{self.input_dir.path}` instead of `{cache_path}`.\"\n            )\n    elif state[\"input_dir_path\"] != self.input_dir.path:\n        raise ValueError(\n            \"The provided `input_dir` path state doesn't match the current one. \"\n            f\"Found `{self.input_dir.path}` instead of `{state['input_dir_path']}`.\"\n        )\n\n    if state[\"input_dir_url\"] != self.input_dir.url:\n        raise ValueError(\n            \"The provided `input_dir` URL state doesn't match the current one. \"\n            f\"Found `{self.input_dir.url}` instead of `{state['input_dir_url']}`.\"\n        )\n\n    if state[\"seed\"] != self.seed:\n        raise ValueError(\n            \"The provided `seed` state doesn't match the current one. \"\n            f\"Found `{self.seed}` instead of `{state['seed']}`.\"\n        )\n\n    if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n        raise ValueError(\n            \"The provided `item_loader` state doesn't match the current one. \"\n            f\"Found `{self.item_loader.state_dict()}` instead of `{state['item_loader']}`.\"\n        )\n\n    if state[\"drop_last\"] != self.drop_last:\n        raise ValueError(\n            \"The provided `drop_last` state doesn't match the current one. \"\n            f\"Found `{self.drop_last}` instead of `{state['drop_last']}`.\"\n        )\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "type": "function", "class_name": null, "function_name": "_try_create_cache_dir", "dependency_all": "# Cross-file Dependency:\nlitdata.constants._DEFAULT_CACHE_DIR\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.constants._DEFAULT_CACHE_DIR\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hashlib\nimport os\nfrom logging import Logger\nfrom time import time\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nfrom torch.utils.data import IterableDataset\n\nfrom litdata.constants import (\n    _DEFAULT_CACHE_DIR,\n    _INDEX_FILENAME,\n)\nfrom litdata.streaming import Cache\nfrom litdata.streaming.item_loader import BaseItemLoader\nfrom litdata.streaming.resolver import Dir, _resolve_dir\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer\nfrom litdata.streaming.shuffle import FullShuffle, NoShuffle, Shuffle\nfrom litdata.utilities.env import _DistributedEnv, _is_in_dataloader_worker, _WorkerEnv\n\nlogger = Logger(__name__)\n\n\nclass StreamingDataset(IterableDataset):\n    \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\"\"\"\n\n    def __init__(\n        self,\n        input_dir: Union[str, \"Dir\"],\n        item_loader: Optional[BaseItemLoader] = None,\n        shuffle: bool = False,\n        drop_last: Optional[bool] = None,\n        seed: int = 42,\n        serializers: Optional[Dict[str, Serializer]] = None,\n        max_cache_size: Union[int, str] = \"100GB\",\n    ) -> None:\n        \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\n\n        Arguments:\n            input_dir: Path to the folder where the input data is stored.\n            item_loader: The logic to load an item from a chunk.\n            shuffle: Whether to shuffle the data.\n            drop_last: If `True`, drops the last items to ensure that\n                all processes/workers return the same amount of data.\n                The argument `drop_last` is set to `True` in a distributed setting\n                and `False` otherwise.\n            seed: Random seed for shuffling.\n            serializers: The serializers used to serialize and deserialize the chunks.\n            max_cache_size: The maximum cache size used by the StreamingDataset.\n\n        \"\"\"\n        super().__init__()\n        if not isinstance(shuffle, bool):\n            raise ValueError(f\"Shuffle should be a boolean. Found {shuffle}\")\n\n        input_dir = _resolve_dir(input_dir)\n\n        self.input_dir = input_dir\n\n        self.item_loader = item_loader\n        self.shuffle: bool = shuffle\n        self.distributed_env = _DistributedEnv.detect()\n\n        if self.distributed_env.world_size > 1:\n            if drop_last is False:\n                logger.warn(\n                    \"You're operating within a distributed environment and have disabled the `drop_last` option. \"\n                    \"Please note that this configuration may lead to training interruptions if your system depends \"\n                    \"on distributed collectives.\"\n                )\n            else:\n                drop_last = True\n\n        self.drop_last = drop_last or False\n\n        self.seed = seed\n        self.max_cache_size = max_cache_size\n\n        self.cache: Optional[Cache] = None\n        self.worker_env: Optional[_WorkerEnv] = None\n        self.worker_chunks: List[int] = []\n        self.worker_intervals: List[List[int]] = []\n        self.current_indexes: List[int] = []\n        self.chunk_index = 0\n        self.num_chunks: Optional[int] = None\n        self.global_index = 0\n        self.index = 0\n        self.has_triggered_download = False\n        self.min_items_per_replica: Optional[int] = None\n        self.current_epoch = 1\n        self.random_state = None\n        self.shuffler: Optional[Shuffle] = None\n        self.serializers = serializers\n        self._state_dict: Optional[Dict[str, Any]] = None\n\n    def set_shuffle(self, shuffle: bool) -> None:\n        self.shuffle = shuffle\n\n    def set_epoch(self, current_epoch: int) -> None:\n        \"\"\"Set the current epoch to the dataset on epoch starts.\n\n        When using the StreamingDataLoader, this is done automatically\n\n        \"\"\"\n        # If the state dict has been reloaded, don't override the current epoch\n        # The StreamingDataloader would clean this out\n        if self._state_dict is None:\n            self.current_epoch = current_epoch\n\n    def _create_cache(self, worker_env: _WorkerEnv) -> Cache:\n        if _should_replace_path(self.input_dir.path):\n            cache_path = _try_create_cache_dir(\n                input_dir=self.input_dir.path if self.input_dir.path else self.input_dir.url\n            )\n            if cache_path is not None:\n                self.input_dir.path = cache_path\n\n        cache = Cache(\n            input_dir=self.input_dir,\n            item_loader=self.item_loader,\n            chunk_bytes=1,\n            serializers=self.serializers,\n            max_cache_size=self.max_cache_size,\n        )\n        cache._reader._try_load_config()\n\n        if not cache.filled:\n            raise ValueError(\n                f\"The provided dataset `{self.input_dir}` doesn't contain any {_INDEX_FILENAME} file.\"\n                \" HINT: Did you successfully optimize a dataset to the provided `input_dir`?\"\n            )\n\n        return cache\n\n    def _create_shuffler(self, cache: Cache) -> Shuffle:\n        seed = self.seed\n        drop_last = self.drop_last\n        if self._state_dict is not None:\n            state: Dict[str, Any] = self._state_dict\n            seed = state[\"seed\"]\n            drop_last = state[\"drop_last\"]\n        return FullShuffle(cache, seed, drop_last) if self.shuffle else NoShuffle(cache, seed, drop_last)\n\n    def __len__(self) -> int:\n        if self.shuffler is None:\n            cache = self._create_cache(worker_env=_WorkerEnv.detect())\n            self.shuffler = self._create_shuffler(cache)\n        return self.shuffler.get_len(self.distributed_env, self.current_epoch)\n\n    def __iter__(self) -> \"StreamingDataset\":\n        # When the StreamingDataset is used within map or optimize, let's refetch the distributed env.\n        if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n            self.distributed_env = _DistributedEnv.detect()\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        # Handle restart\n        if self._state_dict:\n            self._validate_state_dict()\n            state: Dict[str, Any] = self._state_dict\n            self.current_epoch = state[\"current_epoch\"]\n\n        chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n        chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n        intervals_replica = intervals_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(chunks_replica, intervals_replica)\n        else:\n            chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n                self.distributed_env, self.current_epoch\n            )\n            chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n            intervals_replica = intervals_per_replica[\n                self.distributed_env.global_rank % self.distributed_env.world_size\n            ]\n\n            self.worker_chunks = []\n            self.worker_intervals = []\n\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                if i % self.worker_env.world_size != self.worker_env.rank:\n                    continue\n                self.worker_chunks.append(chunk_index)\n                self.worker_intervals.append(chunk_interval)\n\n            self.num_chunks = len(self.worker_chunks)\n\n            self.current_indexes = []\n            self.chunk_index = 0\n            self.global_index = 0\n            self.index = 0\n\n        self.has_triggered_download = False\n        self.last_time = time()\n\n        return self\n\n    def _resume(self, chunks_replica: List[int], intervals_replica: List[Any]) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, chunks_replica, intervals_replica\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index = num_samples_yielded\n\n        # bump the chunk_index\n        self.chunk_index += 1\n\n    def __getitem__(self, index: Union[ChunkedIndex, int]) -> Any:\n        if self.cache is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n        if isinstance(index, int):\n            index = ChunkedIndex(index, self.cache._get_chunk_index_from_index(index))\n        return self.cache[index]\n\n    def __next__(self) -> Any:\n        # Prevent to create more batch on a given process\n        if self.global_index >= len(self):\n            self.current_epoch += 1\n            raise StopIteration\n\n        # Lazily re-populate the interval to reduce memory usage.\n        if len(self.current_indexes) == 0:\n            if self.chunk_index == self.num_chunks:\n                self.current_epoch += 1\n                raise StopIteration\n\n            # reset index\n            self.index = 0\n\n            interval = self.worker_intervals[self.chunk_index]\n            current_indexes = np.arange(interval[0], interval[1])\n\n            assert self.shuffler is not None\n            assert self.num_chunks is not None\n            self.current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n            self.chunk_index += 1\n\n        # Get the first index\n        index = self.current_indexes.pop(0)\n\n        # Call the `__getitem__` method.\n        data = self.__getitem__(\n            ChunkedIndex(\n                index=index,\n                chunk_index=self.worker_chunks[self.chunk_index - 1],\n                # We provide the chunks indexes only one the first\n                chunk_indexes=None if self.has_triggered_download else self.worker_chunks,\n                is_last_index=(self.chunk_index - 1) == len(self.worker_intervals) and len(self.current_indexes) == 1,\n            )\n        )\n\n        self.has_triggered_download = True\n        self.global_index += 1\n        self.index += 1\n\n        return data\n\n    def state_dict(self, num_samples_yielded: int, num_workers: int, batch_size: int) -> Dict[str, Any]:\n        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            self._state_dict[\"num_samples_yielded\"] = num_samples_yielded\n            return self._state_dict\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        if state_dict:\n            # the state is restored within the workers\n            self._state_dict = state_dict\n\n    def _validate_state_dict(self) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                \"The provided `num_workers` state doesn't match the current one. \"\n                f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n            )\n\n        # Note: We need to check whether the path has been resolved to its associated cache.\n        # In this case, validate the cache folder is the same.\n        if _should_replace_path(state[\"input_dir_path\"]):\n            cache_path = _try_create_cache_dir(\n                input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n            )\n            if cache_path != self.input_dir.path:\n                raise ValueError(\n                    \"The provided `input_dir` path state doesn't match the current one. \"\n                    f\"Found `{self.input_dir.path}` instead of `{cache_path}`.\"\n                )\n        elif state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                \"The provided `input_dir` path state doesn't match the current one. \"\n                f\"Found `{self.input_dir.path}` instead of `{state['input_dir_path']}`.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                \"The provided `input_dir` URL state doesn't match the current one. \"\n                f\"Found `{self.input_dir.url}` instead of `{state['input_dir_url']}`.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                \"The provided `seed` state doesn't match the current one. \"\n                f\"Found `{self.seed}` instead of `{state['seed']}`.\"\n            )\n\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                \"The provided `item_loader` state doesn't match the current one. \"\n                f\"Found `{self.item_loader.state_dict()}` instead of `{state['item_loader']}`.\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                \"The provided `drop_last` state doesn't match the current one. \"\n                f\"Found `{self.drop_last}` instead of `{state['drop_last']}`.\"\n            )\n\n\n", "contexts_below": "\n\ndef _should_replace_path(path: Optional[str]) -> bool:\n    \"\"\"Whether the input path is a special path to be replaced.\"\"\"\n    if path is None or path == \"\":\n        return True\n\n    return path.startswith(\"/teamspace/datasets/\") or path.startswith(\"/teamspace/s3_connections/\")\n\n\ndef is_integer(value: str) -> bool:\n    try:\n        int(value)\n        return True\n    except Exception:\n        return False\n\n\ndef _associate_chunks_to_workers(\n    num_workers: int, worker_env: _WorkerEnv, chunks_replica: List[int], intervals_replica: List[Any]\n) -> Any:\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_idx:\n                continue\n\n            worker_chunks.append(chunk_index)\n            worker_intervals.append(chunk_interval)\n\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    return workers_chunks, workers_intervals\n\n\ndef _replay_sampling(num_samples_yielded: int, batch_size: int, num_workers: int) -> Dict[int, int]:\n    \"\"\"This function replays the sampling from the dataloader.\"\"\"\n    divisible_num_batches_yielded = num_samples_yielded // (num_workers * batch_size)\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = divisible_num_batches_yielded * batch_size\n\n    num_samples_yielded = num_samples_yielded - (num_workers * divisible_num_batches_yielded * batch_size)\n\n    # take care of the reminder\n    worker_idx = 0  # reset the worker_idx\n    while True:\n        if num_samples_yielded >= batch_size:\n            indexes[worker_idx] += batch_size\n            worker_idx = (worker_idx + 1) % num_workers\n            num_samples_yielded -= batch_size\n        else:\n            indexes[worker_idx] += num_samples_yielded\n            break\n    return indexes\n\n\ndef _replay_chunks_sampling(\n    workers_intervals: Dict[int, List[Any]], indexes: Dict[int, int]\n) -> Tuple[Dict[int, int], Dict[int, int]]:\n    chunks_index = {}\n\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[-1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes\n", "input_code": "def _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n\n    \"\"\"\n    Attempts to create a cache directory based on the input directory provided. It generates a unique directory name by hashing the input directory. If certain environment variables are not set, it creates the cache directory in a default location; otherwise, it creates it in a specified location.\n\n    Input-Output Arguments\n    :param input_dir: Optional[str]. The directory path to be hashed and used for creating a unique cache directory. If None, an empty string is used for hashing.\n    :return: Optional[str]. The path of the created cache directory. Returns None if the directory cannot be created, although this behavior is not explicitly handled in the function.\n\n    \"\"\"", "reference_steps": "1. Define a function `_try_create_cache_dir` that takes an optional string argument `input_dir`.\n2. Compute the MD5 hash of the `input_dir` or an empty string if `input_dir` is `None`.\n3. Check if the environment variables `LIGHTNING_CLUSTER_ID` and `LIGHTNING_CLOUD_PROJECT_ID` are not set.\n4. If the environment variables are not set, concatenate the default cache directory `_DEFAULT_CACHE_DIR` with the hash of `input_dir`.\n5. Create the cache directory using `os.makedirs` with the `exist_ok` parameter set to `True`, which allows the creation of the directory if it does not exist and does nothing if it already exists.\n6. Return the path to the newly created cache directory.\n7. If the environment variables are set, construct the cache directory path by joining \"/cache\", \"chunks\", and the hash of `input_dir`.\n8. Create this cache directory as well using `os.makedirs` with the `exist_ok` parameter set to `True`.\n9. Return the path to this cache directory.\n10. The function returns an optional string that is the path to the created cache directory.", "reference_code": "def _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n    hash_object = hashlib.md5((input_dir or \"\").encode())\n    if \"LIGHTNING_CLUSTER_ID\" not in os.environ or \"LIGHTNING_CLOUD_PROJECT_ID\" not in os.environ:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    cache_dir = os.path.join(\"/cache\", \"chunks\", hash_object.hexdigest())\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "type": "function", "class_name": null, "function_name": "_associate_chunks_to_workers", "dependency_all": "# Intra-file Dependency:\nlitdata.utilities.env._WorkerEnv.world_size\n\n# Cross-file Dependency:\nlitdata.utilities.env._WorkerEnv\n    class _WorkerEnv:\n        \"\"\"Contains the environment for the current dataloader within the current training process.\n\n        Args:\n            world_size: The number of dataloader workers for the current training process\n            rank: The rank of the current worker within the number of workers\n\n        \"\"\"\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.utilities.env._WorkerEnv\n    class _WorkerEnv:\n        \"\"\"Contains the environment for the current dataloader within the current training process.\n\n        Args:\n            world_size: The number of dataloader workers for the current training process\n            rank: The rank of the current worker within the number of workers\n\n        \"\"\"\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hashlib\nimport os\nfrom logging import Logger\nfrom time import time\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nfrom torch.utils.data import IterableDataset\n\nfrom litdata.constants import (\n    _DEFAULT_CACHE_DIR,\n    _INDEX_FILENAME,\n)\nfrom litdata.streaming import Cache\nfrom litdata.streaming.item_loader import BaseItemLoader\nfrom litdata.streaming.resolver import Dir, _resolve_dir\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer\nfrom litdata.streaming.shuffle import FullShuffle, NoShuffle, Shuffle\nfrom litdata.utilities.env import _DistributedEnv, _is_in_dataloader_worker, _WorkerEnv\n\nlogger = Logger(__name__)\n\n\nclass StreamingDataset(IterableDataset):\n    \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\"\"\"\n\n    def __init__(\n        self,\n        input_dir: Union[str, \"Dir\"],\n        item_loader: Optional[BaseItemLoader] = None,\n        shuffle: bool = False,\n        drop_last: Optional[bool] = None,\n        seed: int = 42,\n        serializers: Optional[Dict[str, Serializer]] = None,\n        max_cache_size: Union[int, str] = \"100GB\",\n    ) -> None:\n        \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\n\n        Arguments:\n            input_dir: Path to the folder where the input data is stored.\n            item_loader: The logic to load an item from a chunk.\n            shuffle: Whether to shuffle the data.\n            drop_last: If `True`, drops the last items to ensure that\n                all processes/workers return the same amount of data.\n                The argument `drop_last` is set to `True` in a distributed setting\n                and `False` otherwise.\n            seed: Random seed for shuffling.\n            serializers: The serializers used to serialize and deserialize the chunks.\n            max_cache_size: The maximum cache size used by the StreamingDataset.\n\n        \"\"\"\n        super().__init__()\n        if not isinstance(shuffle, bool):\n            raise ValueError(f\"Shuffle should be a boolean. Found {shuffle}\")\n\n        input_dir = _resolve_dir(input_dir)\n\n        self.input_dir = input_dir\n\n        self.item_loader = item_loader\n        self.shuffle: bool = shuffle\n        self.distributed_env = _DistributedEnv.detect()\n\n        if self.distributed_env.world_size > 1:\n            if drop_last is False:\n                logger.warn(\n                    \"You're operating within a distributed environment and have disabled the `drop_last` option. \"\n                    \"Please note that this configuration may lead to training interruptions if your system depends \"\n                    \"on distributed collectives.\"\n                )\n            else:\n                drop_last = True\n\n        self.drop_last = drop_last or False\n\n        self.seed = seed\n        self.max_cache_size = max_cache_size\n\n        self.cache: Optional[Cache] = None\n        self.worker_env: Optional[_WorkerEnv] = None\n        self.worker_chunks: List[int] = []\n        self.worker_intervals: List[List[int]] = []\n        self.current_indexes: List[int] = []\n        self.chunk_index = 0\n        self.num_chunks: Optional[int] = None\n        self.global_index = 0\n        self.index = 0\n        self.has_triggered_download = False\n        self.min_items_per_replica: Optional[int] = None\n        self.current_epoch = 1\n        self.random_state = None\n        self.shuffler: Optional[Shuffle] = None\n        self.serializers = serializers\n        self._state_dict: Optional[Dict[str, Any]] = None\n\n    def set_shuffle(self, shuffle: bool) -> None:\n        self.shuffle = shuffle\n\n    def set_epoch(self, current_epoch: int) -> None:\n        \"\"\"Set the current epoch to the dataset on epoch starts.\n\n        When using the StreamingDataLoader, this is done automatically\n\n        \"\"\"\n        # If the state dict has been reloaded, don't override the current epoch\n        # The StreamingDataloader would clean this out\n        if self._state_dict is None:\n            self.current_epoch = current_epoch\n\n    def _create_cache(self, worker_env: _WorkerEnv) -> Cache:\n        if _should_replace_path(self.input_dir.path):\n            cache_path = _try_create_cache_dir(\n                input_dir=self.input_dir.path if self.input_dir.path else self.input_dir.url\n            )\n            if cache_path is not None:\n                self.input_dir.path = cache_path\n\n        cache = Cache(\n            input_dir=self.input_dir,\n            item_loader=self.item_loader,\n            chunk_bytes=1,\n            serializers=self.serializers,\n            max_cache_size=self.max_cache_size,\n        )\n        cache._reader._try_load_config()\n\n        if not cache.filled:\n            raise ValueError(\n                f\"The provided dataset `{self.input_dir}` doesn't contain any {_INDEX_FILENAME} file.\"\n                \" HINT: Did you successfully optimize a dataset to the provided `input_dir`?\"\n            )\n\n        return cache\n\n    def _create_shuffler(self, cache: Cache) -> Shuffle:\n        seed = self.seed\n        drop_last = self.drop_last\n        if self._state_dict is not None:\n            state: Dict[str, Any] = self._state_dict\n            seed = state[\"seed\"]\n            drop_last = state[\"drop_last\"]\n        return FullShuffle(cache, seed, drop_last) if self.shuffle else NoShuffle(cache, seed, drop_last)\n\n    def __len__(self) -> int:\n        if self.shuffler is None:\n            cache = self._create_cache(worker_env=_WorkerEnv.detect())\n            self.shuffler = self._create_shuffler(cache)\n        return self.shuffler.get_len(self.distributed_env, self.current_epoch)\n\n    def __iter__(self) -> \"StreamingDataset\":\n        # When the StreamingDataset is used within map or optimize, let's refetch the distributed env.\n        if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n            self.distributed_env = _DistributedEnv.detect()\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        # Handle restart\n        if self._state_dict:\n            self._validate_state_dict()\n            state: Dict[str, Any] = self._state_dict\n            self.current_epoch = state[\"current_epoch\"]\n\n        chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n        chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n        intervals_replica = intervals_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(chunks_replica, intervals_replica)\n        else:\n            chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n                self.distributed_env, self.current_epoch\n            )\n            chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n            intervals_replica = intervals_per_replica[\n                self.distributed_env.global_rank % self.distributed_env.world_size\n            ]\n\n            self.worker_chunks = []\n            self.worker_intervals = []\n\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                if i % self.worker_env.world_size != self.worker_env.rank:\n                    continue\n                self.worker_chunks.append(chunk_index)\n                self.worker_intervals.append(chunk_interval)\n\n            self.num_chunks = len(self.worker_chunks)\n\n            self.current_indexes = []\n            self.chunk_index = 0\n            self.global_index = 0\n            self.index = 0\n\n        self.has_triggered_download = False\n        self.last_time = time()\n\n        return self\n\n    def _resume(self, chunks_replica: List[int], intervals_replica: List[Any]) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, chunks_replica, intervals_replica\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index = num_samples_yielded\n\n        # bump the chunk_index\n        self.chunk_index += 1\n\n    def __getitem__(self, index: Union[ChunkedIndex, int]) -> Any:\n        if self.cache is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n        if isinstance(index, int):\n            index = ChunkedIndex(index, self.cache._get_chunk_index_from_index(index))\n        return self.cache[index]\n\n    def __next__(self) -> Any:\n        # Prevent to create more batch on a given process\n        if self.global_index >= len(self):\n            self.current_epoch += 1\n            raise StopIteration\n\n        # Lazily re-populate the interval to reduce memory usage.\n        if len(self.current_indexes) == 0:\n            if self.chunk_index == self.num_chunks:\n                self.current_epoch += 1\n                raise StopIteration\n\n            # reset index\n            self.index = 0\n\n            interval = self.worker_intervals[self.chunk_index]\n            current_indexes = np.arange(interval[0], interval[1])\n\n            assert self.shuffler is not None\n            assert self.num_chunks is not None\n            self.current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n            self.chunk_index += 1\n\n        # Get the first index\n        index = self.current_indexes.pop(0)\n\n        # Call the `__getitem__` method.\n        data = self.__getitem__(\n            ChunkedIndex(\n                index=index,\n                chunk_index=self.worker_chunks[self.chunk_index - 1],\n                # We provide the chunks indexes only one the first\n                chunk_indexes=None if self.has_triggered_download else self.worker_chunks,\n                is_last_index=(self.chunk_index - 1) == len(self.worker_intervals) and len(self.current_indexes) == 1,\n            )\n        )\n\n        self.has_triggered_download = True\n        self.global_index += 1\n        self.index += 1\n\n        return data\n\n    def state_dict(self, num_samples_yielded: int, num_workers: int, batch_size: int) -> Dict[str, Any]:\n        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            self._state_dict[\"num_samples_yielded\"] = num_samples_yielded\n            return self._state_dict\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        if state_dict:\n            # the state is restored within the workers\n            self._state_dict = state_dict\n\n    def _validate_state_dict(self) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                \"The provided `num_workers` state doesn't match the current one. \"\n                f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n            )\n\n        # Note: We need to check whether the path has been resolved to its associated cache.\n        # In this case, validate the cache folder is the same.\n        if _should_replace_path(state[\"input_dir_path\"]):\n            cache_path = _try_create_cache_dir(\n                input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n            )\n            if cache_path != self.input_dir.path:\n                raise ValueError(\n                    \"The provided `input_dir` path state doesn't match the current one. \"\n                    f\"Found `{self.input_dir.path}` instead of `{cache_path}`.\"\n                )\n        elif state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                \"The provided `input_dir` path state doesn't match the current one. \"\n                f\"Found `{self.input_dir.path}` instead of `{state['input_dir_path']}`.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                \"The provided `input_dir` URL state doesn't match the current one. \"\n                f\"Found `{self.input_dir.url}` instead of `{state['input_dir_url']}`.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                \"The provided `seed` state doesn't match the current one. \"\n                f\"Found `{self.seed}` instead of `{state['seed']}`.\"\n            )\n\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                \"The provided `item_loader` state doesn't match the current one. \"\n                f\"Found `{self.item_loader.state_dict()}` instead of `{state['item_loader']}`.\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                \"The provided `drop_last` state doesn't match the current one. \"\n                f\"Found `{self.drop_last}` instead of `{state['drop_last']}`.\"\n            )\n\n\ndef _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n    hash_object = hashlib.md5((input_dir or \"\").encode())\n    if \"LIGHTNING_CLUSTER_ID\" not in os.environ or \"LIGHTNING_CLOUD_PROJECT_ID\" not in os.environ:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    cache_dir = os.path.join(\"/cache\", \"chunks\", hash_object.hexdigest())\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n\n\ndef _should_replace_path(path: Optional[str]) -> bool:\n    \"\"\"Whether the input path is a special path to be replaced.\"\"\"\n    if path is None or path == \"\":\n        return True\n\n    return path.startswith(\"/teamspace/datasets/\") or path.startswith(\"/teamspace/s3_connections/\")\n\n\ndef is_integer(value: str) -> bool:\n    try:\n        int(value)\n        return True\n    except Exception:\n        return False\n\n\n", "contexts_below": "\n\ndef _replay_sampling(num_samples_yielded: int, batch_size: int, num_workers: int) -> Dict[int, int]:\n    \"\"\"This function replays the sampling from the dataloader.\"\"\"\n    divisible_num_batches_yielded = num_samples_yielded // (num_workers * batch_size)\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = divisible_num_batches_yielded * batch_size\n\n    num_samples_yielded = num_samples_yielded - (num_workers * divisible_num_batches_yielded * batch_size)\n\n    # take care of the reminder\n    worker_idx = 0  # reset the worker_idx\n    while True:\n        if num_samples_yielded >= batch_size:\n            indexes[worker_idx] += batch_size\n            worker_idx = (worker_idx + 1) % num_workers\n            num_samples_yielded -= batch_size\n        else:\n            indexes[worker_idx] += num_samples_yielded\n            break\n    return indexes\n\n\ndef _replay_chunks_sampling(\n    workers_intervals: Dict[int, List[Any]], indexes: Dict[int, int]\n) -> Tuple[Dict[int, int], Dict[int, int]]:\n    chunks_index = {}\n\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[-1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes\n", "input_code": "def _associate_chunks_to_workers(\n    num_workers: int, worker_env: _WorkerEnv, chunks_replica: List[int], intervals_replica: List[Any]\n) -> Any:\n\n    \"\"\"\n    This function distributes chunks of data and their corresponding intervals across multiple workers based on the number of workers and a worker environment. It ensures that each worker is assigned a subset of chunks and intervals, following a distribution strategy that depends on the worker's index and the total world size defined in the worker environment.\n\n    Input-Output Arguments\n    :param num_workers: int, the total number of workers among which the chunks and intervals are to be distributed.\n    :param worker_env: _WorkerEnv, an instance or object representing the worker environment, which includes details like world size that are used in the distribution logic.\n    :param chunks_replica: List[int], a list of chunk indices that need to be distributed among the workers.\n    :param intervals_replica: List[Any], a list of intervals corresponding to each chunk in chunks_replica. Each interval represents the range or scope of the chunk it corresponds to.\n    :return: A tuple containing two dictionaries. The first dictionary maps worker indices to their assigned chunks, and the second dictionary maps worker indices to the intervals corresponding to their assigned chunks. There is no explicit data type mentioned for the return value, but it is implied to be a tuple of two dictionaries based on the function's implementation.\n    \"\"\"", "reference_steps": "1. Define a function `_associate_chunks_to_workers` that takes four arguments: `num_workers`, `worker_env`, `chunks_replica`, and `intervals_replica`.\n2. Initialize two empty dictionaries, `workers_chunks` and `workers_intervals`, to store the chunks and intervals associated with each worker.\n3. Iterate over the range of `num_workers` using `worker_idx` as the loop variable.\n4. For each worker, create empty lists `worker_chunks` and `worker_intervals` to hold the chunks and intervals assigned to that worker.\n5. Enumerate over the zipped `chunks_replica` and `intervals_replica` lists to get both the index (`i`) and the values (`chunk_index`, `chunk_interval`).\n6. Use a conditional statement to check if the current index `i` modulo `worker_env.world_size` is equal to `worker_idx`. If not, continue to the next iteration.\n7. If the condition is met, append `chunk_index` to `worker_chunks` and `chunk_interval` to `worker_intervals`.\n8. After the inner loop, assign the list of `worker_chunks` to the `workers_chunks` dictionary with the key as `worker_idx`.\n9. Similarly, assign the list of `worker_intervals` to the `workers_intervals` dictionary with the key as `worker_idx`.\n10. Return the two dictionaries `workers_chunks` and `workers_intervals` containing the associated chunks and intervals for each worker.", "reference_code": "def _associate_chunks_to_workers(\n    num_workers: int, worker_env: _WorkerEnv, chunks_replica: List[int], intervals_replica: List[Any]\n) -> Any:\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_idx:\n                continue\n\n            worker_chunks.append(chunk_index)\n            worker_intervals.append(chunk_interval)\n\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    return workers_chunks, workers_intervals\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "type": "method", "class_name": "LocalDownloaderWithCache", "function_name": "download_file", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.downloader.LocalDownloader\n    class LocalDownloader(Downloader):\n\nlitdata.streaming.downloader.LocalDownloader.download_file\n    def download_file(self, remote_chunkpath: str, local_chunkpath: str) -> None:\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.downloader.LocalDownloader.download_file\n    def download_file(self, remote_chunkpath: str, local_chunkpath: str) -> None:\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport shutil\nimport subprocess\nfrom abc import ABC\nfrom typing import Any, Dict, List\nfrom urllib import parse\n\nfrom filelock import FileLock, Timeout\n\nfrom litdata.constants import _INDEX_FILENAME\nfrom litdata.streaming.client import S3Client\n\n\nclass Downloader(ABC):\n    def __init__(self, remote_dir: str, cache_dir: str, chunks: List[Dict[str, Any]]):\n        self._remote_dir = remote_dir\n        self._cache_dir = cache_dir\n        self._chunks = chunks\n\n    def download_chunk_from_index(self, chunk_index: int) -> None:\n        chunk_filename = self._chunks[chunk_index][\"filename\"]\n        local_chunkpath = os.path.join(self._cache_dir, chunk_filename)\n        remote_chunkpath = os.path.join(self._remote_dir, chunk_filename)\n        self.download_file(remote_chunkpath, local_chunkpath)\n\n    def download_file(self, remote_chunkpath: str, local_chunkpath: str) -> None:\n        pass\n\n\nclass S3Downloader(Downloader):\n    def __init__(self, remote_dir: str, cache_dir: str, chunks: List[Dict[str, Any]]):\n        super().__init__(remote_dir, cache_dir, chunks)\n        self._s5cmd_available = os.system(\"s5cmd > /dev/null 2>&1\") == 0\n\n        if not self._s5cmd_available:\n            self._client = S3Client()\n\n    def download_file(self, remote_filepath: str, local_filepath: str) -> None:\n        obj = parse.urlparse(remote_filepath)\n\n        if obj.scheme != \"s3\":\n            raise ValueError(f\"Expected obj.scheme to be `s3`, instead, got {obj.scheme} for remote={remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        try:\n            with FileLock(local_filepath + \".lock\", timeout=3 if obj.path.endswith(_INDEX_FILENAME) else 0):\n                if self._s5cmd_available:\n                    proc = subprocess.Popen(\n                        f\"s5cmd cp {remote_filepath} {local_filepath}\",\n                        shell=True,\n                        stdout=subprocess.PIPE,\n                    )\n                    proc.wait()\n                else:\n                    from boto3.s3.transfer import TransferConfig\n\n                    extra_args: Dict[str, Any] = {}\n\n                    # try:\n                    #     with FileLock(local_filepath + \".lock\", timeout=1):\n                    if not os.path.exists(local_filepath):\n                        # Issue: https://github.com/boto/boto3/issues/3113\n                        self._client.client.download_file(\n                            obj.netloc,\n                            obj.path.lstrip(\"/\"),\n                            local_filepath,\n                            ExtraArgs=extra_args,\n                            Config=TransferConfig(use_threads=False),\n                        )\n        except Timeout:\n            # another process is responsible to download that file, continue\n            pass\n\n\nclass LocalDownloader(Downloader):\n    def download_file(self, remote_filepath: str, local_filepath: str) -> None:\n        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            shutil.copy(remote_filepath, local_filepath)\n\n\nclass LocalDownloaderWithCache(LocalDownloader):\n", "contexts_below": "\n\n_DOWNLOADERS = {\"s3://\": S3Downloader, \"local:\": LocalDownloaderWithCache, \"\": LocalDownloader}\n\n\ndef get_downloader_cls(remote_dir: str, cache_dir: str, chunks: List[Dict[str, Any]]) -> Downloader:\n    for k, cls in _DOWNLOADERS.items():\n        if str(remote_dir).startswith(k):\n            return cls(remote_dir, cache_dir, chunks)\n    raise ValueError(f\"The provided `remote_dir` {remote_dir} doesn't have a downloader associated.\")\n", "input_code": "    def download_file(self, remote_filepath: str, local_filepath: str) -> None:\n\n        \"\"\"\n        The function modifies the remote file path by removing the prefix \"local:\" if present, and then calls the superclass's download_file method to download the file from the modified remote file path to the specified local file path.\n\n        Input-Output Arguments\n        :param self: LocalDownloaderWithCache. An instance of the LocalDownloaderWithCache class.\n        :param remote_filepath: str, The path of the file on the remote server. It may contain a \"local:\" prefix which will be removed before downloading.\n        :param local_filepath: str, The path where the file should be saved locally after downloading.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method named `download_file` with parameters `self`, `remote_filepath`, and `local_filepath`.\n2. Replace the substring \"local:\" in `remote_filepath` with an empty string.\n3. Call the `download_file` method of the superclass with the modified `remote_filepath` and the original `local_filepath`.\n4. The method does not return any value (`None`).", "reference_code": "def download_file(self, remote_filepath: str, local_filepath: str) -> None:\n    remote_filepath = remote_filepath.replace(\"local:\", \"\")\n    super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "type": "method", "class_name": "PILSerializer", "function_name": "serialize", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.serializers.Image\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.serializers.Image\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n", "contexts_below": "\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n\n        \"\"\"\n        Serializes a PIL Image object into a bytes object containing the image's dimensions, mode, and raw pixel data. It returns this serialized data along with None, as a tuple. This can be useful for saving or transmitting the image data in a standardized format.\n\n        Input-Output Arguments\n        :param item: Image, The PIL Image object to be serialized. It is used to extract the image's mode, dimensions, and raw pixel data for serialization.\n        :return: Tuple[bytes, Optional[str]], A tuple where the first element is a bytes object containing the serialized image data and the second element is None. The serialized data includes the image's width, height, mode length as a bytes object, followed by the image mode encoded in UTF-8, and finally the raw pixel data.\n        \"\"\"", "reference_steps": "1. Define a function `serialize` that takes an instance of an `Image` object and returns a tuple containing serialized image data as bytes and an optional string (which is `None` in this case).\n\n2. Encode the image mode (e.g., \"RGB\", \"L\") into UTF-8 bytes and store it in the variable `mode`.\n\n3. Retrieve the width and height from the image size attribute.\n\n4. Convert the image data to raw bytes using the `tobytes` method of the `Image` object and store it in the variable `raw`.\n\n5. Create a NumPy array `ints` with three unsigned 32-bit integers representing the width, height, and the length of the encoded mode.\n\n6. Convert the NumPy array `ints` to bytes using the `tobytes` method.\n\n7. Concatenate the bytes representing the width, height, and mode length with the encoded mode and the raw image data.\n\n8. Return the concatenated bytes as the serialized image data and `None` as the optional string in a tuple.", "reference_code": "def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n    mode = item.mode.encode(\"utf-8\")\n    width, height = item.size\n    raw = item.tobytes()\n    ints = np.array([width, height, len(mode)], np.uint32)\n    return ints.tobytes() + mode + raw, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "type": "method", "class_name": "JPEGSerializer", "function_name": "serialize", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.serializers.Image\n\nlitdata.streaming.serializers.JpegImageFile\n\nlitdata.streaming.serializers.PngImageFile\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.serializers.Image\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n", "contexts_below": "\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n\n        \"\"\"\n        Serializes an image item into JPEG format. If the item is a JPEG and has a defined filename that exists, it reads the file directly. Otherwise, it converts the item into JPEG format in memory. It raises an error if the item is not a supported image type.\n\n        Input-Output Arguments\n        :param item: Image. The image item to be serialized. It should be an instance of Image class or its subclasses.\n        :return: A tuple containing the serialized image data in bytes and an optional error message as None. If the item is not a supported image type, a TypeError is raised.\n\n        \"\"\"", "reference_steps": "1. Define a function `serialize` that takes an `item` of type `Image` and returns a tuple containing bytes and an optional string.\n\n2. Check if the `item` is an instance of `JpegImageFile`.\n\n3. If `item` is a `JpegImageFile`, verify that it has the attribute `filename`. If not, raise a `ValueError` with an informative message.\n\n4. If `item.filename` exists and points to a valid file on the filesystem, open the file in binary read mode (`\"rb\"`).\n\n5. Read the content of the JPEG file and return it as bytes, along with `None` for the optional string.\n\n6. If the JPEG file does not exist on the filesystem, create a `BytesIO` object to hold the image data in memory.\n\n7. Save the `JpegImageFile` to the `BytesIO` object in JPEG format and retrieve the bytes value.\n\n8. Return the bytes of the image and `None` for the optional string.\n\n9. If the `item` is an instance of `PngImageFile`, `WebPImageFile`, `GifImageFile`, or `Image.Image`, create a `BytesIO` object and save the image converted to \"RGB\" in JPEG format with quality 100.\n\n10. Return the bytes of the converted image and `None` for the optional string, and raise a `TypeError` if the `item` is not an instance of the supported image types.", "reference_code": "def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n    if isinstance(item, JpegImageFile):\n        if not hasattr(item, \"filename\"):\n            raise ValueError(\n                \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n            )\n        if item.filename and os.path.isfile(item.filename):\n            # read the content of the file directly\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        else:\n            item_bytes = io.BytesIO()\n            item.save(item_bytes, format=\"JPEG\")\n            item_bytes = item_bytes.getvalue()\n            return item_bytes, None\n\n    if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n        buff = io.BytesIO()\n        item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n        buff.seek(0)\n        return buff.read(), None\n\n    raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "type": "method", "class_name": "PILSerializer", "function_name": "deserialize", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.serializers.Image\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.serializers.Image\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n", "contexts_below": "\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def deserialize(cls, data: bytes) -> Any:\n\n        \"\"\"\n        The function deserializes a byte stream into an image object. It extracts the width, height, and mode of the image from the beginning of the byte stream, then uses these to reconstruct the image from the remaining bytes.\n\n        Input-Output Arguments\n        :param cls: The class that is calling this method. It is used as a decorator to indicate that this method is intended to be a class method.\n        :param data: bytes, The byte stream that contains the serialized image data. This data includes the image's width, height, mode, and the raw image bytes.\n        :return: An image object reconstructed from the byte stream.\n\n        Note: This function assumes the first 12 bytes (3 integers) of the data represent the width, height, and size of the mode string in that order, all as unsigned 32-bit integers. The mode string follows these integers and is of the length specified by the third integer. The rest of the data is the raw image data.\n        \"\"\"", "reference_steps": "1. Define a function `deserialize` that takes a class `cls` and a byte array `data` as input and returns a deserialized object of any type.\n\n2. Set an index `idx` to 12 (3 times 4 bytes), which is the point in the byte array where the image metadata ends and the mode string begins.\n\n3. Extract the width, height, and mode size from the first 12 bytes of the `data` using NumPy's `frombuffer` function, interpreting the data as unsigned 32-bit integers.\n\n4. Calculate the index `idx2` where the mode string ends by adding the mode size to `idx`.\n\n5. Decode the mode string from the byte array using the range from `idx` to `idx2` and UTF-8 encoding.\n\n6. Create a tuple `size` containing the width and height of the image.\n\n7. Extract the raw image data from the byte array starting from `idx2` to the end of the array.\n\n8. Use the `Image.frombytes` method from the PIL library to create an image object with the given mode, size, and raw data.\n\n9. Return the created image object.\n\n10. Ignore type checking for the line with `Image.frombytes` using a comment `# pyright: ignore` to prevent static type checkers from raising an error.", "reference_code": "def deserialize(cls, data: bytes) -> Any:\n    idx = 3 * 4\n    width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n    idx2 = idx + mode_size\n    mode = data[idx:idx2].decode(\"utf-8\")\n    size = width, height\n    raw = data[idx2:]\n    return Image.frombytes(mode, size, raw)  # pyright: ignore\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "type": "method", "class_name": "TensorSerializer", "function_name": "deserialize", "dependency_all": "# Cross-file Dependency:\nlitdata.constants._TORCH_DTYPES_MAPPING\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.constants._TORCH_DTYPES_MAPPING\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n", "contexts_below": "\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def deserialize(self, data: bytes) -> torch.Tensor:\n\n        \"\"\"\n        The function deserializes a byte array into a PyTorch tensor. It extracts the data type and shape information encoded in the byte array, then reconstructs the tensor from the remaining bytes.\n\n        Input-Output Arguments\n        :param data: bytes, the serialized tensor data including information about the tensor's dtype and shape, followed by the tensor's raw data.\n        :return: torch.Tensor, the deserialized tensor reconstructed from the input byte array.\n\n        \"\"\"", "reference_steps": "1. Extract the index for the data type (`dtype_indice`) from the first 4 bytes of the input `data` using `numpy.frombuffer` and convert it to an integer using `item()`.\n2. Map the extracted `dtype_indice` to the corresponding PyTorch data type (`dtype`) using a predefined mapping dictionary `_TORCH_DTYPES_MAPPING`.\n3. Extract the size of the tensor shape (`shape_size`) from bytes 4 to 8 of the input `data` using `numpy.frombuffer` and convert it to an integer using `item()`.\n4. Initialize an empty list `shape` to hold the dimensions of the tensor.\n5. Iterate over the range defined by `shape_size` to extract each dimension of the tensor shape from the `data` and append it to the `shape` list.\n6. Calculate the starting index for the actual tensor data by adding 8 bytes (for the dtype index and shape size) to 4 times the number of shape dimensions.\n7. Create the tensor from the remaining bytes of the input `data` starting from the calculated index to the end, using the extracted `dtype` with `torch.frombuffer`.\n8. Convert the `shape` list to a `torch.Size` object.\n9. Check if the shape of the created tensor matches the extracted `shape`. If it does, return the tensor as is.\n10. If the shapes do not match, reshape the tensor to the extracted `shape` using `torch.reshape` and return the reshaped tensor.", "reference_code": "def deserialize(self, data: bytes) -> torch.Tensor:\n    dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n    dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n    shape_size = np.frombuffer(data[4:8], np.uint32).item()\n    shape = []\n    for shape_idx in range(shape_size):\n        shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n    tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n    shape = torch.Size(shape)\n    if tensor.shape == shape:\n        return tensor\n    return torch.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "type": "method", "class_name": "TensorSerializer", "function_name": "serialize", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.serializers.TensorSerializer._dtype_to_indices\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.serializers.TensorSerializer._dtype_to_indices\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n", "contexts_below": "\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n\n        \"\"\"\n        Serializes a PyTorch tensor into a bytes object containing the tensor's dtype, shape, and raw data. This serialized format can be useful for saving or transmitting tensor data in a compact binary form. The function also returns None as the second element of the tuple, which could be used for additional metadata in future extensions.\n\n        Input-Output Arguments\n        :param item: torch.Tensor, the tensor to be serialized. It is used to extract the dtype, shape, and raw data for serialization.\n        :return: A tuple containing a bytes object and None. The bytes object is the serialized representation of the input tensor, including its dtype, shape, and raw data. The None value indicates that there is no additional metadata associated with this serialization.\n        \"\"\"", "reference_steps": "1. Define a function `serialize` that takes a PyTorch tensor `item` as input and returns a tuple containing bytes and an optional string.\n\n2. Create a mapping from PyTorch data types to indices (not shown in the code snippet) and use it to find the index corresponding to the data type of the input tensor.\n\n3. Convert the data type index to a 32-bit unsigned integer and convert it to bytes, then add it to a list called `data`.\n\n4. Convert the length of the tensor's shape (number of dimensions) to a 32-bit unsigned integer and convert it to bytes, then append it to the `data` list.\n\n5. Iterate over each dimension in the tensor's shape:\n   - Convert the dimension size to a 32-bit unsigned integer and convert it to bytes.\n   - Append the bytes representing the dimension size to the `data` list.\n\n6. Convert the tensor to a NumPy array and then to bytes in C-contiguous order (row-major order).\n\n7. Append the bytes representing the tensor's data to the `data` list.\n\n8. Join all byte sequences in the `data` list into a single bytes object.\n\n9. Return the single bytes object as the first element of the tuple.\n\n10. Return `None` as the second element of the tuple, indicating that there is no additional metadata or string to accompany the bytes.", "reference_code": "def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n    dtype_indice = self._dtype_to_indices[item.dtype]\n    data = [np.uint32(dtype_indice).tobytes()]\n    data.append(np.uint32(len(item.shape)).tobytes())\n    for dim in item.shape:\n        data.append(np.uint32(dim).tobytes())\n    data.append(item.numpy().tobytes(order=\"C\"))\n    return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "type": "method", "class_name": "JPEGSerializer", "function_name": "deserialize", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.serializers.JpegImageFile\n\nlitdata.streaming.serializers.PILSerializer\n    class PILSerializer(Serializer):\n        \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\nlitdata.streaming.serializers.PILSerializer.deserialize\n    def deserialize(self, data: bytes) -> Any:\n\nlitdata.streaming.serializers._TORCH_VISION_AVAILABLE\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.serializers._TORCH_VISION_AVAILABLE\n\nlitdata.streaming.serializers.PILSerializer\n    class PILSerializer(Serializer):\n        \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n", "contexts_below": "\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n\n        \"\"\"\n        The function attempts to deserialize the given byte data into a JPEG image. If PyTorch's torchvision is available and the data can be decoded as a JPEG, it returns a decoded JPEG image. If the decoding fails due to a runtime error (e.g., the data is actually a PNG with a JPEG extension), it falls back to using PIL to deserialize the data. If torchvision is available, it further converts the PIL image to a PyTorch tensor before returning.\n\n        Input-Output Arguments\n        :param data: bytes, the byte data of the image to be deserialized.\n        :return: Union[JpegImageFile, torch.Tensor], the deserialized image, which can be either a JpegImageFile object or a torch.Tensor, depending on whether torchvision is available and the type of image data.\n        \"\"\"", "reference_steps": "1. Define a function `deserialize` that takes a byte stream `data` as input and returns either a `JpegImageFile` object or a `torch.Tensor`.\n\n2. Check if the `torchvision` library is available by evaluating the `_TORCH_VISION_AVAILABLE` flag.\n\n3. If `torchvision` is available, create a `torch.Tensor` from the input byte stream `data` using `torch.frombuffer` with `dtype` set to `torch.uint8`.\n\n4. Attempt to decode the tensor as a JPEG image using the `decode_jpeg` function from `torchvision`.\n\n5. If a `RuntimeError` occurs during decoding (indicating the data may not be a JPEG), handle the exception by doing nothing (passing) to allow for a fallback to PIL (Python Imaging Library) for decoding.\n\n6. If the decoding with `torchvision` fails or `torchvision` is not available, use the `PILSerializer.deserialize` method to convert the byte stream `data` into a PIL image.\n\n7. Check again if `torchvision` is available after falling back to PIL.\n\n8. If `torchvision` is available, convert the PIL image to a `torch.Tensor` using the `pil_to_tensor` function from `torchvision`.\n\n9. Return the resulting `torch.Tensor` or `JpegImageFile` object.\n\n10. The function is designed to handle both JPEG images that can be decoded directly with `torchvision` and images that may require decoding with PIL (such as mislabeled PNG images with a JPEG extension).", "reference_code": "def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n    if _TORCH_VISION_AVAILABLE:\n        array = torch.frombuffer(data, dtype=torch.uint8)\n        try:\n            return decode_jpeg(array)\n        except RuntimeError:\n            # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n            pass\n\n    img = PILSerializer.deserialize(data)\n    if _TORCH_VISION_AVAILABLE:\n        img = pil_to_tensor(img)\n    return img\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "type": "method", "class_name": "NoHeaderTensorSerializer", "function_name": "serialize", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderTensorSerializer._dtype_to_indices\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderTensorSerializer._dtype_to_indices\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n", "contexts_below": "\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n\n        \"\"\"\n        Serializes a PyTorch tensor into a bytes object and a string representing the tensor's data type. The serialization process converts the tensor to a NumPy array and then to bytes. The data type of the tensor is mapped to an index, which is included in the returned string for identification.\n\n        Input-Output Arguments\n        :param self: NoHeaderTensorSerializer. An instance of the NoHeaderTensorSerializer class, which is responsible for serializing PyTorch tensors without including header information.\n        :param item: torch.Tensor, The PyTorch tensor to be serialized. It is used to extract the tensor's data and data type for serialization.\n        :return: Tuple[bytes, Optional[str]], A tuple where the first element is the serialized tensor data as bytes, and the second element is a string representing the tensor's data type through an index. The string is prefixed with \"no_header_tensor:\" followed by the data type index.\n        \"\"\"", "reference_steps": "1. Define a method called `serialize` that takes two parameters: `self` and `item`, where `item` is expected to be a `torch.Tensor`.\n2. Retrieve the corresponding index for the data type of the `item` from a predefined mapping (`self._dtype_to_indices`).\n3. Convert the `torch.Tensor` `item` to a NumPy array using `item.numpy()`.\n4. Serialize the NumPy array to a byte representation with C-like memory order using the `tobytes(order=\"C\")` method.\n5. Construct a string that contains the prefix \"no_header_tensor:\" followed by the data type index obtained earlier.\n6. Return a tuple containing the serialized bytes of the tensor and the constructed string with the data type information.", "reference_code": "def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n    dtype_indice = self._dtype_to_indices[item.dtype]\n    return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "type": "method", "class_name": "NoHeaderTensorSerializer", "function_name": "deserialize", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderTensorSerializer._dtype\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderTensorSerializer._dtype\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n", "contexts_below": "\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def deserialize(self, data: bytes) -> torch.Tensor:\n\n        \"\"\"\n        The function deserializes the given byte data into a PyTorch tensor using the specified data type stored in the instance.\n\n        Input-Output Arguments\n        :param self: NoHeaderTensorSerializer. An instance of the NoHeaderTensorSerializer class, which must have a predefined data type (_dtype) for the tensor.\n        :param data: bytes, The byte data to be deserialized into a PyTorch tensor.\n        :return: torch.Tensor. The deserialized PyTorch tensor.\n        \"\"\"", "reference_steps": "1. Define a function named `deserialize` that takes `self` and a `data` parameter of type `bytes`.\n2. Ensure that the `_dtype` attribute of the instance (`self`) is set and not `None` using an `assert` statement.\n3. Convert the `data` (which is a bytes object) into a PyTorch tensor using the `torch.frombuffer` function.\n4. Specify the data type (`dtype`) for the new tensor using the `_dtype` attribute of the instance.\n5. Return the newly created PyTorch tensor.", "reference_code": "def deserialize(self, data: bytes) -> torch.Tensor:\n    assert self._dtype\n    return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "type": "method", "class_name": "NumpySerializer", "function_name": "deserialize", "dependency_all": "# Cross-file Dependency:\nlitdata.constants._NUMPY_DTYPES_MAPPING\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.constants._NUMPY_DTYPES_MAPPING\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n", "contexts_below": "\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def deserialize(self, data: bytes) -> np.ndarray:\n\n        \"\"\"\n        The function deserializes a byte array back into a numpy array. It first extracts the data type and shape information from the byte array, then reconstructs the numpy array based on this information.\n\n        Input-Output Arguments\n        :param data: bytes, the serialized numpy array data that includes information about the data type, shape, and the array's actual data.\n        :return: np.ndarray, the deserialized numpy array reconstructed from the input byte data.\n\n        \"\"\"", "reference_steps": "1. Define a function `deserialize` that takes a bytes object `data` as input and returns a NumPy array.\n2. Extract the first 4 bytes from `data` and convert them to an unsigned 32-bit integer to get the index of the data type (`dtype_indice`).\n3. Use the `dtype_indice` to retrieve the corresponding NumPy data type from a predefined mapping (`_NUMPY_DTYPES_MAPPING`).\n4. Extract the next 4 bytes from `data` and convert them to an unsigned 32-bit integer to get the size of the shape (`shape_size`).\n5. Initialize an empty list `shape` to hold the dimensions of the array.\n6. Iterate over the range of `shape_size` to extract each dimension of the shape from `data` and append it to the `shape` list.\n   - Calculate the start and end positions for each shape dimension within the byte stream.\n   - Extract and convert each shape dimension from the byte stream to an unsigned 32-bit integer and append to the `shape` list.\n7. Determine the start position of the actual array data in `data` by calculating the offset from the beginning (8 bytes for data type and shape size) plus 4 bytes for each shape dimension.\n8. Extract the remainder of `data` from the calculated start position to the end as the array data bytes.\n9. Convert the array data bytes to a NumPy array using the previously retrieved data type (`dtype`).\n10. Check if the shape of the extracted tensor matches the deserialized shape. If it does, return the tensor; if not, reshape the tensor to the deserialized shape and return it.", "reference_code": "def deserialize(self, data: bytes) -> np.ndarray:\n    dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n    dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n    shape_size = np.frombuffer(data[4:8], np.uint32).item()\n    shape = []\n    # deserialize the shape header\n    # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n    for shape_idx in range(shape_size):\n        shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n    # deserialize the numpy array bytes\n    tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n    if tensor.shape == shape:\n        return tensor\n    return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "type": "method", "class_name": "NoHeaderNumpySerializer", "function_name": "deserialize", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderNumpySerializer._dtype\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderNumpySerializer._dtype\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n", "contexts_below": "\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def deserialize(self, data: bytes) -> np.ndarray:\n\n        \"\"\"\n        Deserialize the given bytes data into a NumPy array using the predefined data type of the serializer instance.\n        Input-Output Arguments\n        :param self: NoHeaderNumpySerializer. An instance of the NoHeaderNumpySerializer class. It uses the _dtype attribute of the instance to determine the data type for the NumPy array.\n        :param data: bytes, The bytes data to be deserialized into a NumPy array.\n        :return: np.ndarray, The deserialized data as a NumPy array.\n        \"\"\"", "reference_steps": "1. Define a method called `deserialize`.\n2. The method takes two parameters: `self` and `data`, where `data` is of type `bytes`.\n3. Ensure that the instance variable `_dtype` is set (not `None` or `False`) using an assert statement.\n4. Convert the byte data into a NumPy array using the `np.frombuffer` function.\n5. Use the `_dtype` instance variable as the `dtype` argument for `np.frombuffer` to specify the data type of the resulting array.\n6. Return the resulting NumPy array.", "reference_code": "def deserialize(self, data: bytes) -> np.ndarray:\n    assert self._dtype\n    return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "type": "method", "class_name": "NoHeaderNumpySerializer", "function_name": "serialize", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderNumpySerializer._dtype_to_indices\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.serializers.NoHeaderNumpySerializer._dtype_to_indices\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n", "contexts_below": "\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n\n        \"\"\"\n        Serializes a NumPy array into a bytes object and generates a corresponding dtype identifier string. This method is designed for scenarios where header information is not required or desired in the serialized output.\n\n        Input-Output Arguments\n        :param self: NoHeaderNumpySerializer. An instance of the NoHeaderNumpySerializer class.\n        :param item: np.ndarray, The NumPy array to be serialized. It is used to convert the array into a bytes object and to determine the data type for generating the dtype identifier.\n        :return: Tuple[bytes, Optional[str]], A tuple where the first element is the serialized bytes object of the NumPy array and the second element is a string representing the data type of the array in a custom format (\"no_header_numpy:<indice>\").\n        \"\"\"", "reference_steps": "1. Define a method `serialize` that takes two parameters: `self` and `item`, where `item` is expected to be a NumPy array (`np.ndarray`).\n\n2. Access a dictionary or mapping (`self._dtype_to_indices`) that associates NumPy data types to specific indices.\n\n3. Retrieve the index (`dtype_indice`) corresponding to the data type (`dtype`) of the `item`.\n\n4. Convert the NumPy array `item` to a bytes object using the `tobytes` method with the order parameter set to \"C\" (row-major order).\n\n5. Construct a string that includes both a fixed prefix (\"no_header_numpy:\") and the retrieved data type index (`dtype_indice`).\n\n6. Return a tuple containing the bytes object from step 4 and the constructed string from step 5.", "reference_code": "def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n    dtype_indice: int = self._dtype_to_indices[item.dtype]\n    return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "type": "method", "class_name": "NumpySerializer", "function_name": "serialize", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.serializers.NumpySerializer._dtype_to_indices\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.serializers.NumpySerializer._dtype_to_indices\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nimport pickle\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom lightning_utilities.core.imports import RequirementCache\n\nfrom litdata.constants import _NUMPY_DTYPES_MAPPING, _TORCH_DTYPES_MAPPING\n\n_PIL_AVAILABLE = RequirementCache(\"PIL\")\n_TORCH_VISION_AVAILABLE = RequirementCache(\"torchvision\")\n_AV_AVAILABLE = RequirementCache(\"av\")\n\nif _PIL_AVAILABLE:\n    from PIL import Image\n    from PIL.GifImagePlugin import GifImageFile\n    from PIL.JpegImagePlugin import JpegImageFile\n    from PIL.PngImagePlugin import PngImageFile\n    from PIL.WebPImagePlugin import WebPImageFile\nelse:\n    Image = None\n    JpegImageFile = None\n    PngImageFile = None\n\nif _TORCH_VISION_AVAILABLE:\n    from torchvision.io import decode_jpeg\n    from torchvision.transforms.functional import pil_to_tensor\n\n\nclass Serializer(ABC):\n    \"\"\"The base interface for any serializers.\n\n    A Serializer serialize and deserialize to and from bytes.\n\n    \"\"\"\n\n    @abstractmethod\n    def serialize(self, data: Any) -> Tuple[bytes, Optional[str]]:\n        pass\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        pass\n\n    @abstractmethod\n    def can_serialize(self, data: Any) -> bool:\n        pass\n\n    def setup(self, metadata: Any) -> None:\n        pass\n\n\nclass PILSerializer(Serializer):\n    \"\"\"The PILSerializer serialize and deserialize PIL Image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n\nclass JPEGSerializer(Serializer):\n    \"\"\"The JPEGSerializer serialize and deserialize JPEG image to and from bytes.\"\"\"\n\n    def serialize(self, item: Image) -> Tuple[bytes, Optional[str]]:\n        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n\n    def deserialize(self, data: bytes) -> Union[JpegImageFile, torch.Tensor]:\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_PIL_AVAILABLE) and isinstance(item, JpegImageFile)\n\n\nclass BytesSerializer(Serializer):\n    \"\"\"The BytesSerializer serialize and deserialize integer to and from bytes.\"\"\"\n\n    def serialize(self, item: bytes) -> Tuple[bytes, Optional[str]]:\n        return item, None\n\n    def deserialize(self, item: bytes) -> bytes:\n        return item\n\n    def can_serialize(self, item: bytes) -> bool:\n        return isinstance(item, bytes)\n\n\nclass TensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n\nclass NoHeaderTensorSerializer(Serializer):\n    \"\"\"The TensorSerializer serialize and deserialize tensor to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _TORCH_DTYPES_MAPPING.items()}\n        self._dtype: Optional[torch.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _TORCH_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: torch.Tensor) -> Tuple[bytes, Optional[str]]:\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) == 1\n\n\nclass NumpySerializer(Serializer):\n    \"\"\"The NumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n\n", "contexts_below": "\n    def deserialize(self, data: bytes) -> np.ndarray:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) > 1\n\n\nclass NoHeaderNumpySerializer(Serializer):\n    \"\"\"The NoHeaderNumpySerializer serialize and deserialize numpy to and from bytes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._dtype_to_indices = {v: k for k, v in _NUMPY_DTYPES_MAPPING.items()}\n        self._dtype: Optional[np.dtype] = None\n\n    def setup(self, data_format: str) -> None:\n        self._dtype = _NUMPY_DTYPES_MAPPING[int(data_format.split(\":\")[1])]\n\n    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n\nclass PickleSerializer(Serializer):\n    \"\"\"The PickleSerializer serialize and deserialize python objects to and from bytes.\"\"\"\n\n    def serialize(self, item: Any) -> Tuple[bytes, Optional[str]]:\n        return pickle.dumps(item), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n    def can_serialize(self, _: Any) -> bool:\n        return True\n\n\nclass FileSerializer(Serializer):\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"file:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        return data\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data)\n\n\nclass VideoSerializer(Serializer):\n    _EXTENSIONS = (\"mp4\", \"ogv\", \"mjpeg\", \"avi\", \"mov\", \"h264\", \"mpg\", \"webm\", \"wmv\")\n\n    def serialize(self, filepath: str) -> Tuple[bytes, Optional[str]]:\n        _, file_extension = os.path.splitext(filepath)\n        with open(filepath, \"rb\") as f:\n            file_extension = file_extension.replace(\".\", \"\").lower()\n            return f.read(), f\"video:{file_extension}\"\n\n    def deserialize(self, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and any(data.endswith(ext) for ext in self._EXTENSIONS)\n\n\nclass StringSerializer(Serializer):\n    def serialize(self, obj: str) -> Tuple[bytes, Optional[str]]:\n        return obj.encode(\"utf-8\"), None\n\n    def deserialize(self, data: bytes) -> str:\n        return data.decode(\"utf-8\")\n\n    def can_serialize(self, data: str) -> bool:\n        return isinstance(data, str) and not os.path.isfile(data)\n\n\nclass NumericSerializer:\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def serialize(self, obj: Any) -> Tuple[bytes, Optional[str]]:\n        return self.dtype(obj).tobytes(), None\n\n    def deserialize(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass IntegerSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.int64)\n\n    def can_serialize(self, data: int) -> bool:\n        return isinstance(data, int)\n\n\nclass FloatSerializer(NumericSerializer, Serializer):\n    def __init__(self) -> None:\n        super().__init__(np.float64)\n\n    def can_serialize(self, data: float) -> bool:\n        return isinstance(data, float)\n\n\n_SERIALIZERS = OrderedDict(**{\n    \"str\": StringSerializer(),\n    \"int\": IntegerSerializer(),\n    \"float\": FloatSerializer(),\n    \"video\": VideoSerializer(),\n    \"tif\": FileSerializer(),\n    \"file\": FileSerializer(),\n    \"pil\": PILSerializer(),\n    \"jpeg\": JPEGSerializer(),\n    \"bytes\": BytesSerializer(),\n    \"no_header_numpy\": NoHeaderNumpySerializer(),\n    \"numpy\": NumpySerializer(),\n    \"no_header_tensor\": NoHeaderTensorSerializer(),\n    \"tensor\": TensorSerializer(),\n    \"pickle\": PickleSerializer(),\n})\n\n\ndef _get_serializers(serializers: Optional[Dict[str, Serializer]]) -> Dict[str, Serializer]:\n    if serializers:\n        serializers = OrderedDict(**serializers)\n        serializers.update(_SERIALIZERS)\n    else:\n        serializers = _SERIALIZERS\n    return serializers\n", "input_code": "    def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n\n        \"\"\"\n        Serializes a NumPy array into a bytes object, including metadata about the array's data type and shape, and returns it along with None as a placeholder for potential metadata that is not used in this implementation.\n        Input-Output Arguments\n        :param item: np.ndarray, the NumPy array to be serialized, used to extract the data type, shape, and the array's binary content.\n        :return: Tuple[bytes, Optional[str]], the serialized bytes object of the NumPy array and None. The bytes object includes the array's data type index, the number of dimensions, each dimension's size, and the array's binary data.\n        \"\"\"", "reference_steps": "1. Define a function `serialize` that takes an `item` which is a NumPy array and returns a tuple consisting of bytes and an optional string.\n\n2. Retrieve the index corresponding to the data type (`dtype`) of the `item` from a mapping (`self._dtype_to_indices`).\n\n3. Convert the dtype index into a 32-bit unsigned integer and then into bytes, and add this to the `data` list.\n\n4. Convert the length of the `item`'s shape (number of dimensions) into a 32-bit unsigned integer and then into bytes, and add this to the `data` list.\n\n5. Iterate over each dimension in the `item`'s shape:\n   - Convert each dimension size into a 32-bit unsigned integer and then into bytes.\n   - Append this byte representation to the `data` list.\n\n6. Convert the actual data of the `item` into bytes using the 'C' order (row-major order) and append this to the `data` list.\n\n7. Join all byte representations in the `data` list into a single bytes object.\n\n8. Return the single bytes object and `None` as the second element of the tuple (since the function signature specifies an optional string which is not used in this case).", "reference_code": "def serialize(self, item: np.ndarray) -> Tuple[bytes, Optional[str]]:\n    dtype_indice = self._dtype_to_indices[item.dtype]\n    data = [np.uint32(dtype_indice).tobytes()]\n    data.append(np.uint32(len(item.shape)).tobytes())\n    for dim in item.shape:\n        data.append(np.uint32(dim).tobytes())\n    data.append(item.tobytes(order=\"C\"))\n    return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "type": "method", "class_name": "StreamingDataLoader", "function_name": "state_dict", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.dataloader.StreamingDataLoader._latest_worker_idx\n\nlitdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_combined\n\nlitdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_streaming\n\nlitdata.streaming.dataloader.StreamingDataLoader.batch_size\n\nlitdata.streaming.dataloader.StreamingDataLoader.current_epoch\n\nlitdata.streaming.dataloader.StreamingDataLoader.num_workers\n\nlitdata.streaming.dataloader.StreamingDataLoader.restore\n\n# Cross-file Dependency:\nlitdata.streaming.dataset.StreamingDataset\n    class StreamingDataset(IterableDataset):\n        \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\"\"\"\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.dataloader.StreamingDataLoader.restore\n\nlitdata.streaming.dataloader.StreamingDataLoader.current_epoch\n\nlitdata.streaming.dataloader.StreamingDataLoader.batch_size\n\nlitdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_streaming\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport inspect\nimport logging\nimport os\nfrom copy import deepcopy\nfrom importlib import reload\nfrom itertools import cycle\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport torch\nfrom torch.utils.data import Dataset, IterableDataset\nfrom torch.utils.data._utils.collate import default_collate\nfrom torch.utils.data._utils.fetch import _BaseDatasetFetcher\nfrom torch.utils.data.dataloader import (\n    DataLoader,\n    _BaseDataLoaderIter,\n    _DatasetKind,\n    _MultiProcessingDataLoaderIter,\n    _SingleProcessDataLoaderIter,\n)\nfrom torch.utils.data.sampler import BatchSampler, Sampler\n\nfrom litdata.constants import _DEFAULT_CHUNK_BYTES, _TORCH_GREATER_EQUAL_2_1_0, _VIZ_TRACKER_AVAILABLE\nfrom litdata.streaming import Cache\nfrom litdata.streaming.combined import (\n    __NUM_SAMPLES_YIELDED_KEY__,\n    __SAMPLES_KEY__,\n    CombinedStreamingDataset,\n)\nfrom litdata.streaming.dataset import StreamingDataset\nfrom litdata.streaming.sampler import CacheBatchSampler\nfrom litdata.utilities.env import _DistributedEnv\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten\n\nlogger = logging.Logger(__name__)\n\n\ndef _equal_items(data_1: Any, data_2: Any) -> bool:\n    data_1_flattened, _ = tree_flatten(data_1)\n    data_2_flattened, _ = tree_flatten(data_2)\n\n    if len(data_1_flattened) != len(data_2_flattened):\n        return False\n\n    return all(_equal_item(d1, d2) for d1, d2 in zip(data_1_flattened, data_2_flattened))\n\n\ndef _equal_item(d1: Any, d2: Any) -> bool:\n    if not isinstance(d1, type(d2)):\n        return False\n    equality = d1 == d2\n    if isinstance(equality, torch.Tensor):\n        return bool(equality.all().item())\n    if equality is True:\n        return True\n    return False\n\n\nclass CacheDataset(Dataset):\n    def __init__(\n        self,\n        dataset: Any,\n        cache_dir: str,\n        chunk_bytes: Optional[int],\n        chunk_size: Optional[int],\n        compression: Optional[str],\n    ):\n        \"\"\"The `CacheDataset` is a dataset wraper to provide a beginner experience with the Cache.\n\n        Arguments:\n            dataset: The dataset of the user\n            cache_dir: The folder where the chunks are written to.\n            chunk_bytes: The maximal number of bytes to write within a chunk.\n            chunk_sie: The maximal number of items to write to a chunk.\n            compression: The compression algorithm to use to reduce the size of the chunk.\n\n        \"\"\"\n        self._dataset = dataset\n        self._cache = Cache(cache_dir, chunk_bytes=chunk_bytes, chunk_size=chunk_size, compression=compression)\n        self._is_deterministic = False\n\n    def __len__(self) -> int:\n        return len(self._cache) if self._cache.filled else len(self._dataset)\n\n    def __getitem__(self, index: int) -> Any:\n        data_1 = self._cache[index] if self._cache.filled else self._dataset[index]\n        if not self._cache.filled:\n            if not self._is_deterministic:\n                data2 = self._dataset[index]\n                if not _equal_items(data_1, data2):\n                    raise ValueError(\n                        f\"Your dataset items aren't deterministic. Found {data_1} and {data2} for index {index}.\"\n                        \" HINT: Use the `litdata.cache.Cache` directly within your dataset.\"\n                    )\n                self._is_deterministic = True\n            self._cache[index] = data_1\n        return data_1\n\n\nclass CacheCollateFn:\n    \"\"\"This CacheCollateFn is used to accelerate the processing of the data generated using the Cache.\n\n    During the chunking phase, there is no need to return any data from the DataLoader reducing some time.\n\n    Additionally, if the user makes their __getitem__ asynchronous, the collate executes them in parallel.\n\n    \"\"\"\n\n    def __init__(self, collate_fn: Optional[Callable] = None) -> None:\n        self.collate_fn = collate_fn or default_collate\n\n    def __call__(self, items: List[Any]) -> Any:\n        if all(item is None for item in items):\n            return None\n\n        # If the __getitem__ method is asynchornous, collect all the items.\n        if all(inspect.iscoroutine(item) for item in items):\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            items = loop.run_until_complete(asyncio.gather(*items))\n\n        return self.collate_fn([item for item in items if item is not None])\n\n\nclass _SingleProcessDataLoaderIterPatch(_SingleProcessDataLoaderIter):\n    \"\"\"This is overriden to inform the cache is done chunking.\"\"\"\n\n    def _next_data(self) -> Any:\n        try:\n            data = None\n            while data is None:\n                data = super()._next_data()\n            return data\n        except StopIteration:\n            for v in self._dataset_fetcher.dataset.__dict__.values():\n                if isinstance(v, Cache):\n                    v.done()\n                    if not v.filled:\n                        v.merge(1)\n            raise StopIteration()\n\n\nclass WorkerLoop:\n    \"\"\"Wrap the PyTorch DataLoader WorkerLoop to perform caching and profiling.\"\"\"\n\n    def __init__(self, global_rank: int, profile: bool = False) -> None:\n        self._global_rank = global_rank\n        self._profile = profile\n\n    def __call__(\n        self,\n        dataset_kind: Any,\n        dataset: Any,\n        index_queue: Any,\n        data_queue: Any,\n        done_event: Any,\n        auto_collation: Any,\n        collate_fn: Any,\n        drop_last: Any,\n        base_seed: Any,\n        init_fn: Any,\n        worker_id: Any,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        from torch.utils.data._utils import worker\n\n        from litdata.streaming.cache import Cache\n\n        enable_profiling = self._global_rank == 0 and worker_id == 0 and _VIZ_TRACKER_AVAILABLE and self._profile\n\n        if enable_profiling:\n            from viztracer import VizTracer\n\n            tracer = VizTracer(output_file=os.path.join(os.getcwd(), \"trace.json\"))\n            tracer.start()\n\n        # Reload to remove the patching\n        reloaded_worker = reload(worker)\n        create_fetcher = _DatasetKind.create_fetcher\n        fetcher = None\n\n        def create_fetcher_fn(*args: Any, **kwargs: Any) -> \"_BaseDatasetFetcher\":\n            nonlocal fetcher\n            fetcher = create_fetcher(*args, **kwargs)\n            return fetcher\n\n        _DatasetKind.create_fetcher = create_fetcher_fn  # type: ignore\n\n        reloaded_worker._worker_loop(\n            dataset_kind,\n            dataset,\n            index_queue,\n            data_queue,\n            done_event,\n            auto_collation,\n            collate_fn,\n            drop_last,\n            base_seed,\n            init_fn,\n            worker_id,\n            *args,\n            **kwargs,\n        )\n\n        if dataset_kind == _DatasetKind.Map:\n            assert fetcher\n            for v in fetcher.dataset.__dict__.values():\n                if isinstance(v, Cache):\n                    v.done()\n\n        if enable_profiling:\n            tracer.stop()\n            tracer.save()\n\n\nclass _MultiProcessingDataLoaderIterPatch(_MultiProcessingDataLoaderIter):\n    def __init__(self, loader: DataLoader) -> None:\n        self._cache = loader._cache\n        self._num_workers = loader.num_workers\n        # Patch PyTorch worker loop to call the `cache.done()` method.\n        from torch.utils.data._utils import worker\n\n        worker._worker_loop = WorkerLoop(loader._global_rank, loader._profile)\n        super().__init__(loader)\n\n    def _shutdown_workers(self) -> None:\n        super()._shutdown_workers()\n\n        # If the data isn't filled, we trigger an indedm merge\n        if not self._cache.filled:\n            self._cache.merge(self._num_workers)\n\n    def _next_data(self) -> Any:\n        try:\n            data = None\n            while data is None:\n                data = super()._next_data()\n            return data\n        except StopIteration as e:\n            raise e\n\n\nclass CacheDataLoader(DataLoader):\n    __doc__ = DataLoader.__doc__\n\n    def __init__(\n        self,\n        dataset: Any,\n        *args: Any,\n        sampler: Optional[Sampler] = None,\n        batch_sampler: Optional[BatchSampler] = None,\n        num_workers: int = 0,\n        shuffle: bool = False,\n        generator: Optional[torch.Generator] = None,\n        batch_size: Optional[int] = None,\n        drop_last: bool = False,\n        cache_dir: Optional[str] = None,\n        chunk_bytes: Optional[int] = _DEFAULT_CHUNK_BYTES,\n        compression: Optional[str] = None,\n        profile: bool = False,\n        collate_fn: Optional[Callable] = None,\n        **kwargs: Any,\n    ) -> None:\n        if sampler:\n            raise ValueError(\n                \"The CacheDataLoader relies on its own internal sampler. Passing a sampler isn't supported.\"\n            )\n\n        if batch_sampler:\n            raise ValueError(\n                \"The CacheDataLoader relies on its own internal sampler. Passing a batch_sampler isn't supported.\"\n            )\n\n        if isinstance(dataset, IterableDataset):\n            raise ValueError(\"Only map-based dataset are supported by the CacheDataLoader for now.\")\n\n        if profile and not _VIZ_TRACKER_AVAILABLE:\n            raise ModuleNotFoundError(\"To enable DataLoader profiling, run `pip install viztracer`.\")\n\n        cache_list = [v for v in dataset.__dict__.values() if isinstance(v, Cache)]\n\n        if len(cache_list) > 1:\n            raise ValueError(\n                \"We found several Cache used as attributes from your dataset. Only one is support for now.\"\n            )\n\n        if len(cache_list) == 0:\n            if cache_dir is None:\n                raise ValueError(\"You should provide a `cache_dir` filepath to the CacheDataLoader.\")\n\n            dataset = CacheDataset(dataset, cache_dir, chunk_bytes, batch_size, compression)\n            cache = dataset._cache\n        else:\n            cache = cache_list[0]\n\n        if not cache.filled and shuffle:\n            logger.info(\"Shuffle is ignored during the caching phase phase.\")\n\n        self._cache = cache\n\n        distributed_env = _DistributedEnv.detect()\n        self._global_rank = distributed_env.global_rank\n\n        batch_sampler = CacheBatchSampler(\n            len(dataset),\n            distributed_env.world_size,\n            self._global_rank,\n            num_workers,\n            batch_size or 1,\n            drop_last,\n            shuffle,\n            cache,\n        )\n\n        self._profile = profile\n\n        super().__init__(\n            dataset,\n            *args,\n            batch_sampler=batch_sampler,  # type: ignore\n            collate_fn=CacheCollateFn(collate_fn),\n            num_workers=num_workers,\n            **kwargs,\n        )\n\n    def _get_iterator(self) -> \"_BaseDataLoaderIter\":\n        \"\"\"Overriden to ensure the `Cache.done()` method is triggered on iteration done.\"\"\"\n        if self.num_workers == 0:\n            return _SingleProcessDataLoaderIterPatch(self)\n        self.check_worker_number_rationality()\n        return _MultiProcessingDataLoaderIterPatch(self)\n\n\ndef _wrapper(fetcher: Any, func: Callable, tracer: Any, profile: int, profile_dir: str) -> Callable:\n    counter = 0\n\n    def wrap(*args: Any, **kwargs: Any) -> Any:\n        nonlocal counter\n        result = func(*args, **kwargs)\n\n        if tracer.enable and counter == profile:\n            tracer.stop()\n            tracer.save()\n            print(\n                f\"Saved {os.path.join(profile_dir, 'result.json')} file after {profile} batches.\"\n                \"Use chrome://tracing/ to view it.\"\n            )\n            fetcher.fetch = func\n\n        counter += 1\n        return result\n\n    return wrap\n\n\nclass _ProfileWorkerLoop:\n    \"\"\"Wrap the PyTorch DataLoader WorkerLoop to add profiling.\"\"\"\n\n    def __init__(self, profile: Union[int, bool], profile_dir: Optional[str] = None):\n        self._profile = profile\n        self._profile_dir = profile_dir if profile_dir else os.getcwd()\n\n    def __call__(\n        self,\n        dataset_kind: Any,\n        dataset: Any,\n        index_queue: Any,\n        data_queue: Any,\n        done_event: Any,\n        auto_collation: Any,\n        collate_fn: Any,\n        drop_last: Any,\n        base_seed: Any,\n        init_fn: Any,\n        worker_id: Any,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        from torch.utils.data._utils import worker\n        from viztracer import VizTracer\n\n        if worker_id == 0:\n            output_file = os.path.join(self._profile_dir, \"result.json\")\n\n            if os.path.exists(output_file):\n                os.remove(output_file)\n\n            tracer = VizTracer(output_file=output_file, verbose=0)\n            tracer.start()\n\n        # Reload to remove the patching\n        reloaded_worker = reload(worker)\n        create_fetcher = _DatasetKind.create_fetcher\n        fetcher = None\n\n        def create_fetcher_fn(*args: Any, **kwargs: Any) -> \"_BaseDatasetFetcher\":\n            nonlocal fetcher\n            fetcher = create_fetcher(*args, **kwargs)\n\n            if worker_id == 0 and isinstance(self._profile, int):\n                fetcher.fetch = _wrapper(fetcher, fetcher.fetch, tracer, self._profile, self._profile_dir)\n            return fetcher\n\n        _DatasetKind.create_fetcher = create_fetcher_fn  # type: ignore\n\n        reloaded_worker._worker_loop(\n            dataset_kind,\n            dataset,\n            index_queue,\n            data_queue,\n            done_event,\n            auto_collation,\n            collate_fn,\n            drop_last,\n            base_seed,\n            init_fn,\n            worker_id,\n            *args,\n            **kwargs,\n        )\n\n        if worker_id == 0 and isinstance(self._profile, bool):\n            tracer.stop()\n            tracer.save()\n\n\nclass _StreamingMultiProcessingDataLoaderIter(_MultiProcessingDataLoaderIter):\n    def __init__(self, loader: DataLoader) -> None:\n        self._loader = loader\n        self._indexes = (\n            list(range(self._loader._latest_worker_idx, self._loader.num_workers))\n            if self._loader._latest_worker_idx > 0\n            else []\n        )\n        self._num_workers = loader.num_workers\n\n        distributed_env = _DistributedEnv.detect()\n\n        if self._loader._profile_batches and distributed_env.global_rank == 0 and _VIZ_TRACKER_AVAILABLE:\n            from torch.utils.data._utils import worker\n\n            worker._worker_loop = _ProfileWorkerLoop(self._loader._profile_batches, self._loader._profile_dir)\n\n        super().__init__(loader)\n\n    def _try_put_index(self) -> None:\n        # Used to restart on the right DataLoader worker\n        if self._loader.restore and self._indexes:\n            assert self._tasks_outstanding < self._prefetch_factor * self._num_workers\n\n            try:\n                index = self._next_index()\n            except StopIteration:\n                return\n            worker_queue_idx = self._indexes.pop(0)\n\n            self._index_queues[worker_queue_idx].put((self._send_idx, index))\n            self._task_info[self._send_idx] = (worker_queue_idx,)\n            self._tasks_outstanding += 1\n            self._send_idx += 1\n        else:\n            super()._try_put_index()\n\n\nclass StreamingDataLoader(DataLoader):\n    r\"\"\"The StreamingDataLoader combines a dataset and a sampler, and provides an iterable over the given dataset.\n\n    The :class:`~litdata.streaming.dataloader.StreamingDataLoader` supports either a\n    StreamingDataset and CombinedStreamingDataset datasets with single- or multi-process loading,\n    customizing\n    loading order and optional automatic batching (collation) and memory pinning.\n\n    See :py:mod:`torch.utils.data` documentation page for more details.\n\n    Args:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: ``1``).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: ``False``).\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. ``0`` means that the data will be loaded in the main process.\n            (default: ``0``)\n        collate_fn (Callable, optional): merges a list of samples to form a\n            mini-batch of Tensor(s).  Used when using batched loading from a\n            map-style dataset.\n        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n            into device/CUDA pinned memory before returning them.  If your data elements\n            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n            see the example below.\n        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative. (default: ``0``)\n        worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n            input, after seeding and before data loading. (default: ``None``)\n        multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n            ``None``, the default `multiprocessing context`_ of your operating system will\n            be used. (default: ``None``)\n        generator (torch.Generator, optional): If not ``None``, this RNG will be used\n            by RandomSampler to generate random indexes and multiprocessing to generate\n            ``base_seed`` for workers. (default: ``None``)\n        prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n            in advance by each worker. ``2`` means there will be a total of\n            2 * num_workers batches prefetched across all workers. (default value depends\n            on the set value for num_workers. If value of num_workers=0 default is ``None``.\n            Otherwise, if value of ``num_workers > 0`` default is ``2``).\n        persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n            the worker processes after a dataset has been consumed once. This allows to\n            maintain the workers `Dataset` instances alive. (default: ``False``)\n        pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n            ``True``.\n        profile_batches (int, bool, optional): Whether to record data loading profile and generate a result.json file.\n        profile_dir (int, bool,  optional): Where to store the recorded trace when profile_batches is enabled.\n\n    \"\"\"\n\n    __doc__ = DataLoader.__doc__\n\n    def __init__(\n        self,\n        dataset: Union[StreamingDataset, CombinedStreamingDataset],\n        *args: Any,\n        batch_size: int = 1,\n        num_workers: int = 0,\n        profile_batches: Union[bool, int] = False,\n        profile_dir: Optional[str] = None,\n        prefetch_factor: Optional[int] = None,\n        shuffle: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> None:  # pyright: ignore\n        if not isinstance(dataset, (StreamingDataset, CombinedStreamingDataset)):\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {dataset}.\"\n            )\n\n        if shuffle is not None:\n            dataset.set_shuffle(shuffle)\n\n        shuffle = None\n\n        if profile_batches and not _VIZ_TRACKER_AVAILABLE:\n            raise ModuleNotFoundError(\"To use profile_batches, viztracer is required. Run `pip install viztracer`\")\n\n        if profile_batches and num_workers == 0:\n            raise ValueError(\"Profiling is supported only with num_workers >= 1.\")\n\n        self.current_epoch = 0\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self._profile_batches = profile_batches\n        self._profile_dir = profile_dir\n        self._num_samples_yielded_streaming = 0\n        self._num_samples_yielded_combined: Dict[int, List[Any]] = {}\n        self.rng_state: Optional[Any] = None\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter: Optional[Any] = None\n        self._latest_worker_idx = 0\n        self.restore = False\n        super().__init__(\n            dataset,\n            *args,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            prefetch_factor=(10 if num_workers > 0 else None) if prefetch_factor is None else prefetch_factor,\n            **kwargs,\n        )  # type: ignore\n\n    def __iter__(self) -> Any:\n        if not self.restore:\n            self._latest_worker_idx = 0\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.current_epoch += 1\n            self._num_samples_yielded_combined = {}\n            self._num_samples_yielded_streaming = 0\n\n        self.dataset.set_epoch(self.current_epoch)\n\n        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            for batch in super().__iter__():\n                self._latest_worker_idx = next(self._worker_idx_iter)  # type: ignore\n                self._num_samples_yielded_streaming += self.batch_size\n                yield batch\n        else:\n            self.dataset._set_use_streaming_dataloader(True)\n            assert self.batch_size\n            # TODO: Inject a custom collate function to avoid collating the __NUM_SAMPLES_YIELDED__ key\n            for batch in super().__iter__():\n                self._latest_worker_idx = next(self._worker_idx_iter)  # type: ignore\n                if isinstance(batch, dict) and __NUM_SAMPLES_YIELDED_KEY__ in batch:\n                    self._num_samples_yielded_combined[self._latest_worker_idx] = [\n                        sample[-1].item() if self.batch_size > 1 else sample.item()\n                        for sample in batch[__NUM_SAMPLES_YIELDED_KEY__]\n                    ]\n\n                    yield batch[__SAMPLES_KEY__]\n                else:\n                    yield batch\n\n        self.restore = False\n\n", "contexts_below": "\n    def load_state_dict(self, obj: Dict[str, Any]) -> None:\n        \"\"\"Load a dict containing training state (called from non-worker process).\n\n        This is called on each copy of the dataset when resuming.\n\n        Args:\n            obj (Any): The state.\n\n        \"\"\"\n        self.current_epoch = obj[\"current_epoch\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        else:\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n\n        # Used to restart on the next DataLoader worker from the previous run.\n        self._latest_worker_idx = obj[\"latest_worker_idx\"] + 1\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)\n\n        # Inform we are resuming and disable resetting the StreamingDataLoader state.\n        # This is toggle back to False when the `__iter__` method of the StreamingDataLoader completes.\n        self.restore = True\n\n        if isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset._set_use_streaming_dataloader(True)\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"The provided dataset should be a `StreamingDataset` or a `CombinedStreamingDataset`.\")\n\n    def _get_iterator(self) -> \"_BaseDataLoaderIter\":\n        \"\"\"Overriden to ensure the `Cache.done()` method is triggered on iteration done.\"\"\"\n        if self.num_workers == 0:\n            return _SingleProcessDataLoaderIter(self)\n        self.check_worker_number_rationality()\n        return _StreamingMultiProcessingDataLoaderIter(self)\n", "input_code": "    def state_dict(self) -> Dict[str, Any]:\n\n        \"\"\"\n        This function generates and returns a dictionary containing the state of the StreamingDataLoader instance, which includes information about the dataset, current epoch, number of samples yielded, and the index of the latest worker. The structure of the returned state dictionary varies depending on whether the dataset is an instance of StreamingDataset.\n\n        Input-Output Arguments\n        :param self: StreamingDataLoader. An instance of the StreamingDataLoader class. It is used to access the dataset, batch size, number of workers, current epoch, and other attributes to construct the state dictionary.\n        :return: Dict[str, Any]. A dictionary representing the state of the StreamingDataLoader instance. The keys include \"dataset\", \"current_epoch\", \"num_samples_yielded\", and \"latest_worker_idx\". The structure of the \"dataset\" part of the dictionary depends on whether the dataset is a StreamingDataset or not.\n        \"\"\"", "reference_steps": "1. Define a method `state_dict` that returns a dictionary containing the state of the dataset and other relevant information.\n   \n2. Check if the dataset is an instance of `StreamingDataset`.\n\n3. If it is a `StreamingDataset`, assert that `self.batch_size` is not None.\n\n4. Create a dictionary with the state of the `StreamingDataset` by calling its `state_dict` method with parameters `self._num_samples_yielded_streaming`, `self.num_workers`, and `self.batch_size`.\n\n5. Add additional state information to the dictionary, such as `current_epoch`, `num_samples_yielded`, and `latest_worker_idx`.\n\n6. If the dataset is not a `StreamingDataset`, initialize a list `num_samples_yieled` with zeros, with the length equal to the number of datasets managed by workers.\n\n7. Loop over each worker index in `self._num_samples_yielded_combined`.\n\n8. Within the loop, enumerate over `self._num_samples_yielded_combined[worker_idx]` and increment the corresponding index in `num_samples_yieled` by the number of samples yielded.\n\n9. Create a dictionary with the state of the dataset by calling its `state_dict` method with parameters `self.num_workers`, `self.batch_size`, and the `num_samples_yieled` list.\n\n10. Add additional state information to the dictionary, such as `current_epoch` (adjusted if `self.restore` is False), `latest_worker_idx`, and a deep copy of `self._num_samples_yielded_combined`.", "reference_code": "def state_dict(self) -> Dict[str, Any]:\n    if isinstance(self.dataset, StreamingDataset):\n        assert self.batch_size\n        return {\n            \"dataset\": self.dataset.state_dict(\n                self._num_samples_yielded_streaming, self.num_workers, self.batch_size\n            ),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n    num_samples_yieled = [0 for _ in range(len(list(self._num_samples_yielded_combined.values())[0]))]\n    for worker_idx in self._num_samples_yielded_combined:\n        for dataset_idx, samples_yieled in enumerate(self._num_samples_yielded_combined[worker_idx]):\n            num_samples_yieled[dataset_idx] += samples_yieled\n\n    return {\n        \"dataset\": self.dataset.state_dict(self.num_workers, self.batch_size, num_samples_yieled),\n        \"current_epoch\": self.current_epoch if self.restore else self.current_epoch - 1,\n        \"latest_worker_idx\": self._latest_worker_idx,\n        \"num_samples_yielded\": deepcopy(self._num_samples_yielded_combined),\n    }\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "type": "method", "class_name": "BinaryWriter", "function_name": "done", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.writer.BinaryWriter._is_done\n\nlitdata.streaming.writer.BinaryWriter._serialized_items\n\nlitdata.streaming.writer.BinaryWriter._should_write\n    def _should_write(self) -> bool:\n\nlitdata.streaming.writer.BinaryWriter.filled\n    def filled(self) -> bool:\n        \"\"\"Returns whether the caching phase is done.\"\"\"\n\nlitdata.streaming.writer.BinaryWriter.write_chunk\n    def write_chunk(self, on_done: bool = False) -> str:\n        \"\"\"Write a chunk to the filesystem.\"\"\"\n\nlitdata.streaming.writer.BinaryWriter.write_chunks_index\n    def write_chunks_index(self) -> str:\n        \"\"\"Write the chunks index to a JSON file.\"\"\"\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.writer.BinaryWriter._serialized_items\n\nlitdata.streaming.writer.BinaryWriter.write_chunk\n    def write_chunk(self, on_done: bool = False) -> str:\n        \"\"\"Write a chunk to the filesystem.\"\"\"\n\nlitdata.streaming.writer.BinaryWriter.filled\n    def filled(self) -> bool:\n        \"\"\"Returns whether the caching phase is done.\"\"\"\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom time import sleep\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\n\nfrom litdata.constants import _INDEX_FILENAME, _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.processing.utilities import get_worker_rank\nfrom litdata.streaming.compression import _COMPRESSORS, Compressor\nfrom litdata.streaming.serializers import Serializer, _get_serializers\nfrom litdata.utilities.env import _DistributedEnv, _WorkerEnv\nfrom litdata.utilities.format import _convert_bytes_to_int, _human_readable_bytes\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import PyTree, tree_flatten, treespec_dumps\n\n\n@dataclass\nclass Item:\n    index: int\n    data: bytes\n    bytes: int\n    dim: Optional[int] = None\n\n    def __len__(self) -> int:\n        return self.bytes\n\n\nclass BinaryWriter:\n    def __init__(\n        self,\n        cache_dir: str,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n        follow_tensor_dimension: bool = True,\n        serializers: Optional[Dict[str, Serializer]] = None,\n    ):\n        \"\"\"The BinaryWriter enables to chunk dataset into an efficient streaming format for cloud training.\n\n        Arguments:\n            cache_dir: The path to where the chunks will be saved.\n            chunk_bytes: The maximum number of bytes within a chunk.\n            chunk_size: The maximum number of items within a chunk.\n            compression: The compression algorithm to use.\n            serializers: Provide your own serializers.\n\n        \"\"\"\n        self._cache_dir = cache_dir\n\n        if (isinstance(self._cache_dir, str) and not os.path.exists(self._cache_dir)) or self._cache_dir is None:\n            raise FileNotFoundError(f\"The provided cache directory `{self._cache_dir}` doesn't exist.\")\n\n        if (chunk_size is None and chunk_bytes is None) or (chunk_size and chunk_bytes):\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self._serializers: Dict[str, Serializer] = _get_serializers(serializers)\n        self._serializers_extra: Dict[str, Serializer] = {}\n        self._chunk_size = chunk_size\n        self._chunk_bytes = _convert_bytes_to_int(chunk_bytes) if isinstance(chunk_bytes, str) else chunk_bytes\n        self._compression = compression\n\n        self._data_format: Optional[List[str]] = None\n        self._data_spec: Optional[PyTree] = None\n\n        if self._compression:\n            if len(_COMPRESSORS) == 0:\n                raise ValueError(\"No compresion algorithms are installed.\")\n\n            if self._compression not in _COMPRESSORS:\n                raise ValueError(\n                    f\"The provided compression {self._compression} isn't available in {sorted(_COMPRESSORS)}\"\n                )\n            self._compressor: Compressor = _COMPRESSORS[self._compression]\n\n        self._serialized_items: Dict[int, Item] = {}\n        self._chunk_index = 0\n        self._min_index: Optional[int] = None\n        self._max_index: Optional[int] = None\n        self._chunks_info: List[Dict[str, Any]] = []\n        self._worker_env: Optional[_WorkerEnv] = None\n        self._rank: Optional[int] = None\n        self._is_done = False\n        self._distributed_env = _DistributedEnv.detect()\n        self._follow_tensor_dimension = follow_tensor_dimension\n\n    @property\n    def filled(self) -> bool:\n        \"\"\"Returns whether the caching phase is done.\"\"\"\n        if self._is_done:\n            return True\n        files = os.listdir(self._cache_dir)\n        index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n        worker_env = _WorkerEnv.detect()\n        data_optimiser_num_workers = os.getenv(\"DATA_OPTIMIZER_NUM_WORKERS\", None)\n        if data_optimiser_num_workers is not None:\n            self._is_done = len(index_files) == int(data_optimiser_num_workers)\n        else:\n            self._is_done = len(index_files) == self._distributed_env.world_size * worker_env.world_size\n        return self._is_done\n\n    @property\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n        if self._rank is None:\n            rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n            if rank:\n                self._rank = int(rank)\n            else:\n                self._worker_env = _WorkerEnv.detect()\n                self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n        return self._rank\n\n    def get_config(self) -> Dict[str, Any]:\n        \"\"\"Returns the config of the writer.\"\"\"\n        out = {\n            \"compression\": self._compression,\n            \"chunk_size\": self._chunk_size,\n            \"chunk_bytes\": self._chunk_bytes,\n            \"data_format\": self._data_format,\n            \"data_spec\": treespec_dumps(self._data_spec) if self._data_spec else None,\n        }\n        return out\n\n    def serialize(self, items: Any) -> Tuple[bytes, Optional[int]]:\n        \"\"\"Serialize a dictionary into its binary format.\"\"\"\n\n        # Flatten the items provided by the users\n        flattened, data_spec = tree_flatten(items)\n\n        is_single_tensor = len(flattened) == 1 and isinstance(flattened[0], torch.Tensor)\n\n        # Collect the sizes and associated bytes for each item\n        sizes: List[int] = []\n        data: List[bytes] = []\n\n        if self._data_format is None:\n            data_format: List[str] = []\n            for item in flattened:\n                data_format.append(self._serialize(item, sizes, data))\n\n            worker_rank = get_worker_rank()\n            if worker_rank is not None:\n                print(f\"Rank {worker_rank} inferred the following `{data_format}` data format.\")\n            self._data_format = data_format\n            self._data_spec = data_spec\n        else:\n            # tiny optimization to avoid looping over all the data format\n            self._serialize_with_data_format(flattened, sizes, data, self._data_format)\n\n        # If there is a single element and it is a tensor, enable continous array.\n        if is_single_tensor:\n            return data[0], flattened[0].shape[0]\n\n        # Concatenante into a single byte array\n        head = np.array(sizes, np.uint32).tobytes()\n        body = b\"\".join(data)\n        return head + body, None\n\n    def _serialize(self, item: Any, sizes: List[int], data: List[bytes]) -> str:\n        \"\"\"Serialize a given item and append its size and bytes to the sizes and data array.\"\"\"\n        for serializer_name, serializer in self._serializers.items():\n            if serializer.can_serialize(item):\n                serialized_item, name = serializer.serialize(item)\n                data.append(serialized_item)\n                sizes.append(serializer.size if hasattr(serializer, \"size\") else len(serialized_item))\n                name = name or serializer_name\n                if name and name not in self._serializers_extra:\n                    self._serializers_extra[name] = serializer\n                return name\n        raise ValueError(f\"The provided item isn't serializable. Found {item}\")\n\n    def _serialize_with_data_format(\n        self, item: Any, sizes: List[int], data: List[bytes], data_format: List[str]\n    ) -> None:\n        \"\"\"Serialize a given item and append its size and bytes to the sizes and data array.\"\"\"\n        assert data_format\n        for element, item_format in zip(item, data_format):\n            serializer = self._serializers_extra[item_format]\n            serialized_item, _ = serializer.serialize(element)\n            data.append(serialized_item)\n            sizes.append(serializer.size if hasattr(serializer, \"size\") else len(serialized_item))\n\n    def _create_chunk(self, filename: str, on_done: bool = False) -> bytes:\n        \"\"\"Create a binary chunk from all the binarized items.\"\"\"\n        items = []\n\n        if on_done:\n            indices = sorted(self._serialized_items.keys())\n            for i in range(len(indices) - 1):\n                assert indices[i] == indices[i + 1] - 1, indices\n            items = [self._serialized_items.pop(index) for index in indices]\n        else:\n            assert self._max_index is not None, (self._max_index, self._min_index)\n            assert self._min_index is not None, (self._max_index, self._min_index)\n            if self._max_index == self._min_index:\n                # A single item is larger than the target chunk size; allow the chunk to be bigger than the target size\n                items.append(self._serialized_items.pop(self._max_index))\n            items.extend(self._serialized_items.pop(index) for index in range(self._min_index, self._max_index))\n\n        if len(items) == 0:\n            raise RuntimeError(\n                \"The items shouldn't have an empty length. Something went wrong.\"\n                f\" Found {self._pretty_serialized_items()} with boundaries: {self._min_index}, {self._max_index}.\"\n            )\n\n        num_items = np.uint32(len(items))\n        sizes = list(map(len, items))\n        offsets = np.array([0] + sizes).cumsum().astype(np.uint32)\n        offsets += len(num_items.tobytes()) + len(offsets.tobytes())\n        sample_data = b\"\".join([item.data for item in items])\n        data = num_items.tobytes() + offsets.tobytes() + sample_data\n\n        current_chunk_bytes = sum([item.bytes for item in items])\n\n        if self._chunk_bytes and current_chunk_bytes > self._chunk_bytes:\n            warnings.warn(\n                f\"An item was larger than the target chunk size ({_human_readable_bytes(self._chunk_bytes)}).\"\n                f\" The current chunk will be {_human_readable_bytes(current_chunk_bytes)} in size.\",\n                UserWarning,\n            )\n\n        if self._chunk_size:\n            assert num_items.item() <= self._chunk_size\n\n        dim: Optional[int] = None\n        if items[0].dim:\n            dim = sum([item.dim if item.dim is not None else 0 for item in items])\n\n        chunk_info = {\n            \"chunk_bytes\": current_chunk_bytes,\n            \"chunk_size\": num_items.item(),\n            \"filename\": filename,\n            \"dim\": dim,\n        }\n\n        self._chunks_info.append(chunk_info)\n\n        return data\n\n    def get_chunk_filename(self) -> str:\n        if self._compression:\n            return f\"chunk-{self.rank}-{self._chunk_index}.{self._compression}.bin\"\n        return f\"chunk-{self.rank}-{self._chunk_index}.bin\"\n\n    def write_chunk(self, on_done: bool = False) -> str:\n        \"\"\"Write a chunk to the filesystem.\"\"\"\n        filename = self.get_chunk_filename()\n        self.write_chunk_to_file(self._create_chunk(filename, on_done=on_done), filename)\n        self._chunk_index += 1\n        return os.path.join(self._cache_dir, filename)\n\n    def __setitem__(self, index: int, items: Any) -> None:\n        \"\"\"Store an item to a chunk.\n\n        The index needs to be provided in order.\n\n        This is handled by the samplers automatically. This ensures we can map an index to a shard from an interval.\n\n        \"\"\"\n        self.add_item(index, items)\n\n    def add_item(self, index: int, items: Any) -> Optional[str]:\n        # Track the minimum index provided to the writer\n        # Serialize the items and store an Item object.\n        if index in self._serialized_items:\n            raise ValueError(f\"The provided index {index} already exists in the cache.\")\n\n        data, dim = self.serialize(items)\n        self._serialized_items[index] = Item(\n            index=index,\n            data=data,\n            bytes=len(data),\n            dim=dim,\n        )\n\n        if self._should_write():\n            filepath = os.path.join(self._cache_dir, self.get_chunk_filename())\n            self.write_chunk()\n            self._min_index = None\n            self._max_index = None\n            return filepath\n\n    def _should_write(self) -> bool:\n        # TODO: Misleading method name, it modifies `self._min_index` and `self._max_index`!\n        if not self._serialized_items:\n            return False\n        indexes = list(self._serialized_items.keys())\n        self._min_index = index = indexes[0] if len(indexes) == 1 else min(*indexes)\n        num_bytes = 0\n        num_items = 0\n        while True:\n            item = self._serialized_items.get(index, None)\n            if item:\n                num_bytes += item.bytes\n                num_items += item.dim if item.dim else 1\n                index += 1\n                if (self._chunk_bytes and self._chunk_bytes < num_bytes) or (\n                    self._chunk_size and num_items > self._chunk_size\n                ):\n                    self._max_index = index - 1\n                    return True\n            else:\n                return False\n\n    def write_chunk_to_file(\n        self,\n        raw_data: bytes,\n        filename: str,\n    ) -> None:\n        \"\"\"Write chunk bytes to a file.\"\"\"\n        # Whether to compress the raw bytes\n        if self._compression:\n            raw_data = self._compressor.compress(raw_data)\n\n        # Write the binary chunk file\n        with open(os.path.join(self._cache_dir, filename), \"wb\") as out:\n            out.write(raw_data)\n\n    def write_chunks_index(self) -> str:\n        \"\"\"Write the chunks index to a JSON file.\"\"\"\n        if len(self._chunks_info) == 0:\n            return \"\"\n        filepath = os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\")\n        config = self.get_config()\n        with open(filepath, \"w\") as out:\n            json.dump({\"chunks\": self._chunks_info, \"config\": config}, out, sort_keys=True)\n        return filepath\n\n", "contexts_below": "\n    def merge(self, num_workers: int = 1, node_rank: Optional[int] = None) -> None:\n        \"\"\"Once all the workers have written their own index, the merge function is responsible to read and merge them\n        into a single index.\"\"\"\n        num_workers = num_workers or 1\n\n        # Only for non rank 0\n        if self.rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(0.01)\n            return\n\n        # Wait for all indexes to be available\n        is_done = False\n        while not is_done:\n            files = os.listdir(self._cache_dir)\n\n            # Return if the index already exists\n            if _INDEX_FILENAME in files:\n                return\n\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n            # When using the Data Optimizer, we don't use multi processes.\n            is_done = len(index_files) == self._distributed_env.world_size * num_workers\n            sleep(0.01)\n\n        self._merge_no_wait(node_rank=node_rank)\n\n    def _merge_no_wait(self, node_rank: Optional[int] = None) -> None:\n        \"\"\"Once all the workers have written their own index, the merge function is responsible to read and merge them\n        into a single index.\"\"\"\n        files = os.listdir(self._cache_dir)\n        index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n        chunks_info = []\n        config = None\n        for index_filename in sorted(index_files):\n            chunk_path = os.path.join(self._cache_dir, index_filename)\n            with open(chunk_path) as f:\n                data = json.load(f)\n\n                if config is None:\n                    config = data[\"config\"]\n\n                elif config != data[\"config\"]:\n                    raise Exception(\n                        \"The config isn't consistent between chunks. This shouldn't have happened.\"\n                        f\"Found {config} {data['config']}.\"\n                    )\n\n                chunks_info.extend(data[\"chunks\"])\n\n            os.remove(chunk_path)\n\n        if node_rank is None:\n            with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as f:\n                json.dump({\"chunks\": chunks_info, \"config\": config}, f, sort_keys=True)\n        else:\n            with open(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\"), \"w\") as f:\n                json.dump({\"chunks\": chunks_info, \"config\": config}, f, sort_keys=True)\n\n    def _should_raise(self, data_format_1: List[str], data_format_2: List[str]) -> bool:\n        if len(data_format_1) != len(data_format_2):\n            return True\n\n        def is_non_valid(f1: str, f2: str) -> bool:\n            if f1 in [\"pil\", \"jpeg\"] and f2 in [\"pil\", \"jpeg\"]:\n                return False\n            return f1 != f2\n\n        return any(is_non_valid(f1, f2) for f1, f2 in zip(data_format_1, data_format_2))\n\n    def _pretty_serialized_items(self) -> Dict[int, Item]:\n        out = {}\n        for key, value in self._serialized_items.items():\n            # drop `data` as it would make logs unreadable.\n            out[key] = Item(\n                index=value.index,\n                bytes=value.bytes,\n                dim=value.dim,\n                data=b\"\",\n            )\n        return out\n", "input_code": "    def done(self) -> List[str]:\n\n        \"\"\"\n        The function finalizes the writing process for a BinaryWriter instance. It attempts to write any remaining chunks of data to files and generates an index file for these chunks. It returns a list of file paths to the written chunks. The function marks the writing process as complete once it is done.\n\n        Input-Output Arguments\n        :param self: BinaryWriter. An instance of the BinaryWriter class. It uses internal state to manage the writing process.\n        :return: List[str]. A list of file paths to the chunks that have been written. This list is returned once the writing process is finalized and all chunks are written.\n\n        Note: The function uses internal methods and properties of the BinaryWriter class, such as `_should_write`, `write_chunk`, and `write_chunks_index`, to manage the writing of chunks and the index file. It also checks if the writing process is already marked as complete by inspecting `self.filled` and marks it as complete by setting `self._is_done` to True at the end.\n        \"\"\"", "reference_steps": "1. Initialize an empty list `filepaths` to store the paths of files written.\n2. Check if the `filled` attribute is True, and if so, return the empty `filepaths` list immediately.\n3. Enter a while loop that continues as long as the `_should_write()` method returns True.\n4. Inside the loop, call the `write_chunk()` method and append the returned file path to the `filepaths` list.\n5. After the loop, check if there are any serialized items left in `_serialized_items`.\n6. If there are remaining items, call the `write_chunk()` method with `True` as an argument to write the last chunk and append the returned file path to the list.\n7. Call the `write_chunks_index()` method to write an index file for the chunks.\n8. Set the `_is_done` attribute to True, indicating that the operation is complete.\n9. Return the `filepaths` list containing the paths of all the chunks written.\n10. Note that the actual implementations of `write_chunk()`, `_should_write()`, and `write_chunks_index()` are not provided in the reference code.", "reference_code": "def done(self) -> List[str]:\n    \"\"\"Called when StopIteration is triggered.\"\"\"\n    filepaths: List[str] = []\n    if self.filled:\n        return filepaths\n\n    # Try writing down an chunks\n    while self._should_write():\n        filepaths.append(self.write_chunk())\n\n    # If any elements is left, try writing one last chunk\n    if self._serialized_items:\n        filepaths.append(self.write_chunk(True))\n\n    # Write down the index file\n    self.write_chunks_index()\n\n    self._is_done = True\n    return filepaths\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "type": "method", "class_name": "StreamingDataLoader", "function_name": "load_state_dict", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.dataloader.StreamingDataLoader._latest_worker_idx\n\nlitdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_combined\n\nlitdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_streaming\n\nlitdata.streaming.dataloader.StreamingDataLoader._worker_idx\n\nlitdata.streaming.dataloader.StreamingDataLoader._worker_idx_iter\n\nlitdata.streaming.dataloader.StreamingDataLoader.current_epoch\n\nlitdata.streaming.dataloader.StreamingDataLoader.restore\n\n# Cross-file Dependency:\nlitdata.streaming.combined.CombinedStreamingDataset\n    class CombinedStreamingDataset(IterableDataset):\n        \"\"\"The `CombinedStreamingDataset` enables to stream data from multiple StreamingDataset with the sampling ratio of\n        your choice.\n\n        Addtionally, the `CombinedStreamingDataset` keeps track of the number of samples fetched to enable resumability\n        of the datasets.\n\n        Note that due to the random sampling, the number of samples returned from the iterator is variable and a function\n        of the given seed. The combined dataset will raise a StopIteration as soon as any of the datasets is exhausted.\n\n        \"\"\"\n\nlitdata.streaming.dataset.StreamingDataset\n    class StreamingDataset(IterableDataset):\n        \"\"\"The streaming dataset can be used once your data have been optimised using the DatasetOptimiser class.\"\"\"\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_combined\n\nlitdata.streaming.dataloader.StreamingDataLoader._latest_worker_idx\n\nlitdata.streaming.dataloader.StreamingDataLoader.restore\n\n# Cross-file Dependency:\nlitdata.streaming.combined.CombinedStreamingDataset\n    class CombinedStreamingDataset(IterableDataset):\n        \"\"\"The `CombinedStreamingDataset` enables to stream data from multiple StreamingDataset with the sampling ratio of\n        your choice.\n\n        Addtionally, the `CombinedStreamingDataset` keeps track of the number of samples fetched to enable resumability\n        of the datasets.\n\n        Note that due to the random sampling, the number of samples returned from the iterator is variable and a function\n        of the given seed. The combined dataset will raise a StopIteration as soon as any of the datasets is exhausted.\n\n        \"\"\"\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport inspect\nimport logging\nimport os\nfrom copy import deepcopy\nfrom importlib import reload\nfrom itertools import cycle\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport torch\nfrom torch.utils.data import Dataset, IterableDataset\nfrom torch.utils.data._utils.collate import default_collate\nfrom torch.utils.data._utils.fetch import _BaseDatasetFetcher\nfrom torch.utils.data.dataloader import (\n    DataLoader,\n    _BaseDataLoaderIter,\n    _DatasetKind,\n    _MultiProcessingDataLoaderIter,\n    _SingleProcessDataLoaderIter,\n)\nfrom torch.utils.data.sampler import BatchSampler, Sampler\n\nfrom litdata.constants import _DEFAULT_CHUNK_BYTES, _TORCH_GREATER_EQUAL_2_1_0, _VIZ_TRACKER_AVAILABLE\nfrom litdata.streaming import Cache\nfrom litdata.streaming.combined import (\n    __NUM_SAMPLES_YIELDED_KEY__,\n    __SAMPLES_KEY__,\n    CombinedStreamingDataset,\n)\nfrom litdata.streaming.dataset import StreamingDataset\nfrom litdata.streaming.sampler import CacheBatchSampler\nfrom litdata.utilities.env import _DistributedEnv\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten\n\nlogger = logging.Logger(__name__)\n\n\ndef _equal_items(data_1: Any, data_2: Any) -> bool:\n    data_1_flattened, _ = tree_flatten(data_1)\n    data_2_flattened, _ = tree_flatten(data_2)\n\n    if len(data_1_flattened) != len(data_2_flattened):\n        return False\n\n    return all(_equal_item(d1, d2) for d1, d2 in zip(data_1_flattened, data_2_flattened))\n\n\ndef _equal_item(d1: Any, d2: Any) -> bool:\n    if not isinstance(d1, type(d2)):\n        return False\n    equality = d1 == d2\n    if isinstance(equality, torch.Tensor):\n        return bool(equality.all().item())\n    if equality is True:\n        return True\n    return False\n\n\nclass CacheDataset(Dataset):\n    def __init__(\n        self,\n        dataset: Any,\n        cache_dir: str,\n        chunk_bytes: Optional[int],\n        chunk_size: Optional[int],\n        compression: Optional[str],\n    ):\n        \"\"\"The `CacheDataset` is a dataset wraper to provide a beginner experience with the Cache.\n\n        Arguments:\n            dataset: The dataset of the user\n            cache_dir: The folder where the chunks are written to.\n            chunk_bytes: The maximal number of bytes to write within a chunk.\n            chunk_sie: The maximal number of items to write to a chunk.\n            compression: The compression algorithm to use to reduce the size of the chunk.\n\n        \"\"\"\n        self._dataset = dataset\n        self._cache = Cache(cache_dir, chunk_bytes=chunk_bytes, chunk_size=chunk_size, compression=compression)\n        self._is_deterministic = False\n\n    def __len__(self) -> int:\n        return len(self._cache) if self._cache.filled else len(self._dataset)\n\n    def __getitem__(self, index: int) -> Any:\n        data_1 = self._cache[index] if self._cache.filled else self._dataset[index]\n        if not self._cache.filled:\n            if not self._is_deterministic:\n                data2 = self._dataset[index]\n                if not _equal_items(data_1, data2):\n                    raise ValueError(\n                        f\"Your dataset items aren't deterministic. Found {data_1} and {data2} for index {index}.\"\n                        \" HINT: Use the `litdata.cache.Cache` directly within your dataset.\"\n                    )\n                self._is_deterministic = True\n            self._cache[index] = data_1\n        return data_1\n\n\nclass CacheCollateFn:\n    \"\"\"This CacheCollateFn is used to accelerate the processing of the data generated using the Cache.\n\n    During the chunking phase, there is no need to return any data from the DataLoader reducing some time.\n\n    Additionally, if the user makes their __getitem__ asynchronous, the collate executes them in parallel.\n\n    \"\"\"\n\n    def __init__(self, collate_fn: Optional[Callable] = None) -> None:\n        self.collate_fn = collate_fn or default_collate\n\n    def __call__(self, items: List[Any]) -> Any:\n        if all(item is None for item in items):\n            return None\n\n        # If the __getitem__ method is asynchornous, collect all the items.\n        if all(inspect.iscoroutine(item) for item in items):\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            items = loop.run_until_complete(asyncio.gather(*items))\n\n        return self.collate_fn([item for item in items if item is not None])\n\n\nclass _SingleProcessDataLoaderIterPatch(_SingleProcessDataLoaderIter):\n    \"\"\"This is overriden to inform the cache is done chunking.\"\"\"\n\n    def _next_data(self) -> Any:\n        try:\n            data = None\n            while data is None:\n                data = super()._next_data()\n            return data\n        except StopIteration:\n            for v in self._dataset_fetcher.dataset.__dict__.values():\n                if isinstance(v, Cache):\n                    v.done()\n                    if not v.filled:\n                        v.merge(1)\n            raise StopIteration()\n\n\nclass WorkerLoop:\n    \"\"\"Wrap the PyTorch DataLoader WorkerLoop to perform caching and profiling.\"\"\"\n\n    def __init__(self, global_rank: int, profile: bool = False) -> None:\n        self._global_rank = global_rank\n        self._profile = profile\n\n    def __call__(\n        self,\n        dataset_kind: Any,\n        dataset: Any,\n        index_queue: Any,\n        data_queue: Any,\n        done_event: Any,\n        auto_collation: Any,\n        collate_fn: Any,\n        drop_last: Any,\n        base_seed: Any,\n        init_fn: Any,\n        worker_id: Any,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        from torch.utils.data._utils import worker\n\n        from litdata.streaming.cache import Cache\n\n        enable_profiling = self._global_rank == 0 and worker_id == 0 and _VIZ_TRACKER_AVAILABLE and self._profile\n\n        if enable_profiling:\n            from viztracer import VizTracer\n\n            tracer = VizTracer(output_file=os.path.join(os.getcwd(), \"trace.json\"))\n            tracer.start()\n\n        # Reload to remove the patching\n        reloaded_worker = reload(worker)\n        create_fetcher = _DatasetKind.create_fetcher\n        fetcher = None\n\n        def create_fetcher_fn(*args: Any, **kwargs: Any) -> \"_BaseDatasetFetcher\":\n            nonlocal fetcher\n            fetcher = create_fetcher(*args, **kwargs)\n            return fetcher\n\n        _DatasetKind.create_fetcher = create_fetcher_fn  # type: ignore\n\n        reloaded_worker._worker_loop(\n            dataset_kind,\n            dataset,\n            index_queue,\n            data_queue,\n            done_event,\n            auto_collation,\n            collate_fn,\n            drop_last,\n            base_seed,\n            init_fn,\n            worker_id,\n            *args,\n            **kwargs,\n        )\n\n        if dataset_kind == _DatasetKind.Map:\n            assert fetcher\n            for v in fetcher.dataset.__dict__.values():\n                if isinstance(v, Cache):\n                    v.done()\n\n        if enable_profiling:\n            tracer.stop()\n            tracer.save()\n\n\nclass _MultiProcessingDataLoaderIterPatch(_MultiProcessingDataLoaderIter):\n    def __init__(self, loader: DataLoader) -> None:\n        self._cache = loader._cache\n        self._num_workers = loader.num_workers\n        # Patch PyTorch worker loop to call the `cache.done()` method.\n        from torch.utils.data._utils import worker\n\n        worker._worker_loop = WorkerLoop(loader._global_rank, loader._profile)\n        super().__init__(loader)\n\n    def _shutdown_workers(self) -> None:\n        super()._shutdown_workers()\n\n        # If the data isn't filled, we trigger an indedm merge\n        if not self._cache.filled:\n            self._cache.merge(self._num_workers)\n\n    def _next_data(self) -> Any:\n        try:\n            data = None\n            while data is None:\n                data = super()._next_data()\n            return data\n        except StopIteration as e:\n            raise e\n\n\nclass CacheDataLoader(DataLoader):\n    __doc__ = DataLoader.__doc__\n\n    def __init__(\n        self,\n        dataset: Any,\n        *args: Any,\n        sampler: Optional[Sampler] = None,\n        batch_sampler: Optional[BatchSampler] = None,\n        num_workers: int = 0,\n        shuffle: bool = False,\n        generator: Optional[torch.Generator] = None,\n        batch_size: Optional[int] = None,\n        drop_last: bool = False,\n        cache_dir: Optional[str] = None,\n        chunk_bytes: Optional[int] = _DEFAULT_CHUNK_BYTES,\n        compression: Optional[str] = None,\n        profile: bool = False,\n        collate_fn: Optional[Callable] = None,\n        **kwargs: Any,\n    ) -> None:\n        if sampler:\n            raise ValueError(\n                \"The CacheDataLoader relies on its own internal sampler. Passing a sampler isn't supported.\"\n            )\n\n        if batch_sampler:\n            raise ValueError(\n                \"The CacheDataLoader relies on its own internal sampler. Passing a batch_sampler isn't supported.\"\n            )\n\n        if isinstance(dataset, IterableDataset):\n            raise ValueError(\"Only map-based dataset are supported by the CacheDataLoader for now.\")\n\n        if profile and not _VIZ_TRACKER_AVAILABLE:\n            raise ModuleNotFoundError(\"To enable DataLoader profiling, run `pip install viztracer`.\")\n\n        cache_list = [v for v in dataset.__dict__.values() if isinstance(v, Cache)]\n\n        if len(cache_list) > 1:\n            raise ValueError(\n                \"We found several Cache used as attributes from your dataset. Only one is support for now.\"\n            )\n\n        if len(cache_list) == 0:\n            if cache_dir is None:\n                raise ValueError(\"You should provide a `cache_dir` filepath to the CacheDataLoader.\")\n\n            dataset = CacheDataset(dataset, cache_dir, chunk_bytes, batch_size, compression)\n            cache = dataset._cache\n        else:\n            cache = cache_list[0]\n\n        if not cache.filled and shuffle:\n            logger.info(\"Shuffle is ignored during the caching phase phase.\")\n\n        self._cache = cache\n\n        distributed_env = _DistributedEnv.detect()\n        self._global_rank = distributed_env.global_rank\n\n        batch_sampler = CacheBatchSampler(\n            len(dataset),\n            distributed_env.world_size,\n            self._global_rank,\n            num_workers,\n            batch_size or 1,\n            drop_last,\n            shuffle,\n            cache,\n        )\n\n        self._profile = profile\n\n        super().__init__(\n            dataset,\n            *args,\n            batch_sampler=batch_sampler,  # type: ignore\n            collate_fn=CacheCollateFn(collate_fn),\n            num_workers=num_workers,\n            **kwargs,\n        )\n\n    def _get_iterator(self) -> \"_BaseDataLoaderIter\":\n        \"\"\"Overriden to ensure the `Cache.done()` method is triggered on iteration done.\"\"\"\n        if self.num_workers == 0:\n            return _SingleProcessDataLoaderIterPatch(self)\n        self.check_worker_number_rationality()\n        return _MultiProcessingDataLoaderIterPatch(self)\n\n\ndef _wrapper(fetcher: Any, func: Callable, tracer: Any, profile: int, profile_dir: str) -> Callable:\n    counter = 0\n\n    def wrap(*args: Any, **kwargs: Any) -> Any:\n        nonlocal counter\n        result = func(*args, **kwargs)\n\n        if tracer.enable and counter == profile:\n            tracer.stop()\n            tracer.save()\n            print(\n                f\"Saved {os.path.join(profile_dir, 'result.json')} file after {profile} batches.\"\n                \"Use chrome://tracing/ to view it.\"\n            )\n            fetcher.fetch = func\n\n        counter += 1\n        return result\n\n    return wrap\n\n\nclass _ProfileWorkerLoop:\n    \"\"\"Wrap the PyTorch DataLoader WorkerLoop to add profiling.\"\"\"\n\n    def __init__(self, profile: Union[int, bool], profile_dir: Optional[str] = None):\n        self._profile = profile\n        self._profile_dir = profile_dir if profile_dir else os.getcwd()\n\n    def __call__(\n        self,\n        dataset_kind: Any,\n        dataset: Any,\n        index_queue: Any,\n        data_queue: Any,\n        done_event: Any,\n        auto_collation: Any,\n        collate_fn: Any,\n        drop_last: Any,\n        base_seed: Any,\n        init_fn: Any,\n        worker_id: Any,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        from torch.utils.data._utils import worker\n        from viztracer import VizTracer\n\n        if worker_id == 0:\n            output_file = os.path.join(self._profile_dir, \"result.json\")\n\n            if os.path.exists(output_file):\n                os.remove(output_file)\n\n            tracer = VizTracer(output_file=output_file, verbose=0)\n            tracer.start()\n\n        # Reload to remove the patching\n        reloaded_worker = reload(worker)\n        create_fetcher = _DatasetKind.create_fetcher\n        fetcher = None\n\n        def create_fetcher_fn(*args: Any, **kwargs: Any) -> \"_BaseDatasetFetcher\":\n            nonlocal fetcher\n            fetcher = create_fetcher(*args, **kwargs)\n\n            if worker_id == 0 and isinstance(self._profile, int):\n                fetcher.fetch = _wrapper(fetcher, fetcher.fetch, tracer, self._profile, self._profile_dir)\n            return fetcher\n\n        _DatasetKind.create_fetcher = create_fetcher_fn  # type: ignore\n\n        reloaded_worker._worker_loop(\n            dataset_kind,\n            dataset,\n            index_queue,\n            data_queue,\n            done_event,\n            auto_collation,\n            collate_fn,\n            drop_last,\n            base_seed,\n            init_fn,\n            worker_id,\n            *args,\n            **kwargs,\n        )\n\n        if worker_id == 0 and isinstance(self._profile, bool):\n            tracer.stop()\n            tracer.save()\n\n\nclass _StreamingMultiProcessingDataLoaderIter(_MultiProcessingDataLoaderIter):\n    def __init__(self, loader: DataLoader) -> None:\n        self._loader = loader\n        self._indexes = (\n            list(range(self._loader._latest_worker_idx, self._loader.num_workers))\n            if self._loader._latest_worker_idx > 0\n            else []\n        )\n        self._num_workers = loader.num_workers\n\n        distributed_env = _DistributedEnv.detect()\n\n        if self._loader._profile_batches and distributed_env.global_rank == 0 and _VIZ_TRACKER_AVAILABLE:\n            from torch.utils.data._utils import worker\n\n            worker._worker_loop = _ProfileWorkerLoop(self._loader._profile_batches, self._loader._profile_dir)\n\n        super().__init__(loader)\n\n    def _try_put_index(self) -> None:\n        # Used to restart on the right DataLoader worker\n        if self._loader.restore and self._indexes:\n            assert self._tasks_outstanding < self._prefetch_factor * self._num_workers\n\n            try:\n                index = self._next_index()\n            except StopIteration:\n                return\n            worker_queue_idx = self._indexes.pop(0)\n\n            self._index_queues[worker_queue_idx].put((self._send_idx, index))\n            self._task_info[self._send_idx] = (worker_queue_idx,)\n            self._tasks_outstanding += 1\n            self._send_idx += 1\n        else:\n            super()._try_put_index()\n\n\nclass StreamingDataLoader(DataLoader):\n    r\"\"\"The StreamingDataLoader combines a dataset and a sampler, and provides an iterable over the given dataset.\n\n    The :class:`~litdata.streaming.dataloader.StreamingDataLoader` supports either a\n    StreamingDataset and CombinedStreamingDataset datasets with single- or multi-process loading,\n    customizing\n    loading order and optional automatic batching (collation) and memory pinning.\n\n    See :py:mod:`torch.utils.data` documentation page for more details.\n\n    Args:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: ``1``).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: ``False``).\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. ``0`` means that the data will be loaded in the main process.\n            (default: ``0``)\n        collate_fn (Callable, optional): merges a list of samples to form a\n            mini-batch of Tensor(s).  Used when using batched loading from a\n            map-style dataset.\n        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n            into device/CUDA pinned memory before returning them.  If your data elements\n            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n            see the example below.\n        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative. (default: ``0``)\n        worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n            input, after seeding and before data loading. (default: ``None``)\n        multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n            ``None``, the default `multiprocessing context`_ of your operating system will\n            be used. (default: ``None``)\n        generator (torch.Generator, optional): If not ``None``, this RNG will be used\n            by RandomSampler to generate random indexes and multiprocessing to generate\n            ``base_seed`` for workers. (default: ``None``)\n        prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n            in advance by each worker. ``2`` means there will be a total of\n            2 * num_workers batches prefetched across all workers. (default value depends\n            on the set value for num_workers. If value of num_workers=0 default is ``None``.\n            Otherwise, if value of ``num_workers > 0`` default is ``2``).\n        persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n            the worker processes after a dataset has been consumed once. This allows to\n            maintain the workers `Dataset` instances alive. (default: ``False``)\n        pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n            ``True``.\n        profile_batches (int, bool, optional): Whether to record data loading profile and generate a result.json file.\n        profile_dir (int, bool,  optional): Where to store the recorded trace when profile_batches is enabled.\n\n    \"\"\"\n\n    __doc__ = DataLoader.__doc__\n\n    def __init__(\n        self,\n        dataset: Union[StreamingDataset, CombinedStreamingDataset],\n        *args: Any,\n        batch_size: int = 1,\n        num_workers: int = 0,\n        profile_batches: Union[bool, int] = False,\n        profile_dir: Optional[str] = None,\n        prefetch_factor: Optional[int] = None,\n        shuffle: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> None:  # pyright: ignore\n        if not isinstance(dataset, (StreamingDataset, CombinedStreamingDataset)):\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {dataset}.\"\n            )\n\n        if shuffle is not None:\n            dataset.set_shuffle(shuffle)\n\n        shuffle = None\n\n        if profile_batches and not _VIZ_TRACKER_AVAILABLE:\n            raise ModuleNotFoundError(\"To use profile_batches, viztracer is required. Run `pip install viztracer`\")\n\n        if profile_batches and num_workers == 0:\n            raise ValueError(\"Profiling is supported only with num_workers >= 1.\")\n\n        self.current_epoch = 0\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self._profile_batches = profile_batches\n        self._profile_dir = profile_dir\n        self._num_samples_yielded_streaming = 0\n        self._num_samples_yielded_combined: Dict[int, List[Any]] = {}\n        self.rng_state: Optional[Any] = None\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter: Optional[Any] = None\n        self._latest_worker_idx = 0\n        self.restore = False\n        super().__init__(\n            dataset,\n            *args,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            prefetch_factor=(10 if num_workers > 0 else None) if prefetch_factor is None else prefetch_factor,\n            **kwargs,\n        )  # type: ignore\n\n    def __iter__(self) -> Any:\n        if not self.restore:\n            self._latest_worker_idx = 0\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.current_epoch += 1\n            self._num_samples_yielded_combined = {}\n            self._num_samples_yielded_streaming = 0\n\n        self.dataset.set_epoch(self.current_epoch)\n\n        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            for batch in super().__iter__():\n                self._latest_worker_idx = next(self._worker_idx_iter)  # type: ignore\n                self._num_samples_yielded_streaming += self.batch_size\n                yield batch\n        else:\n            self.dataset._set_use_streaming_dataloader(True)\n            assert self.batch_size\n            # TODO: Inject a custom collate function to avoid collating the __NUM_SAMPLES_YIELDED__ key\n            for batch in super().__iter__():\n                self._latest_worker_idx = next(self._worker_idx_iter)  # type: ignore\n                if isinstance(batch, dict) and __NUM_SAMPLES_YIELDED_KEY__ in batch:\n                    self._num_samples_yielded_combined[self._latest_worker_idx] = [\n                        sample[-1].item() if self.batch_size > 1 else sample.item()\n                        for sample in batch[__NUM_SAMPLES_YIELDED_KEY__]\n                    ]\n\n                    yield batch[__SAMPLES_KEY__]\n                else:\n                    yield batch\n\n        self.restore = False\n\n    def state_dict(self) -> Dict[str, Any]:\n        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            return {\n                \"dataset\": self.dataset.state_dict(\n                    self._num_samples_yielded_streaming, self.num_workers, self.batch_size\n                ),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        num_samples_yieled = [0 for _ in range(len(list(self._num_samples_yielded_combined.values())[0]))]\n        for worker_idx in self._num_samples_yielded_combined:\n            for dataset_idx, samples_yieled in enumerate(self._num_samples_yielded_combined[worker_idx]):\n                num_samples_yieled[dataset_idx] += samples_yieled\n\n        return {\n            \"dataset\": self.dataset.state_dict(self.num_workers, self.batch_size, num_samples_yieled),\n            \"current_epoch\": self.current_epoch if self.restore else self.current_epoch - 1,\n            \"latest_worker_idx\": self._latest_worker_idx,\n            \"num_samples_yielded\": deepcopy(self._num_samples_yielded_combined),\n        }\n\n", "contexts_below": "\n    def _get_iterator(self) -> \"_BaseDataLoaderIter\":\n        \"\"\"Overriden to ensure the `Cache.done()` method is triggered on iteration done.\"\"\"\n        if self.num_workers == 0:\n            return _SingleProcessDataLoaderIter(self)\n        self.check_worker_number_rationality()\n        return _StreamingMultiProcessingDataLoaderIter(self)\n", "input_code": "    def load_state_dict(self, obj: Dict[str, Any]) -> None:\n\n        \"\"\"\n        This function loads a dictionary containing the training state into the StreamingDataLoader instance. It is designed to be called from a non-worker process when resuming training. It updates the current epoch, the number of samples yielded, and the latest worker index based on the provided state. Additionally, it prepares the DataLoader for resuming by adjusting internal iterators and flags.\n\n        Input-Output Arguments\n        :param self: StreamingDataLoader. An instance of the StreamingDataLoader class.\n        :param obj: Dict[str, Any]. A dictionary containing the training state, including the current epoch, the number of samples yielded, and the latest worker index. It is used to update the state of the StreamingDataLoader and its dataset if applicable.\n        :return: No return values. This method updates the state of the StreamingDataLoader instance in place.\n\n        This method also handles specific logic based on the type of dataset associated with the StreamingDataLoader, updating the dataset's state if it is a StreamingDataset or a CombinedStreamingDataset. It raises a RuntimeError if the dataset is neither.\n        \"\"\"", "reference_steps": "1. Define a method `load_state_dict` that takes a dictionary `obj` representing the training state and loads it into the current object.\n\n2. Set the `current_epoch` attribute of the object to the value from the `obj` dictionary using the key `\"current_epoch\"`.\n\n3. Check if the `dataset` attribute of the object is an instance of `StreamingDataset`.\n\n4. If the dataset is a `StreamingDataset`, set the `_num_samples_yielded_streaming` attribute to the value from the `obj` dictionary using the key `\"num_samples_yielded\"`.\n\n5. If the dataset is not a `StreamingDataset`, set the `_num_samples_yielded_combined` attribute to the value from the `obj` dictionary using the key `\"num_samples_yielded\"`.\n\n6. Update the `_latest_worker_idx` attribute by incrementing the value from the `obj` dictionary using the key `\"latest_worker_idx\"` by 1.\n\n7. Initialize an iterator `_worker_idx_iter` from the `_worker_idx` attribute of the object.\n\n8. Iterate through the `_worker_idx_iter` the number of times equal to `_latest_worker_idx` to reach the state of the last worker index.\n\n9. Set the `restore` attribute to `True` to indicate that the state is being restored and to disable resetting the `StreamingDataLoader` state.\n\n10. Check if the dataset is an instance of `CombinedStreamingDataset` or `StreamingDataset` and call the `load_state_dict` method on the dataset with the appropriate part of the `obj` dictionary. If the dataset is neither, raise a `RuntimeError`.", "reference_code": "def load_state_dict(self, obj: Dict[str, Any]) -> None:\n    \"\"\"Load a dict containing training state (called from non-worker process).\n\n    This is called on each copy of the dataset when resuming.\n\n    Args:\n        obj (Any): The state.\n\n    \"\"\"\n    self.current_epoch = obj[\"current_epoch\"]\n\n    if isinstance(self.dataset, StreamingDataset):\n        self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n    else:\n        self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n\n    # Used to restart on the next DataLoader worker from the previous run.\n    self._latest_worker_idx = obj[\"latest_worker_idx\"] + 1\n    self._worker_idx_iter = iter(self._worker_idx)\n    for _ in range(self._latest_worker_idx):\n        next(self._worker_idx_iter)\n\n    # Inform we are resuming and disable resetting the StreamingDataLoader state.\n    # This is toggle back to False when the `__iter__` method of the StreamingDataLoader completes.\n    self.restore = True\n\n    if isinstance(self.dataset, CombinedStreamingDataset):\n        self.dataset._set_use_streaming_dataloader(True)\n        self.dataset.load_state_dict(obj)\n    elif isinstance(self.dataset, StreamingDataset):\n        self.dataset.load_state_dict(obj[\"dataset\"])\n    else:\n        raise RuntimeError(\"The provided dataset should be a `StreamingDataset` or a `CombinedStreamingDataset`.\")\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "type": "method", "class_name": "CombinedStreamingDataset", "function_name": "state_dict", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.combined.CombinedStreamingDataset._datasets\n\nlitdata.streaming.combined.CombinedStreamingDataset._iterator\n\n# Intra-file Dependency:\nlitdata.streaming.combined._CombinedDatasetIterator.state_dict\n    def state_dict(\n        self, num_workers: int, batch_size: int, num_samples_yielded: Optional[List[int]] = None\n    ) -> Dict[str, Any]:\n\nlitdata.streaming.combined._state_dict\n    def _state_dict(\n        datasets: List[StreamingDataset], num_samples_yielded: List[int], num_workers: int = 0, batch_size: int = 1\n    ) -> Dict[str, Any]:\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.combined.CombinedStreamingDataset._iterator\n\n# Intra-file Dependency:\nlitdata.streaming.combined._CombinedDatasetIterator.state_dict\n    def state_dict(\n        self, num_workers: int, batch_size: int, num_samples_yielded: Optional[List[int]] = None\n    ) -> Dict[str, Any]:\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nfrom typing import Any, Dict, Iterator, List, Optional, Sequence\n\nfrom torch.utils.data import IterableDataset\n\nfrom litdata.streaming.dataset import StreamingDataset\nfrom litdata.utilities.env import _WorkerEnv\n\n__NUM_SAMPLES_YIELDED_KEY__ = \"__NUM_SAMPLES_YIELDED__\"\n__SAMPLES_KEY__ = \"__SAMPLES__\"\n\n\nclass CombinedStreamingDataset(IterableDataset):\n    \"\"\"The `CombinedStreamingDataset` enables to stream data from multiple StreamingDataset with the sampling ratio of\n    your choice.\n\n    Addtionally, the `CombinedStreamingDataset` keeps track of the number of samples fetched to enable resumability\n    of the datasets.\n\n    Note that due to the random sampling, the number of samples returned from the iterator is variable and a function\n    of the given seed. The combined dataset will raise a StopIteration as soon as any of the datasets is exhausted.\n\n    \"\"\"\n\n    def __init__(\n        self, datasets: List[StreamingDataset], seed: int = 42, weights: Optional[Sequence[float]] = None\n    ) -> None:\n        self._check_datasets(datasets)\n\n        self._seed = seed\n        self._datasets = datasets\n        self._weights = weights\n        num_datasets = len(datasets)\n\n        if weights is None:\n            # Inversely weighted based on length\n            self._weights = [1 / float(num_datasets)] * num_datasets\n        else:\n            self._weights = [w / sum(weights) for w in weights]\n\n        self._iterator: Optional[_CombinedDatasetIterator] = None\n        self._use_streaming_dataloader = False\n        self._num_samples_yielded: Optional[List[int]] = None\n        self._current_epoch = 0\n\n    def set_epoch(self, current_epoch: int) -> None:\n        \"\"\"Set the current epoch to the datasets on epoch starts.\n\n        When using the StreamingDataLoader, this is done automatically\n\n        \"\"\"\n        self._current_epoch = current_epoch\n        for dataset in self._datasets:\n            dataset.set_epoch(current_epoch)\n\n    def set_shuffle(self, shuffle: bool) -> None:\n        \"\"\"Set the current shuffle to the datasets.\"\"\"\n        for dataset in self._datasets:\n            dataset.set_shuffle(shuffle)\n\n    def _check_datasets(self, datasets: List[StreamingDataset]) -> None:\n        if any(not isinstance(d, StreamingDataset) for d in datasets):\n            raise RuntimeError(\"The provided datasets should be instances of the StreamingDataset.\")\n\n    def _set_use_streaming_dataloader(self, use_streaming_dataloader: bool) -> None:\n        # Used to prevent returning num_samples_yielded when using PyTorch DataLoader\n        self._use_streaming_dataloader = use_streaming_dataloader\n\n    def __iter__(self) -> Iterator[Any]:\n        assert self._weights\n\n        worker_env = _WorkerEnv.detect()\n\n        num_samples_yielded = None\n\n        if self._num_samples_yielded is not None and worker_env.rank in self._num_samples_yielded:\n            num_samples_yielded = self._num_samples_yielded[worker_env.rank]\n\n        self._iterator = _CombinedDatasetIterator(\n            self._datasets,\n            self._seed,\n            self._weights,\n            self._use_streaming_dataloader,\n            num_samples_yielded,\n        )\n        return self._iterator\n\n", "contexts_below": "\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        if not state_dict:\n            return\n\n        if len(state_dict[\"dataset\"]) != len(self._datasets):\n            raise RuntimeError(f\"The provided state doesn't match the current number of datasets: {self._datasets}.\")\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            if str(dataset_idx) not in state_dict[\"dataset\"]:\n                raise RuntimeError(f\"The provided state doesn't contain the index {dataset_idx}.\")\n\n            dataset.load_state_dict(state_dict[\"dataset\"][str(dataset_idx)])\n\n        # Used to iterate over the sampler to avoid sampling the same samples\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[\"num_samples_yielded\"]\n\n\nclass _CombinedDatasetIterator(Iterator):\n    def __init__(\n        self,\n        datasets: List[StreamingDataset],\n        seed: int,\n        weights: Sequence[float],\n        use_streaming_dataloader: bool,\n        num_samples_yielded: Optional[Any] = None,\n    ) -> None:\n        self._datasets = datasets\n        self._dataset_iters = [iter(dataset) for dataset in datasets]\n        self._dataset_indexes = list(range(len(datasets)))\n        self._num_samples_yielded = [0 for _ in range(len(datasets))]\n        self._weights = weights\n        self._rng = random.Random(seed)\n\n        if num_samples_yielded is not None:\n            self._num_samples_yielded = num_samples_yielded\n            for _ in range(sum(num_samples_yielded)):\n                self._rng.choices(self._dataset_indexes, weights=self._weights, k=1)\n\n        self._use_streaming_dataloader = use_streaming_dataloader\n\n    def __next__(self) -> Any:\n        # randomly select a dataset index\n        (dataset_index,) = self._rng.choices(self._dataset_indexes, weights=self._weights, k=1)\n\n        # keep track the sample was fetched\n        self._num_samples_yielded[dataset_index] += 1\n\n        sample = next(self._dataset_iters[dataset_index])\n\n        # return a new sample\n        if self._use_streaming_dataloader:\n            return {\n                __SAMPLES_KEY__: sample,\n                __NUM_SAMPLES_YIELDED_KEY__: self._num_samples_yielded,\n            }\n        return sample\n\n    def state_dict(self, num_workers: int = 0, batch_size: int = 1) -> Dict[str, Any]:\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)\n\n\ndef _state_dict(\n    datasets: List[StreamingDataset], num_samples_yielded: List[int], num_workers: int = 0, batch_size: int = 1\n) -> Dict[str, Any]:\n    return {\n        str(dataset_idx): dataset.state_dict(\n            num_samples_yielded=num_samples_yielded[dataset_idx], num_workers=num_workers, batch_size=batch_size\n        )\n        for dataset_idx, dataset in enumerate(datasets)\n    }\n", "input_code": "    def state_dict(\n        self, num_workers: int, batch_size: int, num_samples_yielded: Optional[List[int]] = None\n    ) -> Dict[str, Any]:\n\n        \"\"\"\n        The function `state_dict` generates a state dictionary for the CombinedStreamingDataset instance. It returns an empty dictionary if the internal iterator is None and `num_samples_yielded` is not provided. Otherwise, it either returns a state dictionary created from the internal datasets or directly from the internal iterator's state dictionary, depending on whether the iterator is None.\n\n        Input-Output Arguments\n        :param self: CombinedStreamingDataset. An instance of the CombinedStreamingDataset class.\n        :param num_workers: int, The number of workers used in the dataset streaming process.\n        :param batch_size: int, The size of the batch used in the dataset streaming process.\n        :param num_samples_yielded: Optional[List[int]], A list of integers representing the number of samples yielded. This parameter is optional and defaults to None.\n        :return: Dict[str, Any], A dictionary representing the state of the CombinedStreamingDataset instance. The dictionary is empty if the internal iterator is None and `num_samples_yielded` is not provided. Otherwise, it contains the state information relevant to the dataset streaming process.\n        \"\"\"", "reference_steps": "1. Define a function `state_dict` that takes four parameters: `self`, `num_workers`, `batch_size`, and an optional `num_samples_yielded` which defaults to `None`.\n2. Check if the instance variable `_iterator` is `None`.\n3. If `_iterator` is `None` and `num_samples_yielded` is also `None`, return an empty dictionary.\n4. If `_iterator` is `None` but `num_samples_yielded` is not `None`, call a helper function `_state_dict` with the instance variable `_datasets`, `num_samples_yielded`, `num_workers`, and `batch_size` as arguments, and return its result.\n5. If `_iterator` is not `None`, call the `state_dict` method on `_iterator` with `num_workers` and `batch_size` as arguments, and return its result.", "reference_code": "def state_dict(\n    self, num_workers: int, batch_size: int, num_samples_yielded: Optional[List[int]] = None\n) -> Dict[str, Any]:\n    if self._iterator is None:\n        if num_samples_yielded is None:\n            return {}\n        return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n    return self._iterator.state_dict(num_workers, batch_size)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "type": "method", "class_name": "CombinedStreamingDataset", "function_name": "load_state_dict", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.combined.CombinedStreamingDataset._datasets\n\nlitdata.streaming.combined.CombinedStreamingDataset._num_samples_yielded\n\nlitdata.streaming.combined.CombinedStreamingDataset._use_streaming_dataloader\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.combined.CombinedStreamingDataset._datasets\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nfrom typing import Any, Dict, Iterator, List, Optional, Sequence\n\nfrom torch.utils.data import IterableDataset\n\nfrom litdata.streaming.dataset import StreamingDataset\nfrom litdata.utilities.env import _WorkerEnv\n\n__NUM_SAMPLES_YIELDED_KEY__ = \"__NUM_SAMPLES_YIELDED__\"\n__SAMPLES_KEY__ = \"__SAMPLES__\"\n\n\nclass CombinedStreamingDataset(IterableDataset):\n    \"\"\"The `CombinedStreamingDataset` enables to stream data from multiple StreamingDataset with the sampling ratio of\n    your choice.\n\n    Addtionally, the `CombinedStreamingDataset` keeps track of the number of samples fetched to enable resumability\n    of the datasets.\n\n    Note that due to the random sampling, the number of samples returned from the iterator is variable and a function\n    of the given seed. The combined dataset will raise a StopIteration as soon as any of the datasets is exhausted.\n\n    \"\"\"\n\n    def __init__(\n        self, datasets: List[StreamingDataset], seed: int = 42, weights: Optional[Sequence[float]] = None\n    ) -> None:\n        self._check_datasets(datasets)\n\n        self._seed = seed\n        self._datasets = datasets\n        self._weights = weights\n        num_datasets = len(datasets)\n\n        if weights is None:\n            # Inversely weighted based on length\n            self._weights = [1 / float(num_datasets)] * num_datasets\n        else:\n            self._weights = [w / sum(weights) for w in weights]\n\n        self._iterator: Optional[_CombinedDatasetIterator] = None\n        self._use_streaming_dataloader = False\n        self._num_samples_yielded: Optional[List[int]] = None\n        self._current_epoch = 0\n\n    def set_epoch(self, current_epoch: int) -> None:\n        \"\"\"Set the current epoch to the datasets on epoch starts.\n\n        When using the StreamingDataLoader, this is done automatically\n\n        \"\"\"\n        self._current_epoch = current_epoch\n        for dataset in self._datasets:\n            dataset.set_epoch(current_epoch)\n\n    def set_shuffle(self, shuffle: bool) -> None:\n        \"\"\"Set the current shuffle to the datasets.\"\"\"\n        for dataset in self._datasets:\n            dataset.set_shuffle(shuffle)\n\n    def _check_datasets(self, datasets: List[StreamingDataset]) -> None:\n        if any(not isinstance(d, StreamingDataset) for d in datasets):\n            raise RuntimeError(\"The provided datasets should be instances of the StreamingDataset.\")\n\n    def _set_use_streaming_dataloader(self, use_streaming_dataloader: bool) -> None:\n        # Used to prevent returning num_samples_yielded when using PyTorch DataLoader\n        self._use_streaming_dataloader = use_streaming_dataloader\n\n    def __iter__(self) -> Iterator[Any]:\n        assert self._weights\n\n        worker_env = _WorkerEnv.detect()\n\n        num_samples_yielded = None\n\n        if self._num_samples_yielded is not None and worker_env.rank in self._num_samples_yielded:\n            num_samples_yielded = self._num_samples_yielded[worker_env.rank]\n\n        self._iterator = _CombinedDatasetIterator(\n            self._datasets,\n            self._seed,\n            self._weights,\n            self._use_streaming_dataloader,\n            num_samples_yielded,\n        )\n        return self._iterator\n\n    def state_dict(\n        self, num_workers: int, batch_size: int, num_samples_yielded: Optional[List[int]] = None\n    ) -> Dict[str, Any]:\n        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n        return self._iterator.state_dict(num_workers, batch_size)\n\n", "contexts_below": "\n\nclass _CombinedDatasetIterator(Iterator):\n    def __init__(\n        self,\n        datasets: List[StreamingDataset],\n        seed: int,\n        weights: Sequence[float],\n        use_streaming_dataloader: bool,\n        num_samples_yielded: Optional[Any] = None,\n    ) -> None:\n        self._datasets = datasets\n        self._dataset_iters = [iter(dataset) for dataset in datasets]\n        self._dataset_indexes = list(range(len(datasets)))\n        self._num_samples_yielded = [0 for _ in range(len(datasets))]\n        self._weights = weights\n        self._rng = random.Random(seed)\n\n        if num_samples_yielded is not None:\n            self._num_samples_yielded = num_samples_yielded\n            for _ in range(sum(num_samples_yielded)):\n                self._rng.choices(self._dataset_indexes, weights=self._weights, k=1)\n\n        self._use_streaming_dataloader = use_streaming_dataloader\n\n    def __next__(self) -> Any:\n        # randomly select a dataset index\n        (dataset_index,) = self._rng.choices(self._dataset_indexes, weights=self._weights, k=1)\n\n        # keep track the sample was fetched\n        self._num_samples_yielded[dataset_index] += 1\n\n        sample = next(self._dataset_iters[dataset_index])\n\n        # return a new sample\n        if self._use_streaming_dataloader:\n            return {\n                __SAMPLES_KEY__: sample,\n                __NUM_SAMPLES_YIELDED_KEY__: self._num_samples_yielded,\n            }\n        return sample\n\n    def state_dict(self, num_workers: int = 0, batch_size: int = 1) -> Dict[str, Any]:\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)\n\n\ndef _state_dict(\n    datasets: List[StreamingDataset], num_samples_yielded: List[int], num_workers: int = 0, batch_size: int = 1\n) -> Dict[str, Any]:\n    return {\n        str(dataset_idx): dataset.state_dict(\n            num_samples_yielded=num_samples_yielded[dataset_idx], num_workers=num_workers, batch_size=batch_size\n        )\n        for dataset_idx, dataset in enumerate(datasets)\n    }\n", "input_code": "    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n\n        \"\"\"\n        This function loads the state of a CombinedStreamingDataset instance from a given state dictionary. It updates the state of each dataset within the CombinedStreamingDataset and, if applicable, updates the number of samples yielded by the streaming dataloader to avoid repeating samples.\n\n        Input-Output Arguments\n        :param self: CombinedStreamingDataset. An instance of the CombinedStreamingDataset class.\n        :param state_dict: Dict[str, Any]. A dictionary containing the state information for the CombinedStreamingDataset and its constituent datasets. It is used to restore the state of the CombinedStreamingDataset and its datasets.\n        :return: None. There are no return values.\n\n        \"\"\"", "reference_steps": "1. Define a function `load_state_dict` that takes a dictionary `state_dict` as its argument, which is meant to contain the state information for loading into the current object.\n\n2. Check if the `state_dict` is empty or None, and if so, immediately return from the function without performing any action.\n\n3. Verify that the number of datasets in `state_dict[\"dataset\"]` matches the number of datasets in the current object's `_datasets` attribute. If not, raise a `RuntimeError` with an appropriate message.\n\n4. Iterate over each dataset in the current object's `_datasets` using its index.\n\n5. Check if the current dataset index (as a string) exists in the `state_dict[\"dataset\"]`. If the index is missing, raise a `RuntimeError` with an appropriate message indicating the missing index.\n\n6. For each dataset in the `_datasets` list, call its `load_state_dict` method, passing the corresponding state information from `state_dict[\"dataset\"][str(dataset_idx)]`.\n\n7. If the current object is using a streaming dataloader, indicated by the `_use_streaming_dataloader` attribute being `True`, proceed to the next step.\n\n8. Update the object's `_num_samples_yielded` attribute with the value from `state_dict[\"num_samples_yielded\"]` to keep track of the number of samples already yielded by the sampler.\n\n9. The function does not return any value; it updates the state of the object in place.\n\n10. The function ensures that the object's datasets and sampler state are synchronized with the provided `state_dict`.", "reference_code": "def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n    if not state_dict:\n        return\n\n    if len(state_dict[\"dataset\"]) != len(self._datasets):\n        raise RuntimeError(f\"The provided state doesn't match the current number of datasets: {self._datasets}.\")\n\n    for dataset_idx, dataset in enumerate(self._datasets):\n        if str(dataset_idx) not in state_dict[\"dataset\"]:\n            raise RuntimeError(f\"The provided state doesn't contain the index {dataset_idx}.\")\n\n        dataset.load_state_dict(state_dict[\"dataset\"][str(dataset_idx)])\n\n    # Used to iterate over the sampler to avoid sampling the same samples\n    if self._use_streaming_dataloader:\n        self._num_samples_yielded = state_dict[\"num_samples_yielded\"]\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "type": "function", "class_name": null, "function_name": "_resolve_dir", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.resolver.Dir\n    class Dir:\n        \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\nlitdata.streaming.resolver._resolve_datasets\n    def _resolve_datasets(dir_path: str) -> Dir:\n\nlitdata.streaming.resolver._resolve_s3_connections\n    def _resolve_s3_connections(dir_path: str) -> Dir:\n\nlitdata.streaming.resolver._resolve_studio\n    def _resolve_studio(dir_path: str, target_name: Optional[str], target_id: Optional[str]) -> Dir:\n\nlitdata.streaming.resolver._resolve_time_template\n    def _resolve_time_template(path: str) -> str:\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.resolver._resolve_studio\n    def _resolve_studio(dir_path: str, target_name: Optional[str], target_id: Optional[str]) -> Dir:\n\nlitdata.streaming.resolver.Dir\n    class Dir:\n        \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\n", "contexts_above": "import datetime\nimport os\nimport re\nimport sys\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom time import sleep\nfrom typing import Optional, Union\nfrom urllib import parse\n\nfrom lightning_cloud.openapi import V1CloudSpace\nfrom lightning_cloud.rest_client import LightningClient\n\n# To avoid adding lightning_utilities as a dependency for now.\ntry:\n    import boto3\n    import botocore\n\n    _BOTO3_AVAILABLE = True\nexcept Exception:\n    _BOTO3_AVAILABLE = False\n\n\ntry:\n    from lightning_sdk import Machine, Studio\n\n    _LIGHTNING_SDK_AVAILABLE = True\nexcept (ImportError, ModuleNotFoundError):\n\n    class Machine:  # type: ignore\n        pass\n\n    _LIGHTNING_SDK_AVAILABLE = False\n\n\n@dataclass\nclass Dir:\n    \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\n    path: Optional[str] = None\n    url: Optional[str] = None\n\n\n", "contexts_below": "\n\ndef _match_studio(target_id: Optional[str], target_name: Optional[str], cloudspace: V1CloudSpace) -> bool:\n    if cloudspace.name is not None and target_name is not None and cloudspace.name.lower() == target_name.lower():\n        return True\n\n    if target_id is not None and cloudspace.id == target_id:\n        return True\n\n    if (\n        cloudspace.display_name is not None\n        and target_name is not None\n        and cloudspace.display_name.lower() == target_name.lower()\n    ):\n        return True\n\n    return False\n\n\ndef _resolve_studio(dir_path: str, target_name: Optional[str], target_id: Optional[str]) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    cloudspaces = client.cloud_space_service_list_cloud_spaces(project_id=project_id, cluster_id=cluster_id).cloudspaces\n    target_cloud_space = [cloudspace for cloudspace in cloudspaces if _match_studio(target_id, target_name, cloudspace)]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided name `{target_name}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    bucket_name = target_cluster[0].spec.aws_v1.bucket_name\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{bucket_name}/projects/{project_id}/cloudspaces/{target_cloud_space[0].id}/code/content\",\n            *dir_path.split(\"/\")[4:],\n        ),\n    )\n\n\ndef _resolve_s3_connections(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    target_name = dir_path.split(\"/\")[3]\n\n    data_connections = client.data_connection_service_list_data_connections(project_id).data_connections\n\n    data_connection = [dc for dc in data_connections if dc.name == target_name]\n\n    if not data_connection:\n        raise ValueError(f\"We didn't find any matching data connection with the provided name `{target_name}`.\")\n\n    return Dir(path=dir_path, url=os.path.join(data_connection[0].aws.source, *dir_path.split(\"/\")[4:]))\n\n\ndef _resolve_datasets(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    target_cloud_space = [\n        cloudspace\n        for cloudspace in client.cloud_space_service_list_cloud_spaces(\n            project_id=project_id, cluster_id=cluster_id\n        ).cloudspaces\n        if cloudspace.id == cloud_space_id\n    ]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided id `{cloud_space_id}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{target_cluster[0].spec.aws_v1.bucket_name}/projects/{project_id}/datasets/\",\n            *dir_path.split(\"/\")[3:],\n        ),\n    )\n\n\ndef _assert_dir_is_empty(output_dir: Dir, append: bool = False, overwrite: bool = False) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n\ndef _assert_dir_has_index_file(output_dir: Dir) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()\n\n\ndef _get_lightning_cloud_url() -> str:\n    # detect local development\n    if os.getenv(\"VSCODE_PROXY_URI\", \"\").startswith(\"http://localhost:9800\"):\n        return \"http://localhost:9800\"\n    # DO NOT CHANGE!\n    return os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\")\n\n\ndef _resolve_time_template(path: str) -> str:\n    match = re.search(\"^.*{%.*}$\", path)\n    if match is None:\n        return path\n\n    pattern = path.split(\"{\")[1].split(\"}\")[0]\n\n    return path.replace(\"{\" + pattern + \"}\", datetime.datetime.now().strftime(pattern))\n\n\ndef _execute(\n    name: str,\n    num_nodes: int,\n    machine: Optional[Machine] = None,\n    command: Optional[str] = None,\n) -> None:\n    \"\"\"Remotely execute the current operator.\"\"\"\n\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The `lightning_sdk` is required.\")\n\n    lightning_skip_install = os.getenv(\"LIGHTNING_SKIP_INSTALL\", \"\")\n    if lightning_skip_install:\n        lightning_skip_install = f\" LIGHTNING_SKIP_INSTALL={lightning_skip_install} \"\n\n    lightning_branch = os.getenv(\"LIGHTNING_BRANCH\", \"\")\n    if lightning_branch:\n        lightning_branch = f\" LIGHTNING_BRANCH={lightning_branch} \"\n\n    studio = Studio()\n    job = studio._studio_api.create_data_prep_machine_job(\n        command or f\"cd {os.getcwd()} &&{lightning_skip_install}{lightning_branch} python {' '.join(sys.argv)}\",\n        name=name,\n        num_instances=num_nodes,\n        studio_id=studio._studio.id,\n        teamspace_id=studio._teamspace.id,\n        cluster_id=studio._studio.cluster_id,\n        machine=machine or studio._studio_api.get_machine(studio._studio.id, studio._teamspace.id),\n    )\n\n    has_printed = False\n\n    while True:\n        curr_job = studio._studio_api._client.lightningapp_instance_service_get_lightningapp_instance(\n            project_id=studio._teamspace.id, id=job.id\n        )\n        if not has_printed:\n            cloud_url = os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\").replace(\":443\", \"\")\n            job_url = f\"{cloud_url}/{studio.owner}/{studio._teamspace.name}\"\n            job_url += f\"/studios/{studio.name}/app?app_id=data-prep&job_name={curr_job.name}\"\n            print(f\"Find your job at {job_url}\")\n            has_printed = True\n\n        if curr_job.status.phase == \"LIGHTNINGAPP_INSTANCE_STATE_FAILED\":\n            raise RuntimeError(f\"job {curr_job.name} failed!\")\n\n        if curr_job.status.phase in [\"LIGHTNINGAPP_INSTANCE_STATE_STOPPED\", \"LIGHTNINGAPP_INSTANCE_STATE_COMPLETED\"]:\n            break\n\n        sleep(1)\n", "input_code": "def _resolve_dir(dir_path: Optional[Union[str, Dir]]) -> Dir:\n\n    \"\"\"\n    Resolves a directory path into a Dir object, handling various path formats including local paths, S3 URLs, and specific project paths. It also processes paths based on their prefixes to determine the appropriate Dir object configuration.\n\n    Input-Output Arguments\n    :param dir_path: Optional[Union[str, Dir]]. The directory path or Dir object to be resolved. It is used to determine the final Dir object based on the path's format and location.\n    :return: Dir. The resolved Dir object with appropriately set path and/or URL attributes based on the input directory path or Dir object.\n    \"\"\"", "reference_steps": "1. Define a function `_resolve_dir` that takes an argument `dir_path`, which can be of type `str`, `Dir`, or `None`.\n\n2. If `dir_path` is an instance of `Dir`, create a new `Dir` object with its `path` and `url` attributes converted to strings, if they exist; otherwise, use `None`.\n\n3. If `dir_path` is `None`, return a new, empty `Dir` object.\n\n4. If `dir_path` is not a `Dir` instance or a string, raise a `ValueError`.\n\n5. Check if `dir_path` is a string starting with \"s3://\" or \"local:\" and return a new `Dir` object with the `url` set to `dir_path` and `path` set to `None`.\n\n6. Process `dir_path` through a function `_resolve_time_template` to potentially replace any time-based placeholders in the path.\n\n7. Resolve `dir_path` to an absolute path using `Path(dir_path).absolute().resolve()` and convert it to a string.\n\n8. If the absolute path starts with \"/teamspace/studios/this_studio\", return a new `Dir` object with the `path` set to the absolute path and `url` set to `None`.\n\n9. If the absolute path starts with \"/.project\", keep `dir_path` unchanged.\n\n10. Check if the absolute path starts with certain predefined directories like \"/teamspace/studios\", \"/teamspace/s3_connections\", or \"/teamspace/datasets\" and has a sufficient number of path components. If so, call the respective resolution functions (`_resolve_studio`, `_resolve_s3_connections`, `_resolve_datasets`) and return their results.\n\n11. If none of the above conditions are met, return a new `Dir` object with the `path` set to the absolute path and `url` set to `None`.", "reference_code": "def _resolve_dir(dir_path: Optional[Union[str, Dir]]) -> Dir:\n    if isinstance(dir_path, Dir):\n        return Dir(path=str(dir_path.path) if dir_path.path else None, url=str(dir_path.url) if dir_path.url else None)\n\n    if dir_path is None:\n        return Dir()\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"`dir_path` must be a `Dir` or a string, got: {dir_path}\")\n\n    assert isinstance(dir_path, str)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path)\n\n    if dir_path.startswith(\"local:\"):\n        return Dir(path=None, url=dir_path)\n\n    dir_path = _resolve_time_template(dir_path)\n\n    dir_path_absolute = str(Path(dir_path).absolute().resolve())\n\n    if dir_path_absolute.startswith(\"/teamspace/studios/this_studio\"):\n        return Dir(path=dir_path_absolute, url=None)\n\n    if dir_path_absolute.startswith(\"/.project\"):\n        dir_path_absolute = dir_path\n\n    if dir_path_absolute.startswith(\"/teamspace/studios\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_studio(dir_path_absolute, dir_path_absolute.split(\"/\")[3], None)\n\n    if dir_path_absolute.startswith(\"/teamspace/s3_connections\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_s3_connections(dir_path_absolute)\n\n    if dir_path_absolute.startswith(\"/teamspace/datasets\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_datasets(dir_path_absolute)\n\n    return Dir(path=dir_path_absolute, url=None)\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "type": "function", "class_name": null, "function_name": "_assert_dir_is_empty", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.resolver.Dir\n    class Dir:\n        \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\nlitdata.streaming.resolver.Dir.path\n\nlitdata.streaming.resolver.Dir.url\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.resolver.Dir.path\n\n", "contexts_above": "import datetime\nimport os\nimport re\nimport sys\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom time import sleep\nfrom typing import Optional, Union\nfrom urllib import parse\n\nfrom lightning_cloud.openapi import V1CloudSpace\nfrom lightning_cloud.rest_client import LightningClient\n\n# To avoid adding lightning_utilities as a dependency for now.\ntry:\n    import boto3\n    import botocore\n\n    _BOTO3_AVAILABLE = True\nexcept Exception:\n    _BOTO3_AVAILABLE = False\n\n\ntry:\n    from lightning_sdk import Machine, Studio\n\n    _LIGHTNING_SDK_AVAILABLE = True\nexcept (ImportError, ModuleNotFoundError):\n\n    class Machine:  # type: ignore\n        pass\n\n    _LIGHTNING_SDK_AVAILABLE = False\n\n\n@dataclass\nclass Dir:\n    \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\n    path: Optional[str] = None\n    url: Optional[str] = None\n\n\ndef _resolve_dir(dir_path: Optional[Union[str, Dir]]) -> Dir:\n    if isinstance(dir_path, Dir):\n        return Dir(path=str(dir_path.path) if dir_path.path else None, url=str(dir_path.url) if dir_path.url else None)\n\n    if dir_path is None:\n        return Dir()\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"`dir_path` must be a `Dir` or a string, got: {dir_path}\")\n\n    assert isinstance(dir_path, str)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path)\n\n    if dir_path.startswith(\"local:\"):\n        return Dir(path=None, url=dir_path)\n\n    dir_path = _resolve_time_template(dir_path)\n\n    dir_path_absolute = str(Path(dir_path).absolute().resolve())\n\n    if dir_path_absolute.startswith(\"/teamspace/studios/this_studio\"):\n        return Dir(path=dir_path_absolute, url=None)\n\n    if dir_path_absolute.startswith(\"/.project\"):\n        dir_path_absolute = dir_path\n\n    if dir_path_absolute.startswith(\"/teamspace/studios\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_studio(dir_path_absolute, dir_path_absolute.split(\"/\")[3], None)\n\n    if dir_path_absolute.startswith(\"/teamspace/s3_connections\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_s3_connections(dir_path_absolute)\n\n    if dir_path_absolute.startswith(\"/teamspace/datasets\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_datasets(dir_path_absolute)\n\n    return Dir(path=dir_path_absolute, url=None)\n\n\ndef _match_studio(target_id: Optional[str], target_name: Optional[str], cloudspace: V1CloudSpace) -> bool:\n    if cloudspace.name is not None and target_name is not None and cloudspace.name.lower() == target_name.lower():\n        return True\n\n    if target_id is not None and cloudspace.id == target_id:\n        return True\n\n    if (\n        cloudspace.display_name is not None\n        and target_name is not None\n        and cloudspace.display_name.lower() == target_name.lower()\n    ):\n        return True\n\n    return False\n\n\ndef _resolve_studio(dir_path: str, target_name: Optional[str], target_id: Optional[str]) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    cloudspaces = client.cloud_space_service_list_cloud_spaces(project_id=project_id, cluster_id=cluster_id).cloudspaces\n    target_cloud_space = [cloudspace for cloudspace in cloudspaces if _match_studio(target_id, target_name, cloudspace)]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided name `{target_name}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    bucket_name = target_cluster[0].spec.aws_v1.bucket_name\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{bucket_name}/projects/{project_id}/cloudspaces/{target_cloud_space[0].id}/code/content\",\n            *dir_path.split(\"/\")[4:],\n        ),\n    )\n\n\ndef _resolve_s3_connections(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    target_name = dir_path.split(\"/\")[3]\n\n    data_connections = client.data_connection_service_list_data_connections(project_id).data_connections\n\n    data_connection = [dc for dc in data_connections if dc.name == target_name]\n\n    if not data_connection:\n        raise ValueError(f\"We didn't find any matching data connection with the provided name `{target_name}`.\")\n\n    return Dir(path=dir_path, url=os.path.join(data_connection[0].aws.source, *dir_path.split(\"/\")[4:]))\n\n\ndef _resolve_datasets(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    target_cloud_space = [\n        cloudspace\n        for cloudspace in client.cloud_space_service_list_cloud_spaces(\n            project_id=project_id, cluster_id=cluster_id\n        ).cloudspaces\n        if cloudspace.id == cloud_space_id\n    ]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided id `{cloud_space_id}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{target_cluster[0].spec.aws_v1.bucket_name}/projects/{project_id}/datasets/\",\n            *dir_path.split(\"/\")[3:],\n        ),\n    )\n\n\n", "contexts_below": "\n\ndef _assert_dir_has_index_file(output_dir: Dir) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()\n\n\ndef _get_lightning_cloud_url() -> str:\n    # detect local development\n    if os.getenv(\"VSCODE_PROXY_URI\", \"\").startswith(\"http://localhost:9800\"):\n        return \"http://localhost:9800\"\n    # DO NOT CHANGE!\n    return os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\")\n\n\ndef _resolve_time_template(path: str) -> str:\n    match = re.search(\"^.*{%.*}$\", path)\n    if match is None:\n        return path\n\n    pattern = path.split(\"{\")[1].split(\"}\")[0]\n\n    return path.replace(\"{\" + pattern + \"}\", datetime.datetime.now().strftime(pattern))\n\n\ndef _execute(\n    name: str,\n    num_nodes: int,\n    machine: Optional[Machine] = None,\n    command: Optional[str] = None,\n) -> None:\n    \"\"\"Remotely execute the current operator.\"\"\"\n\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The `lightning_sdk` is required.\")\n\n    lightning_skip_install = os.getenv(\"LIGHTNING_SKIP_INSTALL\", \"\")\n    if lightning_skip_install:\n        lightning_skip_install = f\" LIGHTNING_SKIP_INSTALL={lightning_skip_install} \"\n\n    lightning_branch = os.getenv(\"LIGHTNING_BRANCH\", \"\")\n    if lightning_branch:\n        lightning_branch = f\" LIGHTNING_BRANCH={lightning_branch} \"\n\n    studio = Studio()\n    job = studio._studio_api.create_data_prep_machine_job(\n        command or f\"cd {os.getcwd()} &&{lightning_skip_install}{lightning_branch} python {' '.join(sys.argv)}\",\n        name=name,\n        num_instances=num_nodes,\n        studio_id=studio._studio.id,\n        teamspace_id=studio._teamspace.id,\n        cluster_id=studio._studio.cluster_id,\n        machine=machine or studio._studio_api.get_machine(studio._studio.id, studio._teamspace.id),\n    )\n\n    has_printed = False\n\n    while True:\n        curr_job = studio._studio_api._client.lightningapp_instance_service_get_lightningapp_instance(\n            project_id=studio._teamspace.id, id=job.id\n        )\n        if not has_printed:\n            cloud_url = os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\").replace(\":443\", \"\")\n            job_url = f\"{cloud_url}/{studio.owner}/{studio._teamspace.name}\"\n            job_url += f\"/studios/{studio.name}/app?app_id=data-prep&job_name={curr_job.name}\"\n            print(f\"Find your job at {job_url}\")\n            has_printed = True\n\n        if curr_job.status.phase == \"LIGHTNINGAPP_INSTANCE_STATE_FAILED\":\n            raise RuntimeError(f\"job {curr_job.name} failed!\")\n\n        if curr_job.status.phase in [\"LIGHTNINGAPP_INSTANCE_STATE_STOPPED\", \"LIGHTNINGAPP_INSTANCE_STATE_COMPLETED\"]:\n            break\n\n        sleep(1)\n", "input_code": "def _assert_dir_is_empty(output_dir: Dir, append: bool = False, overwrite: bool = False) -> None:\n\n    \"\"\"\n    This function checks if a specified directory, particularly in an S3 bucket, is empty. It raises errors if the directory is not a Dir object, does not start with \"s3://\", or already contains data. Currently, it does not support appending or overwriting data in the directory.\n\n    Input-Output Arguments\n    :param output_dir: Dir. The directory to be checked. It must be an instance of the Dir class.\n    :param append: Bool, optional. Indicates if appending data to the directory is allowed. Currently not implemented.\n    :param overwrite: Bool, optional. Indicates if overwriting data in the directory is allowed. Currently not implemented.\n    :return: No return values. This function either completes successfully or raises an error.\n    \"\"\"", "reference_steps": "1. Define a function `_assert_dir_is_empty` that takes an `output_dir` of type `Dir`, and optional boolean arguments `append` and `overwrite`.\n2. Check if `output_dir` is an instance of the `Dir` class, raising a `ValueError` if it is not.\n3. Return early if the `output_dir`'s URL attribute is `None`.\n4. Parse the URL of the `output_dir` using `urllib.parse.urlparse`.\n5. Raise a `ValueError` if the URL scheme is not \"s3\", indicating that the directory must be an S3 bucket path.\n6. Create an S3 client using the `boto3` library.\n7. List the objects in the specified S3 bucket and prefix using the `list_objects_v2` method of the S3 client.\n8. Check if the `KeyCount` in the returned objects is greater than 0, which would indicate that the directory is not empty.\n9. If the directory is not empty, raise a `RuntimeError` stating that the directory already contains data and datasets are meant to be immutable.\n10. Note that the function currently does not support the `append` and `overwrite` functionality, as indicated by the `TODO` comment.", "reference_code": "def _assert_dir_is_empty(output_dir: Dir, append: bool = False, overwrite: bool = False) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "type": "function", "class_name": null, "function_name": "_assert_dir_has_index_file", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.resolver.Dir\n    class Dir:\n        \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\nlitdata.streaming.resolver.Dir.path\n\nlitdata.streaming.resolver.Dir.url\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.resolver.Dir.path\n\n", "contexts_above": "import datetime\nimport os\nimport re\nimport sys\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom time import sleep\nfrom typing import Optional, Union\nfrom urllib import parse\n\nfrom lightning_cloud.openapi import V1CloudSpace\nfrom lightning_cloud.rest_client import LightningClient\n\n# To avoid adding lightning_utilities as a dependency for now.\ntry:\n    import boto3\n    import botocore\n\n    _BOTO3_AVAILABLE = True\nexcept Exception:\n    _BOTO3_AVAILABLE = False\n\n\ntry:\n    from lightning_sdk import Machine, Studio\n\n    _LIGHTNING_SDK_AVAILABLE = True\nexcept (ImportError, ModuleNotFoundError):\n\n    class Machine:  # type: ignore\n        pass\n\n    _LIGHTNING_SDK_AVAILABLE = False\n\n\n@dataclass\nclass Dir:\n    \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\n    path: Optional[str] = None\n    url: Optional[str] = None\n\n\ndef _resolve_dir(dir_path: Optional[Union[str, Dir]]) -> Dir:\n    if isinstance(dir_path, Dir):\n        return Dir(path=str(dir_path.path) if dir_path.path else None, url=str(dir_path.url) if dir_path.url else None)\n\n    if dir_path is None:\n        return Dir()\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"`dir_path` must be a `Dir` or a string, got: {dir_path}\")\n\n    assert isinstance(dir_path, str)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path)\n\n    if dir_path.startswith(\"local:\"):\n        return Dir(path=None, url=dir_path)\n\n    dir_path = _resolve_time_template(dir_path)\n\n    dir_path_absolute = str(Path(dir_path).absolute().resolve())\n\n    if dir_path_absolute.startswith(\"/teamspace/studios/this_studio\"):\n        return Dir(path=dir_path_absolute, url=None)\n\n    if dir_path_absolute.startswith(\"/.project\"):\n        dir_path_absolute = dir_path\n\n    if dir_path_absolute.startswith(\"/teamspace/studios\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_studio(dir_path_absolute, dir_path_absolute.split(\"/\")[3], None)\n\n    if dir_path_absolute.startswith(\"/teamspace/s3_connections\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_s3_connections(dir_path_absolute)\n\n    if dir_path_absolute.startswith(\"/teamspace/datasets\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_datasets(dir_path_absolute)\n\n    return Dir(path=dir_path_absolute, url=None)\n\n\ndef _match_studio(target_id: Optional[str], target_name: Optional[str], cloudspace: V1CloudSpace) -> bool:\n    if cloudspace.name is not None and target_name is not None and cloudspace.name.lower() == target_name.lower():\n        return True\n\n    if target_id is not None and cloudspace.id == target_id:\n        return True\n\n    if (\n        cloudspace.display_name is not None\n        and target_name is not None\n        and cloudspace.display_name.lower() == target_name.lower()\n    ):\n        return True\n\n    return False\n\n\ndef _resolve_studio(dir_path: str, target_name: Optional[str], target_id: Optional[str]) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    cloudspaces = client.cloud_space_service_list_cloud_spaces(project_id=project_id, cluster_id=cluster_id).cloudspaces\n    target_cloud_space = [cloudspace for cloudspace in cloudspaces if _match_studio(target_id, target_name, cloudspace)]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided name `{target_name}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    bucket_name = target_cluster[0].spec.aws_v1.bucket_name\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{bucket_name}/projects/{project_id}/cloudspaces/{target_cloud_space[0].id}/code/content\",\n            *dir_path.split(\"/\")[4:],\n        ),\n    )\n\n\ndef _resolve_s3_connections(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    target_name = dir_path.split(\"/\")[3]\n\n    data_connections = client.data_connection_service_list_data_connections(project_id).data_connections\n\n    data_connection = [dc for dc in data_connections if dc.name == target_name]\n\n    if not data_connection:\n        raise ValueError(f\"We didn't find any matching data connection with the provided name `{target_name}`.\")\n\n    return Dir(path=dir_path, url=os.path.join(data_connection[0].aws.source, *dir_path.split(\"/\")[4:]))\n\n\ndef _resolve_datasets(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    target_cloud_space = [\n        cloudspace\n        for cloudspace in client.cloud_space_service_list_cloud_spaces(\n            project_id=project_id, cluster_id=cluster_id\n        ).cloudspaces\n        if cloudspace.id == cloud_space_id\n    ]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided id `{cloud_space_id}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{target_cluster[0].spec.aws_v1.bucket_name}/projects/{project_id}/datasets/\",\n            *dir_path.split(\"/\")[3:],\n        ),\n    )\n\n\ndef _assert_dir_is_empty(output_dir: Dir, append: bool = False, overwrite: bool = False) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n\n", "contexts_below": "\n\ndef _get_lightning_cloud_url() -> str:\n    # detect local development\n    if os.getenv(\"VSCODE_PROXY_URI\", \"\").startswith(\"http://localhost:9800\"):\n        return \"http://localhost:9800\"\n    # DO NOT CHANGE!\n    return os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\")\n\n\ndef _resolve_time_template(path: str) -> str:\n    match = re.search(\"^.*{%.*}$\", path)\n    if match is None:\n        return path\n\n    pattern = path.split(\"{\")[1].split(\"}\")[0]\n\n    return path.replace(\"{\" + pattern + \"}\", datetime.datetime.now().strftime(pattern))\n\n\ndef _execute(\n    name: str,\n    num_nodes: int,\n    machine: Optional[Machine] = None,\n    command: Optional[str] = None,\n) -> None:\n    \"\"\"Remotely execute the current operator.\"\"\"\n\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The `lightning_sdk` is required.\")\n\n    lightning_skip_install = os.getenv(\"LIGHTNING_SKIP_INSTALL\", \"\")\n    if lightning_skip_install:\n        lightning_skip_install = f\" LIGHTNING_SKIP_INSTALL={lightning_skip_install} \"\n\n    lightning_branch = os.getenv(\"LIGHTNING_BRANCH\", \"\")\n    if lightning_branch:\n        lightning_branch = f\" LIGHTNING_BRANCH={lightning_branch} \"\n\n    studio = Studio()\n    job = studio._studio_api.create_data_prep_machine_job(\n        command or f\"cd {os.getcwd()} &&{lightning_skip_install}{lightning_branch} python {' '.join(sys.argv)}\",\n        name=name,\n        num_instances=num_nodes,\n        studio_id=studio._studio.id,\n        teamspace_id=studio._teamspace.id,\n        cluster_id=studio._studio.cluster_id,\n        machine=machine or studio._studio_api.get_machine(studio._studio.id, studio._teamspace.id),\n    )\n\n    has_printed = False\n\n    while True:\n        curr_job = studio._studio_api._client.lightningapp_instance_service_get_lightningapp_instance(\n            project_id=studio._teamspace.id, id=job.id\n        )\n        if not has_printed:\n            cloud_url = os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\").replace(\":443\", \"\")\n            job_url = f\"{cloud_url}/{studio.owner}/{studio._teamspace.name}\"\n            job_url += f\"/studios/{studio.name}/app?app_id=data-prep&job_name={curr_job.name}\"\n            print(f\"Find your job at {job_url}\")\n            has_printed = True\n\n        if curr_job.status.phase == \"LIGHTNINGAPP_INSTANCE_STATE_FAILED\":\n            raise RuntimeError(f\"job {curr_job.name} failed!\")\n\n        if curr_job.status.phase in [\"LIGHTNINGAPP_INSTANCE_STATE_STOPPED\", \"LIGHTNINGAPP_INSTANCE_STATE_COMPLETED\"]:\n            break\n\n        sleep(1)\n", "input_code": "def _assert_dir_has_index_file(output_dir: Dir) -> None:\n\n    \"\"\"\n    This function asserts that a given directory, expected to be an S3 bucket directory, does not contain an index file named \"index.json\". If the directory is not an S3 bucket or already contains an index file, it raises an error. Additionally, if an index file is not found, it deletes all objects within the specified prefix in the bucket.\n\n    Input-Output Arguments\n    :param output_dir: Dir. The directory object which is expected to be an S3 bucket directory. It is used to check for the presence of an \"index.json\" file and to perform operations based on its existence.\n    :return: None. There are no return values from this function.\n    \"\"\"", "reference_steps": "1. Define a function `_assert_dir_has_index_file` that takes an argument `output_dir` which should be an instance of the `Dir` class.\n\n2. Check if `output_dir` is an instance of the `Dir` class; if not, raise a `ValueError`.\n\n3. Return from the function if the `url` attribute of `output_dir` is `None`.\n\n4. Parse the `url` of `output_dir` using `urlparse` from the `parse` module to obtain its components.\n\n5. Raise a `ValueError` if the scheme of the URL is not \"s3\".\n\n6. Create an S3 client using `boto3.client`.\n\n7. Define the `prefix` for the S3 bucket by stripping leading and trailing slashes from the path component of the URL and appending a slash.\n\n8. List the objects in the S3 bucket using the `list_objects_v2` method with the specified `Bucket`, `Delimiter`, and `Prefix`.\n\n9. Check if an \"index.json\" file exists in the specified directory by using the `head_object` method. If the file exists, set `has_index_file` to `True`, otherwise handle the `ClientError` and set `has_index_file` to `False`.\n\n10. If an index file exists, raise a `RuntimeError` indicating the directory already contains an optimized immutable dataset. If no index file is found, iterate over the objects in the S3 bucket with the specified prefix and delete them using the `delete` method on each S3 `Object`.", "reference_code": "def _assert_dir_has_index_file(output_dir: Dir) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "type": "method", "class_name": "BinaryWriter", "function_name": "merge", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.writer.BinaryWriter._cache_dir\n\nlitdata.streaming.writer.BinaryWriter._distributed_env\n\nlitdata.streaming.writer.BinaryWriter._merge_no_wait\n    def _merge_no_wait(self, node_rank: Optional[int] = None) -> None:\n        \"\"\"Once all the workers have written their own index, the merge function is responsible to read and merge them\n        into a single index.\"\"\"\n\nlitdata.streaming.writer.BinaryWriter.rank\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n\n# Cross-file Dependency:\nlitdata.constants._INDEX_FILENAME\n\nlitdata.utilities.env._DistributedEnv.world_size\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.writer.BinaryWriter.rank\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n\nlitdata.streaming.writer.BinaryWriter._merge_no_wait\n    def _merge_no_wait(self, node_rank: Optional[int] = None) -> None:\n        \"\"\"Once all the workers have written their own index, the merge function is responsible to read and merge them\n        into a single index.\"\"\"\n\n# Cross-file Dependency:\nlitdata.constants._INDEX_FILENAME\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom time import sleep\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\n\nfrom litdata.constants import _INDEX_FILENAME, _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.processing.utilities import get_worker_rank\nfrom litdata.streaming.compression import _COMPRESSORS, Compressor\nfrom litdata.streaming.serializers import Serializer, _get_serializers\nfrom litdata.utilities.env import _DistributedEnv, _WorkerEnv\nfrom litdata.utilities.format import _convert_bytes_to_int, _human_readable_bytes\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import PyTree, tree_flatten, treespec_dumps\n\n\n@dataclass\nclass Item:\n    index: int\n    data: bytes\n    bytes: int\n    dim: Optional[int] = None\n\n    def __len__(self) -> int:\n        return self.bytes\n\n\nclass BinaryWriter:\n    def __init__(\n        self,\n        cache_dir: str,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n        follow_tensor_dimension: bool = True,\n        serializers: Optional[Dict[str, Serializer]] = None,\n    ):\n        \"\"\"The BinaryWriter enables to chunk dataset into an efficient streaming format for cloud training.\n\n        Arguments:\n            cache_dir: The path to where the chunks will be saved.\n            chunk_bytes: The maximum number of bytes within a chunk.\n            chunk_size: The maximum number of items within a chunk.\n            compression: The compression algorithm to use.\n            serializers: Provide your own serializers.\n\n        \"\"\"\n        self._cache_dir = cache_dir\n\n        if (isinstance(self._cache_dir, str) and not os.path.exists(self._cache_dir)) or self._cache_dir is None:\n            raise FileNotFoundError(f\"The provided cache directory `{self._cache_dir}` doesn't exist.\")\n\n        if (chunk_size is None and chunk_bytes is None) or (chunk_size and chunk_bytes):\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self._serializers: Dict[str, Serializer] = _get_serializers(serializers)\n        self._serializers_extra: Dict[str, Serializer] = {}\n        self._chunk_size = chunk_size\n        self._chunk_bytes = _convert_bytes_to_int(chunk_bytes) if isinstance(chunk_bytes, str) else chunk_bytes\n        self._compression = compression\n\n        self._data_format: Optional[List[str]] = None\n        self._data_spec: Optional[PyTree] = None\n\n        if self._compression:\n            if len(_COMPRESSORS) == 0:\n                raise ValueError(\"No compresion algorithms are installed.\")\n\n            if self._compression not in _COMPRESSORS:\n                raise ValueError(\n                    f\"The provided compression {self._compression} isn't available in {sorted(_COMPRESSORS)}\"\n                )\n            self._compressor: Compressor = _COMPRESSORS[self._compression]\n\n        self._serialized_items: Dict[int, Item] = {}\n        self._chunk_index = 0\n        self._min_index: Optional[int] = None\n        self._max_index: Optional[int] = None\n        self._chunks_info: List[Dict[str, Any]] = []\n        self._worker_env: Optional[_WorkerEnv] = None\n        self._rank: Optional[int] = None\n        self._is_done = False\n        self._distributed_env = _DistributedEnv.detect()\n        self._follow_tensor_dimension = follow_tensor_dimension\n\n    @property\n    def filled(self) -> bool:\n        \"\"\"Returns whether the caching phase is done.\"\"\"\n        if self._is_done:\n            return True\n        files = os.listdir(self._cache_dir)\n        index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n        worker_env = _WorkerEnv.detect()\n        data_optimiser_num_workers = os.getenv(\"DATA_OPTIMIZER_NUM_WORKERS\", None)\n        if data_optimiser_num_workers is not None:\n            self._is_done = len(index_files) == int(data_optimiser_num_workers)\n        else:\n            self._is_done = len(index_files) == self._distributed_env.world_size * worker_env.world_size\n        return self._is_done\n\n    @property\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n        if self._rank is None:\n            rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n            if rank:\n                self._rank = int(rank)\n            else:\n                self._worker_env = _WorkerEnv.detect()\n                self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n        return self._rank\n\n    def get_config(self) -> Dict[str, Any]:\n        \"\"\"Returns the config of the writer.\"\"\"\n        out = {\n            \"compression\": self._compression,\n            \"chunk_size\": self._chunk_size,\n            \"chunk_bytes\": self._chunk_bytes,\n            \"data_format\": self._data_format,\n            \"data_spec\": treespec_dumps(self._data_spec) if self._data_spec else None,\n        }\n        return out\n\n    def serialize(self, items: Any) -> Tuple[bytes, Optional[int]]:\n        \"\"\"Serialize a dictionary into its binary format.\"\"\"\n\n        # Flatten the items provided by the users\n        flattened, data_spec = tree_flatten(items)\n\n        is_single_tensor = len(flattened) == 1 and isinstance(flattened[0], torch.Tensor)\n\n        # Collect the sizes and associated bytes for each item\n        sizes: List[int] = []\n        data: List[bytes] = []\n\n        if self._data_format is None:\n            data_format: List[str] = []\n            for item in flattened:\n                data_format.append(self._serialize(item, sizes, data))\n\n            worker_rank = get_worker_rank()\n            if worker_rank is not None:\n                print(f\"Rank {worker_rank} inferred the following `{data_format}` data format.\")\n            self._data_format = data_format\n            self._data_spec = data_spec\n        else:\n            # tiny optimization to avoid looping over all the data format\n            self._serialize_with_data_format(flattened, sizes, data, self._data_format)\n\n        # If there is a single element and it is a tensor, enable continous array.\n        if is_single_tensor:\n            return data[0], flattened[0].shape[0]\n\n        # Concatenante into a single byte array\n        head = np.array(sizes, np.uint32).tobytes()\n        body = b\"\".join(data)\n        return head + body, None\n\n    def _serialize(self, item: Any, sizes: List[int], data: List[bytes]) -> str:\n        \"\"\"Serialize a given item and append its size and bytes to the sizes and data array.\"\"\"\n        for serializer_name, serializer in self._serializers.items():\n            if serializer.can_serialize(item):\n                serialized_item, name = serializer.serialize(item)\n                data.append(serialized_item)\n                sizes.append(serializer.size if hasattr(serializer, \"size\") else len(serialized_item))\n                name = name or serializer_name\n                if name and name not in self._serializers_extra:\n                    self._serializers_extra[name] = serializer\n                return name\n        raise ValueError(f\"The provided item isn't serializable. Found {item}\")\n\n    def _serialize_with_data_format(\n        self, item: Any, sizes: List[int], data: List[bytes], data_format: List[str]\n    ) -> None:\n        \"\"\"Serialize a given item and append its size and bytes to the sizes and data array.\"\"\"\n        assert data_format\n        for element, item_format in zip(item, data_format):\n            serializer = self._serializers_extra[item_format]\n            serialized_item, _ = serializer.serialize(element)\n            data.append(serialized_item)\n            sizes.append(serializer.size if hasattr(serializer, \"size\") else len(serialized_item))\n\n    def _create_chunk(self, filename: str, on_done: bool = False) -> bytes:\n        \"\"\"Create a binary chunk from all the binarized items.\"\"\"\n        items = []\n\n        if on_done:\n            indices = sorted(self._serialized_items.keys())\n            for i in range(len(indices) - 1):\n                assert indices[i] == indices[i + 1] - 1, indices\n            items = [self._serialized_items.pop(index) for index in indices]\n        else:\n            assert self._max_index is not None, (self._max_index, self._min_index)\n            assert self._min_index is not None, (self._max_index, self._min_index)\n            if self._max_index == self._min_index:\n                # A single item is larger than the target chunk size; allow the chunk to be bigger than the target size\n                items.append(self._serialized_items.pop(self._max_index))\n            items.extend(self._serialized_items.pop(index) for index in range(self._min_index, self._max_index))\n\n        if len(items) == 0:\n            raise RuntimeError(\n                \"The items shouldn't have an empty length. Something went wrong.\"\n                f\" Found {self._pretty_serialized_items()} with boundaries: {self._min_index}, {self._max_index}.\"\n            )\n\n        num_items = np.uint32(len(items))\n        sizes = list(map(len, items))\n        offsets = np.array([0] + sizes).cumsum().astype(np.uint32)\n        offsets += len(num_items.tobytes()) + len(offsets.tobytes())\n        sample_data = b\"\".join([item.data for item in items])\n        data = num_items.tobytes() + offsets.tobytes() + sample_data\n\n        current_chunk_bytes = sum([item.bytes for item in items])\n\n        if self._chunk_bytes and current_chunk_bytes > self._chunk_bytes:\n            warnings.warn(\n                f\"An item was larger than the target chunk size ({_human_readable_bytes(self._chunk_bytes)}).\"\n                f\" The current chunk will be {_human_readable_bytes(current_chunk_bytes)} in size.\",\n                UserWarning,\n            )\n\n        if self._chunk_size:\n            assert num_items.item() <= self._chunk_size\n\n        dim: Optional[int] = None\n        if items[0].dim:\n            dim = sum([item.dim if item.dim is not None else 0 for item in items])\n\n        chunk_info = {\n            \"chunk_bytes\": current_chunk_bytes,\n            \"chunk_size\": num_items.item(),\n            \"filename\": filename,\n            \"dim\": dim,\n        }\n\n        self._chunks_info.append(chunk_info)\n\n        return data\n\n    def get_chunk_filename(self) -> str:\n        if self._compression:\n            return f\"chunk-{self.rank}-{self._chunk_index}.{self._compression}.bin\"\n        return f\"chunk-{self.rank}-{self._chunk_index}.bin\"\n\n    def write_chunk(self, on_done: bool = False) -> str:\n        \"\"\"Write a chunk to the filesystem.\"\"\"\n        filename = self.get_chunk_filename()\n        self.write_chunk_to_file(self._create_chunk(filename, on_done=on_done), filename)\n        self._chunk_index += 1\n        return os.path.join(self._cache_dir, filename)\n\n    def __setitem__(self, index: int, items: Any) -> None:\n        \"\"\"Store an item to a chunk.\n\n        The index needs to be provided in order.\n\n        This is handled by the samplers automatically. This ensures we can map an index to a shard from an interval.\n\n        \"\"\"\n        self.add_item(index, items)\n\n    def add_item(self, index: int, items: Any) -> Optional[str]:\n        # Track the minimum index provided to the writer\n        # Serialize the items and store an Item object.\n        if index in self._serialized_items:\n            raise ValueError(f\"The provided index {index} already exists in the cache.\")\n\n        data, dim = self.serialize(items)\n        self._serialized_items[index] = Item(\n            index=index,\n            data=data,\n            bytes=len(data),\n            dim=dim,\n        )\n\n        if self._should_write():\n            filepath = os.path.join(self._cache_dir, self.get_chunk_filename())\n            self.write_chunk()\n            self._min_index = None\n            self._max_index = None\n            return filepath\n\n    def _should_write(self) -> bool:\n        # TODO: Misleading method name, it modifies `self._min_index` and `self._max_index`!\n        if not self._serialized_items:\n            return False\n        indexes = list(self._serialized_items.keys())\n        self._min_index = index = indexes[0] if len(indexes) == 1 else min(*indexes)\n        num_bytes = 0\n        num_items = 0\n        while True:\n            item = self._serialized_items.get(index, None)\n            if item:\n                num_bytes += item.bytes\n                num_items += item.dim if item.dim else 1\n                index += 1\n                if (self._chunk_bytes and self._chunk_bytes < num_bytes) or (\n                    self._chunk_size and num_items > self._chunk_size\n                ):\n                    self._max_index = index - 1\n                    return True\n            else:\n                return False\n\n    def write_chunk_to_file(\n        self,\n        raw_data: bytes,\n        filename: str,\n    ) -> None:\n        \"\"\"Write chunk bytes to a file.\"\"\"\n        # Whether to compress the raw bytes\n        if self._compression:\n            raw_data = self._compressor.compress(raw_data)\n\n        # Write the binary chunk file\n        with open(os.path.join(self._cache_dir, filename), \"wb\") as out:\n            out.write(raw_data)\n\n    def write_chunks_index(self) -> str:\n        \"\"\"Write the chunks index to a JSON file.\"\"\"\n        if len(self._chunks_info) == 0:\n            return \"\"\n        filepath = os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\")\n        config = self.get_config()\n        with open(filepath, \"w\") as out:\n            json.dump({\"chunks\": self._chunks_info, \"config\": config}, out, sort_keys=True)\n        return filepath\n\n    def done(self) -> List[str]:\n        \"\"\"Called when StopIteration is triggered.\"\"\"\n        filepaths: List[str] = []\n        if self.filled:\n            return filepaths\n\n        # Try writing down an chunks\n        while self._should_write():\n            filepaths.append(self.write_chunk())\n\n        # If any elements is left, try writing one last chunk\n        if self._serialized_items:\n            filepaths.append(self.write_chunk(True))\n\n        # Write down the index file\n        self.write_chunks_index()\n\n        self._is_done = True\n        return filepaths\n\n", "contexts_below": "\n    def _merge_no_wait(self, node_rank: Optional[int] = None) -> None:\n        \"\"\"Once all the workers have written their own index, the merge function is responsible to read and merge them\n        into a single index.\"\"\"\n        files = os.listdir(self._cache_dir)\n        index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n        chunks_info = []\n        config = None\n        for index_filename in sorted(index_files):\n            chunk_path = os.path.join(self._cache_dir, index_filename)\n            with open(chunk_path) as f:\n                data = json.load(f)\n\n                if config is None:\n                    config = data[\"config\"]\n\n                elif config != data[\"config\"]:\n                    raise Exception(\n                        \"The config isn't consistent between chunks. This shouldn't have happened.\"\n                        f\"Found {config} {data['config']}.\"\n                    )\n\n                chunks_info.extend(data[\"chunks\"])\n\n            os.remove(chunk_path)\n\n        if node_rank is None:\n            with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as f:\n                json.dump({\"chunks\": chunks_info, \"config\": config}, f, sort_keys=True)\n        else:\n            with open(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\"), \"w\") as f:\n                json.dump({\"chunks\": chunks_info, \"config\": config}, f, sort_keys=True)\n\n    def _should_raise(self, data_format_1: List[str], data_format_2: List[str]) -> bool:\n        if len(data_format_1) != len(data_format_2):\n            return True\n\n        def is_non_valid(f1: str, f2: str) -> bool:\n            if f1 in [\"pil\", \"jpeg\"] and f2 in [\"pil\", \"jpeg\"]:\n                return False\n            return f1 != f2\n\n        return any(is_non_valid(f1, f2) for f1, f2 in zip(data_format_1, data_format_2))\n\n    def _pretty_serialized_items(self) -> Dict[int, Item]:\n        out = {}\n        for key, value in self._serialized_items.items():\n            # drop `data` as it would make logs unreadable.\n            out[key] = Item(\n                index=value.index,\n                bytes=value.bytes,\n                dim=value.dim,\n                data=b\"\",\n            )\n        return out\n", "input_code": "    def merge(self, num_workers: int = 1, node_rank: Optional[int] = None) -> None:\n\n        \"\"\"\n        The merge function is responsible for combining individual index files created by multiple workers into a single, unified index file. This is particularly useful in distributed environments where each worker generates a part of the index. The function waits until all parts are available, then proceeds with the merge only if it's the master node (rank 0).\n\n        Input-Output Arguments\n        :param self: BinaryWriter. An instance of the BinaryWriter class, which manages the merging process.\n        :param num_workers: int, optional. The number of workers that are expected to contribute to the index. It defaults to 1, assuming a non-distributed environment unless specified otherwise. It's used to determine when all parts of the index are ready for merging.\n        :param node_rank: Optional[int], optional. The rank of the node in the distributed environment. This parameter is used to identify the master node (rank 0) which is responsible for the actual merging process. If None, the function assumes a non-distributed environment or that the rank determination is handled externally.\n        :return: No return values. The function's purpose is to perform the merge operation, and it does not return any value.\n\n        Note: The function includes a mechanism to wait for all index parts to be available by periodically checking the presence of index files in a cache directory. It also includes a special condition for non-master nodes (rank != 0) to wait until the merged index file is available.\n        \"\"\"", "reference_steps": "1. Define a function `merge` that accepts the number of workers (`num_workers`) and the rank of the node (`node_rank`) as optional parameters, with the purpose of merging individual index files into a single index file.\n\n2. Set the number of workers to 1 if it is not provided or is falsy.\n\n3. If the rank of the current process (`self.rank`) is not 0 (indicating it is not the master process), enter a loop where the process waits until a file named `_INDEX_FILENAME` appears in the cache directory (`self._cache_dir`), then return without doing anything further.\n\n4. If the rank of the current process is 0 (master process), enter a loop to wait for all index files to become available.\n\n5. Within the loop, list all the files in the cache directory.\n\n6. Check if the final merged index file (`_INDEX_FILENAME`) already exists in the cache directory; if it does, return immediately as the merge is not needed.\n\n7. Filter the list of files to include only those that end with `_INDEX_FILENAME`, which are the individual index files from each worker.\n\n8. Check if the number of index files matches the expected count, which is the world size of the distributed environment (`self._distributed_env.world_size`) multiplied by the number of workers. If it does, set `is_done` to `True` to exit the loop.\n\n9. If the expected number of index files is not present, wait for a brief period (`sleep(0.01)`) before checking again.\n\n10. Once all index files are available, call the method `_merge_no_wait` with the provided `node_rank` to perform the actual merging of index files into a single index file.", "reference_code": "def merge(self, num_workers: int = 1, node_rank: Optional[int] = None) -> None:\n    \"\"\"Once all the workers have written their own index, the merge function is responsible to read and merge them\n    into a single index.\"\"\"\n    num_workers = num_workers or 1\n\n    # Only for non rank 0\n    if self.rank != 0:\n        while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n            sleep(0.01)\n        return\n\n    # Wait for all indexes to be available\n    is_done = False\n    while not is_done:\n        files = os.listdir(self._cache_dir)\n\n        # Return if the index already exists\n        if _INDEX_FILENAME in files:\n            return\n\n        index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n        # When using the Data Optimizer, we don't use multi processes.\n        is_done = len(index_files) == self._distributed_env.world_size * num_workers\n        sleep(0.01)\n\n    self._merge_no_wait(node_rank=node_rank)\n"}
{"namespace": "litdata.streaming.resolver._execute", "type": "function", "class_name": null, "function_name": "_execute", "dependency_all": "# Intra-file Dependency:\nlitdata.streaming.resolver.Machine\n        class Machine:  # type: ignore\n            pass\n\n        _LIGHTNING_SDK_AVAILABLE = False\n\n\n    @dataclass\n    class Dir:\n        \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\nlitdata.streaming.resolver._LIGHTNING_SDK_AVAILABLE\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.streaming.resolver.Machine\n        class Machine:  # type: ignore\n            pass\n\n        _LIGHTNING_SDK_AVAILABLE = False\n\n\n    @dataclass\n    class Dir:\n        \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\n", "contexts_above": "import datetime\nimport os\nimport re\nimport sys\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom time import sleep\nfrom typing import Optional, Union\nfrom urllib import parse\n\nfrom lightning_cloud.openapi import V1CloudSpace\nfrom lightning_cloud.rest_client import LightningClient\n\n# To avoid adding lightning_utilities as a dependency for now.\ntry:\n    import boto3\n    import botocore\n\n    _BOTO3_AVAILABLE = True\nexcept Exception:\n    _BOTO3_AVAILABLE = False\n\n\ntry:\n    from lightning_sdk import Machine, Studio\n\n    _LIGHTNING_SDK_AVAILABLE = True\nexcept (ImportError, ModuleNotFoundError):\n\n    class Machine:  # type: ignore\n        pass\n\n    _LIGHTNING_SDK_AVAILABLE = False\n\n\n@dataclass\nclass Dir:\n    \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\n    path: Optional[str] = None\n    url: Optional[str] = None\n\n\ndef _resolve_dir(dir_path: Optional[Union[str, Dir]]) -> Dir:\n    if isinstance(dir_path, Dir):\n        return Dir(path=str(dir_path.path) if dir_path.path else None, url=str(dir_path.url) if dir_path.url else None)\n\n    if dir_path is None:\n        return Dir()\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"`dir_path` must be a `Dir` or a string, got: {dir_path}\")\n\n    assert isinstance(dir_path, str)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path)\n\n    if dir_path.startswith(\"local:\"):\n        return Dir(path=None, url=dir_path)\n\n    dir_path = _resolve_time_template(dir_path)\n\n    dir_path_absolute = str(Path(dir_path).absolute().resolve())\n\n    if dir_path_absolute.startswith(\"/teamspace/studios/this_studio\"):\n        return Dir(path=dir_path_absolute, url=None)\n\n    if dir_path_absolute.startswith(\"/.project\"):\n        dir_path_absolute = dir_path\n\n    if dir_path_absolute.startswith(\"/teamspace/studios\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_studio(dir_path_absolute, dir_path_absolute.split(\"/\")[3], None)\n\n    if dir_path_absolute.startswith(\"/teamspace/s3_connections\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_s3_connections(dir_path_absolute)\n\n    if dir_path_absolute.startswith(\"/teamspace/datasets\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_datasets(dir_path_absolute)\n\n    return Dir(path=dir_path_absolute, url=None)\n\n\ndef _match_studio(target_id: Optional[str], target_name: Optional[str], cloudspace: V1CloudSpace) -> bool:\n    if cloudspace.name is not None and target_name is not None and cloudspace.name.lower() == target_name.lower():\n        return True\n\n    if target_id is not None and cloudspace.id == target_id:\n        return True\n\n    if (\n        cloudspace.display_name is not None\n        and target_name is not None\n        and cloudspace.display_name.lower() == target_name.lower()\n    ):\n        return True\n\n    return False\n\n\ndef _resolve_studio(dir_path: str, target_name: Optional[str], target_id: Optional[str]) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    cloudspaces = client.cloud_space_service_list_cloud_spaces(project_id=project_id, cluster_id=cluster_id).cloudspaces\n    target_cloud_space = [cloudspace for cloudspace in cloudspaces if _match_studio(target_id, target_name, cloudspace)]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided name `{target_name}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    bucket_name = target_cluster[0].spec.aws_v1.bucket_name\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{bucket_name}/projects/{project_id}/cloudspaces/{target_cloud_space[0].id}/code/content\",\n            *dir_path.split(\"/\")[4:],\n        ),\n    )\n\n\ndef _resolve_s3_connections(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    target_name = dir_path.split(\"/\")[3]\n\n    data_connections = client.data_connection_service_list_data_connections(project_id).data_connections\n\n    data_connection = [dc for dc in data_connections if dc.name == target_name]\n\n    if not data_connection:\n        raise ValueError(f\"We didn't find any matching data connection with the provided name `{target_name}`.\")\n\n    return Dir(path=dir_path, url=os.path.join(data_connection[0].aws.source, *dir_path.split(\"/\")[4:]))\n\n\ndef _resolve_datasets(dir_path: str) -> Dir:\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    target_cloud_space = [\n        cloudspace\n        for cloudspace in client.cloud_space_service_list_cloud_spaces(\n            project_id=project_id, cluster_id=cluster_id\n        ).cloudspaces\n        if cloudspace.id == cloud_space_id\n    ]\n\n    if not target_cloud_space:\n        raise ValueError(f\"We didn't find any matching Studio for the provided id `{cloud_space_id}`.\")\n\n    target_cluster = [cluster for cluster in clusters if cluster.id == target_cloud_space[0].cluster_id]\n\n    if not target_cluster:\n        raise ValueError(\n            f\"We didn't find a matching cluster associated with the id {target_cloud_space[0].cluster_id}.\"\n        )\n\n    return Dir(\n        path=dir_path,\n        url=os.path.join(\n            f\"s3://{target_cluster[0].spec.aws_v1.bucket_name}/projects/{project_id}/datasets/\",\n            *dir_path.split(\"/\")[3:],\n        ),\n    )\n\n\ndef _assert_dir_is_empty(output_dir: Dir, append: bool = False, overwrite: bool = False) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n\ndef _assert_dir_has_index_file(output_dir: Dir) -> None:\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()\n\n\ndef _get_lightning_cloud_url() -> str:\n    # detect local development\n    if os.getenv(\"VSCODE_PROXY_URI\", \"\").startswith(\"http://localhost:9800\"):\n        return \"http://localhost:9800\"\n    # DO NOT CHANGE!\n    return os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\")\n\n\ndef _resolve_time_template(path: str) -> str:\n    match = re.search(\"^.*{%.*}$\", path)\n    if match is None:\n        return path\n\n    pattern = path.split(\"{\")[1].split(\"}\")[0]\n\n    return path.replace(\"{\" + pattern + \"}\", datetime.datetime.now().strftime(pattern))\n\n\n", "contexts_below": "", "input_code": "def _execute(\n    name: str,\n    num_nodes: int,\n    machine: Optional[Machine] = None,\n    command: Optional[str] = None,\n) -> None:\n\n    \"\"\"\n    This function remotely executes the current operator by creating a data preparation machine job through the Studio API. It continuously checks the job status and prints the job URL when it starts. The function raises exceptions if the required SDK is not available or if the job fails.\n\n    Input-Output Arguments\n    :param name: str, The name of the job to be executed.\n    :param num_nodes: int, The number of instances (nodes) required for the job.\n    :param machine: Optional[Machine], The machine configuration for the job. If not provided, a default machine configuration is fetched.\n    :param command: Optional[str], The command to be executed in the job. If not provided, a default command that includes the current working directory and environment variables is used.\n    :return: No return values.\n    \"\"\"", "reference_steps": "1. Define a function `_execute` that takes parameters `name`, `num_nodes`, `machine`, and `command` with `machine` and `command` being optional.\n2. Check if the `lightning_sdk` module is available; if not, raise a `ModuleNotFoundError`.\n3. Retrieve the environment variables `LIGHTNING_SKIP_INSTALL` and `LIGHTNING_BRANCH`, and format them for inclusion in a command string if they are set.\n4. Create an instance of `Studio` class.\n5. Construct the command to be executed, using the current working directory and any provided or environment-specified arguments.\n6. Create a remote machine job using the `studio._studio_api.create_data_prep_machine_job` method with the constructed command and other provided parameters.\n7. Enter a loop that continuously checks the status of the job by querying the API with the job's ID.\n8. Print the URL of the job to the console the first time the loop runs.\n9. Check the job's status in each loop iteration; if it has failed, raise a `RuntimeError`.\n10. Exit the loop when the job's status is either stopped or completed, indicating the job has finished.", "reference_code": "def _execute(\n    name: str,\n    num_nodes: int,\n    machine: Optional[Machine] = None,\n    command: Optional[str] = None,\n) -> None:\n    \"\"\"Remotely execute the current operator.\"\"\"\n\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The `lightning_sdk` is required.\")\n\n    lightning_skip_install = os.getenv(\"LIGHTNING_SKIP_INSTALL\", \"\")\n    if lightning_skip_install:\n        lightning_skip_install = f\" LIGHTNING_SKIP_INSTALL={lightning_skip_install} \"\n\n    lightning_branch = os.getenv(\"LIGHTNING_BRANCH\", \"\")\n    if lightning_branch:\n        lightning_branch = f\" LIGHTNING_BRANCH={lightning_branch} \"\n\n    studio = Studio()\n    job = studio._studio_api.create_data_prep_machine_job(\n        command or f\"cd {os.getcwd()} &&{lightning_skip_install}{lightning_branch} python {' '.join(sys.argv)}\",\n        name=name,\n        num_instances=num_nodes,\n        studio_id=studio._studio.id,\n        teamspace_id=studio._teamspace.id,\n        cluster_id=studio._studio.cluster_id,\n        machine=machine or studio._studio_api.get_machine(studio._studio.id, studio._teamspace.id),\n    )\n\n    has_printed = False\n\n    while True:\n        curr_job = studio._studio_api._client.lightningapp_instance_service_get_lightningapp_instance(\n            project_id=studio._teamspace.id, id=job.id\n        )\n        if not has_printed:\n            cloud_url = os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\").replace(\":443\", \"\")\n            job_url = f\"{cloud_url}/{studio.owner}/{studio._teamspace.name}\"\n            job_url += f\"/studios/{studio.name}/app?app_id=data-prep&job_name={curr_job.name}\"\n            print(f\"Find your job at {job_url}\")\n            has_printed = True\n\n        if curr_job.status.phase == \"LIGHTNINGAPP_INSTANCE_STATE_FAILED\":\n            raise RuntimeError(f\"job {curr_job.name} failed!\")\n\n        if curr_job.status.phase in [\"LIGHTNINGAPP_INSTANCE_STATE_STOPPED\", \"LIGHTNINGAPP_INSTANCE_STATE_COMPLETED\"]:\n            break\n\n        sleep(1)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "type": "method", "class_name": "PrepareChunksThread", "function_name": "delete", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.reader.PrepareChunksThread._to_delete_queue\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.reader.PrepareChunksThread._to_delete_queue\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport multiprocessing\nimport os\nimport warnings\nfrom logging import Logger\nfrom queue import Empty\nfrom threading import Thread\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.streaming.config import ChunksConfig\nfrom litdata.streaming.item_loader import BaseItemLoader, PyTreeLoader\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer, _get_serializers\nfrom litdata.utilities.env import _DistributedEnv, _WorkerEnv\n\nwarnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    pass\n\n\nlogger = Logger(__name__)\n\n\n_END_TOKEN = \"END\"\n\n# Note: The timeout here should not be too short. We need to prevent the caller from aggressively\n# querying the queue and consuming too many CPU cycles.\n_DEFAULT_TIMEOUT = 0.1\n_LONG_DEFAULT_TIMEOUT = 5\n\n\nclass PrepareChunksThread(Thread):\n    \"\"\"This thread is responsible to download the chunks associated to a given worker.\"\"\"\n\n    def __init__(\n        self,\n        config: ChunksConfig,\n        item_loader: BaseItemLoader,\n        distributed_env: _DistributedEnv,\n        max_cache_size: Optional[int] = None,\n        max_pre_download: int = 2,\n    ) -> None:\n        super().__init__(daemon=True)\n        self._config = config\n        self._item_loader = item_loader\n        self._max_pre_download = max_pre_download\n        self._pre_download_counter = 0\n        self._distributed_env = distributed_env\n\n        self._chunks_index_to_be_deleted: List[int] = []\n        self._max_cache_size = max_cache_size\n        self._parent_cache_dir = os.path.dirname(self._config._cache_dir)\n        self._to_download_queue: multiprocessing.Queue = multiprocessing.Queue()\n        self._to_delete_queue: multiprocessing.Queue = multiprocessing.Queue()\n\n        # Check whether a dataset slice fits on the node\n        num_bytes_per_nodes = self._config.num_bytes // self._distributed_env.num_nodes\n        self._delete_chunks_when_processed = num_bytes_per_nodes > max_cache_size if max_cache_size else False\n        self._has_exited = False\n\n    def download(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n", "contexts_below": "\n    def _delete(self, chunk_index: int) -> None:\n        \"\"\"Inform the item loader of the chunk to delete.\"\"\"\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.delete(chunk_index, chunk_filepath)\n\n    def stop(self) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        self._to_download_queue.put(_END_TOKEN)\n\n    def _maybe_delete_chunks(self) -> None:\n        reached_pre_download = self._pre_download_counter == self._max_pre_download\n\n        # we have already pre-downloaded some chunks, we just need to wait for them to be processed.\n        chunk_index = _get_from_queue(\n            self._to_delete_queue, timeout=_LONG_DEFAULT_TIMEOUT if reached_pre_download else _DEFAULT_TIMEOUT\n        )\n\n        if chunk_index is not None:\n            self._pre_download_counter -= 1\n\n            # Store the current chunk index\n            self._chunks_index_to_be_deleted.append(chunk_index)\n\n        # Get the current cache size and decide whether we need to start cleanup. Otherwise, keep track of it\n        while self._max_cache_size and self._chunks_index_to_be_deleted and self._can_delete_chunk():\n            # Delete the oldest chunk\n            self._delete(self._chunks_index_to_be_deleted.pop(0))\n\n        return\n\n    def _can_delete_chunk(self) -> bool:\n        if self._delete_chunks_when_processed:\n            return self._pre_download_counter >= self._max_pre_download - 1\n        return self._max_cache_size is not None and _get_folder_size(self._parent_cache_dir) >= self._max_cache_size\n\n    def _pre_load_chunk(self, chunk_index: int) -> None:\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.pre_load_chunk(chunk_index, chunk_filepath)\n\n    def run(self) -> None:\n        while True:\n            if self._pre_download_counter < self._max_pre_download:\n                chunk_index = _get_from_queue(self._to_download_queue)\n                if chunk_index == _END_TOKEN:\n                    self._has_exited = True\n                    return\n\n                if chunk_index is not None:\n                    self._config.download_chunk_from_index(chunk_index)\n\n                    # Preload item if possible to gain some time but only\n                    # if this is one of the pre-downloaded chunk\n                    if self._pre_download_counter > 0:\n                        self._pre_load_chunk(chunk_index)\n\n                    # Avoid downloading too many chunks in advance at the risk of over using the disk space\n                    self._pre_download_counter += 1\n\n            if self._max_cache_size:\n                self._maybe_delete_chunks()\n\n\nclass BinaryReader:\n    def __init__(\n        self,\n        cache_dir: str,\n        max_cache_size: Optional[Union[int, str]] = None,\n        remote_input_dir: Optional[str] = None,\n        compression: Optional[str] = None,\n        item_loader: Optional[BaseItemLoader] = None,\n        serializers: Optional[Dict[str, Serializer]] = None,\n    ) -> None:\n        \"\"\"The BinaryReader enables to read chunked dataset in an efficient way.\n\n        Arguments:\n            cache_dir: The path to cache folder.\n            remote_input_dir: The path to a remote folder where the data are located.\n                The scheme needs to be added to the path.\n            compression: The algorithm to decompress the chunks.\n            item_loader: The chunk sampler to create sub arrays from a chunk.\n            max_cache_size: The maximum cache size used by the reader when fetching the chunks.\n            serializers: Provide your own serializers.\n\n        \"\"\"\n        super().__init__()\n        warnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\n        self._cache_dir = cache_dir\n        self._remote_input_dir = remote_input_dir\n\n        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        self._compression = compression\n        self._intervals: Optional[List[str]] = None\n\n        self._serializers: Dict[str, Serializer] = _get_serializers(serializers)\n        self._distributed_env = _DistributedEnv.detect()\n        self._rank: Optional[int] = None\n        self._config: Optional[ChunksConfig] = None\n        self._prepare_thread: Optional[PrepareChunksThread] = None\n        self._item_loader = item_loader or PyTreeLoader()\n        self._last_chunk_index: Optional[int] = None\n        self._max_cache_size = int(os.getenv(\"MAX_CACHE_SIZE\", max_cache_size or 0))\n\n    def _get_chunk_index_from_index(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index(index)  # type: ignore\n\n    def _try_load_config(self) -> Optional[ChunksConfig]:\n        \"\"\"Try to load the chunks config if the index files are available.\"\"\"\n        self._config = ChunksConfig.load(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n        return self._config\n\n    @property\n    def config(self) -> ChunksConfig:\n        if self._config is None:\n            raise RuntimeError(\"The config should be defined.\")\n        return self._config\n\n    @property\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n        if self._rank is None:\n            self._worker_env = _WorkerEnv.detect()\n            self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n        return self._rank\n\n    def read(self, index: ChunkedIndex) -> Any:\n        \"\"\"Read an item for the given from a chunk.\n\n        If the chunk isn't available locally or in memory, it will be downloaded.\n\n        Prefetching should reduce the wait time to be the batch available.\n\n        \"\"\"\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The Reader.read(...) method expects a chunked Index.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._config and (self._config._remote_dir or self._config._compressor):\n            # Create and start the prepare chunks thread\n            if self._prepare_thread is None and self._config:\n                self._prepare_thread = PrepareChunksThread(\n                    self._config, self._item_loader, self._distributed_env, self._max_cache_size\n                )\n                self._prepare_thread.start()\n                if index.chunk_indexes:\n                    self._prepare_thread.download(index.chunk_indexes)\n\n            # If the chunk_index is new, request for it to be downloaded.\n            if index.chunk_index != self._last_chunk_index:\n                assert self._prepare_thread\n                self._prepare_thread.download([index.chunk_index])\n\n            if self._last_chunk_index is None:\n                self._last_chunk_index = index.chunk_index\n\n        # Fetch the element\n        chunk_filepath, begin, _ = self.config[index]\n        item = self._item_loader.load_item_from_chunk(index.index, index.chunk_index, chunk_filepath, begin)\n\n        # We need to request deletion after the latest element has been loaded.\n        # Otherwise, this could trigger segmentation fault error depending on the item loader used.\n        if self._config and self._config._remote_dir and index.chunk_index != self._last_chunk_index:\n            assert self._prepare_thread\n            assert self._last_chunk_index is not None\n\n            # inform the chunk has been completely consumed\n            self._prepare_thread.delete([self._last_chunk_index])\n\n            # track the new chunk index as the latest one\n            self._last_chunk_index = index.chunk_index\n\n        if index.is_last_index and self._prepare_thread:\n            # inform the thread it is time to stop\n            self._prepare_thread.stop()\n            self._prepare_thread = None\n\n        return item\n\n    def get_length(self) -> int:\n        \"\"\"Get the number of samples across all chunks.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return len(self.config)\n\n    def get_chunk_intervals(self) -> List[Tuple[int, int]]:\n        \"\"\"Get the index interval of each chunk.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self.config.intervals\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = self.__dict__.copy()\n        state[\"_prepare_thread\"] = None\n        return state\n\n\ndef _get_folder_size(path: str) -> int:\n    \"\"\"Collect the size of each files within a folder.\n\n    This method is robust to file deletion races\n\n    \"\"\"\n    size = 0\n    for dirpath, _, filenames in os.walk(str(path)):\n        for filename in filenames:\n            with contextlib.suppress(FileNotFoundError):\n                size += os.stat(os.path.join(dirpath, filename)).st_size\n    return size\n\n\ndef _get_from_queue(queue: multiprocessing.Queue, timeout: float = _DEFAULT_TIMEOUT) -> Optional[Any]:\n    try:\n        return queue.get(timeout=timeout)\n    except Empty:\n        pass\n    except OSError as err:\n        # handle closed queue before the thread terminates\n        if \"handle is closed\" in str(err) or \"Bad file descriptor\" in str(err):\n            logger.debug(err)\n        else:\n            raise err\n    except EOFError as err:\n        logger.debug(err)\n    return None\n", "input_code": "    def delete(self, chunk_indexes: List[int]) -> None:\n\n        \"\"\"\n        Deletes the specified chunks by adding their indices to a deletion queue. This is typically used to manage chunks of data that are no longer needed for the current epoch in a threaded environment.\n\n        Input-Output Arguments\n        :param self: PrepareChunksThread. An instance of the PrepareChunksThread class.\n        :param chunk_indexes: List[int], The indices of the chunks to be deleted. These indices are added to a queue for deletion.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method called `delete` within a class that takes `self` and `chunk_indexes` as parameters, where `chunk_indexes` is a list of integers.\n2. The method is designed to handle the deletion of chunks based on their indices for a given epoch.\n3. Iterate over each `chunk_index` in the `chunk_indexes` list.\n4. For each `chunk_index`, add it to a queue (`_to_delete_queue`) that is presumably an attribute of the class instance.\n5. The queue is used to store the indices of chunks that are scheduled for deletion.\n6. The method does not return any value (`None` is implied by the absence of a return statement).", "reference_code": "def delete(self, chunk_indexes: List[int]) -> None:\n    \"\"\"Receive the list of the chunk indices to delete for the current epoch.\"\"\"\n    for chunk_index in chunk_indexes:\n        self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "type": "method", "class_name": "BinaryReader", "function_name": "_try_load_config", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.reader.BinaryReader._cache_dir\n\nlitdata.streaming.reader.BinaryReader._config\n\nlitdata.streaming.reader.BinaryReader._item_loader\n\nlitdata.streaming.reader.BinaryReader._remote_input_dir\n\nlitdata.streaming.reader.BinaryReader._serializers\n\n# Cross-file Dependency:\nlitdata.streaming.config.ChunksConfig\n    class ChunksConfig:\n\nlitdata.streaming.config.ChunksConfig.load\n    def load(\n        cls,\n        cache_dir: str,\n        serializers: Dict[str, Serializer],\n        remote_dir: Optional[str] = None,\n        item_loader: Optional[BaseItemLoader] = None,\n    ) -> Optional[\"ChunksConfig\"]:\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.reader.BinaryReader._serializers\n\nlitdata.streaming.reader.BinaryReader._item_loader\n\n# Cross-file Dependency:\nlitdata.streaming.config.ChunksConfig\n    class ChunksConfig:\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport multiprocessing\nimport os\nimport warnings\nfrom logging import Logger\nfrom queue import Empty\nfrom threading import Thread\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.streaming.config import ChunksConfig\nfrom litdata.streaming.item_loader import BaseItemLoader, PyTreeLoader\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer, _get_serializers\nfrom litdata.utilities.env import _DistributedEnv, _WorkerEnv\n\nwarnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    pass\n\n\nlogger = Logger(__name__)\n\n\n_END_TOKEN = \"END\"\n\n# Note: The timeout here should not be too short. We need to prevent the caller from aggressively\n# querying the queue and consuming too many CPU cycles.\n_DEFAULT_TIMEOUT = 0.1\n_LONG_DEFAULT_TIMEOUT = 5\n\n\nclass PrepareChunksThread(Thread):\n    \"\"\"This thread is responsible to download the chunks associated to a given worker.\"\"\"\n\n    def __init__(\n        self,\n        config: ChunksConfig,\n        item_loader: BaseItemLoader,\n        distributed_env: _DistributedEnv,\n        max_cache_size: Optional[int] = None,\n        max_pre_download: int = 2,\n    ) -> None:\n        super().__init__(daemon=True)\n        self._config = config\n        self._item_loader = item_loader\n        self._max_pre_download = max_pre_download\n        self._pre_download_counter = 0\n        self._distributed_env = distributed_env\n\n        self._chunks_index_to_be_deleted: List[int] = []\n        self._max_cache_size = max_cache_size\n        self._parent_cache_dir = os.path.dirname(self._config._cache_dir)\n        self._to_download_queue: multiprocessing.Queue = multiprocessing.Queue()\n        self._to_delete_queue: multiprocessing.Queue = multiprocessing.Queue()\n\n        # Check whether a dataset slice fits on the node\n        num_bytes_per_nodes = self._config.num_bytes // self._distributed_env.num_nodes\n        self._delete_chunks_when_processed = num_bytes_per_nodes > max_cache_size if max_cache_size else False\n        self._has_exited = False\n\n    def download(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    def delete(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to delete for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    def _delete(self, chunk_index: int) -> None:\n        \"\"\"Inform the item loader of the chunk to delete.\"\"\"\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.delete(chunk_index, chunk_filepath)\n\n    def stop(self) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        self._to_download_queue.put(_END_TOKEN)\n\n    def _maybe_delete_chunks(self) -> None:\n        reached_pre_download = self._pre_download_counter == self._max_pre_download\n\n        # we have already pre-downloaded some chunks, we just need to wait for them to be processed.\n        chunk_index = _get_from_queue(\n            self._to_delete_queue, timeout=_LONG_DEFAULT_TIMEOUT if reached_pre_download else _DEFAULT_TIMEOUT\n        )\n\n        if chunk_index is not None:\n            self._pre_download_counter -= 1\n\n            # Store the current chunk index\n            self._chunks_index_to_be_deleted.append(chunk_index)\n\n        # Get the current cache size and decide whether we need to start cleanup. Otherwise, keep track of it\n        while self._max_cache_size and self._chunks_index_to_be_deleted and self._can_delete_chunk():\n            # Delete the oldest chunk\n            self._delete(self._chunks_index_to_be_deleted.pop(0))\n\n        return\n\n    def _can_delete_chunk(self) -> bool:\n        if self._delete_chunks_when_processed:\n            return self._pre_download_counter >= self._max_pre_download - 1\n        return self._max_cache_size is not None and _get_folder_size(self._parent_cache_dir) >= self._max_cache_size\n\n    def _pre_load_chunk(self, chunk_index: int) -> None:\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.pre_load_chunk(chunk_index, chunk_filepath)\n\n    def run(self) -> None:\n        while True:\n            if self._pre_download_counter < self._max_pre_download:\n                chunk_index = _get_from_queue(self._to_download_queue)\n                if chunk_index == _END_TOKEN:\n                    self._has_exited = True\n                    return\n\n                if chunk_index is not None:\n                    self._config.download_chunk_from_index(chunk_index)\n\n                    # Preload item if possible to gain some time but only\n                    # if this is one of the pre-downloaded chunk\n                    if self._pre_download_counter > 0:\n                        self._pre_load_chunk(chunk_index)\n\n                    # Avoid downloading too many chunks in advance at the risk of over using the disk space\n                    self._pre_download_counter += 1\n\n            if self._max_cache_size:\n                self._maybe_delete_chunks()\n\n\nclass BinaryReader:\n    def __init__(\n        self,\n        cache_dir: str,\n        max_cache_size: Optional[Union[int, str]] = None,\n        remote_input_dir: Optional[str] = None,\n        compression: Optional[str] = None,\n        item_loader: Optional[BaseItemLoader] = None,\n        serializers: Optional[Dict[str, Serializer]] = None,\n    ) -> None:\n        \"\"\"The BinaryReader enables to read chunked dataset in an efficient way.\n\n        Arguments:\n            cache_dir: The path to cache folder.\n            remote_input_dir: The path to a remote folder where the data are located.\n                The scheme needs to be added to the path.\n            compression: The algorithm to decompress the chunks.\n            item_loader: The chunk sampler to create sub arrays from a chunk.\n            max_cache_size: The maximum cache size used by the reader when fetching the chunks.\n            serializers: Provide your own serializers.\n\n        \"\"\"\n        super().__init__()\n        warnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\n        self._cache_dir = cache_dir\n        self._remote_input_dir = remote_input_dir\n\n        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        self._compression = compression\n        self._intervals: Optional[List[str]] = None\n\n        self._serializers: Dict[str, Serializer] = _get_serializers(serializers)\n        self._distributed_env = _DistributedEnv.detect()\n        self._rank: Optional[int] = None\n        self._config: Optional[ChunksConfig] = None\n        self._prepare_thread: Optional[PrepareChunksThread] = None\n        self._item_loader = item_loader or PyTreeLoader()\n        self._last_chunk_index: Optional[int] = None\n        self._max_cache_size = int(os.getenv(\"MAX_CACHE_SIZE\", max_cache_size or 0))\n\n    def _get_chunk_index_from_index(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index(index)  # type: ignore\n\n", "contexts_below": "\n    @property\n    def config(self) -> ChunksConfig:\n        if self._config is None:\n            raise RuntimeError(\"The config should be defined.\")\n        return self._config\n\n    @property\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n        if self._rank is None:\n            self._worker_env = _WorkerEnv.detect()\n            self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n        return self._rank\n\n    def read(self, index: ChunkedIndex) -> Any:\n        \"\"\"Read an item for the given from a chunk.\n\n        If the chunk isn't available locally or in memory, it will be downloaded.\n\n        Prefetching should reduce the wait time to be the batch available.\n\n        \"\"\"\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The Reader.read(...) method expects a chunked Index.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._config and (self._config._remote_dir or self._config._compressor):\n            # Create and start the prepare chunks thread\n            if self._prepare_thread is None and self._config:\n                self._prepare_thread = PrepareChunksThread(\n                    self._config, self._item_loader, self._distributed_env, self._max_cache_size\n                )\n                self._prepare_thread.start()\n                if index.chunk_indexes:\n                    self._prepare_thread.download(index.chunk_indexes)\n\n            # If the chunk_index is new, request for it to be downloaded.\n            if index.chunk_index != self._last_chunk_index:\n                assert self._prepare_thread\n                self._prepare_thread.download([index.chunk_index])\n\n            if self._last_chunk_index is None:\n                self._last_chunk_index = index.chunk_index\n\n        # Fetch the element\n        chunk_filepath, begin, _ = self.config[index]\n        item = self._item_loader.load_item_from_chunk(index.index, index.chunk_index, chunk_filepath, begin)\n\n        # We need to request deletion after the latest element has been loaded.\n        # Otherwise, this could trigger segmentation fault error depending on the item loader used.\n        if self._config and self._config._remote_dir and index.chunk_index != self._last_chunk_index:\n            assert self._prepare_thread\n            assert self._last_chunk_index is not None\n\n            # inform the chunk has been completely consumed\n            self._prepare_thread.delete([self._last_chunk_index])\n\n            # track the new chunk index as the latest one\n            self._last_chunk_index = index.chunk_index\n\n        if index.is_last_index and self._prepare_thread:\n            # inform the thread it is time to stop\n            self._prepare_thread.stop()\n            self._prepare_thread = None\n\n        return item\n\n    def get_length(self) -> int:\n        \"\"\"Get the number of samples across all chunks.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return len(self.config)\n\n    def get_chunk_intervals(self) -> List[Tuple[int, int]]:\n        \"\"\"Get the index interval of each chunk.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self.config.intervals\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = self.__dict__.copy()\n        state[\"_prepare_thread\"] = None\n        return state\n\n\ndef _get_folder_size(path: str) -> int:\n    \"\"\"Collect the size of each files within a folder.\n\n    This method is robust to file deletion races\n\n    \"\"\"\n    size = 0\n    for dirpath, _, filenames in os.walk(str(path)):\n        for filename in filenames:\n            with contextlib.suppress(FileNotFoundError):\n                size += os.stat(os.path.join(dirpath, filename)).st_size\n    return size\n\n\ndef _get_from_queue(queue: multiprocessing.Queue, timeout: float = _DEFAULT_TIMEOUT) -> Optional[Any]:\n    try:\n        return queue.get(timeout=timeout)\n    except Empty:\n        pass\n    except OSError as err:\n        # handle closed queue before the thread terminates\n        if \"handle is closed\" in str(err) or \"Bad file descriptor\" in str(err):\n            logger.debug(err)\n        else:\n            raise err\n    except EOFError as err:\n        logger.debug(err)\n    return None\n", "input_code": "    def _try_load_config(self) -> Optional[ChunksConfig]:\n\n        \"\"\"\n        This function attempts to load the chunks configuration for a BinaryReader instance if the index files are available. It updates the instance's configuration with the loaded ChunksConfig object.\n        Input-Output Arguments\n        :param self: BinaryReader. An instance of the BinaryReader class. It uses its attributes such as _cache_dir, _serializers, _remote_input_dir, and _item_loader to attempt to load the configuration.\n        :return: Optional[ChunksConfig]. The loaded ChunksConfig object if the configuration could be successfully loaded, otherwise None.\n        \"\"\"", "reference_steps": "1. Define a method `_try_load_config` that attempts to load a configuration for chunks.\n2. The method returns an instance of `ChunksConfig` or `None`.\n3. Inside the method, set an instance variable `_config` to the result of the `load` method of `ChunksConfig`.\n4. Pass the following parameters to the `load` method: `_cache_dir`, `_serializers`, `_remote_input_dir`, and `_item_loader`.\n5. The `load` method is likely responsible for reading the configuration from the cache directory and initializing it with the given serializers, remote input directory, and item loader.\n6. The method returns the loaded configuration stored in `_config`.\n7. The return type of the method is hinted to be `Optional[ChunksConfig]`, which means it can return a `ChunksConfig` object or `None` if the configuration cannot be loaded.\n8. The method is a part of a class (implied by the use of `self`) that manages chunk configurations, possibly for a caching system or data processing pipeline.\n9. The method does not take any parameters besides `self`, indicating that all necessary data is already present within the object's state.\n10. The method is intended to be called internally within the class, as suggested by the leading underscore in its name, which is a common convention in Python to denote private methods.", "reference_code": "def _try_load_config(self) -> Optional[ChunksConfig]:\n    \"\"\"Try to load the chunks config if the index files are available.\"\"\"\n    self._config = ChunksConfig.load(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n    return self._config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "type": "method", "class_name": "PrepareChunksThread", "function_name": "download", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.reader.PrepareChunksThread._to_download_queue\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.reader.PrepareChunksThread._to_download_queue\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport multiprocessing\nimport os\nimport warnings\nfrom logging import Logger\nfrom queue import Empty\nfrom threading import Thread\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.streaming.config import ChunksConfig\nfrom litdata.streaming.item_loader import BaseItemLoader, PyTreeLoader\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer, _get_serializers\nfrom litdata.utilities.env import _DistributedEnv, _WorkerEnv\n\nwarnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    pass\n\n\nlogger = Logger(__name__)\n\n\n_END_TOKEN = \"END\"\n\n# Note: The timeout here should not be too short. We need to prevent the caller from aggressively\n# querying the queue and consuming too many CPU cycles.\n_DEFAULT_TIMEOUT = 0.1\n_LONG_DEFAULT_TIMEOUT = 5\n\n\nclass PrepareChunksThread(Thread):\n    \"\"\"This thread is responsible to download the chunks associated to a given worker.\"\"\"\n\n    def __init__(\n        self,\n        config: ChunksConfig,\n        item_loader: BaseItemLoader,\n        distributed_env: _DistributedEnv,\n        max_cache_size: Optional[int] = None,\n        max_pre_download: int = 2,\n    ) -> None:\n        super().__init__(daemon=True)\n        self._config = config\n        self._item_loader = item_loader\n        self._max_pre_download = max_pre_download\n        self._pre_download_counter = 0\n        self._distributed_env = distributed_env\n\n        self._chunks_index_to_be_deleted: List[int] = []\n        self._max_cache_size = max_cache_size\n        self._parent_cache_dir = os.path.dirname(self._config._cache_dir)\n        self._to_download_queue: multiprocessing.Queue = multiprocessing.Queue()\n        self._to_delete_queue: multiprocessing.Queue = multiprocessing.Queue()\n\n        # Check whether a dataset slice fits on the node\n        num_bytes_per_nodes = self._config.num_bytes // self._distributed_env.num_nodes\n        self._delete_chunks_when_processed = num_bytes_per_nodes > max_cache_size if max_cache_size else False\n        self._has_exited = False\n\n", "contexts_below": "\n    def delete(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to delete for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    def _delete(self, chunk_index: int) -> None:\n        \"\"\"Inform the item loader of the chunk to delete.\"\"\"\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.delete(chunk_index, chunk_filepath)\n\n    def stop(self) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        self._to_download_queue.put(_END_TOKEN)\n\n    def _maybe_delete_chunks(self) -> None:\n        reached_pre_download = self._pre_download_counter == self._max_pre_download\n\n        # we have already pre-downloaded some chunks, we just need to wait for them to be processed.\n        chunk_index = _get_from_queue(\n            self._to_delete_queue, timeout=_LONG_DEFAULT_TIMEOUT if reached_pre_download else _DEFAULT_TIMEOUT\n        )\n\n        if chunk_index is not None:\n            self._pre_download_counter -= 1\n\n            # Store the current chunk index\n            self._chunks_index_to_be_deleted.append(chunk_index)\n\n        # Get the current cache size and decide whether we need to start cleanup. Otherwise, keep track of it\n        while self._max_cache_size and self._chunks_index_to_be_deleted and self._can_delete_chunk():\n            # Delete the oldest chunk\n            self._delete(self._chunks_index_to_be_deleted.pop(0))\n\n        return\n\n    def _can_delete_chunk(self) -> bool:\n        if self._delete_chunks_when_processed:\n            return self._pre_download_counter >= self._max_pre_download - 1\n        return self._max_cache_size is not None and _get_folder_size(self._parent_cache_dir) >= self._max_cache_size\n\n    def _pre_load_chunk(self, chunk_index: int) -> None:\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.pre_load_chunk(chunk_index, chunk_filepath)\n\n    def run(self) -> None:\n        while True:\n            if self._pre_download_counter < self._max_pre_download:\n                chunk_index = _get_from_queue(self._to_download_queue)\n                if chunk_index == _END_TOKEN:\n                    self._has_exited = True\n                    return\n\n                if chunk_index is not None:\n                    self._config.download_chunk_from_index(chunk_index)\n\n                    # Preload item if possible to gain some time but only\n                    # if this is one of the pre-downloaded chunk\n                    if self._pre_download_counter > 0:\n                        self._pre_load_chunk(chunk_index)\n\n                    # Avoid downloading too many chunks in advance at the risk of over using the disk space\n                    self._pre_download_counter += 1\n\n            if self._max_cache_size:\n                self._maybe_delete_chunks()\n\n\nclass BinaryReader:\n    def __init__(\n        self,\n        cache_dir: str,\n        max_cache_size: Optional[Union[int, str]] = None,\n        remote_input_dir: Optional[str] = None,\n        compression: Optional[str] = None,\n        item_loader: Optional[BaseItemLoader] = None,\n        serializers: Optional[Dict[str, Serializer]] = None,\n    ) -> None:\n        \"\"\"The BinaryReader enables to read chunked dataset in an efficient way.\n\n        Arguments:\n            cache_dir: The path to cache folder.\n            remote_input_dir: The path to a remote folder where the data are located.\n                The scheme needs to be added to the path.\n            compression: The algorithm to decompress the chunks.\n            item_loader: The chunk sampler to create sub arrays from a chunk.\n            max_cache_size: The maximum cache size used by the reader when fetching the chunks.\n            serializers: Provide your own serializers.\n\n        \"\"\"\n        super().__init__()\n        warnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\n        self._cache_dir = cache_dir\n        self._remote_input_dir = remote_input_dir\n\n        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        self._compression = compression\n        self._intervals: Optional[List[str]] = None\n\n        self._serializers: Dict[str, Serializer] = _get_serializers(serializers)\n        self._distributed_env = _DistributedEnv.detect()\n        self._rank: Optional[int] = None\n        self._config: Optional[ChunksConfig] = None\n        self._prepare_thread: Optional[PrepareChunksThread] = None\n        self._item_loader = item_loader or PyTreeLoader()\n        self._last_chunk_index: Optional[int] = None\n        self._max_cache_size = int(os.getenv(\"MAX_CACHE_SIZE\", max_cache_size or 0))\n\n    def _get_chunk_index_from_index(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index(index)  # type: ignore\n\n    def _try_load_config(self) -> Optional[ChunksConfig]:\n        \"\"\"Try to load the chunks config if the index files are available.\"\"\"\n        self._config = ChunksConfig.load(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n        return self._config\n\n    @property\n    def config(self) -> ChunksConfig:\n        if self._config is None:\n            raise RuntimeError(\"The config should be defined.\")\n        return self._config\n\n    @property\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n        if self._rank is None:\n            self._worker_env = _WorkerEnv.detect()\n            self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n        return self._rank\n\n    def read(self, index: ChunkedIndex) -> Any:\n        \"\"\"Read an item for the given from a chunk.\n\n        If the chunk isn't available locally or in memory, it will be downloaded.\n\n        Prefetching should reduce the wait time to be the batch available.\n\n        \"\"\"\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The Reader.read(...) method expects a chunked Index.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._config and (self._config._remote_dir or self._config._compressor):\n            # Create and start the prepare chunks thread\n            if self._prepare_thread is None and self._config:\n                self._prepare_thread = PrepareChunksThread(\n                    self._config, self._item_loader, self._distributed_env, self._max_cache_size\n                )\n                self._prepare_thread.start()\n                if index.chunk_indexes:\n                    self._prepare_thread.download(index.chunk_indexes)\n\n            # If the chunk_index is new, request for it to be downloaded.\n            if index.chunk_index != self._last_chunk_index:\n                assert self._prepare_thread\n                self._prepare_thread.download([index.chunk_index])\n\n            if self._last_chunk_index is None:\n                self._last_chunk_index = index.chunk_index\n\n        # Fetch the element\n        chunk_filepath, begin, _ = self.config[index]\n        item = self._item_loader.load_item_from_chunk(index.index, index.chunk_index, chunk_filepath, begin)\n\n        # We need to request deletion after the latest element has been loaded.\n        # Otherwise, this could trigger segmentation fault error depending on the item loader used.\n        if self._config and self._config._remote_dir and index.chunk_index != self._last_chunk_index:\n            assert self._prepare_thread\n            assert self._last_chunk_index is not None\n\n            # inform the chunk has been completely consumed\n            self._prepare_thread.delete([self._last_chunk_index])\n\n            # track the new chunk index as the latest one\n            self._last_chunk_index = index.chunk_index\n\n        if index.is_last_index and self._prepare_thread:\n            # inform the thread it is time to stop\n            self._prepare_thread.stop()\n            self._prepare_thread = None\n\n        return item\n\n    def get_length(self) -> int:\n        \"\"\"Get the number of samples across all chunks.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return len(self.config)\n\n    def get_chunk_intervals(self) -> List[Tuple[int, int]]:\n        \"\"\"Get the index interval of each chunk.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self.config.intervals\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = self.__dict__.copy()\n        state[\"_prepare_thread\"] = None\n        return state\n\n\ndef _get_folder_size(path: str) -> int:\n    \"\"\"Collect the size of each files within a folder.\n\n    This method is robust to file deletion races\n\n    \"\"\"\n    size = 0\n    for dirpath, _, filenames in os.walk(str(path)):\n        for filename in filenames:\n            with contextlib.suppress(FileNotFoundError):\n                size += os.stat(os.path.join(dirpath, filename)).st_size\n    return size\n\n\ndef _get_from_queue(queue: multiprocessing.Queue, timeout: float = _DEFAULT_TIMEOUT) -> Optional[Any]:\n    try:\n        return queue.get(timeout=timeout)\n    except Empty:\n        pass\n    except OSError as err:\n        # handle closed queue before the thread terminates\n        if \"handle is closed\" in str(err) or \"Bad file descriptor\" in str(err):\n            logger.debug(err)\n        else:\n            raise err\n    except EOFError as err:\n        logger.debug(err)\n    return None\n", "input_code": "    def download(self, chunk_indexes: List[int]) -> None:\n\n        \"\"\"\n        The download function takes a list of chunk indices and enqueues them into a download queue for processing. This is typically used to prepare chunks of data for downloading in a multi-threaded or asynchronous environment, where each chunk is identified by its index.\n\n        Input-Output Arguments\n        :param self: PrepareChunksThread. An instance of the PrepareChunksThread class, which should have access to a queue attribute for storing the chunk indices to be downloaded.\n        :param chunk_indexes: List[int]. A list of integers where each integer represents the index of a chunk that needs to be downloaded. These indices are used to identify and enqueue the specific chunks for downloading.\n        :return: No return values. This method modifies the state of the _to_download_queue attribute by adding the chunk indices to it but does not return any value.\n        \"\"\"", "reference_steps": "1. Define a method `download` that accepts `self` and a list of integers `chunk_indexes` as parameters.\n2. The method is intended to handle the list of chunk indices that need to be downloaded for the current epoch.\n3. Iterate over each `chunk_index` in the `chunk_indexes` list.\n4. For each `chunk_index`, place it into a queue `self._to_download_queue` for processing.\n5. The queue is likely used for managing download tasks asynchronously or in parallel.", "reference_code": "def download(self, chunk_indexes: List[int]) -> None:\n    \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n    for chunk_index in chunk_indexes:\n        self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "type": "method", "class_name": "BinaryReader", "function_name": "config", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.reader.BinaryReader._config\n\n# Cross-file Dependency:\nlitdata.streaming.config.ChunksConfig\n    class ChunksConfig:\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.reader.BinaryReader._config\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport multiprocessing\nimport os\nimport warnings\nfrom logging import Logger\nfrom queue import Empty\nfrom threading import Thread\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.streaming.config import ChunksConfig\nfrom litdata.streaming.item_loader import BaseItemLoader, PyTreeLoader\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer, _get_serializers\nfrom litdata.utilities.env import _DistributedEnv, _WorkerEnv\n\nwarnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    pass\n\n\nlogger = Logger(__name__)\n\n\n_END_TOKEN = \"END\"\n\n# Note: The timeout here should not be too short. We need to prevent the caller from aggressively\n# querying the queue and consuming too many CPU cycles.\n_DEFAULT_TIMEOUT = 0.1\n_LONG_DEFAULT_TIMEOUT = 5\n\n\nclass PrepareChunksThread(Thread):\n    \"\"\"This thread is responsible to download the chunks associated to a given worker.\"\"\"\n\n    def __init__(\n        self,\n        config: ChunksConfig,\n        item_loader: BaseItemLoader,\n        distributed_env: _DistributedEnv,\n        max_cache_size: Optional[int] = None,\n        max_pre_download: int = 2,\n    ) -> None:\n        super().__init__(daemon=True)\n        self._config = config\n        self._item_loader = item_loader\n        self._max_pre_download = max_pre_download\n        self._pre_download_counter = 0\n        self._distributed_env = distributed_env\n\n        self._chunks_index_to_be_deleted: List[int] = []\n        self._max_cache_size = max_cache_size\n        self._parent_cache_dir = os.path.dirname(self._config._cache_dir)\n        self._to_download_queue: multiprocessing.Queue = multiprocessing.Queue()\n        self._to_delete_queue: multiprocessing.Queue = multiprocessing.Queue()\n\n        # Check whether a dataset slice fits on the node\n        num_bytes_per_nodes = self._config.num_bytes // self._distributed_env.num_nodes\n        self._delete_chunks_when_processed = num_bytes_per_nodes > max_cache_size if max_cache_size else False\n        self._has_exited = False\n\n    def download(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    def delete(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to delete for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    def _delete(self, chunk_index: int) -> None:\n        \"\"\"Inform the item loader of the chunk to delete.\"\"\"\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.delete(chunk_index, chunk_filepath)\n\n    def stop(self) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        self._to_download_queue.put(_END_TOKEN)\n\n    def _maybe_delete_chunks(self) -> None:\n        reached_pre_download = self._pre_download_counter == self._max_pre_download\n\n        # we have already pre-downloaded some chunks, we just need to wait for them to be processed.\n        chunk_index = _get_from_queue(\n            self._to_delete_queue, timeout=_LONG_DEFAULT_TIMEOUT if reached_pre_download else _DEFAULT_TIMEOUT\n        )\n\n        if chunk_index is not None:\n            self._pre_download_counter -= 1\n\n            # Store the current chunk index\n            self._chunks_index_to_be_deleted.append(chunk_index)\n\n        # Get the current cache size and decide whether we need to start cleanup. Otherwise, keep track of it\n        while self._max_cache_size and self._chunks_index_to_be_deleted and self._can_delete_chunk():\n            # Delete the oldest chunk\n            self._delete(self._chunks_index_to_be_deleted.pop(0))\n\n        return\n\n    def _can_delete_chunk(self) -> bool:\n        if self._delete_chunks_when_processed:\n            return self._pre_download_counter >= self._max_pre_download - 1\n        return self._max_cache_size is not None and _get_folder_size(self._parent_cache_dir) >= self._max_cache_size\n\n    def _pre_load_chunk(self, chunk_index: int) -> None:\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.pre_load_chunk(chunk_index, chunk_filepath)\n\n    def run(self) -> None:\n        while True:\n            if self._pre_download_counter < self._max_pre_download:\n                chunk_index = _get_from_queue(self._to_download_queue)\n                if chunk_index == _END_TOKEN:\n                    self._has_exited = True\n                    return\n\n                if chunk_index is not None:\n                    self._config.download_chunk_from_index(chunk_index)\n\n                    # Preload item if possible to gain some time but only\n                    # if this is one of the pre-downloaded chunk\n                    if self._pre_download_counter > 0:\n                        self._pre_load_chunk(chunk_index)\n\n                    # Avoid downloading too many chunks in advance at the risk of over using the disk space\n                    self._pre_download_counter += 1\n\n            if self._max_cache_size:\n                self._maybe_delete_chunks()\n\n\nclass BinaryReader:\n    def __init__(\n        self,\n        cache_dir: str,\n        max_cache_size: Optional[Union[int, str]] = None,\n        remote_input_dir: Optional[str] = None,\n        compression: Optional[str] = None,\n        item_loader: Optional[BaseItemLoader] = None,\n        serializers: Optional[Dict[str, Serializer]] = None,\n    ) -> None:\n        \"\"\"The BinaryReader enables to read chunked dataset in an efficient way.\n\n        Arguments:\n            cache_dir: The path to cache folder.\n            remote_input_dir: The path to a remote folder where the data are located.\n                The scheme needs to be added to the path.\n            compression: The algorithm to decompress the chunks.\n            item_loader: The chunk sampler to create sub arrays from a chunk.\n            max_cache_size: The maximum cache size used by the reader when fetching the chunks.\n            serializers: Provide your own serializers.\n\n        \"\"\"\n        super().__init__()\n        warnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\n        self._cache_dir = cache_dir\n        self._remote_input_dir = remote_input_dir\n\n        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        self._compression = compression\n        self._intervals: Optional[List[str]] = None\n\n        self._serializers: Dict[str, Serializer] = _get_serializers(serializers)\n        self._distributed_env = _DistributedEnv.detect()\n        self._rank: Optional[int] = None\n        self._config: Optional[ChunksConfig] = None\n        self._prepare_thread: Optional[PrepareChunksThread] = None\n        self._item_loader = item_loader or PyTreeLoader()\n        self._last_chunk_index: Optional[int] = None\n        self._max_cache_size = int(os.getenv(\"MAX_CACHE_SIZE\", max_cache_size or 0))\n\n    def _get_chunk_index_from_index(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index(index)  # type: ignore\n\n    def _try_load_config(self) -> Optional[ChunksConfig]:\n        \"\"\"Try to load the chunks config if the index files are available.\"\"\"\n        self._config = ChunksConfig.load(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n        return self._config\n\n    @property\n", "contexts_below": "\n    @property\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n        if self._rank is None:\n            self._worker_env = _WorkerEnv.detect()\n            self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n        return self._rank\n\n    def read(self, index: ChunkedIndex) -> Any:\n        \"\"\"Read an item for the given from a chunk.\n\n        If the chunk isn't available locally or in memory, it will be downloaded.\n\n        Prefetching should reduce the wait time to be the batch available.\n\n        \"\"\"\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The Reader.read(...) method expects a chunked Index.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._config and (self._config._remote_dir or self._config._compressor):\n            # Create and start the prepare chunks thread\n            if self._prepare_thread is None and self._config:\n                self._prepare_thread = PrepareChunksThread(\n                    self._config, self._item_loader, self._distributed_env, self._max_cache_size\n                )\n                self._prepare_thread.start()\n                if index.chunk_indexes:\n                    self._prepare_thread.download(index.chunk_indexes)\n\n            # If the chunk_index is new, request for it to be downloaded.\n            if index.chunk_index != self._last_chunk_index:\n                assert self._prepare_thread\n                self._prepare_thread.download([index.chunk_index])\n\n            if self._last_chunk_index is None:\n                self._last_chunk_index = index.chunk_index\n\n        # Fetch the element\n        chunk_filepath, begin, _ = self.config[index]\n        item = self._item_loader.load_item_from_chunk(index.index, index.chunk_index, chunk_filepath, begin)\n\n        # We need to request deletion after the latest element has been loaded.\n        # Otherwise, this could trigger segmentation fault error depending on the item loader used.\n        if self._config and self._config._remote_dir and index.chunk_index != self._last_chunk_index:\n            assert self._prepare_thread\n            assert self._last_chunk_index is not None\n\n            # inform the chunk has been completely consumed\n            self._prepare_thread.delete([self._last_chunk_index])\n\n            # track the new chunk index as the latest one\n            self._last_chunk_index = index.chunk_index\n\n        if index.is_last_index and self._prepare_thread:\n            # inform the thread it is time to stop\n            self._prepare_thread.stop()\n            self._prepare_thread = None\n\n        return item\n\n    def get_length(self) -> int:\n        \"\"\"Get the number of samples across all chunks.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return len(self.config)\n\n    def get_chunk_intervals(self) -> List[Tuple[int, int]]:\n        \"\"\"Get the index interval of each chunk.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self.config.intervals\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = self.__dict__.copy()\n        state[\"_prepare_thread\"] = None\n        return state\n\n\ndef _get_folder_size(path: str) -> int:\n    \"\"\"Collect the size of each files within a folder.\n\n    This method is robust to file deletion races\n\n    \"\"\"\n    size = 0\n    for dirpath, _, filenames in os.walk(str(path)):\n        for filename in filenames:\n            with contextlib.suppress(FileNotFoundError):\n                size += os.stat(os.path.join(dirpath, filename)).st_size\n    return size\n\n\ndef _get_from_queue(queue: multiprocessing.Queue, timeout: float = _DEFAULT_TIMEOUT) -> Optional[Any]:\n    try:\n        return queue.get(timeout=timeout)\n    except Empty:\n        pass\n    except OSError as err:\n        # handle closed queue before the thread terminates\n        if \"handle is closed\" in str(err) or \"Bad file descriptor\" in str(err):\n            logger.debug(err)\n        else:\n            raise err\n    except EOFError as err:\n        logger.debug(err)\n    return None\n", "input_code": "    def config(self) -> ChunksConfig:\n\n        \"\"\"\n        This function returns the configuration for a BinaryReader instance. If the configuration is not set (None), it raises a RuntimeError indicating that the configuration should be defined before accessing it.\n        Input-Output Arguments\n        :param self: BinaryReader. An instance of the BinaryReader class. It uses the _config attribute to determine if the configuration is set.\n        :return: ChunksConfig. The configuration of the BinaryReader instance.\n        \"\"\"", "reference_steps": "1. Define a method named `config` in a class that returns an instance of `ChunksConfig`.\n2. Check if the instance variable `_config` is `None`.\n3. If `_config` is `None`, raise a `RuntimeError` with the message \"The config should be defined.\"\n4. If `_config` is not `None`, return the value of `_config`.", "reference_code": "def config(self) -> ChunksConfig:\n    if self._config is None:\n        raise RuntimeError(\"The config should be defined.\")\n    return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "type": "method", "class_name": "BinaryReader", "function_name": "read", "dependency_all": "# Intra-class Dependency:\nlitdata.streaming.reader.BinaryReader._config\n\nlitdata.streaming.reader.BinaryReader._distributed_env\n\nlitdata.streaming.reader.BinaryReader._item_loader\n\nlitdata.streaming.reader.BinaryReader._last_chunk_index\n\nlitdata.streaming.reader.BinaryReader._max_cache_size\n\nlitdata.streaming.reader.BinaryReader._prepare_thread\n\nlitdata.streaming.reader.BinaryReader._try_load_config\n    def _try_load_config(self) -> Optional[ChunksConfig]:\n        \"\"\"Try to load the chunks config if the index files are available.\"\"\"\n\nlitdata.streaming.reader.BinaryReader.config\n    def config(self) -> ChunksConfig:\n\n# Intra-file Dependency:\nlitdata.streaming.reader.PrepareChunksThread\n    class PrepareChunksThread(Thread):\n        \"\"\"This thread is responsible to download the chunks associated to a given worker.\"\"\"\n\nlitdata.streaming.reader.PrepareChunksThread.__init__\n    def __init__(\n        self,\n        config: ChunksConfig,\n        item_loader: BaseItemLoader,\n        distributed_env: _DistributedEnv,\n        max_cache_size: Optional[int] = None,\n        max_pre_download: int = 2,\n    ) -> None:\n\nlitdata.streaming.reader.PrepareChunksThread.delete\n    def delete(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to delete for the current epoch.\"\"\"\n\nlitdata.streaming.reader.PrepareChunksThread.download\n    def download(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n\nlitdata.streaming.reader.PrepareChunksThread.stop\n    def stop(self) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n\nlitdata.streaming.sampler.ChunkedIndex.chunk_indexes\n\nlitdata.streaming.sampler.ChunkedIndex.index\n\nlitdata.streaming.sampler.ChunkedIndex.is_last_index\n\n# Cross-file Dependency:\nlitdata.streaming.item_loader.PyTreeLoader.load_item_from_chunk\n    def load_item_from_chunk(self, index: int, chunk_index: int, chunk_filepath: str, begin: int) -> Any:\n        \"\"\"Returns an item loaded from a chunk.\"\"\"\n\nlitdata.streaming.sampler.ChunkedIndex\n    class ChunkedIndex:\n\nlitdata.streaming.sampler.ChunkedIndex.chunk_index\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.streaming.reader.BinaryReader._prepare_thread\n\nlitdata.streaming.reader.BinaryReader._config\n\nlitdata.streaming.reader.BinaryReader._max_cache_size\n\n# Intra-file Dependency:\nlitdata.streaming.reader.PrepareChunksThread.__init__\n    def __init__(\n        self,\n        config: ChunksConfig,\n        item_loader: BaseItemLoader,\n        distributed_env: _DistributedEnv,\n        max_cache_size: Optional[int] = None,\n        max_pre_download: int = 2,\n    ) -> None:\n\nlitdata.streaming.sampler.ChunkedIndex.chunk_indexes\n\nlitdata.streaming.sampler.ChunkedIndex.index\n\nlitdata.streaming.reader.PrepareChunksThread.download\n    def download(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n\nlitdata.streaming.reader.PrepareChunksThread\n    class PrepareChunksThread(Thread):\n        \"\"\"This thread is responsible to download the chunks associated to a given worker.\"\"\"\n\n# Cross-file Dependency:\nlitdata.streaming.sampler.ChunkedIndex\n    class ChunkedIndex:\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport multiprocessing\nimport os\nimport warnings\nfrom logging import Logger\nfrom queue import Empty\nfrom threading import Thread\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom litdata.constants import _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.streaming.config import ChunksConfig\nfrom litdata.streaming.item_loader import BaseItemLoader, PyTreeLoader\nfrom litdata.streaming.sampler import ChunkedIndex\nfrom litdata.streaming.serializers import Serializer, _get_serializers\nfrom litdata.utilities.env import _DistributedEnv, _WorkerEnv\n\nwarnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    pass\n\n\nlogger = Logger(__name__)\n\n\n_END_TOKEN = \"END\"\n\n# Note: The timeout here should not be too short. We need to prevent the caller from aggressively\n# querying the queue and consuming too many CPU cycles.\n_DEFAULT_TIMEOUT = 0.1\n_LONG_DEFAULT_TIMEOUT = 5\n\n\nclass PrepareChunksThread(Thread):\n    \"\"\"This thread is responsible to download the chunks associated to a given worker.\"\"\"\n\n    def __init__(\n        self,\n        config: ChunksConfig,\n        item_loader: BaseItemLoader,\n        distributed_env: _DistributedEnv,\n        max_cache_size: Optional[int] = None,\n        max_pre_download: int = 2,\n    ) -> None:\n        super().__init__(daemon=True)\n        self._config = config\n        self._item_loader = item_loader\n        self._max_pre_download = max_pre_download\n        self._pre_download_counter = 0\n        self._distributed_env = distributed_env\n\n        self._chunks_index_to_be_deleted: List[int] = []\n        self._max_cache_size = max_cache_size\n        self._parent_cache_dir = os.path.dirname(self._config._cache_dir)\n        self._to_download_queue: multiprocessing.Queue = multiprocessing.Queue()\n        self._to_delete_queue: multiprocessing.Queue = multiprocessing.Queue()\n\n        # Check whether a dataset slice fits on the node\n        num_bytes_per_nodes = self._config.num_bytes // self._distributed_env.num_nodes\n        self._delete_chunks_when_processed = num_bytes_per_nodes > max_cache_size if max_cache_size else False\n        self._has_exited = False\n\n    def download(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    def delete(self, chunk_indexes: List[int]) -> None:\n        \"\"\"Receive the list of the chunk indices to delete for the current epoch.\"\"\"\n        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    def _delete(self, chunk_index: int) -> None:\n        \"\"\"Inform the item loader of the chunk to delete.\"\"\"\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.delete(chunk_index, chunk_filepath)\n\n    def stop(self) -> None:\n        \"\"\"Receive the list of the chunk indices to download for the current epoch.\"\"\"\n        self._to_download_queue.put(_END_TOKEN)\n\n    def _maybe_delete_chunks(self) -> None:\n        reached_pre_download = self._pre_download_counter == self._max_pre_download\n\n        # we have already pre-downloaded some chunks, we just need to wait for them to be processed.\n        chunk_index = _get_from_queue(\n            self._to_delete_queue, timeout=_LONG_DEFAULT_TIMEOUT if reached_pre_download else _DEFAULT_TIMEOUT\n        )\n\n        if chunk_index is not None:\n            self._pre_download_counter -= 1\n\n            # Store the current chunk index\n            self._chunks_index_to_be_deleted.append(chunk_index)\n\n        # Get the current cache size and decide whether we need to start cleanup. Otherwise, keep track of it\n        while self._max_cache_size and self._chunks_index_to_be_deleted and self._can_delete_chunk():\n            # Delete the oldest chunk\n            self._delete(self._chunks_index_to_be_deleted.pop(0))\n\n        return\n\n    def _can_delete_chunk(self) -> bool:\n        if self._delete_chunks_when_processed:\n            return self._pre_download_counter >= self._max_pre_download - 1\n        return self._max_cache_size is not None and _get_folder_size(self._parent_cache_dir) >= self._max_cache_size\n\n    def _pre_load_chunk(self, chunk_index: int) -> None:\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.pre_load_chunk(chunk_index, chunk_filepath)\n\n    def run(self) -> None:\n        while True:\n            if self._pre_download_counter < self._max_pre_download:\n                chunk_index = _get_from_queue(self._to_download_queue)\n                if chunk_index == _END_TOKEN:\n                    self._has_exited = True\n                    return\n\n                if chunk_index is not None:\n                    self._config.download_chunk_from_index(chunk_index)\n\n                    # Preload item if possible to gain some time but only\n                    # if this is one of the pre-downloaded chunk\n                    if self._pre_download_counter > 0:\n                        self._pre_load_chunk(chunk_index)\n\n                    # Avoid downloading too many chunks in advance at the risk of over using the disk space\n                    self._pre_download_counter += 1\n\n            if self._max_cache_size:\n                self._maybe_delete_chunks()\n\n\nclass BinaryReader:\n    def __init__(\n        self,\n        cache_dir: str,\n        max_cache_size: Optional[Union[int, str]] = None,\n        remote_input_dir: Optional[str] = None,\n        compression: Optional[str] = None,\n        item_loader: Optional[BaseItemLoader] = None,\n        serializers: Optional[Dict[str, Serializer]] = None,\n    ) -> None:\n        \"\"\"The BinaryReader enables to read chunked dataset in an efficient way.\n\n        Arguments:\n            cache_dir: The path to cache folder.\n            remote_input_dir: The path to a remote folder where the data are located.\n                The scheme needs to be added to the path.\n            compression: The algorithm to decompress the chunks.\n            item_loader: The chunk sampler to create sub arrays from a chunk.\n            max_cache_size: The maximum cache size used by the reader when fetching the chunks.\n            serializers: Provide your own serializers.\n\n        \"\"\"\n        super().__init__()\n        warnings.filterwarnings(\"ignore\", message=\".*The given buffer is not writable.*\")\n\n        self._cache_dir = cache_dir\n        self._remote_input_dir = remote_input_dir\n\n        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        self._compression = compression\n        self._intervals: Optional[List[str]] = None\n\n        self._serializers: Dict[str, Serializer] = _get_serializers(serializers)\n        self._distributed_env = _DistributedEnv.detect()\n        self._rank: Optional[int] = None\n        self._config: Optional[ChunksConfig] = None\n        self._prepare_thread: Optional[PrepareChunksThread] = None\n        self._item_loader = item_loader or PyTreeLoader()\n        self._last_chunk_index: Optional[int] = None\n        self._max_cache_size = int(os.getenv(\"MAX_CACHE_SIZE\", max_cache_size or 0))\n\n    def _get_chunk_index_from_index(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index(index)  # type: ignore\n\n    def _try_load_config(self) -> Optional[ChunksConfig]:\n        \"\"\"Try to load the chunks config if the index files are available.\"\"\"\n        self._config = ChunksConfig.load(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n        return self._config\n\n    @property\n    def config(self) -> ChunksConfig:\n        if self._config is None:\n            raise RuntimeError(\"The config should be defined.\")\n        return self._config\n\n    @property\n    def rank(self) -> int:\n        \"\"\"Returns the rank of the writer.\"\"\"\n        if self._rank is None:\n            self._worker_env = _WorkerEnv.detect()\n            self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n        return self._rank\n\n", "contexts_below": "\n    def get_length(self) -> int:\n        \"\"\"Get the number of samples across all chunks.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return len(self.config)\n\n    def get_chunk_intervals(self) -> List[Tuple[int, int]]:\n        \"\"\"Get the index interval of each chunk.\"\"\"\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self.config.intervals\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = self.__dict__.copy()\n        state[\"_prepare_thread\"] = None\n        return state\n\n\ndef _get_folder_size(path: str) -> int:\n    \"\"\"Collect the size of each files within a folder.\n\n    This method is robust to file deletion races\n\n    \"\"\"\n    size = 0\n    for dirpath, _, filenames in os.walk(str(path)):\n        for filename in filenames:\n            with contextlib.suppress(FileNotFoundError):\n                size += os.stat(os.path.join(dirpath, filename)).st_size\n    return size\n\n\ndef _get_from_queue(queue: multiprocessing.Queue, timeout: float = _DEFAULT_TIMEOUT) -> Optional[Any]:\n    try:\n        return queue.get(timeout=timeout)\n    except Empty:\n        pass\n    except OSError as err:\n        # handle closed queue before the thread terminates\n        if \"handle is closed\" in str(err) or \"Bad file descriptor\" in str(err):\n            logger.debug(err)\n        else:\n            raise err\n    except EOFError as err:\n        logger.debug(err)\n    return None\n", "input_code": "    def read(self, index: ChunkedIndex) -> Any:\n\n        \"\"\"\n        The read function in the BinaryReader class is designed to read and return an item from a specified chunk. It ensures that the chunk is available either locally or in memory, and if not, it initiates its download. The function supports prefetching to minimize wait times and handles the lifecycle of chunks, including their deletion once they are fully consumed to avoid errors such as segmentation faults.\n\n        Input-Output Arguments\n        :param self: BinaryReader. An instance of the BinaryReader class.\n        :param index: ChunkedIndex, The index of the chunk from which an item is to be read. This index is used to determine the specific chunk and the item within that chunk to be loaded.\n        :return: Any, The item read from the specified chunk. The type of the item can vary depending on the implementation of the item loader.\n\n        Note: The function raises a ValueError if the provided index is not an instance of ChunkedIndex. It also raises an Exception if the reader's index configuration is not defined. Additionally, the function asserts the presence of a prepare thread during certain operations to ensure the thread is correctly managing the chunks.\n        \"\"\"", "reference_steps": "1. Verify that the `index` parameter is an instance of `ChunkedIndex`; raise a `ValueError` if not.\n2. Attempt to load the configuration (containing the index) if it is not already loaded; raise an exception if the configuration cannot be defined.\n3. Check if the configuration exists and has either a remote directory or compressor defined.\n4. If the prepare chunks thread does not exist and the configuration is valid, create and start a new `PrepareChunksThread` with the appropriate parameters.\n5. If the current `chunk_index` is new (i.e., not the same as `self._last_chunk_index`), request the prepare thread to download the chunk by its index.\n6. If `self._last_chunk_index` is `None`, set it to the current `chunk_index`.\n7. Fetch the element by retrieving the chunk file path, the beginning byte, and the index from the configuration, and then use `_item_loader` to load the item from the chunk.\n8. If the current chunk index is different from the last chunk index and the configuration has a remote directory, request the prepare thread to delete the last chunk index from cache or memory.\n9. Update `self._last_chunk_index` to the current chunk index after the chunk has been completely consumed.\n10. If the current index is the last one and the prepare thread exists, signal the thread to stop and set `self._prepare_thread` to `None`.\n11. Return the loaded item.", "reference_code": "def read(self, index: ChunkedIndex) -> Any:\n    \"\"\"Read an item for the given from a chunk.\n\n    If the chunk isn't available locally or in memory, it will be downloaded.\n\n    Prefetching should reduce the wait time to be the batch available.\n\n    \"\"\"\n    if not isinstance(index, ChunkedIndex):\n        raise ValueError(\"The Reader.read(...) method expects a chunked Index.\")\n\n    # Load the config containing the index\n    if self._config is None and self._try_load_config() is None:\n        raise Exception(\"The reader index isn't defined.\")\n\n    if self._config and (self._config._remote_dir or self._config._compressor):\n        # Create and start the prepare chunks thread\n        if self._prepare_thread is None and self._config:\n            self._prepare_thread = PrepareChunksThread(\n                self._config, self._item_loader, self._distributed_env, self._max_cache_size\n            )\n            self._prepare_thread.start()\n            if index.chunk_indexes:\n                self._prepare_thread.download(index.chunk_indexes)\n\n        # If the chunk_index is new, request for it to be downloaded.\n        if index.chunk_index != self._last_chunk_index:\n            assert self._prepare_thread\n            self._prepare_thread.download([index.chunk_index])\n\n        if self._last_chunk_index is None:\n            self._last_chunk_index = index.chunk_index\n\n    # Fetch the element\n    chunk_filepath, begin, _ = self.config[index]\n    item = self._item_loader.load_item_from_chunk(index.index, index.chunk_index, chunk_filepath, begin)\n\n    # We need to request deletion after the latest element has been loaded.\n    # Otherwise, this could trigger segmentation fault error depending on the item loader used.\n    if self._config and self._config._remote_dir and index.chunk_index != self._last_chunk_index:\n        assert self._prepare_thread\n        assert self._last_chunk_index is not None\n\n        # inform the chunk has been completely consumed\n        self._prepare_thread.delete([self._last_chunk_index])\n\n        # track the new chunk index as the latest one\n        self._last_chunk_index = index.chunk_index\n\n    if index.is_last_index and self._prepare_thread:\n        # inform the thread it is time to stop\n        self._prepare_thread.stop()\n        self._prepare_thread = None\n\n    return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "type": "function", "class_name": null, "function_name": "broadcast_object", "dependency_all": "# Intra-file Dependency:\nlitdata.utilities.broadcast._ImmutableDistributedMap\n    class _ImmutableDistributedMap:\n        \"\"\"The _ImmutableDistributedMap enables to create a distributed key value pair in the cloud.\n\n        The first process to perform the set operation defines its value.\n\n        \"\"\"\n\nlitdata.utilities.broadcast._ImmutableDistributedMap.__init__\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n\nlitdata.utilities.broadcast._ImmutableDistributedMap.set_and_get\n    def set_and_get(self, key: str, value: Any) -> Any:\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.utilities.broadcast._ImmutableDistributedMap\n    class _ImmutableDistributedMap:\n        \"\"\"The _ImmutableDistributedMap enables to create a distributed key value pair in the cloud.\n\n        The first process to perform the set operation defines its value.\n\n        \"\"\"\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport pickle\nfrom logging import Logger\nfrom typing import Any, Callable, Dict, Optional\nfrom urllib.parse import urljoin\n\nimport requests\nimport urllib3\n\n# for backwards compatibility\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\nlogger = Logger(__name__)\n\n_CONNECTION_RETRY_TOTAL = 2880\n_CONNECTION_RETRY_BACKOFF_FACTOR = 0.5\n_DEFAULT_REQUEST_TIMEOUT = 30  # seconds\n\n\nclass _CustomRetryAdapter(HTTPAdapter):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self.timeout = kwargs.pop(\"timeout\", _DEFAULT_REQUEST_TIMEOUT)\n        super().__init__(*args, **kwargs)\n\n    def send(self, request: Any, *args: Any, **kwargs: Any) -> Any:\n        kwargs[\"timeout\"] = kwargs.get(\"timeout\", self.timeout)\n        return super().send(request, **kwargs)\n\n\ndef _response(r: Any, *args: Any, **kwargs: Any) -> Any:\n    return r.raise_for_status()\n\n\nclass _HTTPClient:\n    \"\"\"A wrapper class around the requests library which handles chores like logging, retries, and timeouts\n    automatically.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        auth_token: Optional[str] = None,\n        log_callback: Optional[Callable] = None,\n        use_retry: bool = True,\n    ) -> None:\n        self.base_url = base_url\n        retry_strategy = Retry(\n            # wait time between retries increases exponentially according to: backoff_factor * (2 ** (retry - 1))\n            # but the the maximum wait time is 120 secs. By setting a large value (2880), we'll make sure clients\n            # are going to be alive for a very long time (~ 4 days) but retries every 120 seconds\n            total=_CONNECTION_RETRY_TOTAL,\n            backoff_factor=_CONNECTION_RETRY_BACKOFF_FACTOR,\n            status_forcelist=[\n                408,  # Request Timeout\n                429,  # Too Many Requests\n                500,  # Internal Server Error\n                502,  # Bad Gateway\n                503,  # Service Unavailable\n                504,  # Gateway Timeout\n            ],\n        )\n        adapter = _CustomRetryAdapter(max_retries=retry_strategy, timeout=_DEFAULT_REQUEST_TIMEOUT)\n        self.session = requests.Session()\n\n        self.session.hooks = {\"response\": _response}\n\n        if use_retry:\n            self.session.mount(\"http://\", adapter)\n            self.session.mount(\"https://\", adapter)\n\n        if auth_token:\n            self.session.headers.update({\"Authorization\": f\"Bearer {auth_token}\"})\n\n    def get(self, path: str) -> Any:\n        url = urljoin(self.base_url, path)\n        return self.session.get(url)\n\n    def post(\n        self, path: str, *, query_params: Optional[Dict] = None, data: Optional[bytes] = None, json: Any = None\n    ) -> Any:\n        url = urljoin(self.base_url, path)\n        return self.session.post(url, data=data, params=query_params, json=json)\n\n    def delete(self, path: str) -> Any:\n        url = urljoin(self.base_url, path)\n        return self.session.delete(url)\n\n\nclass _ImmutableDistributedMap:\n    \"\"\"The _ImmutableDistributedMap enables to create a distributed key value pair in the cloud.\n\n    The first process to perform the set operation defines its value.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        token = _get_token()\n\n        lightning_app_external_url = os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\")\n        if lightning_app_external_url is None:\n            raise RuntimeError(\"The `LIGHTNING_APP_EXTERNAL_URL` should be set.\")\n\n        self.public_client: _HTTPClient = _HTTPClient(lightning_app_external_url, auth_token=token, use_retry=False)\n\n        lightning_app_state_url = os.getenv(\"LIGHTNING_APP_STATE_URL\")\n        if lightning_app_state_url is None:\n            raise RuntimeError(\"The `LIGHTNING_APP_STATE_URL` should be set.\")\n\n        self.private_client: _HTTPClient = _HTTPClient(lightning_app_state_url, auth_token=token, use_retry=False)\n\n    def set_and_get(self, key: str, value: Any) -> Any:\n        payload = {\"key\": key, \"value\": pickle.dumps(value, 0).decode()}\n\n        # Try the public address first\n        try:\n            resp = self.public_client.post(\"/broadcast\", json=payload)\n        except (requests.exceptions.ConnectionError, urllib3.exceptions.MaxRetryError):\n            # fallback to the private one\n            resp = self.private_client.post(\"/broadcast\", json=payload)\n\n        if resp.status_code != 200:\n            raise RuntimeError(f\"Failed to broadcast the following {key=} {value=}.\")\n        return pickle.loads(bytes(resp.json()[\"value\"], \"utf-8\"))\n\n\n", "contexts_below": "\n\ndef _get_token() -> Optional[str]:\n    \"\"\"This function tries to retrieve a temporary token.\"\"\"\n    if os.getenv(\"LIGHTNING_CLOUD_URL\") is None:\n        return None\n\n    payload = {\"apiKey\": os.getenv(\"LIGHTNING_API_KEY\"), \"username\": os.getenv(\"LIGHTNING_USERNAME\")}\n    url_login = os.getenv(\"LIGHTNING_CLOUD_URL\", \"\") + \"/v1/auth/login\"\n    res = requests.post(url_login, data=json.dumps(payload))\n    if \"token\" not in res.json():\n        raise RuntimeError(\n            f\"You haven't properly setup your environment variables with {url_login} and data: \\n{payload}\"\n        )\n    return res.json()[\"token\"]\n", "input_code": "def broadcast_object(key: str, obj: Any) -> Any:\n\n    \"\"\"\n    This function broadcasts an object across machines in a distributed environment. It checks if the application is running in an environment with an external URL (indicating a distributed setting) and uses an immutable distributed map to share the object. If not in such an environment, it simply returns the object as is.\n\n    Input-Output Arguments\n    :param key: String, the key associated with the object to be broadcasted. It is used to identify the object in the distributed map.\n    :param obj: Any, the object to be broadcasted. This can be of any type and is the object that is either shared across machines or returned directly.\n    :return: Any. The object that was either broadcasted and retrieved from the distributed map or the original object if not in a distributed environment.\n    \"\"\"", "reference_steps": "1. Define a function `broadcast_object` that takes two parameters: `key` (a string) and `obj` (an object of any type).\n2. Check if the environment variable `LIGHTNING_APP_EXTERNAL_URL` is set (not `None`).\n3. If the environment variable is set, indicating an external URL for a distributed environment, use the `_ImmutableDistributedMap` class to distribute the object.\n4. Call the `set_and_get` method of `_ImmutableDistributedMap` with the given `key` and `obj` to store and retrieve the object across machines.\n5. If the environment variable is not set, indicating a non-distributed environment, simply return the original `obj`.\n6. The function returns the object that was either distributed and retrieved from the distributed map or the original object for a non-distributed environment.", "reference_code": "def broadcast_object(key: str, obj: Any) -> Any:\n    \"\"\"This function enables to broadcast object across machines.\"\"\"\n    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "type": "function", "class_name": null, "function_name": "_intra_node_chunk_shuffle", "dependency_all": "# Intra-file Dependency:\nlitdata.utilities.env._DistributedEnv.world_size\n\n# Cross-file Dependency:\nlitdata.utilities.env._DistributedEnv\n    class _DistributedEnv:\n        \"\"\"The environment of the distributed training.\n\n        Args:\n            world_size: The number of total distributed training processes\n            global_rank: The rank of the current process within this pool of training processes\n\n        \"\"\"\n\nlitdata.utilities.env._DistributedEnv.num_nodes\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.utilities.env._DistributedEnv.num_nodes\n\n", "contexts_above": "from typing import Any, List, Tuple\n\nimport numpy as np\n\nfrom litdata.utilities.env import _DistributedEnv\n\n\n", "contexts_below": "\n\ndef _associate_chunks_and_internals_to_ranks(\n    distributed_env: _DistributedEnv,\n    indexes: Any,\n    chunk_intervals: Any,\n    drop_last: bool,\n) -> Tuple[List[List[int]], List[Any]]:\n    num_items = sum([(interval[-1] - interval[0]) for interval in chunk_intervals])\n    num_items_per_ranks: List[int] = [\n        num_items // distributed_env.world_size + num_items % distributed_env.world_size\n        if rank == distributed_env.world_size - 1 and not drop_last\n        else num_items // distributed_env.world_size\n        for rank in range(distributed_env.world_size)\n    ]\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[List[int]]] = [[] for _ in range(distributed_env.world_size)]\n\n    # 4. Assign the chunk & intervals to each rank\n    for chunk_index, chunk_interval in zip(indexes, chunk_intervals):\n        rank = 0\n\n        while True:\n            if rank == len(num_items_per_ranks):\n                break\n\n            items_left_to_assign = num_items_per_ranks[rank]\n\n            if items_left_to_assign == 0:\n                rank += 1\n                continue\n\n            items_in_chunk = chunk_interval[-1] - chunk_interval[0]\n\n            if items_in_chunk == 0:\n                break\n\n            if items_in_chunk > items_left_to_assign:\n                chunks_per_ranks[rank].append(chunk_index)\n                begin, end = chunk_interval\n                intervals_per_ranks[rank].append([begin, begin + items_left_to_assign])\n                chunk_interval = (begin + items_left_to_assign, end)\n                num_items_per_ranks[rank] = 0\n                rank += 1\n            else:\n                chunks_per_ranks[rank].append(chunk_index)\n                intervals_per_ranks[rank].append(chunk_interval)\n                num_items_per_ranks[rank] -= items_in_chunk\n                break\n\n    return chunks_per_ranks, intervals_per_ranks\n", "input_code": "def _intra_node_chunk_shuffle(\n    distributed_env: _DistributedEnv,\n    chunks_per_ranks: List[List[int]],\n    seed: int,\n    current_epoch: int,\n) -> List[int]:\n\n    \"\"\"\n    This function shuffles the chunks of data assigned to each node in a distributed environment. It ensures that each node receives a randomized set of chunk indexes based on the provided seed and the current epoch, aiming to distribute data more evenly and randomly across nodes for each training epoch.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The environment of the distributed system, containing information like the number of nodes and the world size.\n    :param chunks_per_ranks: List[List[int]]. A list where each sublist contains the chunk indexes assigned to a specific rank.\n    :param seed: int. The seed used for random number generation to ensure reproducibility of the shuffle.\n    :param current_epoch: int. The current epoch number, used to modify the seed for each epoch to ensure different shuffles.\n    :return: List[int]. A flattened list of shuffled chunk indexes across all nodes.\n    \"\"\"", "reference_steps": "1. Define a function `_intra_node_chunk_shuffle` that takes a distributed environment object `distributed_env`, a list of lists `chunks_per_ranks` where each sublist contains chunk indices for a specific rank, a `seed` for random number generation, and the `current_epoch`.\n\n2. Initialize a list of lists `chunk_indexes_per_nodes` with empty sublists, one for each node in the distributed environment.\n\n3. Calculate the number of processes per node by dividing the total world size by the number of nodes.\n\n4. Iterate over each rank and its corresponding chunks in `chunks_per_ranks`.\n\n5. For each rank, append its chunks to the appropriate sublist in `chunk_indexes_per_nodes` based on the rank's node index. If there is only one node, append to the first sublist.\n\n6. Iterate over the `chunk_indexes_per_nodes` list.\n\n7. For each sublist in `chunk_indexes_per_nodes`, shuffle the chunk indices using a `numpy` random state seeded with the `seed` plus the `current_epoch`. This ensures a unique shuffle for each epoch.\n\n8. Replace each sublist in `chunk_indexes_per_nodes` with the shuffled version.\n\n9. Flatten the `chunk_indexes_per_nodes` list of lists into a single list of chunk indices.\n\n10. Return the flattened list of shuffled chunk indices.", "reference_code": "def _intra_node_chunk_shuffle(\n    distributed_env: _DistributedEnv,\n    chunks_per_ranks: List[List[int]],\n    seed: int,\n    current_epoch: int,\n) -> List[int]:\n    chunk_indexes_per_nodes: Any = [[] for _ in range(distributed_env.num_nodes)]\n    process_per_node = distributed_env.world_size // distributed_env.num_nodes\n    for rank, chunks_per_rank in enumerate(chunks_per_ranks):\n        chunk_indexes_per_nodes[0 if distributed_env.num_nodes == 1 else rank // process_per_node].extend(\n            chunks_per_rank\n        )\n\n    # shuffle the chunks associated to the node\n    for i in range(len(chunk_indexes_per_nodes)):\n        # permute the indexes within the node\n        chunk_indexes_per_nodes[i] = np.random.RandomState(seed=seed + current_epoch).permutation(\n            chunk_indexes_per_nodes[i]\n        )\n\n    return [index for chunks in chunk_indexes_per_nodes for index in chunks]\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "type": "function", "class_name": null, "function_name": "_get_input_dir", "dependency_all": "# Intra-file Dependency:\nlitdata.processing.functions._get_indexed_paths\n    def _get_indexed_paths(data: Any) -> Dict[int, str]:\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.processing.functions._get_indexed_paths\n    def _get_indexed_paths(data: Any) -> Dict[int, str]:\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport concurrent.futures\nimport inspect\nimport os\nfrom datetime import datetime\nfrom functools import partial\nfrom pathlib import Path\nfrom types import FunctionType\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport torch\n\nfrom litdata.constants import _IS_IN_STUDIO, _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.processing.data_processor import DataChunkRecipe, DataProcessor, DataTransformRecipe\nfrom litdata.processing.readers import BaseReader\nfrom litdata.processing.utilities import optimize_dns_context\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import (\n    Dir,\n    _assert_dir_has_index_file,\n    _assert_dir_is_empty,\n    _execute,\n    _resolve_dir,\n)\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten\n\n\ndef _get_indexed_paths(data: Any) -> Dict[int, str]:\n    flattened_item, _ = tree_flatten(data)\n\n    indexed_paths = {\n        index: element\n        for index, element in enumerate(flattened_item)\n        if isinstance(element, str) and os.path.exists(element)\n    }\n\n    return indexed_paths\n\n\n", "contexts_below": "\n\ndef _get_default_num_workers() -> int:\n    if torch.cuda.is_available():\n        return torch.cuda.device_count()\n    return os.cpu_count() or 1\n\n\nclass LambdaDataTransformRecipe(DataTransformRecipe):\n    def __init__(self, fn: Callable[[str, Any], None], inputs: Sequence[Any]):\n        super().__init__()\n        self._fn = fn\n        self._inputs = inputs\n        self._device: Optional[str] = None\n\n        _fn = self._fn if isinstance(self._fn, FunctionType) else self._fn.__call__  # type: ignore\n        params = inspect.signature(_fn).parameters\n        self._contains_device = \"device\" in params\n        self._contains_is_last = \"is_last\" in params\n\n    def prepare_structure(self, _: Optional[str]) -> Any:\n        return self._inputs\n\n    def prepare_item(self, item_metadata: Any, output_dir: str, is_last: bool) -> None:\n        if self._contains_device and self._device is None:\n            self._find_device()\n\n        kwargs: Dict[str, Any] = {}\n\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        if isinstance(self._fn, (FunctionType, partial)):\n            self._fn(item_metadata, output_dir, **kwargs)\n\n        elif callable(self._fn):\n            self._fn.__call__(item_metadata, output_dir, **kwargs)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n    def _find_device(self) -> None:\n        global_rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n        if torch.cuda.is_available() and global_rank:\n            num_gpus = torch.cuda.device_count()\n            device = int(global_rank) % num_gpus\n            self._device = f\"cuda:{device}\"\n\n\nclass LambdaDataChunkRecipe(DataChunkRecipe):\n    def __init__(\n        self,\n        fn: Callable[[Any], None],\n        inputs: Sequence[Any],\n        chunk_size: Optional[int],\n        chunk_bytes: Optional[Union[int, str]],\n        compression: Optional[str],\n    ):\n        super().__init__(chunk_size=chunk_size, chunk_bytes=chunk_bytes, compression=compression)\n        self._fn = fn\n        self._inputs = inputs\n\n    def prepare_structure(self, input_dir: Optional[str]) -> Any:\n        return self._inputs\n\n    def prepare_item(self, item_metadata: Any) -> Any:\n        if isinstance(self._fn, partial):\n            yield from self._fn(item_metadata)\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                yield from self._fn(item_metadata)\n            else:\n                yield self._fn(item_metadata)\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                yield from self._fn.__call__(item_metadata)  # type: ignore\n            else:\n                yield self._fn.__call__(item_metadata)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n\ndef map(\n    fn: Callable[[str, Any], None],\n    inputs: Sequence[Any],\n    output_dir: Union[str, Dir],\n    weights: Optional[List[int]] = None,\n    num_workers: Optional[int] = None,\n    fast_dev_run: Union[bool, int] = False,\n    num_nodes: Optional[int] = None,\n    machine: Optional[str] = None,\n    num_downloaders: Optional[int] = None,\n    num_uploaders: Optional[int] = None,\n    reorder_files: bool = True,\n    error_when_not_empty: bool = False,\n    reader: Optional[BaseReader] = None,\n    batch_size: Optional[int] = None,\n) -> None:\n    \"\"\"This function map a callbable over a collection of files possibly in a distributed way.\n\n    Arguments:\n        fn: A function to be executed over each input element\n        inputs: A sequence of input to be processed by the `fn` function.\n            Each input should contain at least a valid filepath.\n        output_dir: The folder where the processed data should be stored.\n        weights: Provide an associated weight to each input. This is used to balance work among workers.\n        num_workers: The number of workers to use during processing\n        fast_dev_run: Whether to use process only a sub part of the inputs\n        num_nodes: When doing remote execution, the number of nodes to use. Only supported on https://lightning.ai/.\n        machine: When doing remote execution, the machine to use. Only supported on https://lightning.ai/.\n        num_downloaders: The number of downloaders per worker.\n        num_uploaders: The number of uploaders per workers.\n        reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n            Set this to ``False`` if the order in which samples are processed should be preserved.\n        error_when_not_empty: Whether we should error if the output folder isn't empty.\n        batch_size: Group the inputs into batches of batch_size length.\n\n    \"\"\"\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(LambdaDataTransformRecipe(fn, inputs))\n    return _execute(\n        f\"data-prep-map-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )\n\n\ndef optimize(\n    fn: Callable[[Any], Any],\n    inputs: Sequence[Any],\n    output_dir: str,\n    weights: Optional[List[int]] = None,\n    chunk_size: Optional[int] = None,\n    chunk_bytes: Optional[Union[int, str]] = None,\n    compression: Optional[str] = None,\n    num_workers: Optional[int] = None,\n    fast_dev_run: bool = False,\n    num_nodes: Optional[int] = None,\n    machine: Optional[str] = None,\n    num_downloaders: Optional[int] = None,\n    num_uploaders: Optional[int] = None,\n    reorder_files: bool = True,\n    reader: Optional[BaseReader] = None,\n    batch_size: Optional[int] = None,\n) -> None:\n    \"\"\"This function converts a dataset into chunks possibly in a distributed way.\n\n    Arguments:\n        fn: A function to be executed over each input element\n        inputs: A sequence of input to be processed by the `fn` function.\n            Each input should contain at least a valid filepath.\n        output_dir: The folder where the processed data should be stored.\n        weights: Provide an associated weight to each input. This is used to balance work among workers.\n        chunk_size: The maximum number of elements to hold within a chunk.\n        chunk_bytes: The maximum number of bytes to hold within a chunk.\n        compression: The compression algorithm to use over the chunks.\n        num_workers: The number of workers to use during processing\n        fast_dev_run: Whether to use process only a sub part of the inputs\n        num_nodes: When doing remote execution, the number of nodes to use. Only supported on https://lightning.ai/.\n        machine: When doing remote execution, the machine to use. Only supported on https://lightning.ai/.\n        num_downloaders: The number of downloaders per worker.\n        num_uploaders: The numbers of uploaders per worker.\n        reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n            Set this to ``False`` if the order in which samples are processed should be preserved.\n        batch_size: Group the inputs into batches of batch_size length.\n\n    \"\"\"\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if chunk_size is None and chunk_bytes is None:\n        raise ValueError(\"Either `chunk_size` or `chunk_bytes` needs to be defined.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            reader=reader,\n        )\n\n        with optimize_dns_context(True):\n            data_processor.run(\n                LambdaDataChunkRecipe(\n                    fn,\n                    inputs,\n                    chunk_size=chunk_size,\n                    chunk_bytes=chunk_bytes,\n                    compression=compression,\n                )\n            )\n        return None\n    return _execute(\n        f\"data-prep-optimize-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )\n\n\ndef _listdir(folder: str) -> Tuple[str, List[str]]:\n    return folder, os.listdir(folder)\n\n\nclass walk:\n    \"\"\"This class is an optimized version of os.walk for listing files and folders from cloud filesystem.\n\n    Note: The order of files and folders yielded aren't depth-first anymore due to the asynchronous listing call.\n\n    \"\"\"\n\n    def __init__(self, folder: str, max_workers: Optional[int] = os.cpu_count()) -> None:\n        self.folders = [folder]\n        self.max_workers = max_workers or 1\n        self.futures: List[concurrent.futures.Future] = []\n\n        if not _IS_IN_STUDIO:\n            print(\"This method is optimized to run on https://lightning.ai/. Don't use it otherwise.\")\n\n    def __iter__(self) -> Any:\n        \"\"\"This function queues the folders to perform listdir across multiple workers.\"\"\"\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            while len(self.folders):\n                folder = self.folders.pop(0)\n                future = executor.submit(_listdir, folder)\n                self.futures.append(future)\n\n            while self.futures:\n                for future in concurrent.futures.as_completed(self.futures):\n                    filenames = []\n                    folders = []\n\n                    folder, files_or_folders = future.result()\n                    self.futures = [f for f in self.futures if f != future]\n\n                    for file_or_folder in files_or_folders:\n                        if os.path.isfile(os.path.join(folder, file_or_folder)):\n                            filenames.append(file_or_folder)\n                        else:\n                            folders.append(file_or_folder)\n                            self.folders.append(os.path.join(folder, file_or_folder))\n\n                    yield folder, folders, filenames\n\n                    while len(self.folders) and len(self.futures) <= self.max_workers * 2:\n                        folder = self.folders.pop(0)\n                        future = executor.submit(_listdir, folder)\n                        self.futures.append(future)\n        return\n", "input_code": "def _get_input_dir(inputs: Sequence[Any]) -> Optional[str]:\n\n    \"\"\"\n    Determines the input directory from a sequence of inputs by extracting and resolving indexed paths. It checks the first two elements of the input sequence for valid file paths, raises an error if inconsistent file paths are found, and formats the path to include the project root or a specified depth in the file system.\n\n    Input-Output Arguments\n    :param inputs: Sequence[Any]. A sequence of inputs from which file paths are to be extracted and analyzed.\n    :return: Optional[str]. The absolute path to the input directory determined from the inputs, or None if no valid file paths are found.\n    \"\"\"", "reference_steps": "1. Define a function `_get_input_dir` that takes a sequence of inputs and returns an optional string.\n2. Call a function `_get_indexed_paths` on the first element of the inputs sequence to retrieve a dictionary of indexed paths.\n3. Check if the dictionary `indexed_paths` is empty. If it is, proceed to the next step; if not, skip to step 6.\n4. Call `_get_indexed_paths` on the second element of the inputs sequence if the first element had no indexed paths.\n5. If the second element also has no indexed paths, return `None`. If it does have paths but the first element did not, raise a `ValueError` indicating inconsistency.\n6. Resolve the first path in `indexed_paths` to an absolute path.\n7. Check if the string \"/.project\" is part of the absolute path.\n8. If \"/.project\" is found, construct a new path by joining the first four segments of the path starting from the root \"/\".\n9. If \"/.project\" is not found, construct a new path by joining the first four segments of the absolute path starting from the root \"/\".\n10. Return the constructed path as the output of the function.", "reference_code": "def _get_input_dir(inputs: Sequence[Any]) -> Optional[str]:\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(indexed_paths) == 0:\n        # Check whether the second element has any input_path\n        indexed_paths = _get_indexed_paths(inputs[1])\n        if len(indexed_paths) == 0:\n            return None\n\n        # Every element should have filepaths if any contains one.\n        raise ValueError(f\"The provided item {inputs[0]} didn't contain any filepaths.\")\n\n    absolute_path = str(Path(list(indexed_paths.values())[0]).resolve())\n\n    if \"/.project\" in absolute_path:\n        return \"/\" + os.path.join(*str(list(indexed_paths.values())[0]).split(\"/\")[:4])\n\n    return \"/\" + os.path.join(*str(absolute_path).split(\"/\")[:4])\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "type": "function", "class_name": null, "function_name": "optimize_dns_context", "dependency_all": "# Intra-file Dependency:\nlitdata.processing.utilities.optimize_dns\n    def optimize_dns(enable: bool) -> None:\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.processing.utilities.optimize_dns\n    def optimize_dns(enable: bool) -> None:\n\n", "contexts_above": "import io\nimport os\nimport urllib\nfrom contextlib import contextmanager\nfrom subprocess import DEVNULL, Popen\nfrom typing import Any, Callable, List, Optional, Tuple, Union\n\nfrom litdata.constants import _IS_IN_STUDIO, _LIGHTNING_CLOUD_LATEST\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import (\n        ProjectIdDatasetsBody,\n        V1DatasetType,\n    )\n    from lightning_cloud.openapi.rest import ApiException\n    from lightning_cloud.rest_client import LightningClient\n\n\ndef _create_dataset(\n    input_dir: Optional[str],\n    storage_dir: str,\n    dataset_type: V1DatasetType,\n    empty: Optional[bool] = None,\n    size: Optional[int] = None,\n    num_bytes: Optional[str] = None,\n    data_format: Optional[Union[str, Tuple[str]]] = None,\n    compression: Optional[str] = None,\n    num_chunks: Optional[int] = None,\n    num_bytes_per_chunk: Optional[List[int]] = None,\n    name: Optional[str] = None,\n    version: Optional[int] = None,\n) -> None:\n    \"\"\"Create a dataset with metadata information about its source and destination.\"\"\"\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    user_id = os.getenv(\"LIGHTNING_USER_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n    lightning_app_id = os.getenv(\"LIGHTNING_CLOUD_APP_ID\", None)\n\n    if project_id is None:\n        return\n\n    if not storage_dir:\n        raise ValueError(\"The storage_dir should be defined.\")\n\n    client = LightningClient(retry=False)\n\n    try:\n        client.dataset_service_create_dataset(\n            body=ProjectIdDatasetsBody(\n                cloud_space_id=cloud_space_id if lightning_app_id is None else None,\n                cluster_id=cluster_id,\n                creator_id=user_id,\n                empty=empty,\n                input_dir=input_dir,\n                lightning_app_id=lightning_app_id,\n                name=name,\n                size=size,\n                num_bytes=num_bytes,\n                data_format=str(data_format) if data_format else data_format,\n                compression=compression,\n                num_chunks=num_chunks,\n                num_bytes_per_chunk=num_bytes_per_chunk,\n                storage_dir=storage_dir,\n                type=dataset_type,\n                version=version,\n            ),\n            project_id=project_id,\n        )\n    except ApiException as ex:\n        if \"already exists\" in str(ex.body):\n            pass\n        else:\n            raise ex\n\n\ndef get_worker_rank() -> Optional[str]:\n    return os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\")\n\n\ndef catch(func: Callable) -> Callable:\n    def _wrapper(*args: Any, **kwargs: Any) -> Tuple[Any, Optional[Exception]]:\n        try:\n            return func(*args, **kwargs), None\n        except Exception as e:\n            return None, e\n\n    return _wrapper\n\n\n# Credit to the https://github.com/rom1504/img2dataset Github repo\n# The code was taken from there. It has a MIT License.\n\n\ndef make_request(\n    url: str,\n    timeout: int = 10,\n    user_agent_token: str = \"pytorch-lightning\",\n) -> io.BytesIO:\n    \"\"\"Download an image with urllib.\"\"\"\n    user_agent_string = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\"\n    if user_agent_token:\n        user_agent_string += f\" (compatible; {user_agent_token}; +https://github.com/Lightning-AI/pytorch-lightning)\"\n\n    with urllib.request.urlopen(  # noqa: S310\n        urllib.request.Request(url, data=None, headers={\"User-Agent\": user_agent_string}), timeout=timeout\n    ) as r:\n        img_stream = io.BytesIO(r.read())\n    return img_stream\n\n\n@contextmanager\n", "contexts_below": "\n\ndef optimize_dns(enable: bool) -> None:\n    if not _IS_IN_STUDIO:\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n\ndef _optimize_dns(enable: bool) -> None:\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    write_lines = []\n    for line in lines:\n        if \"nameserver 127\" in line:\n            if enable:\n                write_lines.append(\"nameserver 127.0.0.1\\n\")\n            else:\n                write_lines.append(\"nameserver 127.0.0.53\\n\")\n        else:\n            write_lines.append(line)\n\n    with open(\"/etc/resolv.conf\", \"w\") as f:\n        for line in write_lines:\n            f.write(line)\n", "input_code": "def optimize_dns_context(enable: bool) -> Any:\n\n    \"\"\"\n    This function is designed to temporarily enable or disable DNS optimization based on the input parameter. It attempts to perform operations within a context where DNS optimization is enabled or disabled as specified, and ensures that DNS optimization is always disabled after these operations, even if an exception occurs.\n\n    Input-Output Arguments\n    :param enable: Bool, indicates whether DNS optimization should be enabled (True) or disabled (False) during the execution of the context.\n    :return: No return values. This function is a context manager that does not return any value but ensures DNS optimization is set according to the 'enable' parameter and then reset to disabled afterwards.\n    \"\"\"", "reference_steps": "1. Define a function `optimize_dns_context` that accepts a boolean parameter `enable`.\n2. Inside the function, call another function `optimize_dns` with the argument `enable`.\n3. Enter a `try` block to yield control back to the caller, allowing them to execute their code within the context of the optimized DNS settings.\n4. After the caller's code has executed (after the `yield`), call `optimize_dns(False)` to ensure DNS optimization is always disabled when the caller's code block finishes.\n5. If an exception occurs within the caller's code block, catch it with an `except` block.\n6. Inside the `except` block, call `optimize_dns(False)` to disable DNS optimization even when an exception is raised.\n7. Re-raise the caught exception to propagate it to the caller.\n8. The `optimize_dns_context` function is designed to be used as a context manager with the `with` statement.\n9. The function ensures that DNS optimization is enabled at the start of the `with` block if `enable` is `True`, and it is always disabled when exiting the block, whether normally or due to an exception.\n10. The function returns a context manager that can be used with the `with` statement to manage the DNS optimization state.", "reference_code": "def optimize_dns_context(enable: bool) -> Any:\n    optimize_dns(enable)\n    try:\n        yield\n        optimize_dns(False)  # always disable the optimize DNS\n    except Exception as e:\n        optimize_dns(False)  # always disable the optimize DNS\n        raise e\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "type": "function", "class_name": null, "function_name": "_associate_chunks_and_internals_to_ranks", "dependency_all": "# Intra-file Dependency:\nlitdata.utilities.env._DistributedEnv.world_size\n\n# Cross-file Dependency:\nlitdata.utilities.env._DistributedEnv\n    class _DistributedEnv:\n        \"\"\"The environment of the distributed training.\n\n        Args:\n            world_size: The number of total distributed training processes\n            global_rank: The rank of the current process within this pool of training processes\n\n        \"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.utilities.env._DistributedEnv.world_size\n\n", "contexts_above": "from typing import Any, List, Tuple\n\nimport numpy as np\n\nfrom litdata.utilities.env import _DistributedEnv\n\n\ndef _intra_node_chunk_shuffle(\n    distributed_env: _DistributedEnv,\n    chunks_per_ranks: List[List[int]],\n    seed: int,\n    current_epoch: int,\n) -> List[int]:\n    chunk_indexes_per_nodes: Any = [[] for _ in range(distributed_env.num_nodes)]\n    process_per_node = distributed_env.world_size // distributed_env.num_nodes\n    for rank, chunks_per_rank in enumerate(chunks_per_ranks):\n        chunk_indexes_per_nodes[0 if distributed_env.num_nodes == 1 else rank // process_per_node].extend(\n            chunks_per_rank\n        )\n\n    # shuffle the chunks associated to the node\n    for i in range(len(chunk_indexes_per_nodes)):\n        # permute the indexes within the node\n        chunk_indexes_per_nodes[i] = np.random.RandomState(seed=seed + current_epoch).permutation(\n            chunk_indexes_per_nodes[i]\n        )\n\n    return [index for chunks in chunk_indexes_per_nodes for index in chunks]\n\n\n", "contexts_below": "", "input_code": "def _associate_chunks_and_internals_to_ranks(\n    distributed_env: _DistributedEnv,\n    indexes: Any,\n    chunk_intervals: Any,\n    drop_last: bool,\n) -> Tuple[List[List[int]], List[Any]]:\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"", "reference_steps": "1. Define a function `_associate_chunks_and_internals_to_ranks` that takes a `distributed_env` object, `indexes`, `chunk_intervals`, and a `drop_last` boolean as arguments and returns a tuple of two lists.\n\n2. Calculate the total number of items across all chunks by summing the differences between the end and start of each interval in `chunk_intervals`.\n\n3. Determine the number of items to assign to each rank in the distributed environment, taking into account whether to drop the last items or not.\n\n4. Initialize two lists of lists, `chunks_per_ranks` and `intervals_per_ranks`, with empty sublists for each rank in the distributed environment.\n\n5. Loop through each `chunk_index` and corresponding `chunk_interval` by zipping `indexes` and `chunk_intervals`.\n\n6. Initialize a variable `rank` to 0 to start assigning chunks to the first rank.\n\n7. Use a `while` loop to iterate over ranks and assign chunks and intervals to them until all items are assigned or there are no more ranks.\n\n8. Check if the current rank has no more items left to assign or if the chunk has no items, and if so, move to the next rank or exit the loop.\n\n9. If the current chunk has more items than the current rank can accept, assign as many items as possible to the current rank, update the `chunk_interval` and `num_items_per_ranks`, and move to the next rank.\n\n10. If the current chunk can be fully assigned to the current rank, do so and update the `num_items_per_ranks` accordingly, then break the loop to move on to the next chunk.\n\n11. Return the `chunks_per_ranks` and `intervals_per_ranks` lists, which now contain the distribution of chunks and intervals across the ranks.", "reference_code": "def _associate_chunks_and_internals_to_ranks(\n    distributed_env: _DistributedEnv,\n    indexes: Any,\n    chunk_intervals: Any,\n    drop_last: bool,\n) -> Tuple[List[List[int]], List[Any]]:\n    num_items = sum([(interval[-1] - interval[0]) for interval in chunk_intervals])\n    num_items_per_ranks: List[int] = [\n        num_items // distributed_env.world_size + num_items % distributed_env.world_size\n        if rank == distributed_env.world_size - 1 and not drop_last\n        else num_items // distributed_env.world_size\n        for rank in range(distributed_env.world_size)\n    ]\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[List[int]]] = [[] for _ in range(distributed_env.world_size)]\n\n    # 4. Assign the chunk & intervals to each rank\n    for chunk_index, chunk_interval in zip(indexes, chunk_intervals):\n        rank = 0\n\n        while True:\n            if rank == len(num_items_per_ranks):\n                break\n\n            items_left_to_assign = num_items_per_ranks[rank]\n\n            if items_left_to_assign == 0:\n                rank += 1\n                continue\n\n            items_in_chunk = chunk_interval[-1] - chunk_interval[0]\n\n            if items_in_chunk == 0:\n                break\n\n            if items_in_chunk > items_left_to_assign:\n                chunks_per_ranks[rank].append(chunk_index)\n                begin, end = chunk_interval\n                intervals_per_ranks[rank].append([begin, begin + items_left_to_assign])\n                chunk_interval = (begin + items_left_to_assign, end)\n                num_items_per_ranks[rank] = 0\n                rank += 1\n            else:\n                chunks_per_ranks[rank].append(chunk_index)\n                intervals_per_ranks[rank].append(chunk_interval)\n                num_items_per_ranks[rank] -= items_in_chunk\n                break\n\n    return chunks_per_ranks, intervals_per_ranks\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "type": "method", "class_name": "LambdaDataTransformRecipe", "function_name": "prepare_item", "dependency_all": "# Intra-class Dependency:\nlitdata.processing.functions.LambdaDataTransformRecipe._contains_device\n\nlitdata.processing.functions.LambdaDataTransformRecipe._contains_is_last\n\nlitdata.processing.functions.LambdaDataTransformRecipe._device\n\nlitdata.processing.functions.LambdaDataTransformRecipe._find_device\n    def _find_device(self) -> None:\n\nlitdata.processing.functions.LambdaDataTransformRecipe._fn\n\n", "dependency_sampled": "# Intra-class Dependency:\nlitdata.processing.functions.LambdaDataTransformRecipe._contains_is_last\n\nlitdata.processing.functions.LambdaDataTransformRecipe._device\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport concurrent.futures\nimport inspect\nimport os\nfrom datetime import datetime\nfrom functools import partial\nfrom pathlib import Path\nfrom types import FunctionType\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport torch\n\nfrom litdata.constants import _IS_IN_STUDIO, _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.processing.data_processor import DataChunkRecipe, DataProcessor, DataTransformRecipe\nfrom litdata.processing.readers import BaseReader\nfrom litdata.processing.utilities import optimize_dns_context\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import (\n    Dir,\n    _assert_dir_has_index_file,\n    _assert_dir_is_empty,\n    _execute,\n    _resolve_dir,\n)\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten\n\n\ndef _get_indexed_paths(data: Any) -> Dict[int, str]:\n    flattened_item, _ = tree_flatten(data)\n\n    indexed_paths = {\n        index: element\n        for index, element in enumerate(flattened_item)\n        if isinstance(element, str) and os.path.exists(element)\n    }\n\n    return indexed_paths\n\n\ndef _get_input_dir(inputs: Sequence[Any]) -> Optional[str]:\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(indexed_paths) == 0:\n        # Check whether the second element has any input_path\n        indexed_paths = _get_indexed_paths(inputs[1])\n        if len(indexed_paths) == 0:\n            return None\n\n        # Every element should have filepaths if any contains one.\n        raise ValueError(f\"The provided item {inputs[0]} didn't contain any filepaths.\")\n\n    absolute_path = str(Path(list(indexed_paths.values())[0]).resolve())\n\n    if \"/.project\" in absolute_path:\n        return \"/\" + os.path.join(*str(list(indexed_paths.values())[0]).split(\"/\")[:4])\n\n    return \"/\" + os.path.join(*str(absolute_path).split(\"/\")[:4])\n\n\ndef _get_default_num_workers() -> int:\n    if torch.cuda.is_available():\n        return torch.cuda.device_count()\n    return os.cpu_count() or 1\n\n\nclass LambdaDataTransformRecipe(DataTransformRecipe):\n    def __init__(self, fn: Callable[[str, Any], None], inputs: Sequence[Any]):\n        super().__init__()\n        self._fn = fn\n        self._inputs = inputs\n        self._device: Optional[str] = None\n\n        _fn = self._fn if isinstance(self._fn, FunctionType) else self._fn.__call__  # type: ignore\n        params = inspect.signature(_fn).parameters\n        self._contains_device = \"device\" in params\n        self._contains_is_last = \"is_last\" in params\n\n    def prepare_structure(self, _: Optional[str]) -> Any:\n        return self._inputs\n\n", "contexts_below": "\n    def _find_device(self) -> None:\n        global_rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n        if torch.cuda.is_available() and global_rank:\n            num_gpus = torch.cuda.device_count()\n            device = int(global_rank) % num_gpus\n            self._device = f\"cuda:{device}\"\n\n\nclass LambdaDataChunkRecipe(DataChunkRecipe):\n    def __init__(\n        self,\n        fn: Callable[[Any], None],\n        inputs: Sequence[Any],\n        chunk_size: Optional[int],\n        chunk_bytes: Optional[Union[int, str]],\n        compression: Optional[str],\n    ):\n        super().__init__(chunk_size=chunk_size, chunk_bytes=chunk_bytes, compression=compression)\n        self._fn = fn\n        self._inputs = inputs\n\n    def prepare_structure(self, input_dir: Optional[str]) -> Any:\n        return self._inputs\n\n    def prepare_item(self, item_metadata: Any) -> Any:\n        if isinstance(self._fn, partial):\n            yield from self._fn(item_metadata)\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                yield from self._fn(item_metadata)\n            else:\n                yield self._fn(item_metadata)\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                yield from self._fn.__call__(item_metadata)  # type: ignore\n            else:\n                yield self._fn.__call__(item_metadata)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n\ndef map(\n    fn: Callable[[str, Any], None],\n    inputs: Sequence[Any],\n    output_dir: Union[str, Dir],\n    weights: Optional[List[int]] = None,\n    num_workers: Optional[int] = None,\n    fast_dev_run: Union[bool, int] = False,\n    num_nodes: Optional[int] = None,\n    machine: Optional[str] = None,\n    num_downloaders: Optional[int] = None,\n    num_uploaders: Optional[int] = None,\n    reorder_files: bool = True,\n    error_when_not_empty: bool = False,\n    reader: Optional[BaseReader] = None,\n    batch_size: Optional[int] = None,\n) -> None:\n    \"\"\"This function map a callbable over a collection of files possibly in a distributed way.\n\n    Arguments:\n        fn: A function to be executed over each input element\n        inputs: A sequence of input to be processed by the `fn` function.\n            Each input should contain at least a valid filepath.\n        output_dir: The folder where the processed data should be stored.\n        weights: Provide an associated weight to each input. This is used to balance work among workers.\n        num_workers: The number of workers to use during processing\n        fast_dev_run: Whether to use process only a sub part of the inputs\n        num_nodes: When doing remote execution, the number of nodes to use. Only supported on https://lightning.ai/.\n        machine: When doing remote execution, the machine to use. Only supported on https://lightning.ai/.\n        num_downloaders: The number of downloaders per worker.\n        num_uploaders: The number of uploaders per workers.\n        reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n            Set this to ``False`` if the order in which samples are processed should be preserved.\n        error_when_not_empty: Whether we should error if the output folder isn't empty.\n        batch_size: Group the inputs into batches of batch_size length.\n\n    \"\"\"\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(LambdaDataTransformRecipe(fn, inputs))\n    return _execute(\n        f\"data-prep-map-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )\n\n\ndef optimize(\n    fn: Callable[[Any], Any],\n    inputs: Sequence[Any],\n    output_dir: str,\n    weights: Optional[List[int]] = None,\n    chunk_size: Optional[int] = None,\n    chunk_bytes: Optional[Union[int, str]] = None,\n    compression: Optional[str] = None,\n    num_workers: Optional[int] = None,\n    fast_dev_run: bool = False,\n    num_nodes: Optional[int] = None,\n    machine: Optional[str] = None,\n    num_downloaders: Optional[int] = None,\n    num_uploaders: Optional[int] = None,\n    reorder_files: bool = True,\n    reader: Optional[BaseReader] = None,\n    batch_size: Optional[int] = None,\n) -> None:\n    \"\"\"This function converts a dataset into chunks possibly in a distributed way.\n\n    Arguments:\n        fn: A function to be executed over each input element\n        inputs: A sequence of input to be processed by the `fn` function.\n            Each input should contain at least a valid filepath.\n        output_dir: The folder where the processed data should be stored.\n        weights: Provide an associated weight to each input. This is used to balance work among workers.\n        chunk_size: The maximum number of elements to hold within a chunk.\n        chunk_bytes: The maximum number of bytes to hold within a chunk.\n        compression: The compression algorithm to use over the chunks.\n        num_workers: The number of workers to use during processing\n        fast_dev_run: Whether to use process only a sub part of the inputs\n        num_nodes: When doing remote execution, the number of nodes to use. Only supported on https://lightning.ai/.\n        machine: When doing remote execution, the machine to use. Only supported on https://lightning.ai/.\n        num_downloaders: The number of downloaders per worker.\n        num_uploaders: The numbers of uploaders per worker.\n        reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n            Set this to ``False`` if the order in which samples are processed should be preserved.\n        batch_size: Group the inputs into batches of batch_size length.\n\n    \"\"\"\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if chunk_size is None and chunk_bytes is None:\n        raise ValueError(\"Either `chunk_size` or `chunk_bytes` needs to be defined.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            reader=reader,\n        )\n\n        with optimize_dns_context(True):\n            data_processor.run(\n                LambdaDataChunkRecipe(\n                    fn,\n                    inputs,\n                    chunk_size=chunk_size,\n                    chunk_bytes=chunk_bytes,\n                    compression=compression,\n                )\n            )\n        return None\n    return _execute(\n        f\"data-prep-optimize-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )\n\n\ndef _listdir(folder: str) -> Tuple[str, List[str]]:\n    return folder, os.listdir(folder)\n\n\nclass walk:\n    \"\"\"This class is an optimized version of os.walk for listing files and folders from cloud filesystem.\n\n    Note: The order of files and folders yielded aren't depth-first anymore due to the asynchronous listing call.\n\n    \"\"\"\n\n    def __init__(self, folder: str, max_workers: Optional[int] = os.cpu_count()) -> None:\n        self.folders = [folder]\n        self.max_workers = max_workers or 1\n        self.futures: List[concurrent.futures.Future] = []\n\n        if not _IS_IN_STUDIO:\n            print(\"This method is optimized to run on https://lightning.ai/. Don't use it otherwise.\")\n\n    def __iter__(self) -> Any:\n        \"\"\"This function queues the folders to perform listdir across multiple workers.\"\"\"\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            while len(self.folders):\n                folder = self.folders.pop(0)\n                future = executor.submit(_listdir, folder)\n                self.futures.append(future)\n\n            while self.futures:\n                for future in concurrent.futures.as_completed(self.futures):\n                    filenames = []\n                    folders = []\n\n                    folder, files_or_folders = future.result()\n                    self.futures = [f for f in self.futures if f != future]\n\n                    for file_or_folder in files_or_folders:\n                        if os.path.isfile(os.path.join(folder, file_or_folder)):\n                            filenames.append(file_or_folder)\n                        else:\n                            folders.append(file_or_folder)\n                            self.folders.append(os.path.join(folder, file_or_folder))\n\n                    yield folder, folders, filenames\n\n                    while len(self.folders) and len(self.futures) <= self.max_workers * 2:\n                        folder = self.folders.pop(0)\n                        future = executor.submit(_listdir, folder)\n                        self.futures.append(future)\n        return\n", "input_code": "    def prepare_item(self, item_metadata: Any, output_dir: str, is_last: bool) -> None:\n\n        \"\"\"\n        The function 'prepare_item' prepares an item by applying a transformation function ('_fn') to the item's metadata, potentially including additional context like device information and a flag indicating if it's the last item. This is used within a data transformation recipe context, where items are processed and transformed according to a specified lambda function or callable.\n\n        Input-Output Arguments\n        :param item_metadata: Any, the metadata of the item to be transformed. It is used as the first argument in the transformation function.\n        :param output_dir: str, the directory where the transformed item's output should be stored. It is passed as the second argument to the transformation function.\n        :param is_last: bool, a flag indicating whether the item is the last one in the sequence to be processed. It is conditionally added to the keyword arguments passed to the transformation function if '_contains_is_last' is True.\n        :return: No return values. The function directly calls the transformation function ('_fn') with the provided arguments and keyword arguments, affecting external state or outputs rather than returning a value.\n        \"\"\"", "reference_steps": "1. Check if the instance requires a device (`self._contains_device`) and if the device is not already set (`self._device is None`), then call the method to find and set the device (`self._find_device()`).\n\n2. Initialize an empty dictionary `kwargs` to hold keyword arguments that may be passed to the function `self._fn`.\n\n3. If the instance requires a device (`self._contains_device`), add the device to the `kwargs` dictionary with the key `\"device\"`.\n\n4. If the instance has an attribute indicating whether the current item is the last (`self._contains_is_last`), add the `is_last` boolean value to the `kwargs` dictionary with the key `\"is_last\"`.\n\n5. Check if the function to be called (`self._fn`) is either a standard Python function (`FunctionType`) or a `partial` function.\n\n6. If `self._fn` is a function or partial, call it with `item_metadata`, `output_dir`, and the additional keyword arguments from `kwargs`.\n\n7. If `self._fn` is not a function or partial but is still callable (e.g., an object with a `__call__` method), use its `__call__` method with `item_metadata`, `output_dir`, and the keyword arguments from `kwargs`.\n\n8. If `self._fn` is neither a function, partial, nor a callable object, raise a `ValueError` indicating that the provided function is not supported.\n\n9. The function `prepare_item` does not return any value; it is intended to execute the provided function `self._fn` with the appropriate arguments and keyword arguments.\n\n10. The function `prepare_item` is designed to be a method within a class, indicated by the `self` parameter, and is meant to be used on an instance of that class.", "reference_code": "def prepare_item(self, item_metadata: Any, output_dir: str, is_last: bool) -> None:\n    if self._contains_device and self._device is None:\n        self._find_device()\n\n    kwargs: Dict[str, Any] = {}\n\n    if self._contains_device:\n        kwargs[\"device\"] = self._device\n\n    if self._contains_is_last:\n        kwargs[\"is_last\"] = is_last\n\n    if isinstance(self._fn, (FunctionType, partial)):\n        self._fn(item_metadata, output_dir, **kwargs)\n\n    elif callable(self._fn):\n        self._fn.__call__(item_metadata, output_dir, **kwargs)  # type: ignore\n    else:\n        raise ValueError(f\"The provided {self._fn} isn't supported.\")\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "type": "function", "class_name": null, "function_name": "_wait_for_file_to_exist", "dependency_all": "# Cross-file Dependency:\nlitdata.streaming.client.S3Client\n    class S3Client:\n\nlitdata.streaming.client.S3Client.client\n    def client(self) -> Any:\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.streaming.client.S3Client\n    class S3Client:\n\n", "contexts_above": "import concurrent\nimport json\nimport logging\nimport os\nimport random\nimport shutil\nimport signal\nimport tempfile\nimport traceback\nimport types\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom multiprocessing import Process, Queue\nfrom pathlib import Path\nfrom queue import Empty\nfrom time import sleep, time\nfrom typing import Any, Dict, List, Optional, Tuple, TypeVar, Union\nfrom urllib import parse\n\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm as _tqdm\n\nfrom litdata.constants import (\n    _BOTO3_AVAILABLE,\n    _DEFAULT_FAST_DEV_RUN_ITEMS,\n    _INDEX_FILENAME,\n    _IS_IN_STUDIO,\n    _LIGHTNING_CLOUD_LATEST,\n    _TORCH_GREATER_EQUAL_2_1_0,\n)\nfrom litdata.processing.readers import BaseReader, StreamingDataLoaderReader\nfrom litdata.processing.utilities import _create_dataset\nfrom litdata.streaming import Cache\nfrom litdata.streaming.cache import Dir\nfrom litdata.streaming.client import S3Client\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.broadcast import broadcast_object\nfrom litdata.utilities.packing import _pack_greedily\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten, tree_unflatten, treespec_loads\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\nif _BOTO3_AVAILABLE:\n    import botocore\n\nlogger = logging.Logger(__name__)\n\n\ndef _get_num_nodes() -> int:\n    \"\"\"Returns the number of nodes.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 1))\n\n\ndef _get_node_rank() -> int:\n    \"\"\"Returns the current node rank of the instance.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NODE_RANK\", 0))\n\n\ndef _get_fast_dev_run() -> int:\n    \"\"\"Returns whether fast dev mode is enabled.\"\"\"\n    return bool(int(os.getenv(\"DATA_OPTIMIZER_FAST_DEV_RUN\", 1)))\n\n\ndef _get_default_cache() -> str:\n    return \"/cache\" if _IS_IN_STUDIO else tempfile.gettempdir()\n\n\ndef _get_cache_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache directory used by the Cache to store the chunks.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_FOLDER\", f\"{_get_default_cache()}/chunks\")\n    if name is None:\n        return cache_dir\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\ndef _get_cache_data_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache data directory used by the DataProcessor workers to download the files.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_DATA_CACHE_FOLDER\", f\"{_get_default_cache()}/data\")\n    if name is None:\n        return os.path.join(cache_dir)\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\n", "contexts_below": "\n\ndef _wait_for_disk_usage_higher_than_threshold(input_dir: str, threshold_in_gb: int = 25, sleep_time: int = 3) -> None:\n    usage = shutil.disk_usage(input_dir)\n\n    while (usage.free / 1000 / 1000 / 1000) <= threshold_in_gb:\n        sleep(sleep_time)\n        usage = shutil.disk_usage(input_dir)\n\n    return\n\n\ndef _download_data_target(input_dir: Dir, cache_dir: str, queue_in: Queue, queue_out: Queue) -> None:\n    \"\"\"This function is used to download data from a remote directory to a cache directory to optimise reading.\"\"\"\n    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            queue_out.put(None)\n            return\n\n        # 4. Unpack\n        index, paths = r\n\n        # 5. Check whether all the files are already downloaded\n        if input_dir.path and all(\n            os.path.exists(p.replace(input_dir.path, cache_dir) if input_dir else p) for p in paths\n        ):\n            queue_out.put(index)\n            continue\n\n        if input_dir.url is not None or input_dir.path is not None:\n            if input_dir.url:\n                # 6. Wait for the removers to catch up when we are downloading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            # 7. Download all the required paths to unblock the current index\n            for path in paths:\n                if input_dir.path:\n                    local_path = path.replace(input_dir.path, cache_dir)\n\n                if input_dir.url and input_dir.path:\n                    path = path.replace(input_dir.path, input_dir.url)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    dirpath = os.path.dirname(local_path)\n\n                    os.makedirs(dirpath, exist_ok=True)\n\n                    with open(local_path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n                elif os.path.isfile(path):\n                    if not path.startswith(\"/teamspace/studios/this_studio\"):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copyfile(path, local_path)\n                else:\n                    raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n\n        # 7. Inform the worker the current files are available\n        queue_out.put(index)\n\n\ndef _remove_target(input_dir: Dir, cache_dir: str, queue_in: Queue) -> None:\n    \"\"\"This function is used to delete files from the cache directory to minimise disk space.\"\"\"\n    while True:\n        # 1. Collect paths\n        paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and delete them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    os.remove(path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.remove(path)\n\n\ndef _upload_fn(upload_queue: Queue, remove_queue: Queue, cache_dir: str, output_dir: Dir) -> None:\n    \"\"\"This function is used to upload optimised chunks from a local to remote dataset directory.\"\"\"\n    obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        tmpdir = None\n\n        if isinstance(data, str) or data is None:\n            local_filepath = data\n        else:\n            tmpdir, local_filepath = data\n\n        # Terminate the process if we received a termination signal\n        if local_filepath is None:\n            return\n\n        # Upload the file to the target cloud storage\n        if not local_filepath.startswith(cache_dir):\n            local_filepath = os.path.join(cache_dir, local_filepath)\n\n        if obj.scheme == \"s3\":\n            try:\n                if tmpdir is None:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n                else:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), local_filepath.replace(tmpdir, \"\")[1:])\n\n                s3.client.upload_file(\n                    local_filepath,\n                    obj.netloc,\n                    output_filepath,\n                )\n            except Exception as e:\n                print(e)\n\n        elif output_dir.path:\n            if tmpdir is None:\n                output_filepath = os.path.join(output_dir.path, os.path.basename(local_filepath))\n            else:\n                output_filepath = os.path.join(output_dir.path, local_filepath.replace(tmpdir, \"\")[1:])\n\n            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n            shutil.move(local_filepath, output_filepath)\n        else:\n            raise ValueError(f\"The provided {output_dir.path} isn't supported.\")\n\n        # Inform the remover to delete the file\n        if remove_queue and os.path.exists(local_filepath):\n            remove_queue.put([local_filepath])\n\n\ndef _map_items_to_workers_sequentially(num_workers: int, user_items: List[Any]) -> List[List[Any]]:\n\n    from typing import List, Any\n    import os\n    total_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = total_nodes * num_workers\n\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return result\n\n\ndef _map_items_to_workers_weighted(\n    num_workers: int,\n    user_items: List[Any],\n    weights: Optional[List[int]] = None,\n    file_size: bool = True,\n) -> List[List[Any]]:\n    # Associate the items to the workers based on number of nodes and node rank.\n    weights = [1] * len(user_items) if weights is None else weights\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    for worker_id, size in worker_weights.items():\n        if worker_id not in worker_ids_this_node:\n            continue\n\n        if file_size:\n            print(f\"Worker {worker_id} gets {size / 1e6:.1f} MB ({len(worker_items[worker_id])} files)\")\n        else:\n            print(f\"Worker {worker_id} gets ({len(worker_items[worker_id])}) items for a total weight of {size}.\")\n\n    return [np.random.permutation(worker_items[worker_id]).tolist() for worker_id in worker_ids_this_node]\n\n\ndef _get_num_bytes(item: Any, base_path: str) -> int:\n    flattened_item, _ = tree_flatten(item)\n\n    num_bytes = 0\n    for element in flattened_item:\n        if isinstance(element, str):\n            element = Path(element).resolve()\n            if not element.exists():\n                continue\n            file_bytes = os.path.getsize(element)\n            if file_bytes == 0:\n                raise RuntimeError(f\"The file {element} has 0 bytes!\")\n            num_bytes += file_bytes\n    return num_bytes\n\n\ndef _get_item_filesizes(items: List[Any], base_path: str = \"\") -> List[int]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        for future in futures:\n            item_sizes.append(future.result())\n    return item_sizes\n\n\ndef _to_path(element: str) -> str:\n    return element if _IS_IN_STUDIO and element.startswith(\"/teamspace\") else str(Path(element).resolve())\n\n\ndef _is_path(input_dir: Optional[str], element: Any) -> bool:\n    if not isinstance(element, str):\n        return False\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n        if element.startswith(input_dir):\n            return True\n\n    return os.path.exists(element)\n\n\nclass BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        data_recipe: \"DataRecipe\",\n        input_dir: Dir,\n        output_dir: Dir,\n        items: List[Any],\n        progress_queue: Queue,\n        error_queue: Queue,\n        stop_queue: Queue,\n        num_downloaders: int,\n        num_uploaders: int,\n        remove: bool,\n        reader: Optional[BaseReader] = None,\n    ) -> None:\n        \"\"\"The BaseWorker is responsible to process the user data.\"\"\"\n        self.worker_index = worker_index\n        self.num_workers = num_workers\n        self.node_rank = node_rank\n        self.data_recipe = data_recipe\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.items = items\n        self.num_items = len(self.items)\n        self.num_downloaders = num_downloaders\n        self.num_uploaders = num_uploaders\n        self.remove = remove\n        self.reader = reader\n        self.paths: List[List[str]] = []\n        self.remover: Optional[Process] = None\n        self.downloaders: List[Process] = []\n        self.uploaders: List[Process] = []\n        self.to_download_queues: List[Queue] = []\n        self.to_upload_queues: List[Queue] = []\n        self.stop_queue = stop_queue\n        self.ready_to_process_queue: Queue = Queue()\n        self.remove_queue: Queue = Queue()\n        self.progress_queue: Queue = progress_queue\n        self.error_queue: Queue = error_queue\n        self._counter = 0\n        self._last_time = time()\n        self._index_counter = 0\n\n    def run(self) -> None:\n        try:\n            self._setup()\n            self._loop()\n        except Exception:\n            traceback_format = traceback.format_exc()\n            print(traceback_format)\n            self.error_queue.put(traceback_format)\n        print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is done.\")\n\n    def _setup(self) -> None:\n        self._set_environ_variables()\n        self._create_cache()\n        self._collect_paths()\n        self._start_downloaders()\n        self._start_uploaders()\n        self._start_remover()\n\n    def _loop(self) -> None:\n        num_downloader_finished = 0\n\n        while True:\n            index = self.ready_to_process_queue.get()\n\n            if index is None:\n                num_downloader_finished += 1\n                if num_downloader_finished == self.num_downloaders:\n                    print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is terminating.\")\n\n                    if isinstance(self.data_recipe, DataChunkRecipe):\n                        self._handle_data_chunk_recipe_end()\n\n                    if self.output_dir.url if self.output_dir.url else self.output_dir.path:\n                        # Inform the uploaders they are doing working\n                        for i in range(self.num_uploaders):\n                            self.to_upload_queues[i].put(None)\n\n                        # Wait for them all to be finished\n                        for uploader in self.uploaders:\n                            uploader.join()\n\n                    if self.remove:\n                        assert self.remover\n                        self.remove_queue.put(None)\n                        self.remover.join()\n\n                    if self.progress_queue:\n                        self.progress_queue.put((self.worker_index, self._counter))\n                    return\n                continue\n\n            if isinstance(self.data_recipe, DataChunkRecipe):\n                self._handle_data_chunk_recipe(index)\n            else:\n                self._handle_data_transform_recipe(index)\n\n            self._counter += 1\n\n            # Don't send the last progress update, so the main thread awaits for the uploader and remover\n            if self.progress_queue and (time() - self._last_time) > 1 and self._counter < (self.num_items - 2):\n                self.progress_queue.put((self.worker_index, self._counter))\n                self._last_time = time()\n\n            if self.remove and self.input_dir.path is not None and self.reader is None:\n                self.remove_queue.put(self.paths[index])\n\n            try:\n                self.stop_queue.get(timeout=0.0001)\n                return\n            except Empty:\n                pass\n\n    def _set_environ_variables(self) -> None:\n        # set the optimizer global rank and world_size\n        os.environ[\"DATA_OPTIMIZER_GLOBAL_RANK\"] = str(_get_node_rank() * self.num_workers + self.worker_index)\n        os.environ[\"DATA_OPTIMIZER_NUM_WORKERS\"] = str(self.num_workers)\n\n    def _create_cache(self) -> None:\n        self.cache_data_dir = _get_cache_data_dir()\n        os.makedirs(self.cache_data_dir, exist_ok=True)\n\n        self.cache_chunks_dir = _get_cache_dir()\n        os.makedirs(self.cache_chunks_dir, exist_ok=True)\n\n        if isinstance(self.data_recipe, DataTransformRecipe):\n            return\n\n        self.cache = Cache(\n            self.cache_chunks_dir,\n            chunk_bytes=self.data_recipe.chunk_bytes,\n            chunk_size=self.data_recipe.chunk_size,\n            compression=self.data_recipe.compression,\n        )\n        self.cache._reader._rank = _get_node_rank() * self.num_workers + self.worker_index\n\n    def _try_upload(self, data: Optional[Union[str, Tuple[str, str]]]) -> None:\n        if not data or (self.output_dir.url if self.output_dir.url else self.output_dir.path) is None:\n            return\n\n        if isinstance(data, str):\n            assert os.path.exists(data), data\n        else:\n            assert os.path.exists(data[-1]), data\n\n        self.to_upload_queues[self._counter % self.num_uploaders].put(data)\n\n    def _collect_paths(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            for index in range(len(self.items)):\n                self.ready_to_process_queue.put(index)\n            for _ in range(self.num_downloaders):\n                self.ready_to_process_queue.put(None)\n            return\n\n        items = []\n        for item in self.items:\n            flattened_item, spec = tree_flatten(item)\n\n            # For speed reasons, we assume starting with `self.input_dir` is enough to be a real file.\n            # Other alternative would be too slow.\n            # TODO: Try using dictionary for higher accurary.\n            indexed_paths = {\n                index: _to_path(element)\n                for index, element in enumerate(flattened_item)\n                if _is_path(self.input_dir.path, element)\n            }\n\n            if len(indexed_paths) == 0:\n                raise ValueError(\n                    f\"The provided item {item} didn't contain any filepaths. The input_dir is {self.input_dir.path}.\"\n                )\n\n            paths = []\n            for index, path in indexed_paths.items():\n                paths.append(path)\n                if self.input_dir and not self.input_dir.path.startswith(\"/teamspace/studios/this_studio\"):\n                    path = path.replace(self.input_dir.path, self.cache_data_dir)\n                flattened_item[index] = path\n\n            self.paths.append(paths)\n\n            items.append(tree_unflatten(flattened_item, spec))\n\n        self.items = items\n\n    def _start_downloaders(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            return\n\n        for _ in range(self.num_downloaders):\n            to_download_queue: Queue = Queue()\n            p = Process(\n                target=_download_data_target,\n                args=(\n                    self.input_dir,\n                    self.cache_data_dir,\n                    to_download_queue,\n                    self.ready_to_process_queue,\n                ),\n            )\n            p.start()\n            self.downloaders.append(p)\n            self.to_download_queues.append(to_download_queue)\n\n        for index, paths in enumerate(self.paths):\n            self.to_download_queues[index % self.num_downloaders].put((index, paths))\n\n        for downloader_index in range(self.num_downloaders):\n            self.to_download_queues[downloader_index].put(None)\n\n    def _start_remover(self) -> None:\n        if not self.remove:\n            return\n\n        self.remover = Process(\n            target=_remove_target,\n            args=(\n                self.input_dir,\n                self.cache_data_dir,\n                self.remove_queue,\n            ),\n        )\n        self.remover.start()\n\n    def _start_uploaders(self) -> None:\n        if self.output_dir.path is None and self.output_dir.url is None:\n            return\n\n        for _ in range(self.num_uploaders):\n            to_upload_queue: Queue = Queue()\n            p = Process(\n                target=_upload_fn,\n                args=(\n                    to_upload_queue,\n                    self.remove_queue,\n                    self.cache_chunks_dir,\n                    self.output_dir,\n                ),\n            )\n            p.start()\n            self.uploaders.append(p)\n            self.to_upload_queues.append(to_upload_queue)\n\n    def _handle_data_chunk_recipe(self, index: int) -> None:\n        try:\n            current_item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n            item_data_or_generator = self.data_recipe.prepare_item(current_item)\n            if isinstance(item_data_or_generator, types.GeneratorType):\n                for item_data in item_data_or_generator:\n                    if item_data is not None:\n                        chunk_filepath = self.cache._add_item(self._index_counter, item_data)\n                        self._try_upload(chunk_filepath)\n                        self._index_counter += 1\n            elif item_data_or_generator is not None:\n                chunk_filepath = self.cache._add_item(self._index_counter, item_data_or_generator)\n                self._try_upload(chunk_filepath)\n                self._index_counter += 1\n        except Exception as e:\n            raise RuntimeError(f\"Failed processing {self.items[index]}\") from e\n\n    def _handle_data_chunk_recipe_end(self) -> None:\n        chunks_filepaths = self.cache.done()\n\n        if chunks_filepaths and len(self.to_upload_queues):\n            for i, chunk_filepath in enumerate(chunks_filepaths):\n                if isinstance(chunk_filepath, str) and os.path.exists(chunk_filepath):\n                    self.to_upload_queues[i % self.num_uploaders].put(chunk_filepath)\n\n    def _handle_data_transform_recipe(self, index: int) -> None:\n        # Don't use a context manager to avoid deleting files that are being uploaded.\n        output_dir = tempfile.mkdtemp()\n        item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n        item_data = self.data_recipe.prepare_item(item, str(output_dir), len(self.items) - 1 == index)\n        if item_data is not None:\n            raise ValueError(\n                \"When using a `DataTransformRecipe`, the `prepare_item` shouldn't return anything.\"\n                \" Simply store your files under the output_dir.\"\n            )\n        filepaths = []\n        for directory, _, filenames in os.walk(output_dir):\n            for filename in filenames:\n                filepaths.append(os.path.join(directory, filename))\n\n        for filepath in filepaths:\n            self._try_upload((output_dir, filepath))\n\n\nclass DataWorkerProcess(BaseWorker, Process):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"The DataWorkerProcess is responsible to process the user data inside processes.\"\"\"\n        BaseWorker.__init__(self, *args, **kwargs)\n        Process.__init__(self)\n\n\n@dataclass\nclass _Result:\n    size: Optional[int] = None\n    num_bytes: Optional[str] = None\n    data_format: Optional[str] = None\n    compression: Optional[str] = None\n    num_chunks: Optional[int] = None\n    num_bytes_per_chunk: Optional[List[int]] = None\n\n\nT = TypeVar(\"T\")\n\n\nclass DataRecipe:\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        pass\n\n    @abstractmethod\n    def prepare_item(self, *args: Any, **kwargs: Any) -> Any:\n        pass\n\n    def __init__(self) -> None:\n        self._name: Optional[str] = None\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        return _Result(size=size)\n\n\nclass DataChunkRecipe(DataRecipe):\n    def __init__(\n        self,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n    ):\n        super().__init__()\n        if chunk_size is not None and chunk_bytes is not None:\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self.chunk_size = chunk_size\n        self.chunk_bytes = 1 << 26 if chunk_size is None else chunk_bytes\n        self.compression = compression\n\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T) -> Any:\n        \"\"\"The return of this `prepare_item` method is persisted in chunked binary files.\"\"\"\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        num_nodes = _get_num_nodes()\n        cache_dir = _get_cache_dir()\n\n        chunks = [file for file in os.listdir(cache_dir) if file.endswith(\".bin\")]\n        if chunks and delete_cached_files and output_dir.path is not None:\n            raise RuntimeError(f\"All the chunks should have been deleted. Found {chunks}\")\n\n        merge_cache = Cache(cache_dir, chunk_bytes=1)\n        node_rank = _get_node_rank()\n        merge_cache._merge_no_wait(node_rank if num_nodes > 1 else None)\n        self._upload_index(output_dir, cache_dir, num_nodes, node_rank)\n\n        if num_nodes == node_rank + 1:\n            with open(os.path.join(cache_dir, _INDEX_FILENAME)) as f:\n                config = json.load(f)\n\n            size = sum([c[\"dim\"] if c[\"dim\"] is not None else c[\"chunk_size\"] for c in config[\"chunks\"]])\n            num_bytes = sum([c[\"chunk_bytes\"] for c in config[\"chunks\"]])\n            if config[\"config\"] is not None:\n                data_format = tree_unflatten(\n                    config[\"config\"][\"data_format\"], treespec_loads(config[\"config\"][\"data_spec\"])\n                )\n            else:\n                data_format = None\n            num_chunks = len(config[\"chunks\"])\n\n            # The platform can't store more than 1024 entries.\n            # Note: This isn't really used right now, so it is fine to skip if too big.\n            num_bytes_per_chunk = [c[\"chunk_size\"] for c in config[\"chunks\"]] if num_chunks < 1024 else []\n\n            return _Result(\n                size=size,\n                num_bytes=num_bytes,\n                data_format=data_format,\n                compression=config[\"config\"][\"compression\"] if config[\"config\"] else None,\n                num_chunks=len(config[\"chunks\"]),\n                num_bytes_per_chunk=num_bytes_per_chunk,\n            )\n        return _Result(\n            size=size,\n        )\n\n    def _upload_index(self, output_dir: Dir, cache_dir: str, num_nodes: int, node_rank: Optional[int]) -> None:\n        \"\"\"This method upload the index file to the remote cloud directory.\"\"\"\n        if output_dir.path is None and output_dir.url is None:\n            return\n\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n        if num_nodes > 1:\n            local_filepath = os.path.join(cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")\n        else:\n            local_filepath = os.path.join(cache_dir, _INDEX_FILENAME)\n\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(\n                local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        if num_nodes == 1 or node_rank is None:\n            return\n\n        # Merge the index files generated by each node.\n        # Note: When using the Data Optimizer, they should be a single process on each node executing this section\n        # So no risk to get race conditon.\n        if num_nodes == node_rank + 1:\n            # Get the index file locally\n            for node_rank in range(num_nodes - 1):\n                output_dir_path = output_dir.url if output_dir.url else output_dir.path\n                assert output_dir_path\n                remote_filepath = os.path.join(output_dir_path, f\"{node_rank}-{_INDEX_FILENAME}\")\n                node_index_filepath = os.path.join(cache_dir, os.path.basename(remote_filepath))\n                if obj.scheme == \"s3\":\n                    obj = parse.urlparse(remote_filepath)\n                    _wait_for_file_to_exist(s3, obj)\n                    with open(node_index_filepath, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                elif output_dir.path and os.path.isdir(output_dir.path):\n                    shutil.copyfile(remote_filepath, node_index_filepath)\n\n            merge_cache = Cache(cache_dir, chunk_bytes=1)\n            merge_cache._merge_no_wait()\n            self._upload_index(output_dir, cache_dir, 1, None)\n\n\nclass DataTransformRecipe(DataRecipe):\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T, output_dir: str, is_last: bool) -> None:\n        \"\"\"Use your item metadata to process your files and save the file outputs into `output_dir`.\"\"\"\n\n\nclass DataProcessor:\n    def __init__(\n        self,\n        input_dir: Union[str, Dir],\n        output_dir: Optional[Union[str, Dir]] = None,\n        num_workers: Optional[int] = None,\n        num_downloaders: Optional[int] = None,\n        num_uploaders: Optional[int] = None,\n        delete_cached_files: bool = True,\n        fast_dev_run: Optional[Union[bool, int]] = None,\n        random_seed: Optional[int] = 42,\n        reorder_files: bool = True,\n        weights: Optional[List[int]] = None,\n        reader: Optional[BaseReader] = None,\n    ):\n        \"\"\"The `DatasetOptimiser` provides an efficient way to process data across multiple machine into chunks to make\n        training faster.\n\n        Arguments:\n            input_dir: The path to where the input data are stored.\n            output_dir: The path to where the output data are stored.\n            num_workers: The number of worker threads to use.\n            num_downloaders: The number of file downloaders to use.\n            num_uploaders: The number of file uploaders to use.\n            delete_cached_files: Whether to delete the cached files.\n            fast_dev_run: Whether to run a quick dev run.\n            random_seed: The random seed to be set before shuffling the data.\n            reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n                Set this to ``False`` if the order in which samples are processed should be preserved.\n            weights: Provide a list of weights associated to the inputs.\n                This is used to evenly split the work among the workers.\n            reader: Map the inputs to worker inputs and provides a read method to read a slice of the data.\n\n        \"\"\"\n        self.input_dir = _resolve_dir(input_dir)\n        self.output_dir = _resolve_dir(output_dir)\n        self.num_workers = num_workers or (1 if fast_dev_run else (os.cpu_count() or 1) * 4)\n        self.num_downloaders = num_downloaders or 2\n        self.num_uploaders = num_uploaders or 5\n        self.delete_cached_files = delete_cached_files\n        self.fast_dev_run = _get_fast_dev_run() if fast_dev_run is None else fast_dev_run\n        self.workers: Any = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.reorder_files = reorder_files\n        self.weights = weights\n        self.reader = reader\n\n        if self.reader is not None and self.weights is not None:\n            raise ValueError(\"Either the reader or the weights needs to be defined.\")\n\n        # Ensure the input dir is the same across all nodes\n        self.input_dir = broadcast_object(\"input_dir\", self.input_dir)\n\n        if self.output_dir:\n            # Ensure the output dir is the same across all nodes\n            self.output_dir = broadcast_object(\"output_dir\", self.output_dir)\n            print(f\"Storing the files under {self.output_dir.path}\")\n\n        self.random_seed = random_seed\n\n    def run(self, data_recipe: DataRecipe) -> None:\n        \"\"\"The `DataProcessor.run(...)` method triggers the data recipe processing over your dataset.\"\"\"\n        if not isinstance(data_recipe, DataRecipe):\n            raise ValueError(\"The provided value should be a data recipe.\")\n\n        t0 = time()\n        print(f\"Setup started with fast_dev_run={self.fast_dev_run}.\")\n\n        # Force random seed to be fixed\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n        torch.manual_seed(self.random_seed)\n\n        # Call the setup method of the user\n        user_items: List[Any] = data_recipe.prepare_structure(self.input_dir.path if self.input_dir else None)\n\n        if not isinstance(user_items, (list, StreamingDataLoader)):\n            raise ValueError(\"The `prepare_structure` should return a list of item metadata.\")\n\n        if isinstance(user_items, StreamingDataLoader):\n            self.reader = StreamingDataLoaderReader(user_items)\n\n        if self.reader:\n            user_items = self.reader.remap_items(user_items, self.num_workers)\n\n        if self.weights is not None:\n            if len(self.weights) != len(user_items):\n                raise ValueError(\"The provided weights length should match the inputs' length.\")\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=self.weights, file_size=False\n            )\n\n        elif self.reorder_files and self.input_dir.path:\n            # TODO: Only do this on node 0, and broadcast the item sizes to the other nodes.\n            item_sizes = _get_item_filesizes(user_items, base_path=self.input_dir.path)\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=item_sizes\n            )\n        else:\n            workers_user_items = _map_items_to_workers_sequentially(num_workers=self.num_workers, user_items=user_items)\n\n        print(f\"Setup finished in {round(time() - t0, 3)} seconds. Found {len(user_items)} items to process.\")\n\n        if self.fast_dev_run:\n            items_to_keep = self.fast_dev_run if type(self.fast_dev_run) is int else _DEFAULT_FAST_DEV_RUN_ITEMS\n            workers_user_items = [w[:items_to_keep] for w in workers_user_items]\n            print(f\"Fast dev run is enabled. Limiting to {items_to_keep} items per process.\")\n\n        num_items = sum([len(items) for items in workers_user_items])\n\n        self._cleanup_cache()\n\n        print(f\"Starting {self.num_workers} workers with {num_items} items.\")\n\n        if self.input_dir is None and self.src_resolver is not None and self.input_dir:\n            self.input_dir = self.src_resolver(self.input_dir)\n            print(f\"The remote_dir is `{self.input_dir}`.\")\n\n        signal.signal(signal.SIGINT, self._signal_handler)\n\n        self._create_process_workers(data_recipe, workers_user_items)\n\n        print(\"Workers are ready ! Starting data processing...\")\n\n        current_total = 0\n        has_failed = False\n        pbar = _tqdm(\n            desc=\"Progress\",\n            total=num_items,\n            smoothing=0,\n            position=-1,\n            mininterval=1,\n            leave=True,\n            dynamic_ncols=True,\n        )\n\n        while True:\n            try:\n                error = self.error_queue.get(timeout=0.001)\n                self._exit_on_error(error)\n            except Empty:\n                assert self.progress_queue\n                try:\n                    index, counter = self.progress_queue.get(timeout=0.001)\n                except Empty:\n                    continue\n                self.workers_tracker[index] = counter\n                new_total = sum(self.workers_tracker.values())\n\n            pbar.update(new_total - current_total)\n\n            current_total = new_total\n            if current_total == num_items:\n                break\n\n            # Exit early if all the workers are done.\n            # This means there were some kinda of errors.\n            if all(not w.is_alive() for w in self.workers):\n                has_failed = True\n                break\n\n        pbar.close()\n\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        # TODO: Understand why it hangs.\n        if num_nodes == 1:\n            for w in self.workers:\n                w.join(0)\n\n        print(\"Workers are finished.\")\n        result = data_recipe._done(len(user_items), self.delete_cached_files, self.output_dir)\n\n        if num_nodes == node_rank + 1 and self.output_dir.url and _IS_IN_STUDIO:\n            assert self.output_dir.path\n            _create_dataset(\n                input_dir=self.input_dir.path,\n                storage_dir=self.output_dir.path,\n                dataset_type=V1DatasetType.CHUNKED\n                if isinstance(data_recipe, DataChunkRecipe)\n                else V1DatasetType.TRANSFORMED,\n                empty=False,\n                size=result.size,\n                num_bytes=result.num_bytes,\n                data_format=result.data_format,\n                compression=result.compression,\n                num_chunks=result.num_chunks,\n                num_bytes_per_chunk=result.num_bytes_per_chunk,\n            )\n\n        print(\"Finished data processing!\")\n\n        # TODO: Understand why it is required to avoid long shutdown.\n        if _get_num_nodes() > 1:\n            os._exit(int(has_failed))\n\n    def _exit_on_error(self, error: str) -> None:\n        for w in self.workers:\n            w.join(0)\n        raise RuntimeError(f\"We found the following error {error}.\")\n\n    def _create_process_workers(self, data_recipe: DataRecipe, workers_user_items: List[List[Any]]) -> None:\n        self.progress_queue = Queue()\n        workers: List[DataWorkerProcess] = []\n        stop_queues: List[Queue] = []\n        for worker_idx, worker_user_items in enumerate(workers_user_items):\n            stop_queues.append(Queue())\n            worker = DataWorkerProcess(\n                worker_idx,\n                self.num_workers,\n                _get_node_rank(),\n                data_recipe,\n                self.input_dir,\n                self.output_dir,\n                worker_user_items,\n                self.progress_queue,\n                self.error_queue,\n                stop_queues[-1],\n                self.num_downloaders,\n                self.num_uploaders,\n                self.delete_cached_files,\n                self.reader,\n            )\n            worker.start()\n            workers.append(worker)\n\n        # Note: Don't store within the loop as weakref aren't serializable\n        self.workers = workers\n        self.stop_queues = stop_queues\n\n    def _signal_handler(self, signal: Any, frame: Any) -> None:\n        \"\"\"On temrination, we stop all the processes to avoid leaking RAM.\"\"\"\n        for stop_queue in self.stop_queues:\n            stop_queue.put(None)\n        for w in self.workers:\n            w.join(0)\n        os._exit(0)\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n", "input_code": "def _wait_for_file_to_exist(s3: S3Client, obj: parse.ParseResult, sleep_time: int = 2) -> Any:\n\n    \"\"\"\n    This function continuously checks if a specified file exists in an S3 bucket. It attempts to retrieve the file metadata using the head_object method. If the file is not found, it waits for a specified amount of time before trying again. If any other error occurs, the error is raised.\n    Input-Output Arguments\n    :param s3: S3Client. The S3 client used to interact with AWS S3.\n    :param obj: parse.ParseResult. The parsed result of the S3 object URL, containing the bucket name and object key.\n    :param sleep_time: int, optional. The amount of time in seconds to wait before retrying if the file does not exist. Defaults to 2 seconds.\n    :return: Any. The response from the head_object method if the file exists. This typically includes metadata about the S3 object.\n    \"\"\"", "reference_steps": "1. Define a function `_wait_for_file_to_exist` that takes an S3 client, a parsed S3 object URL, and an optional sleep time parameter with a default value of 2 seconds.\n\n2. Enter an infinite loop to continuously check for the existence of the specified file in S3.\n\n3. Within the loop, attempt to retrieve the metadata of the object using the `head_object` method of the S3 client.\n\n4. Pass the bucket name (`obj.netloc`) and the object key (`obj.path.lstrip(\"/\")`) to the `head_object` method to specify the S3 object to check.\n\n5. Handle exceptions by catching `botocore.exceptions.ClientError`.\n\n6. If the exception is due to the object not being found (identified by the specific error message \"the HeadObject operation: Not Found\"), pause the loop for the specified sleep time using the `sleep` function.\n\n7. If the exception is not related to the object not being found, re-raise the exception to handle it outside the function.\n\n8. If the `head_object` method succeeds without raising an exception, return the response from the S3 client, effectively breaking the loop.\n\n9. The function will keep checking for the file's existence at the specified interval (sleep time) until it is found or another exception is raised.\n\n10. The function will either return the metadata of the existing S3 object or raise an exception if an error occurs other than the file not being found.", "reference_code": "def _wait_for_file_to_exist(s3: S3Client, obj: parse.ParseResult, sleep_time: int = 2) -> Any:\n    \"\"\"This function check.\"\"\"\n    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if \"the HeadObject operation: Not Found\" in str(e):\n                sleep(sleep_time)\n            else:\n                raise e\n"}
{"namespace": "litdata.processing.functions.optimize", "type": "function", "class_name": null, "function_name": "optimize", "dependency_all": "# Intra-file Dependency:\nlitdata.processing.functions.LambdaDataChunkRecipe\n    class LambdaDataChunkRecipe(DataChunkRecipe):\n\nlitdata.processing.functions.LambdaDataChunkRecipe.__init__\n    def __init__(self, fn: Callable[[str, Any], None], inputs: Sequence[Any]):\n\nlitdata.processing.functions._get_default_num_workers\n    def _get_default_num_workers() -> int:\n\nlitdata.processing.functions._get_input_dir\n    def _get_input_dir(inputs: Sequence[Any]) -> Optional[str]:\n\n# Cross-file Dependency:\nlitdata.constants._IS_IN_STUDIO\n\nlitdata.processing.data_processor.DataProcessor\n    class DataProcessor:\n\nlitdata.processing.data_processor.DataProcessor.run\n    def run(self) -> None:\n\nlitdata.processing.readers.BaseReader\n    class BaseReader(ABC):\n\nlitdata.processing.utilities.optimize_dns_context\n    def optimize_dns_context(enable: bool) -> Any:\n\nlitdata.streaming.dataloader.StreamingDataLoader\n    class StreamingDataLoader(DataLoader):\n\nlitdata.streaming.resolver.Dir\n    class Dir:\n        \"\"\"Holds a directory path and possibly its associated remote URL.\"\"\"\n\nlitdata.streaming.resolver.Dir.path\n\nlitdata.streaming.resolver.Dir.url\n\nlitdata.streaming.resolver._assert_dir_has_index_file\n    def _assert_dir_has_index_file(output_dir: Dir) -> None:\n\nlitdata.streaming.resolver._execute\n    def _execute(\n        name: str,\n        num_nodes: int,\n        machine: Optional[Machine] = None,\n        command: Optional[str] = None,\n    ) -> None:\n        \"\"\"Remotely execute the current operator.\"\"\"\n\nlitdata.streaming.resolver._resolve_dir\n    def _resolve_dir(dir_path: Optional[Union[str, Dir]]) -> Dir:\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.processing.functions.LambdaDataChunkRecipe\n    class LambdaDataChunkRecipe(DataChunkRecipe):\n\nlitdata.processing.functions.LambdaDataChunkRecipe.__init__\n    def __init__(self, fn: Callable[[str, Any], None], inputs: Sequence[Any]):\n\n# Cross-file Dependency:\nlitdata.streaming.dataloader.StreamingDataLoader\n    class StreamingDataLoader(DataLoader):\n\nlitdata.processing.data_processor.DataProcessor\n    class DataProcessor:\n\nlitdata.processing.readers.BaseReader\n    class BaseReader(ABC):\n\nlitdata.streaming.resolver._resolve_dir\n    def _resolve_dir(dir_path: Optional[Union[str, Dir]]) -> Dir:\n\nlitdata.streaming.resolver._assert_dir_has_index_file\n    def _assert_dir_has_index_file(output_dir: Dir) -> None:\n\nlitdata.constants._IS_IN_STUDIO\n\n", "contexts_above": "# Copyright The Lightning AI team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport concurrent.futures\nimport inspect\nimport os\nfrom datetime import datetime\nfrom functools import partial\nfrom pathlib import Path\nfrom types import FunctionType\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport torch\n\nfrom litdata.constants import _IS_IN_STUDIO, _TORCH_GREATER_EQUAL_2_1_0\nfrom litdata.processing.data_processor import DataChunkRecipe, DataProcessor, DataTransformRecipe\nfrom litdata.processing.readers import BaseReader\nfrom litdata.processing.utilities import optimize_dns_context\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import (\n    Dir,\n    _assert_dir_has_index_file,\n    _assert_dir_is_empty,\n    _execute,\n    _resolve_dir,\n)\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten\n\n\ndef _get_indexed_paths(data: Any) -> Dict[int, str]:\n    flattened_item, _ = tree_flatten(data)\n\n    indexed_paths = {\n        index: element\n        for index, element in enumerate(flattened_item)\n        if isinstance(element, str) and os.path.exists(element)\n    }\n\n    return indexed_paths\n\n\ndef _get_input_dir(inputs: Sequence[Any]) -> Optional[str]:\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(indexed_paths) == 0:\n        # Check whether the second element has any input_path\n        indexed_paths = _get_indexed_paths(inputs[1])\n        if len(indexed_paths) == 0:\n            return None\n\n        # Every element should have filepaths if any contains one.\n        raise ValueError(f\"The provided item {inputs[0]} didn't contain any filepaths.\")\n\n    absolute_path = str(Path(list(indexed_paths.values())[0]).resolve())\n\n    if \"/.project\" in absolute_path:\n        return \"/\" + os.path.join(*str(list(indexed_paths.values())[0]).split(\"/\")[:4])\n\n    return \"/\" + os.path.join(*str(absolute_path).split(\"/\")[:4])\n\n\ndef _get_default_num_workers() -> int:\n    if torch.cuda.is_available():\n        return torch.cuda.device_count()\n    return os.cpu_count() or 1\n\n\nclass LambdaDataTransformRecipe(DataTransformRecipe):\n    def __init__(self, fn: Callable[[str, Any], None], inputs: Sequence[Any]):\n        super().__init__()\n        self._fn = fn\n        self._inputs = inputs\n        self._device: Optional[str] = None\n\n        _fn = self._fn if isinstance(self._fn, FunctionType) else self._fn.__call__  # type: ignore\n        params = inspect.signature(_fn).parameters\n        self._contains_device = \"device\" in params\n        self._contains_is_last = \"is_last\" in params\n\n    def prepare_structure(self, _: Optional[str]) -> Any:\n        return self._inputs\n\n    def prepare_item(self, item_metadata: Any, output_dir: str, is_last: bool) -> None:\n        if self._contains_device and self._device is None:\n            self._find_device()\n\n        kwargs: Dict[str, Any] = {}\n\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        if isinstance(self._fn, (FunctionType, partial)):\n            self._fn(item_metadata, output_dir, **kwargs)\n\n        elif callable(self._fn):\n            self._fn.__call__(item_metadata, output_dir, **kwargs)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n    def _find_device(self) -> None:\n        global_rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n        if torch.cuda.is_available() and global_rank:\n            num_gpus = torch.cuda.device_count()\n            device = int(global_rank) % num_gpus\n            self._device = f\"cuda:{device}\"\n\n\nclass LambdaDataChunkRecipe(DataChunkRecipe):\n    def __init__(\n        self,\n        fn: Callable[[Any], None],\n        inputs: Sequence[Any],\n        chunk_size: Optional[int],\n        chunk_bytes: Optional[Union[int, str]],\n        compression: Optional[str],\n    ):\n        super().__init__(chunk_size=chunk_size, chunk_bytes=chunk_bytes, compression=compression)\n        self._fn = fn\n        self._inputs = inputs\n\n    def prepare_structure(self, input_dir: Optional[str]) -> Any:\n        return self._inputs\n\n    def prepare_item(self, item_metadata: Any) -> Any:\n        if isinstance(self._fn, partial):\n            yield from self._fn(item_metadata)\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                yield from self._fn(item_metadata)\n            else:\n                yield self._fn(item_metadata)\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                yield from self._fn.__call__(item_metadata)  # type: ignore\n            else:\n                yield self._fn.__call__(item_metadata)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n\ndef map(\n    fn: Callable[[str, Any], None],\n    inputs: Sequence[Any],\n    output_dir: Union[str, Dir],\n    weights: Optional[List[int]] = None,\n    num_workers: Optional[int] = None,\n    fast_dev_run: Union[bool, int] = False,\n    num_nodes: Optional[int] = None,\n    machine: Optional[str] = None,\n    num_downloaders: Optional[int] = None,\n    num_uploaders: Optional[int] = None,\n    reorder_files: bool = True,\n    error_when_not_empty: bool = False,\n    reader: Optional[BaseReader] = None,\n    batch_size: Optional[int] = None,\n) -> None:\n    \"\"\"This function map a callbable over a collection of files possibly in a distributed way.\n\n    Arguments:\n        fn: A function to be executed over each input element\n        inputs: A sequence of input to be processed by the `fn` function.\n            Each input should contain at least a valid filepath.\n        output_dir: The folder where the processed data should be stored.\n        weights: Provide an associated weight to each input. This is used to balance work among workers.\n        num_workers: The number of workers to use during processing\n        fast_dev_run: Whether to use process only a sub part of the inputs\n        num_nodes: When doing remote execution, the number of nodes to use. Only supported on https://lightning.ai/.\n        machine: When doing remote execution, the machine to use. Only supported on https://lightning.ai/.\n        num_downloaders: The number of downloaders per worker.\n        num_uploaders: The number of uploaders per workers.\n        reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n            Set this to ``False`` if the order in which samples are processed should be preserved.\n        error_when_not_empty: Whether we should error if the output folder isn't empty.\n        batch_size: Group the inputs into batches of batch_size length.\n\n    \"\"\"\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(LambdaDataTransformRecipe(fn, inputs))\n    return _execute(\n        f\"data-prep-map-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )\n\n\n", "contexts_below": "\n\ndef _listdir(folder: str) -> Tuple[str, List[str]]:\n    return folder, os.listdir(folder)\n\n\nclass walk:\n    \"\"\"This class is an optimized version of os.walk for listing files and folders from cloud filesystem.\n\n    Note: The order of files and folders yielded aren't depth-first anymore due to the asynchronous listing call.\n\n    \"\"\"\n\n    def __init__(self, folder: str, max_workers: Optional[int] = os.cpu_count()) -> None:\n        self.folders = [folder]\n        self.max_workers = max_workers or 1\n        self.futures: List[concurrent.futures.Future] = []\n\n        if not _IS_IN_STUDIO:\n            print(\"This method is optimized to run on https://lightning.ai/. Don't use it otherwise.\")\n\n    def __iter__(self) -> Any:\n        \"\"\"This function queues the folders to perform listdir across multiple workers.\"\"\"\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            while len(self.folders):\n                folder = self.folders.pop(0)\n                future = executor.submit(_listdir, folder)\n                self.futures.append(future)\n\n            while self.futures:\n                for future in concurrent.futures.as_completed(self.futures):\n                    filenames = []\n                    folders = []\n\n                    folder, files_or_folders = future.result()\n                    self.futures = [f for f in self.futures if f != future]\n\n                    for file_or_folder in files_or_folders:\n                        if os.path.isfile(os.path.join(folder, file_or_folder)):\n                            filenames.append(file_or_folder)\n                        else:\n                            folders.append(file_or_folder)\n                            self.folders.append(os.path.join(folder, file_or_folder))\n\n                    yield folder, folders, filenames\n\n                    while len(self.folders) and len(self.futures) <= self.max_workers * 2:\n                        folder = self.folders.pop(0)\n                        future = executor.submit(_listdir, folder)\n                        self.futures.append(future)\n        return\n", "input_code": "def optimize(\n    fn: Callable[[Any], Any],\n    inputs: Sequence[Any],\n    output_dir: str,\n    weights: Optional[List[int]] = None,\n    chunk_size: Optional[int] = None,\n    chunk_bytes: Optional[Union[int, str]] = None,\n    compression: Optional[str] = None,\n    num_workers: Optional[int] = None,\n    fast_dev_run: bool = False,\n    num_nodes: Optional[int] = None,\n    machine: Optional[str] = None,\n    num_downloaders: Optional[int] = None,\n    num_uploaders: Optional[int] = None,\n    reorder_files: bool = True,\n    reader: Optional[BaseReader] = None,\n    batch_size: Optional[int] = None,\n) -> None:\n\n    \"\"\"\n    This function is designed to optimize a dataset by converting it into chunks, which can be processed in a distributed manner if required. It applies a given function to each element of the input sequence, supports various configurations for chunking, compression, and parallel processing, and handles both local and remote execution environments.\n\n    Input-Output Arguments\n    :param fn: Callable[[Any], Any]. A function that will be executed over each element of the inputs.\n    :param inputs: Sequence[Any]. A sequence of inputs that the `fn` function will process. Each input should contain at least a valid filepath.\n    :param output_dir: str. The directory where the processed data will be stored.\n    :param weights: Optional[List[int]]. Weights associated with each input, used for balancing work among workers. Default is None.\n    :param chunk_size: Optional[int]. The maximum number of elements each chunk should contain. Default is None.\n    :param chunk_bytes: Optional[Union[int, str]]. The maximum size in bytes that each chunk should be. Default is None.\n    :param compression: Optional[str]. The compression algorithm to use for the chunks. Default is None.\n    :param num_workers: Optional[int]. The number of workers to use for processing. Default is None.\n    :param fast_dev_run: bool. If True, processes only a subset of the inputs for quick development testing. Default is False.\n    :param num_nodes: Optional[int]. The number of nodes to use for remote execution, supported only on https://lightning.ai/. Default is None.\n    :param machine: Optional[str]. Specifies the machine type for remote execution, supported only on https://lightning.ai/. Default is None.\n    :param num_downloaders: Optional[int]. The number of downloaders per worker. Default is None.\n    :param num_uploaders: Optional[int]. The number of uploaders per worker. Default is None.\n    :param reorder_files: bool. If True, reorders files by size to distribute work equally among workers; otherwise, preserves the order of processing. Default is True.\n    :param reader: Optional[BaseReader]. The reader to use for reading inputs. Default is None.\n    :param batch_size: Optional[int]. Groups the inputs into batches of this size. Default is None.\n    :return: None. This function does not return any value.\n    \"\"\"", "reference_steps": "1. Validate inputs: Ensure that `inputs` is a non-empty sequence or a `StreamingDataLoader`. If `inputs` is a `StreamingDataLoader`, `batch_size` and `weights` should not be provided.\n2. Check for required arguments: Ensure that either `chunk_size` or `chunk_bytes` is defined.\n3. Handle environment-specific restrictions: If not running on https://lightning.ai/, disallow the use of `machine` or `num_nodes`.\n4. Resolve the output directory: Convert `output_dir` to a `Dir` object and validate its URL if applicable.\n5. Ensure the output directory has an index file: This is required for the processing to proceed.\n6. Batch inputs if necessary: If `batch_size` is provided and is greater than 1, group the inputs into batches.\n7. Create a `DataProcessor` instance: Initialize it with the appropriate parameters, including the number of workers, downloaders, uploaders, and whether to reorder files.\n8. Set up DNS optimization context: This is likely for performance reasons during data processing.\n9. Run the data processing: Use the `DataProcessor` to process the data with the provided function `fn` and the inputs, along with the specified chunking and compression settings.\n10. Handle remote execution: If `num_nodes` is specified and the environment supports it, execute the data processing in a distributed manner using the specified number of nodes and machine type.", "reference_code": "def optimize(\n    fn: Callable[[Any], Any],\n    inputs: Sequence[Any],\n    output_dir: str,\n    weights: Optional[List[int]] = None,\n    chunk_size: Optional[int] = None,\n    chunk_bytes: Optional[Union[int, str]] = None,\n    compression: Optional[str] = None,\n    num_workers: Optional[int] = None,\n    fast_dev_run: bool = False,\n    num_nodes: Optional[int] = None,\n    machine: Optional[str] = None,\n    num_downloaders: Optional[int] = None,\n    num_uploaders: Optional[int] = None,\n    reorder_files: bool = True,\n    reader: Optional[BaseReader] = None,\n    batch_size: Optional[int] = None,\n) -> None:\n    \"\"\"This function converts a dataset into chunks possibly in a distributed way.\n\n    Arguments:\n        fn: A function to be executed over each input element\n        inputs: A sequence of input to be processed by the `fn` function.\n            Each input should contain at least a valid filepath.\n        output_dir: The folder where the processed data should be stored.\n        weights: Provide an associated weight to each input. This is used to balance work among workers.\n        chunk_size: The maximum number of elements to hold within a chunk.\n        chunk_bytes: The maximum number of bytes to hold within a chunk.\n        compression: The compression algorithm to use over the chunks.\n        num_workers: The number of workers to use during processing\n        fast_dev_run: Whether to use process only a sub part of the inputs\n        num_nodes: When doing remote execution, the number of nodes to use. Only supported on https://lightning.ai/.\n        machine: When doing remote execution, the machine to use. Only supported on https://lightning.ai/.\n        num_downloaders: The number of downloaders per worker.\n        num_uploaders: The numbers of uploaders per worker.\n        reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n            Set this to ``False`` if the order in which samples are processed should be preserved.\n        batch_size: Group the inputs into batches of batch_size length.\n\n    \"\"\"\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if chunk_size is None and chunk_bytes is None:\n        raise ValueError(\"Either `chunk_size` or `chunk_bytes` needs to be defined.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            reader=reader,\n        )\n\n        with optimize_dns_context(True):\n            data_processor.run(\n                LambdaDataChunkRecipe(\n                    fn,\n                    inputs,\n                    chunk_size=chunk_size,\n                    chunk_bytes=chunk_bytes,\n                    compression=compression,\n                )\n            )\n        return None\n    return _execute(\n        f\"data-prep-optimize-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )\n"}
{"namespace": "litdata.processing.data_processor._download_data_target", "type": "function", "class_name": null, "function_name": "_download_data_target", "dependency_all": "# Intra-file Dependency:\nlitdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold\n    def _wait_for_disk_usage_higher_than_threshold(input_dir: str, threshold_in_gb: int = 25, sleep_time: int = 3) -> None:\n\n# Cross-file Dependency:\nlitdata.streaming.client.S3Client\n    class S3Client:\n\nlitdata.streaming.client.S3Client.client\n    def client(self) -> Any:\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.streaming.client.S3Client.client\n    def client(self) -> Any:\n\n", "contexts_above": "import concurrent\nimport json\nimport logging\nimport os\nimport random\nimport shutil\nimport signal\nimport tempfile\nimport traceback\nimport types\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom multiprocessing import Process, Queue\nfrom pathlib import Path\nfrom queue import Empty\nfrom time import sleep, time\nfrom typing import Any, Dict, List, Optional, Tuple, TypeVar, Union\nfrom urllib import parse\n\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm as _tqdm\n\nfrom litdata.constants import (\n    _BOTO3_AVAILABLE,\n    _DEFAULT_FAST_DEV_RUN_ITEMS,\n    _INDEX_FILENAME,\n    _IS_IN_STUDIO,\n    _LIGHTNING_CLOUD_LATEST,\n    _TORCH_GREATER_EQUAL_2_1_0,\n)\nfrom litdata.processing.readers import BaseReader, StreamingDataLoaderReader\nfrom litdata.processing.utilities import _create_dataset\nfrom litdata.streaming import Cache\nfrom litdata.streaming.cache import Dir\nfrom litdata.streaming.client import S3Client\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.broadcast import broadcast_object\nfrom litdata.utilities.packing import _pack_greedily\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten, tree_unflatten, treespec_loads\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\nif _BOTO3_AVAILABLE:\n    import botocore\n\nlogger = logging.Logger(__name__)\n\n\ndef _get_num_nodes() -> int:\n    \"\"\"Returns the number of nodes.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 1))\n\n\ndef _get_node_rank() -> int:\n    \"\"\"Returns the current node rank of the instance.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NODE_RANK\", 0))\n\n\ndef _get_fast_dev_run() -> int:\n    \"\"\"Returns whether fast dev mode is enabled.\"\"\"\n    return bool(int(os.getenv(\"DATA_OPTIMIZER_FAST_DEV_RUN\", 1)))\n\n\ndef _get_default_cache() -> str:\n    return \"/cache\" if _IS_IN_STUDIO else tempfile.gettempdir()\n\n\ndef _get_cache_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache directory used by the Cache to store the chunks.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_FOLDER\", f\"{_get_default_cache()}/chunks\")\n    if name is None:\n        return cache_dir\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\ndef _get_cache_data_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache data directory used by the DataProcessor workers to download the files.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_DATA_CACHE_FOLDER\", f\"{_get_default_cache()}/data\")\n    if name is None:\n        return os.path.join(cache_dir)\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\ndef _wait_for_file_to_exist(s3: S3Client, obj: parse.ParseResult, sleep_time: int = 2) -> Any:\n    \"\"\"This function check.\"\"\"\n    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if \"the HeadObject operation: Not Found\" in str(e):\n                sleep(sleep_time)\n            else:\n                raise e\n\n\ndef _wait_for_disk_usage_higher_than_threshold(input_dir: str, threshold_in_gb: int = 25, sleep_time: int = 3) -> None:\n    usage = shutil.disk_usage(input_dir)\n\n    while (usage.free / 1000 / 1000 / 1000) <= threshold_in_gb:\n        sleep(sleep_time)\n        usage = shutil.disk_usage(input_dir)\n\n    return\n\n\n", "contexts_below": "\n\ndef _remove_target(input_dir: Dir, cache_dir: str, queue_in: Queue) -> None:\n    \"\"\"This function is used to delete files from the cache directory to minimise disk space.\"\"\"\n    while True:\n        # 1. Collect paths\n        paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and delete them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    os.remove(path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.remove(path)\n\n\ndef _upload_fn(upload_queue: Queue, remove_queue: Queue, cache_dir: str, output_dir: Dir) -> None:\n    \"\"\"This function is used to upload optimised chunks from a local to remote dataset directory.\"\"\"\n    obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        tmpdir = None\n\n        if isinstance(data, str) or data is None:\n            local_filepath = data\n        else:\n            tmpdir, local_filepath = data\n\n        # Terminate the process if we received a termination signal\n        if local_filepath is None:\n            return\n\n        # Upload the file to the target cloud storage\n        if not local_filepath.startswith(cache_dir):\n            local_filepath = os.path.join(cache_dir, local_filepath)\n\n        if obj.scheme == \"s3\":\n            try:\n                if tmpdir is None:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n                else:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), local_filepath.replace(tmpdir, \"\")[1:])\n\n                s3.client.upload_file(\n                    local_filepath,\n                    obj.netloc,\n                    output_filepath,\n                )\n            except Exception as e:\n                print(e)\n\n        elif output_dir.path:\n            if tmpdir is None:\n                output_filepath = os.path.join(output_dir.path, os.path.basename(local_filepath))\n            else:\n                output_filepath = os.path.join(output_dir.path, local_filepath.replace(tmpdir, \"\")[1:])\n\n            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n            shutil.move(local_filepath, output_filepath)\n        else:\n            raise ValueError(f\"The provided {output_dir.path} isn't supported.\")\n\n        # Inform the remover to delete the file\n        if remove_queue and os.path.exists(local_filepath):\n            remove_queue.put([local_filepath])\n\n\ndef _map_items_to_workers_sequentially(num_workers: int, user_items: List[Any]) -> List[List[Any]]:\n\n    from typing import List, Any\n    import os\n    total_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = total_nodes * num_workers\n\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return result\n\n\ndef _map_items_to_workers_weighted(\n    num_workers: int,\n    user_items: List[Any],\n    weights: Optional[List[int]] = None,\n    file_size: bool = True,\n) -> List[List[Any]]:\n    # Associate the items to the workers based on number of nodes and node rank.\n    weights = [1] * len(user_items) if weights is None else weights\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    for worker_id, size in worker_weights.items():\n        if worker_id not in worker_ids_this_node:\n            continue\n\n        if file_size:\n            print(f\"Worker {worker_id} gets {size / 1e6:.1f} MB ({len(worker_items[worker_id])} files)\")\n        else:\n            print(f\"Worker {worker_id} gets ({len(worker_items[worker_id])}) items for a total weight of {size}.\")\n\n    return [np.random.permutation(worker_items[worker_id]).tolist() for worker_id in worker_ids_this_node]\n\n\ndef _get_num_bytes(item: Any, base_path: str) -> int:\n    flattened_item, _ = tree_flatten(item)\n\n    num_bytes = 0\n    for element in flattened_item:\n        if isinstance(element, str):\n            element = Path(element).resolve()\n            if not element.exists():\n                continue\n            file_bytes = os.path.getsize(element)\n            if file_bytes == 0:\n                raise RuntimeError(f\"The file {element} has 0 bytes!\")\n            num_bytes += file_bytes\n    return num_bytes\n\n\ndef _get_item_filesizes(items: List[Any], base_path: str = \"\") -> List[int]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        for future in futures:\n            item_sizes.append(future.result())\n    return item_sizes\n\n\ndef _to_path(element: str) -> str:\n    return element if _IS_IN_STUDIO and element.startswith(\"/teamspace\") else str(Path(element).resolve())\n\n\ndef _is_path(input_dir: Optional[str], element: Any) -> bool:\n    if not isinstance(element, str):\n        return False\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n        if element.startswith(input_dir):\n            return True\n\n    return os.path.exists(element)\n\n\nclass BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        data_recipe: \"DataRecipe\",\n        input_dir: Dir,\n        output_dir: Dir,\n        items: List[Any],\n        progress_queue: Queue,\n        error_queue: Queue,\n        stop_queue: Queue,\n        num_downloaders: int,\n        num_uploaders: int,\n        remove: bool,\n        reader: Optional[BaseReader] = None,\n    ) -> None:\n        \"\"\"The BaseWorker is responsible to process the user data.\"\"\"\n        self.worker_index = worker_index\n        self.num_workers = num_workers\n        self.node_rank = node_rank\n        self.data_recipe = data_recipe\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.items = items\n        self.num_items = len(self.items)\n        self.num_downloaders = num_downloaders\n        self.num_uploaders = num_uploaders\n        self.remove = remove\n        self.reader = reader\n        self.paths: List[List[str]] = []\n        self.remover: Optional[Process] = None\n        self.downloaders: List[Process] = []\n        self.uploaders: List[Process] = []\n        self.to_download_queues: List[Queue] = []\n        self.to_upload_queues: List[Queue] = []\n        self.stop_queue = stop_queue\n        self.ready_to_process_queue: Queue = Queue()\n        self.remove_queue: Queue = Queue()\n        self.progress_queue: Queue = progress_queue\n        self.error_queue: Queue = error_queue\n        self._counter = 0\n        self._last_time = time()\n        self._index_counter = 0\n\n    def run(self) -> None:\n        try:\n            self._setup()\n            self._loop()\n        except Exception:\n            traceback_format = traceback.format_exc()\n            print(traceback_format)\n            self.error_queue.put(traceback_format)\n        print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is done.\")\n\n    def _setup(self) -> None:\n        self._set_environ_variables()\n        self._create_cache()\n        self._collect_paths()\n        self._start_downloaders()\n        self._start_uploaders()\n        self._start_remover()\n\n    def _loop(self) -> None:\n        num_downloader_finished = 0\n\n        while True:\n            index = self.ready_to_process_queue.get()\n\n            if index is None:\n                num_downloader_finished += 1\n                if num_downloader_finished == self.num_downloaders:\n                    print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is terminating.\")\n\n                    if isinstance(self.data_recipe, DataChunkRecipe):\n                        self._handle_data_chunk_recipe_end()\n\n                    if self.output_dir.url if self.output_dir.url else self.output_dir.path:\n                        # Inform the uploaders they are doing working\n                        for i in range(self.num_uploaders):\n                            self.to_upload_queues[i].put(None)\n\n                        # Wait for them all to be finished\n                        for uploader in self.uploaders:\n                            uploader.join()\n\n                    if self.remove:\n                        assert self.remover\n                        self.remove_queue.put(None)\n                        self.remover.join()\n\n                    if self.progress_queue:\n                        self.progress_queue.put((self.worker_index, self._counter))\n                    return\n                continue\n\n            if isinstance(self.data_recipe, DataChunkRecipe):\n                self._handle_data_chunk_recipe(index)\n            else:\n                self._handle_data_transform_recipe(index)\n\n            self._counter += 1\n\n            # Don't send the last progress update, so the main thread awaits for the uploader and remover\n            if self.progress_queue and (time() - self._last_time) > 1 and self._counter < (self.num_items - 2):\n                self.progress_queue.put((self.worker_index, self._counter))\n                self._last_time = time()\n\n            if self.remove and self.input_dir.path is not None and self.reader is None:\n                self.remove_queue.put(self.paths[index])\n\n            try:\n                self.stop_queue.get(timeout=0.0001)\n                return\n            except Empty:\n                pass\n\n    def _set_environ_variables(self) -> None:\n        # set the optimizer global rank and world_size\n        os.environ[\"DATA_OPTIMIZER_GLOBAL_RANK\"] = str(_get_node_rank() * self.num_workers + self.worker_index)\n        os.environ[\"DATA_OPTIMIZER_NUM_WORKERS\"] = str(self.num_workers)\n\n    def _create_cache(self) -> None:\n        self.cache_data_dir = _get_cache_data_dir()\n        os.makedirs(self.cache_data_dir, exist_ok=True)\n\n        self.cache_chunks_dir = _get_cache_dir()\n        os.makedirs(self.cache_chunks_dir, exist_ok=True)\n\n        if isinstance(self.data_recipe, DataTransformRecipe):\n            return\n\n        self.cache = Cache(\n            self.cache_chunks_dir,\n            chunk_bytes=self.data_recipe.chunk_bytes,\n            chunk_size=self.data_recipe.chunk_size,\n            compression=self.data_recipe.compression,\n        )\n        self.cache._reader._rank = _get_node_rank() * self.num_workers + self.worker_index\n\n    def _try_upload(self, data: Optional[Union[str, Tuple[str, str]]]) -> None:\n        if not data or (self.output_dir.url if self.output_dir.url else self.output_dir.path) is None:\n            return\n\n        if isinstance(data, str):\n            assert os.path.exists(data), data\n        else:\n            assert os.path.exists(data[-1]), data\n\n        self.to_upload_queues[self._counter % self.num_uploaders].put(data)\n\n    def _collect_paths(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            for index in range(len(self.items)):\n                self.ready_to_process_queue.put(index)\n            for _ in range(self.num_downloaders):\n                self.ready_to_process_queue.put(None)\n            return\n\n        items = []\n        for item in self.items:\n            flattened_item, spec = tree_flatten(item)\n\n            # For speed reasons, we assume starting with `self.input_dir` is enough to be a real file.\n            # Other alternative would be too slow.\n            # TODO: Try using dictionary for higher accurary.\n            indexed_paths = {\n                index: _to_path(element)\n                for index, element in enumerate(flattened_item)\n                if _is_path(self.input_dir.path, element)\n            }\n\n            if len(indexed_paths) == 0:\n                raise ValueError(\n                    f\"The provided item {item} didn't contain any filepaths. The input_dir is {self.input_dir.path}.\"\n                )\n\n            paths = []\n            for index, path in indexed_paths.items():\n                paths.append(path)\n                if self.input_dir and not self.input_dir.path.startswith(\"/teamspace/studios/this_studio\"):\n                    path = path.replace(self.input_dir.path, self.cache_data_dir)\n                flattened_item[index] = path\n\n            self.paths.append(paths)\n\n            items.append(tree_unflatten(flattened_item, spec))\n\n        self.items = items\n\n    def _start_downloaders(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            return\n\n        for _ in range(self.num_downloaders):\n            to_download_queue: Queue = Queue()\n            p = Process(\n                target=_download_data_target,\n                args=(\n                    self.input_dir,\n                    self.cache_data_dir,\n                    to_download_queue,\n                    self.ready_to_process_queue,\n                ),\n            )\n            p.start()\n            self.downloaders.append(p)\n            self.to_download_queues.append(to_download_queue)\n\n        for index, paths in enumerate(self.paths):\n            self.to_download_queues[index % self.num_downloaders].put((index, paths))\n\n        for downloader_index in range(self.num_downloaders):\n            self.to_download_queues[downloader_index].put(None)\n\n    def _start_remover(self) -> None:\n        if not self.remove:\n            return\n\n        self.remover = Process(\n            target=_remove_target,\n            args=(\n                self.input_dir,\n                self.cache_data_dir,\n                self.remove_queue,\n            ),\n        )\n        self.remover.start()\n\n    def _start_uploaders(self) -> None:\n        if self.output_dir.path is None and self.output_dir.url is None:\n            return\n\n        for _ in range(self.num_uploaders):\n            to_upload_queue: Queue = Queue()\n            p = Process(\n                target=_upload_fn,\n                args=(\n                    to_upload_queue,\n                    self.remove_queue,\n                    self.cache_chunks_dir,\n                    self.output_dir,\n                ),\n            )\n            p.start()\n            self.uploaders.append(p)\n            self.to_upload_queues.append(to_upload_queue)\n\n    def _handle_data_chunk_recipe(self, index: int) -> None:\n        try:\n            current_item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n            item_data_or_generator = self.data_recipe.prepare_item(current_item)\n            if isinstance(item_data_or_generator, types.GeneratorType):\n                for item_data in item_data_or_generator:\n                    if item_data is not None:\n                        chunk_filepath = self.cache._add_item(self._index_counter, item_data)\n                        self._try_upload(chunk_filepath)\n                        self._index_counter += 1\n            elif item_data_or_generator is not None:\n                chunk_filepath = self.cache._add_item(self._index_counter, item_data_or_generator)\n                self._try_upload(chunk_filepath)\n                self._index_counter += 1\n        except Exception as e:\n            raise RuntimeError(f\"Failed processing {self.items[index]}\") from e\n\n    def _handle_data_chunk_recipe_end(self) -> None:\n        chunks_filepaths = self.cache.done()\n\n        if chunks_filepaths and len(self.to_upload_queues):\n            for i, chunk_filepath in enumerate(chunks_filepaths):\n                if isinstance(chunk_filepath, str) and os.path.exists(chunk_filepath):\n                    self.to_upload_queues[i % self.num_uploaders].put(chunk_filepath)\n\n    def _handle_data_transform_recipe(self, index: int) -> None:\n        # Don't use a context manager to avoid deleting files that are being uploaded.\n        output_dir = tempfile.mkdtemp()\n        item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n        item_data = self.data_recipe.prepare_item(item, str(output_dir), len(self.items) - 1 == index)\n        if item_data is not None:\n            raise ValueError(\n                \"When using a `DataTransformRecipe`, the `prepare_item` shouldn't return anything.\"\n                \" Simply store your files under the output_dir.\"\n            )\n        filepaths = []\n        for directory, _, filenames in os.walk(output_dir):\n            for filename in filenames:\n                filepaths.append(os.path.join(directory, filename))\n\n        for filepath in filepaths:\n            self._try_upload((output_dir, filepath))\n\n\nclass DataWorkerProcess(BaseWorker, Process):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"The DataWorkerProcess is responsible to process the user data inside processes.\"\"\"\n        BaseWorker.__init__(self, *args, **kwargs)\n        Process.__init__(self)\n\n\n@dataclass\nclass _Result:\n    size: Optional[int] = None\n    num_bytes: Optional[str] = None\n    data_format: Optional[str] = None\n    compression: Optional[str] = None\n    num_chunks: Optional[int] = None\n    num_bytes_per_chunk: Optional[List[int]] = None\n\n\nT = TypeVar(\"T\")\n\n\nclass DataRecipe:\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        pass\n\n    @abstractmethod\n    def prepare_item(self, *args: Any, **kwargs: Any) -> Any:\n        pass\n\n    def __init__(self) -> None:\n        self._name: Optional[str] = None\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        return _Result(size=size)\n\n\nclass DataChunkRecipe(DataRecipe):\n    def __init__(\n        self,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n    ):\n        super().__init__()\n        if chunk_size is not None and chunk_bytes is not None:\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self.chunk_size = chunk_size\n        self.chunk_bytes = 1 << 26 if chunk_size is None else chunk_bytes\n        self.compression = compression\n\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T) -> Any:\n        \"\"\"The return of this `prepare_item` method is persisted in chunked binary files.\"\"\"\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        num_nodes = _get_num_nodes()\n        cache_dir = _get_cache_dir()\n\n        chunks = [file for file in os.listdir(cache_dir) if file.endswith(\".bin\")]\n        if chunks and delete_cached_files and output_dir.path is not None:\n            raise RuntimeError(f\"All the chunks should have been deleted. Found {chunks}\")\n\n        merge_cache = Cache(cache_dir, chunk_bytes=1)\n        node_rank = _get_node_rank()\n        merge_cache._merge_no_wait(node_rank if num_nodes > 1 else None)\n        self._upload_index(output_dir, cache_dir, num_nodes, node_rank)\n\n        if num_nodes == node_rank + 1:\n            with open(os.path.join(cache_dir, _INDEX_FILENAME)) as f:\n                config = json.load(f)\n\n            size = sum([c[\"dim\"] if c[\"dim\"] is not None else c[\"chunk_size\"] for c in config[\"chunks\"]])\n            num_bytes = sum([c[\"chunk_bytes\"] for c in config[\"chunks\"]])\n            if config[\"config\"] is not None:\n                data_format = tree_unflatten(\n                    config[\"config\"][\"data_format\"], treespec_loads(config[\"config\"][\"data_spec\"])\n                )\n            else:\n                data_format = None\n            num_chunks = len(config[\"chunks\"])\n\n            # The platform can't store more than 1024 entries.\n            # Note: This isn't really used right now, so it is fine to skip if too big.\n            num_bytes_per_chunk = [c[\"chunk_size\"] for c in config[\"chunks\"]] if num_chunks < 1024 else []\n\n            return _Result(\n                size=size,\n                num_bytes=num_bytes,\n                data_format=data_format,\n                compression=config[\"config\"][\"compression\"] if config[\"config\"] else None,\n                num_chunks=len(config[\"chunks\"]),\n                num_bytes_per_chunk=num_bytes_per_chunk,\n            )\n        return _Result(\n            size=size,\n        )\n\n    def _upload_index(self, output_dir: Dir, cache_dir: str, num_nodes: int, node_rank: Optional[int]) -> None:\n        \"\"\"This method upload the index file to the remote cloud directory.\"\"\"\n        if output_dir.path is None and output_dir.url is None:\n            return\n\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n        if num_nodes > 1:\n            local_filepath = os.path.join(cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")\n        else:\n            local_filepath = os.path.join(cache_dir, _INDEX_FILENAME)\n\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(\n                local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        if num_nodes == 1 or node_rank is None:\n            return\n\n        # Merge the index files generated by each node.\n        # Note: When using the Data Optimizer, they should be a single process on each node executing this section\n        # So no risk to get race conditon.\n        if num_nodes == node_rank + 1:\n            # Get the index file locally\n            for node_rank in range(num_nodes - 1):\n                output_dir_path = output_dir.url if output_dir.url else output_dir.path\n                assert output_dir_path\n                remote_filepath = os.path.join(output_dir_path, f\"{node_rank}-{_INDEX_FILENAME}\")\n                node_index_filepath = os.path.join(cache_dir, os.path.basename(remote_filepath))\n                if obj.scheme == \"s3\":\n                    obj = parse.urlparse(remote_filepath)\n                    _wait_for_file_to_exist(s3, obj)\n                    with open(node_index_filepath, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                elif output_dir.path and os.path.isdir(output_dir.path):\n                    shutil.copyfile(remote_filepath, node_index_filepath)\n\n            merge_cache = Cache(cache_dir, chunk_bytes=1)\n            merge_cache._merge_no_wait()\n            self._upload_index(output_dir, cache_dir, 1, None)\n\n\nclass DataTransformRecipe(DataRecipe):\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T, output_dir: str, is_last: bool) -> None:\n        \"\"\"Use your item metadata to process your files and save the file outputs into `output_dir`.\"\"\"\n\n\nclass DataProcessor:\n    def __init__(\n        self,\n        input_dir: Union[str, Dir],\n        output_dir: Optional[Union[str, Dir]] = None,\n        num_workers: Optional[int] = None,\n        num_downloaders: Optional[int] = None,\n        num_uploaders: Optional[int] = None,\n        delete_cached_files: bool = True,\n        fast_dev_run: Optional[Union[bool, int]] = None,\n        random_seed: Optional[int] = 42,\n        reorder_files: bool = True,\n        weights: Optional[List[int]] = None,\n        reader: Optional[BaseReader] = None,\n    ):\n        \"\"\"The `DatasetOptimiser` provides an efficient way to process data across multiple machine into chunks to make\n        training faster.\n\n        Arguments:\n            input_dir: The path to where the input data are stored.\n            output_dir: The path to where the output data are stored.\n            num_workers: The number of worker threads to use.\n            num_downloaders: The number of file downloaders to use.\n            num_uploaders: The number of file uploaders to use.\n            delete_cached_files: Whether to delete the cached files.\n            fast_dev_run: Whether to run a quick dev run.\n            random_seed: The random seed to be set before shuffling the data.\n            reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n                Set this to ``False`` if the order in which samples are processed should be preserved.\n            weights: Provide a list of weights associated to the inputs.\n                This is used to evenly split the work among the workers.\n            reader: Map the inputs to worker inputs and provides a read method to read a slice of the data.\n\n        \"\"\"\n        self.input_dir = _resolve_dir(input_dir)\n        self.output_dir = _resolve_dir(output_dir)\n        self.num_workers = num_workers or (1 if fast_dev_run else (os.cpu_count() or 1) * 4)\n        self.num_downloaders = num_downloaders or 2\n        self.num_uploaders = num_uploaders or 5\n        self.delete_cached_files = delete_cached_files\n        self.fast_dev_run = _get_fast_dev_run() if fast_dev_run is None else fast_dev_run\n        self.workers: Any = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.reorder_files = reorder_files\n        self.weights = weights\n        self.reader = reader\n\n        if self.reader is not None and self.weights is not None:\n            raise ValueError(\"Either the reader or the weights needs to be defined.\")\n\n        # Ensure the input dir is the same across all nodes\n        self.input_dir = broadcast_object(\"input_dir\", self.input_dir)\n\n        if self.output_dir:\n            # Ensure the output dir is the same across all nodes\n            self.output_dir = broadcast_object(\"output_dir\", self.output_dir)\n            print(f\"Storing the files under {self.output_dir.path}\")\n\n        self.random_seed = random_seed\n\n    def run(self, data_recipe: DataRecipe) -> None:\n        \"\"\"The `DataProcessor.run(...)` method triggers the data recipe processing over your dataset.\"\"\"\n        if not isinstance(data_recipe, DataRecipe):\n            raise ValueError(\"The provided value should be a data recipe.\")\n\n        t0 = time()\n        print(f\"Setup started with fast_dev_run={self.fast_dev_run}.\")\n\n        # Force random seed to be fixed\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n        torch.manual_seed(self.random_seed)\n\n        # Call the setup method of the user\n        user_items: List[Any] = data_recipe.prepare_structure(self.input_dir.path if self.input_dir else None)\n\n        if not isinstance(user_items, (list, StreamingDataLoader)):\n            raise ValueError(\"The `prepare_structure` should return a list of item metadata.\")\n\n        if isinstance(user_items, StreamingDataLoader):\n            self.reader = StreamingDataLoaderReader(user_items)\n\n        if self.reader:\n            user_items = self.reader.remap_items(user_items, self.num_workers)\n\n        if self.weights is not None:\n            if len(self.weights) != len(user_items):\n                raise ValueError(\"The provided weights length should match the inputs' length.\")\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=self.weights, file_size=False\n            )\n\n        elif self.reorder_files and self.input_dir.path:\n            # TODO: Only do this on node 0, and broadcast the item sizes to the other nodes.\n            item_sizes = _get_item_filesizes(user_items, base_path=self.input_dir.path)\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=item_sizes\n            )\n        else:\n            workers_user_items = _map_items_to_workers_sequentially(num_workers=self.num_workers, user_items=user_items)\n\n        print(f\"Setup finished in {round(time() - t0, 3)} seconds. Found {len(user_items)} items to process.\")\n\n        if self.fast_dev_run:\n            items_to_keep = self.fast_dev_run if type(self.fast_dev_run) is int else _DEFAULT_FAST_DEV_RUN_ITEMS\n            workers_user_items = [w[:items_to_keep] for w in workers_user_items]\n            print(f\"Fast dev run is enabled. Limiting to {items_to_keep} items per process.\")\n\n        num_items = sum([len(items) for items in workers_user_items])\n\n        self._cleanup_cache()\n\n        print(f\"Starting {self.num_workers} workers with {num_items} items.\")\n\n        if self.input_dir is None and self.src_resolver is not None and self.input_dir:\n            self.input_dir = self.src_resolver(self.input_dir)\n            print(f\"The remote_dir is `{self.input_dir}`.\")\n\n        signal.signal(signal.SIGINT, self._signal_handler)\n\n        self._create_process_workers(data_recipe, workers_user_items)\n\n        print(\"Workers are ready ! Starting data processing...\")\n\n        current_total = 0\n        has_failed = False\n        pbar = _tqdm(\n            desc=\"Progress\",\n            total=num_items,\n            smoothing=0,\n            position=-1,\n            mininterval=1,\n            leave=True,\n            dynamic_ncols=True,\n        )\n\n        while True:\n            try:\n                error = self.error_queue.get(timeout=0.001)\n                self._exit_on_error(error)\n            except Empty:\n                assert self.progress_queue\n                try:\n                    index, counter = self.progress_queue.get(timeout=0.001)\n                except Empty:\n                    continue\n                self.workers_tracker[index] = counter\n                new_total = sum(self.workers_tracker.values())\n\n            pbar.update(new_total - current_total)\n\n            current_total = new_total\n            if current_total == num_items:\n                break\n\n            # Exit early if all the workers are done.\n            # This means there were some kinda of errors.\n            if all(not w.is_alive() for w in self.workers):\n                has_failed = True\n                break\n\n        pbar.close()\n\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        # TODO: Understand why it hangs.\n        if num_nodes == 1:\n            for w in self.workers:\n                w.join(0)\n\n        print(\"Workers are finished.\")\n        result = data_recipe._done(len(user_items), self.delete_cached_files, self.output_dir)\n\n        if num_nodes == node_rank + 1 and self.output_dir.url and _IS_IN_STUDIO:\n            assert self.output_dir.path\n            _create_dataset(\n                input_dir=self.input_dir.path,\n                storage_dir=self.output_dir.path,\n                dataset_type=V1DatasetType.CHUNKED\n                if isinstance(data_recipe, DataChunkRecipe)\n                else V1DatasetType.TRANSFORMED,\n                empty=False,\n                size=result.size,\n                num_bytes=result.num_bytes,\n                data_format=result.data_format,\n                compression=result.compression,\n                num_chunks=result.num_chunks,\n                num_bytes_per_chunk=result.num_bytes_per_chunk,\n            )\n\n        print(\"Finished data processing!\")\n\n        # TODO: Understand why it is required to avoid long shutdown.\n        if _get_num_nodes() > 1:\n            os._exit(int(has_failed))\n\n    def _exit_on_error(self, error: str) -> None:\n        for w in self.workers:\n            w.join(0)\n        raise RuntimeError(f\"We found the following error {error}.\")\n\n    def _create_process_workers(self, data_recipe: DataRecipe, workers_user_items: List[List[Any]]) -> None:\n        self.progress_queue = Queue()\n        workers: List[DataWorkerProcess] = []\n        stop_queues: List[Queue] = []\n        for worker_idx, worker_user_items in enumerate(workers_user_items):\n            stop_queues.append(Queue())\n            worker = DataWorkerProcess(\n                worker_idx,\n                self.num_workers,\n                _get_node_rank(),\n                data_recipe,\n                self.input_dir,\n                self.output_dir,\n                worker_user_items,\n                self.progress_queue,\n                self.error_queue,\n                stop_queues[-1],\n                self.num_downloaders,\n                self.num_uploaders,\n                self.delete_cached_files,\n                self.reader,\n            )\n            worker.start()\n            workers.append(worker)\n\n        # Note: Don't store within the loop as weakref aren't serializable\n        self.workers = workers\n        self.stop_queues = stop_queues\n\n    def _signal_handler(self, signal: Any, frame: Any) -> None:\n        \"\"\"On temrination, we stop all the processes to avoid leaking RAM.\"\"\"\n        for stop_queue in self.stop_queues:\n            stop_queue.put(None)\n        for w in self.workers:\n            w.join(0)\n        os._exit(0)\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n", "input_code": "def _download_data_target(input_dir: Dir, cache_dir: str, queue_in: Queue, queue_out: Queue) -> None:\n\n    \"\"\"\n    This function downloads data from a remote directory to a local cache directory to optimize reading. It continuously fetches download tasks from an input queue, checks if the files are already downloaded, downloads missing files, and then signals completion by putting the task index into an output queue.\n\n    Input-Output Arguments\n    :param input_dir: Dir. The directory object representing the source directory from which files are to be downloaded. It is used to determine the source path or URL for the files.\n    :param cache_dir: str. The path to the local cache directory where files are to be downloaded. It is used as the destination for the downloaded files.\n    :param queue_in: Queue. The input queue from which download tasks are fetched. Each task includes an index and a list of file paths to download.\n    :param queue_out: Queue. The output queue where the index of a completed download task is put to signal that the files for that index are available.\n    :return: None. There are no return values as the function's purpose is to perform side effects (downloading files and communicating via queues).\n    \"\"\"", "reference_steps": "1. Define a function `_download_data_target` that takes an input directory object, a cache directory path, and two queues (`queue_in` and `queue_out`) as arguments. This function is designed to download data from a remote directory to a local cache directory to optimize reading.\n\n2. Instantiate an S3 client object for interacting with AWS S3 storage.\n\n3. Enter an infinite loop to continuously process incoming download requests from `queue_in`.\n\n4. Fetch a tuple containing an index and a list of paths from `queue_in`. If the fetched item is `None`, it is a termination signal; put `None` into `queue_out` and return from the function to end the process.\n\n5. Unpack the index and paths from the fetched tuple.\n\n6. Check if all the files specified in the paths list are already present in the cache directory. If they are, put the index into `queue_out` and continue to the next iteration of the loop.\n\n7. If the input directory has a URL or a local path, wait for disk usage to drop below a certain threshold (25%) before proceeding with the download if the input directory has a URL.\n\n8. Iterate over each path in the paths list and determine the local path in the cache directory. If the input directory has a URL, replace the input directory's path with the URL in the path.\n\n9. Download the files from the remote location to the local cache directory, handling different schemes such as \"s3\" for AWS S3 storage or local file paths. Create necessary directories and handle copying of files or downloading from S3 as required.\n\n10. After downloading the required files, put the index into `queue_out` to inform the worker that the current files are available.", "reference_code": "def _download_data_target(input_dir: Dir, cache_dir: str, queue_in: Queue, queue_out: Queue) -> None:\n    \"\"\"This function is used to download data from a remote directory to a cache directory to optimise reading.\"\"\"\n    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            queue_out.put(None)\n            return\n\n        # 4. Unpack\n        index, paths = r\n\n        # 5. Check whether all the files are already downloaded\n        if input_dir.path and all(\n            os.path.exists(p.replace(input_dir.path, cache_dir) if input_dir else p) for p in paths\n        ):\n            queue_out.put(index)\n            continue\n\n        if input_dir.url is not None or input_dir.path is not None:\n            if input_dir.url:\n                # 6. Wait for the removers to catch up when we are downloading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            # 7. Download all the required paths to unblock the current index\n            for path in paths:\n                if input_dir.path:\n                    local_path = path.replace(input_dir.path, cache_dir)\n\n                if input_dir.url and input_dir.path:\n                    path = path.replace(input_dir.path, input_dir.url)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    dirpath = os.path.dirname(local_path)\n\n                    os.makedirs(dirpath, exist_ok=True)\n\n                    with open(local_path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n                elif os.path.isfile(path):\n                    if not path.startswith(\"/teamspace/studios/this_studio\"):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copyfile(path, local_path)\n                else:\n                    raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n\n        # 7. Inform the worker the current files are available\n        queue_out.put(index)\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "type": "function", "class_name": null, "function_name": "_upload_fn", "dependency_all": "# Cross-file Dependency:\nlitdata.streaming.client.S3Client\n    class S3Client:\n\nlitdata.streaming.client.S3Client.client\n    def client(self) -> Any:\n\n", "dependency_sampled": "# Cross-file Dependency:\nlitdata.streaming.client.S3Client\n    class S3Client:\n\n", "contexts_above": "import concurrent\nimport json\nimport logging\nimport os\nimport random\nimport shutil\nimport signal\nimport tempfile\nimport traceback\nimport types\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom multiprocessing import Process, Queue\nfrom pathlib import Path\nfrom queue import Empty\nfrom time import sleep, time\nfrom typing import Any, Dict, List, Optional, Tuple, TypeVar, Union\nfrom urllib import parse\n\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm as _tqdm\n\nfrom litdata.constants import (\n    _BOTO3_AVAILABLE,\n    _DEFAULT_FAST_DEV_RUN_ITEMS,\n    _INDEX_FILENAME,\n    _IS_IN_STUDIO,\n    _LIGHTNING_CLOUD_LATEST,\n    _TORCH_GREATER_EQUAL_2_1_0,\n)\nfrom litdata.processing.readers import BaseReader, StreamingDataLoaderReader\nfrom litdata.processing.utilities import _create_dataset\nfrom litdata.streaming import Cache\nfrom litdata.streaming.cache import Dir\nfrom litdata.streaming.client import S3Client\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.broadcast import broadcast_object\nfrom litdata.utilities.packing import _pack_greedily\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten, tree_unflatten, treespec_loads\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\nif _BOTO3_AVAILABLE:\n    import botocore\n\nlogger = logging.Logger(__name__)\n\n\ndef _get_num_nodes() -> int:\n    \"\"\"Returns the number of nodes.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 1))\n\n\ndef _get_node_rank() -> int:\n    \"\"\"Returns the current node rank of the instance.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NODE_RANK\", 0))\n\n\ndef _get_fast_dev_run() -> int:\n    \"\"\"Returns whether fast dev mode is enabled.\"\"\"\n    return bool(int(os.getenv(\"DATA_OPTIMIZER_FAST_DEV_RUN\", 1)))\n\n\ndef _get_default_cache() -> str:\n    return \"/cache\" if _IS_IN_STUDIO else tempfile.gettempdir()\n\n\ndef _get_cache_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache directory used by the Cache to store the chunks.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_FOLDER\", f\"{_get_default_cache()}/chunks\")\n    if name is None:\n        return cache_dir\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\ndef _get_cache_data_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache data directory used by the DataProcessor workers to download the files.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_DATA_CACHE_FOLDER\", f\"{_get_default_cache()}/data\")\n    if name is None:\n        return os.path.join(cache_dir)\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\ndef _wait_for_file_to_exist(s3: S3Client, obj: parse.ParseResult, sleep_time: int = 2) -> Any:\n    \"\"\"This function check.\"\"\"\n    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if \"the HeadObject operation: Not Found\" in str(e):\n                sleep(sleep_time)\n            else:\n                raise e\n\n\ndef _wait_for_disk_usage_higher_than_threshold(input_dir: str, threshold_in_gb: int = 25, sleep_time: int = 3) -> None:\n    usage = shutil.disk_usage(input_dir)\n\n    while (usage.free / 1000 / 1000 / 1000) <= threshold_in_gb:\n        sleep(sleep_time)\n        usage = shutil.disk_usage(input_dir)\n\n    return\n\n\ndef _download_data_target(input_dir: Dir, cache_dir: str, queue_in: Queue, queue_out: Queue) -> None:\n    \"\"\"This function is used to download data from a remote directory to a cache directory to optimise reading.\"\"\"\n    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            queue_out.put(None)\n            return\n\n        # 4. Unpack\n        index, paths = r\n\n        # 5. Check whether all the files are already downloaded\n        if input_dir.path and all(\n            os.path.exists(p.replace(input_dir.path, cache_dir) if input_dir else p) for p in paths\n        ):\n            queue_out.put(index)\n            continue\n\n        if input_dir.url is not None or input_dir.path is not None:\n            if input_dir.url:\n                # 6. Wait for the removers to catch up when we are downloading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            # 7. Download all the required paths to unblock the current index\n            for path in paths:\n                if input_dir.path:\n                    local_path = path.replace(input_dir.path, cache_dir)\n\n                if input_dir.url and input_dir.path:\n                    path = path.replace(input_dir.path, input_dir.url)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    dirpath = os.path.dirname(local_path)\n\n                    os.makedirs(dirpath, exist_ok=True)\n\n                    with open(local_path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n                elif os.path.isfile(path):\n                    if not path.startswith(\"/teamspace/studios/this_studio\"):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copyfile(path, local_path)\n                else:\n                    raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n\n        # 7. Inform the worker the current files are available\n        queue_out.put(index)\n\n\ndef _remove_target(input_dir: Dir, cache_dir: str, queue_in: Queue) -> None:\n    \"\"\"This function is used to delete files from the cache directory to minimise disk space.\"\"\"\n    while True:\n        # 1. Collect paths\n        paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and delete them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    os.remove(path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.remove(path)\n\n\n", "contexts_below": "\n\ndef _map_items_to_workers_sequentially(num_workers: int, user_items: List[Any]) -> List[List[Any]]:\n\n    from typing import List, Any\n    import os\n    total_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = total_nodes * num_workers\n\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return result\n\n\ndef _map_items_to_workers_weighted(\n    num_workers: int,\n    user_items: List[Any],\n    weights: Optional[List[int]] = None,\n    file_size: bool = True,\n) -> List[List[Any]]:\n    # Associate the items to the workers based on number of nodes and node rank.\n    weights = [1] * len(user_items) if weights is None else weights\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    for worker_id, size in worker_weights.items():\n        if worker_id not in worker_ids_this_node:\n            continue\n\n        if file_size:\n            print(f\"Worker {worker_id} gets {size / 1e6:.1f} MB ({len(worker_items[worker_id])} files)\")\n        else:\n            print(f\"Worker {worker_id} gets ({len(worker_items[worker_id])}) items for a total weight of {size}.\")\n\n    return [np.random.permutation(worker_items[worker_id]).tolist() for worker_id in worker_ids_this_node]\n\n\ndef _get_num_bytes(item: Any, base_path: str) -> int:\n    flattened_item, _ = tree_flatten(item)\n\n    num_bytes = 0\n    for element in flattened_item:\n        if isinstance(element, str):\n            element = Path(element).resolve()\n            if not element.exists():\n                continue\n            file_bytes = os.path.getsize(element)\n            if file_bytes == 0:\n                raise RuntimeError(f\"The file {element} has 0 bytes!\")\n            num_bytes += file_bytes\n    return num_bytes\n\n\ndef _get_item_filesizes(items: List[Any], base_path: str = \"\") -> List[int]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        for future in futures:\n            item_sizes.append(future.result())\n    return item_sizes\n\n\ndef _to_path(element: str) -> str:\n    return element if _IS_IN_STUDIO and element.startswith(\"/teamspace\") else str(Path(element).resolve())\n\n\ndef _is_path(input_dir: Optional[str], element: Any) -> bool:\n    if not isinstance(element, str):\n        return False\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n        if element.startswith(input_dir):\n            return True\n\n    return os.path.exists(element)\n\n\nclass BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        data_recipe: \"DataRecipe\",\n        input_dir: Dir,\n        output_dir: Dir,\n        items: List[Any],\n        progress_queue: Queue,\n        error_queue: Queue,\n        stop_queue: Queue,\n        num_downloaders: int,\n        num_uploaders: int,\n        remove: bool,\n        reader: Optional[BaseReader] = None,\n    ) -> None:\n        \"\"\"The BaseWorker is responsible to process the user data.\"\"\"\n        self.worker_index = worker_index\n        self.num_workers = num_workers\n        self.node_rank = node_rank\n        self.data_recipe = data_recipe\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.items = items\n        self.num_items = len(self.items)\n        self.num_downloaders = num_downloaders\n        self.num_uploaders = num_uploaders\n        self.remove = remove\n        self.reader = reader\n        self.paths: List[List[str]] = []\n        self.remover: Optional[Process] = None\n        self.downloaders: List[Process] = []\n        self.uploaders: List[Process] = []\n        self.to_download_queues: List[Queue] = []\n        self.to_upload_queues: List[Queue] = []\n        self.stop_queue = stop_queue\n        self.ready_to_process_queue: Queue = Queue()\n        self.remove_queue: Queue = Queue()\n        self.progress_queue: Queue = progress_queue\n        self.error_queue: Queue = error_queue\n        self._counter = 0\n        self._last_time = time()\n        self._index_counter = 0\n\n    def run(self) -> None:\n        try:\n            self._setup()\n            self._loop()\n        except Exception:\n            traceback_format = traceback.format_exc()\n            print(traceback_format)\n            self.error_queue.put(traceback_format)\n        print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is done.\")\n\n    def _setup(self) -> None:\n        self._set_environ_variables()\n        self._create_cache()\n        self._collect_paths()\n        self._start_downloaders()\n        self._start_uploaders()\n        self._start_remover()\n\n    def _loop(self) -> None:\n        num_downloader_finished = 0\n\n        while True:\n            index = self.ready_to_process_queue.get()\n\n            if index is None:\n                num_downloader_finished += 1\n                if num_downloader_finished == self.num_downloaders:\n                    print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is terminating.\")\n\n                    if isinstance(self.data_recipe, DataChunkRecipe):\n                        self._handle_data_chunk_recipe_end()\n\n                    if self.output_dir.url if self.output_dir.url else self.output_dir.path:\n                        # Inform the uploaders they are doing working\n                        for i in range(self.num_uploaders):\n                            self.to_upload_queues[i].put(None)\n\n                        # Wait for them all to be finished\n                        for uploader in self.uploaders:\n                            uploader.join()\n\n                    if self.remove:\n                        assert self.remover\n                        self.remove_queue.put(None)\n                        self.remover.join()\n\n                    if self.progress_queue:\n                        self.progress_queue.put((self.worker_index, self._counter))\n                    return\n                continue\n\n            if isinstance(self.data_recipe, DataChunkRecipe):\n                self._handle_data_chunk_recipe(index)\n            else:\n                self._handle_data_transform_recipe(index)\n\n            self._counter += 1\n\n            # Don't send the last progress update, so the main thread awaits for the uploader and remover\n            if self.progress_queue and (time() - self._last_time) > 1 and self._counter < (self.num_items - 2):\n                self.progress_queue.put((self.worker_index, self._counter))\n                self._last_time = time()\n\n            if self.remove and self.input_dir.path is not None and self.reader is None:\n                self.remove_queue.put(self.paths[index])\n\n            try:\n                self.stop_queue.get(timeout=0.0001)\n                return\n            except Empty:\n                pass\n\n    def _set_environ_variables(self) -> None:\n        # set the optimizer global rank and world_size\n        os.environ[\"DATA_OPTIMIZER_GLOBAL_RANK\"] = str(_get_node_rank() * self.num_workers + self.worker_index)\n        os.environ[\"DATA_OPTIMIZER_NUM_WORKERS\"] = str(self.num_workers)\n\n    def _create_cache(self) -> None:\n        self.cache_data_dir = _get_cache_data_dir()\n        os.makedirs(self.cache_data_dir, exist_ok=True)\n\n        self.cache_chunks_dir = _get_cache_dir()\n        os.makedirs(self.cache_chunks_dir, exist_ok=True)\n\n        if isinstance(self.data_recipe, DataTransformRecipe):\n            return\n\n        self.cache = Cache(\n            self.cache_chunks_dir,\n            chunk_bytes=self.data_recipe.chunk_bytes,\n            chunk_size=self.data_recipe.chunk_size,\n            compression=self.data_recipe.compression,\n        )\n        self.cache._reader._rank = _get_node_rank() * self.num_workers + self.worker_index\n\n    def _try_upload(self, data: Optional[Union[str, Tuple[str, str]]]) -> None:\n        if not data or (self.output_dir.url if self.output_dir.url else self.output_dir.path) is None:\n            return\n\n        if isinstance(data, str):\n            assert os.path.exists(data), data\n        else:\n            assert os.path.exists(data[-1]), data\n\n        self.to_upload_queues[self._counter % self.num_uploaders].put(data)\n\n    def _collect_paths(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            for index in range(len(self.items)):\n                self.ready_to_process_queue.put(index)\n            for _ in range(self.num_downloaders):\n                self.ready_to_process_queue.put(None)\n            return\n\n        items = []\n        for item in self.items:\n            flattened_item, spec = tree_flatten(item)\n\n            # For speed reasons, we assume starting with `self.input_dir` is enough to be a real file.\n            # Other alternative would be too slow.\n            # TODO: Try using dictionary for higher accurary.\n            indexed_paths = {\n                index: _to_path(element)\n                for index, element in enumerate(flattened_item)\n                if _is_path(self.input_dir.path, element)\n            }\n\n            if len(indexed_paths) == 0:\n                raise ValueError(\n                    f\"The provided item {item} didn't contain any filepaths. The input_dir is {self.input_dir.path}.\"\n                )\n\n            paths = []\n            for index, path in indexed_paths.items():\n                paths.append(path)\n                if self.input_dir and not self.input_dir.path.startswith(\"/teamspace/studios/this_studio\"):\n                    path = path.replace(self.input_dir.path, self.cache_data_dir)\n                flattened_item[index] = path\n\n            self.paths.append(paths)\n\n            items.append(tree_unflatten(flattened_item, spec))\n\n        self.items = items\n\n    def _start_downloaders(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            return\n\n        for _ in range(self.num_downloaders):\n            to_download_queue: Queue = Queue()\n            p = Process(\n                target=_download_data_target,\n                args=(\n                    self.input_dir,\n                    self.cache_data_dir,\n                    to_download_queue,\n                    self.ready_to_process_queue,\n                ),\n            )\n            p.start()\n            self.downloaders.append(p)\n            self.to_download_queues.append(to_download_queue)\n\n        for index, paths in enumerate(self.paths):\n            self.to_download_queues[index % self.num_downloaders].put((index, paths))\n\n        for downloader_index in range(self.num_downloaders):\n            self.to_download_queues[downloader_index].put(None)\n\n    def _start_remover(self) -> None:\n        if not self.remove:\n            return\n\n        self.remover = Process(\n            target=_remove_target,\n            args=(\n                self.input_dir,\n                self.cache_data_dir,\n                self.remove_queue,\n            ),\n        )\n        self.remover.start()\n\n    def _start_uploaders(self) -> None:\n        if self.output_dir.path is None and self.output_dir.url is None:\n            return\n\n        for _ in range(self.num_uploaders):\n            to_upload_queue: Queue = Queue()\n            p = Process(\n                target=_upload_fn,\n                args=(\n                    to_upload_queue,\n                    self.remove_queue,\n                    self.cache_chunks_dir,\n                    self.output_dir,\n                ),\n            )\n            p.start()\n            self.uploaders.append(p)\n            self.to_upload_queues.append(to_upload_queue)\n\n    def _handle_data_chunk_recipe(self, index: int) -> None:\n        try:\n            current_item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n            item_data_or_generator = self.data_recipe.prepare_item(current_item)\n            if isinstance(item_data_or_generator, types.GeneratorType):\n                for item_data in item_data_or_generator:\n                    if item_data is not None:\n                        chunk_filepath = self.cache._add_item(self._index_counter, item_data)\n                        self._try_upload(chunk_filepath)\n                        self._index_counter += 1\n            elif item_data_or_generator is not None:\n                chunk_filepath = self.cache._add_item(self._index_counter, item_data_or_generator)\n                self._try_upload(chunk_filepath)\n                self._index_counter += 1\n        except Exception as e:\n            raise RuntimeError(f\"Failed processing {self.items[index]}\") from e\n\n    def _handle_data_chunk_recipe_end(self) -> None:\n        chunks_filepaths = self.cache.done()\n\n        if chunks_filepaths and len(self.to_upload_queues):\n            for i, chunk_filepath in enumerate(chunks_filepaths):\n                if isinstance(chunk_filepath, str) and os.path.exists(chunk_filepath):\n                    self.to_upload_queues[i % self.num_uploaders].put(chunk_filepath)\n\n    def _handle_data_transform_recipe(self, index: int) -> None:\n        # Don't use a context manager to avoid deleting files that are being uploaded.\n        output_dir = tempfile.mkdtemp()\n        item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n        item_data = self.data_recipe.prepare_item(item, str(output_dir), len(self.items) - 1 == index)\n        if item_data is not None:\n            raise ValueError(\n                \"When using a `DataTransformRecipe`, the `prepare_item` shouldn't return anything.\"\n                \" Simply store your files under the output_dir.\"\n            )\n        filepaths = []\n        for directory, _, filenames in os.walk(output_dir):\n            for filename in filenames:\n                filepaths.append(os.path.join(directory, filename))\n\n        for filepath in filepaths:\n            self._try_upload((output_dir, filepath))\n\n\nclass DataWorkerProcess(BaseWorker, Process):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"The DataWorkerProcess is responsible to process the user data inside processes.\"\"\"\n        BaseWorker.__init__(self, *args, **kwargs)\n        Process.__init__(self)\n\n\n@dataclass\nclass _Result:\n    size: Optional[int] = None\n    num_bytes: Optional[str] = None\n    data_format: Optional[str] = None\n    compression: Optional[str] = None\n    num_chunks: Optional[int] = None\n    num_bytes_per_chunk: Optional[List[int]] = None\n\n\nT = TypeVar(\"T\")\n\n\nclass DataRecipe:\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        pass\n\n    @abstractmethod\n    def prepare_item(self, *args: Any, **kwargs: Any) -> Any:\n        pass\n\n    def __init__(self) -> None:\n        self._name: Optional[str] = None\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        return _Result(size=size)\n\n\nclass DataChunkRecipe(DataRecipe):\n    def __init__(\n        self,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n    ):\n        super().__init__()\n        if chunk_size is not None and chunk_bytes is not None:\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self.chunk_size = chunk_size\n        self.chunk_bytes = 1 << 26 if chunk_size is None else chunk_bytes\n        self.compression = compression\n\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T) -> Any:\n        \"\"\"The return of this `prepare_item` method is persisted in chunked binary files.\"\"\"\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        num_nodes = _get_num_nodes()\n        cache_dir = _get_cache_dir()\n\n        chunks = [file for file in os.listdir(cache_dir) if file.endswith(\".bin\")]\n        if chunks and delete_cached_files and output_dir.path is not None:\n            raise RuntimeError(f\"All the chunks should have been deleted. Found {chunks}\")\n\n        merge_cache = Cache(cache_dir, chunk_bytes=1)\n        node_rank = _get_node_rank()\n        merge_cache._merge_no_wait(node_rank if num_nodes > 1 else None)\n        self._upload_index(output_dir, cache_dir, num_nodes, node_rank)\n\n        if num_nodes == node_rank + 1:\n            with open(os.path.join(cache_dir, _INDEX_FILENAME)) as f:\n                config = json.load(f)\n\n            size = sum([c[\"dim\"] if c[\"dim\"] is not None else c[\"chunk_size\"] for c in config[\"chunks\"]])\n            num_bytes = sum([c[\"chunk_bytes\"] for c in config[\"chunks\"]])\n            if config[\"config\"] is not None:\n                data_format = tree_unflatten(\n                    config[\"config\"][\"data_format\"], treespec_loads(config[\"config\"][\"data_spec\"])\n                )\n            else:\n                data_format = None\n            num_chunks = len(config[\"chunks\"])\n\n            # The platform can't store more than 1024 entries.\n            # Note: This isn't really used right now, so it is fine to skip if too big.\n            num_bytes_per_chunk = [c[\"chunk_size\"] for c in config[\"chunks\"]] if num_chunks < 1024 else []\n\n            return _Result(\n                size=size,\n                num_bytes=num_bytes,\n                data_format=data_format,\n                compression=config[\"config\"][\"compression\"] if config[\"config\"] else None,\n                num_chunks=len(config[\"chunks\"]),\n                num_bytes_per_chunk=num_bytes_per_chunk,\n            )\n        return _Result(\n            size=size,\n        )\n\n    def _upload_index(self, output_dir: Dir, cache_dir: str, num_nodes: int, node_rank: Optional[int]) -> None:\n        \"\"\"This method upload the index file to the remote cloud directory.\"\"\"\n        if output_dir.path is None and output_dir.url is None:\n            return\n\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n        if num_nodes > 1:\n            local_filepath = os.path.join(cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")\n        else:\n            local_filepath = os.path.join(cache_dir, _INDEX_FILENAME)\n\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(\n                local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        if num_nodes == 1 or node_rank is None:\n            return\n\n        # Merge the index files generated by each node.\n        # Note: When using the Data Optimizer, they should be a single process on each node executing this section\n        # So no risk to get race conditon.\n        if num_nodes == node_rank + 1:\n            # Get the index file locally\n            for node_rank in range(num_nodes - 1):\n                output_dir_path = output_dir.url if output_dir.url else output_dir.path\n                assert output_dir_path\n                remote_filepath = os.path.join(output_dir_path, f\"{node_rank}-{_INDEX_FILENAME}\")\n                node_index_filepath = os.path.join(cache_dir, os.path.basename(remote_filepath))\n                if obj.scheme == \"s3\":\n                    obj = parse.urlparse(remote_filepath)\n                    _wait_for_file_to_exist(s3, obj)\n                    with open(node_index_filepath, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                elif output_dir.path and os.path.isdir(output_dir.path):\n                    shutil.copyfile(remote_filepath, node_index_filepath)\n\n            merge_cache = Cache(cache_dir, chunk_bytes=1)\n            merge_cache._merge_no_wait()\n            self._upload_index(output_dir, cache_dir, 1, None)\n\n\nclass DataTransformRecipe(DataRecipe):\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T, output_dir: str, is_last: bool) -> None:\n        \"\"\"Use your item metadata to process your files and save the file outputs into `output_dir`.\"\"\"\n\n\nclass DataProcessor:\n    def __init__(\n        self,\n        input_dir: Union[str, Dir],\n        output_dir: Optional[Union[str, Dir]] = None,\n        num_workers: Optional[int] = None,\n        num_downloaders: Optional[int] = None,\n        num_uploaders: Optional[int] = None,\n        delete_cached_files: bool = True,\n        fast_dev_run: Optional[Union[bool, int]] = None,\n        random_seed: Optional[int] = 42,\n        reorder_files: bool = True,\n        weights: Optional[List[int]] = None,\n        reader: Optional[BaseReader] = None,\n    ):\n        \"\"\"The `DatasetOptimiser` provides an efficient way to process data across multiple machine into chunks to make\n        training faster.\n\n        Arguments:\n            input_dir: The path to where the input data are stored.\n            output_dir: The path to where the output data are stored.\n            num_workers: The number of worker threads to use.\n            num_downloaders: The number of file downloaders to use.\n            num_uploaders: The number of file uploaders to use.\n            delete_cached_files: Whether to delete the cached files.\n            fast_dev_run: Whether to run a quick dev run.\n            random_seed: The random seed to be set before shuffling the data.\n            reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n                Set this to ``False`` if the order in which samples are processed should be preserved.\n            weights: Provide a list of weights associated to the inputs.\n                This is used to evenly split the work among the workers.\n            reader: Map the inputs to worker inputs and provides a read method to read a slice of the data.\n\n        \"\"\"\n        self.input_dir = _resolve_dir(input_dir)\n        self.output_dir = _resolve_dir(output_dir)\n        self.num_workers = num_workers or (1 if fast_dev_run else (os.cpu_count() or 1) * 4)\n        self.num_downloaders = num_downloaders or 2\n        self.num_uploaders = num_uploaders or 5\n        self.delete_cached_files = delete_cached_files\n        self.fast_dev_run = _get_fast_dev_run() if fast_dev_run is None else fast_dev_run\n        self.workers: Any = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.reorder_files = reorder_files\n        self.weights = weights\n        self.reader = reader\n\n        if self.reader is not None and self.weights is not None:\n            raise ValueError(\"Either the reader or the weights needs to be defined.\")\n\n        # Ensure the input dir is the same across all nodes\n        self.input_dir = broadcast_object(\"input_dir\", self.input_dir)\n\n        if self.output_dir:\n            # Ensure the output dir is the same across all nodes\n            self.output_dir = broadcast_object(\"output_dir\", self.output_dir)\n            print(f\"Storing the files under {self.output_dir.path}\")\n\n        self.random_seed = random_seed\n\n    def run(self, data_recipe: DataRecipe) -> None:\n        \"\"\"The `DataProcessor.run(...)` method triggers the data recipe processing over your dataset.\"\"\"\n        if not isinstance(data_recipe, DataRecipe):\n            raise ValueError(\"The provided value should be a data recipe.\")\n\n        t0 = time()\n        print(f\"Setup started with fast_dev_run={self.fast_dev_run}.\")\n\n        # Force random seed to be fixed\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n        torch.manual_seed(self.random_seed)\n\n        # Call the setup method of the user\n        user_items: List[Any] = data_recipe.prepare_structure(self.input_dir.path if self.input_dir else None)\n\n        if not isinstance(user_items, (list, StreamingDataLoader)):\n            raise ValueError(\"The `prepare_structure` should return a list of item metadata.\")\n\n        if isinstance(user_items, StreamingDataLoader):\n            self.reader = StreamingDataLoaderReader(user_items)\n\n        if self.reader:\n            user_items = self.reader.remap_items(user_items, self.num_workers)\n\n        if self.weights is not None:\n            if len(self.weights) != len(user_items):\n                raise ValueError(\"The provided weights length should match the inputs' length.\")\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=self.weights, file_size=False\n            )\n\n        elif self.reorder_files and self.input_dir.path:\n            # TODO: Only do this on node 0, and broadcast the item sizes to the other nodes.\n            item_sizes = _get_item_filesizes(user_items, base_path=self.input_dir.path)\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=item_sizes\n            )\n        else:\n            workers_user_items = _map_items_to_workers_sequentially(num_workers=self.num_workers, user_items=user_items)\n\n        print(f\"Setup finished in {round(time() - t0, 3)} seconds. Found {len(user_items)} items to process.\")\n\n        if self.fast_dev_run:\n            items_to_keep = self.fast_dev_run if type(self.fast_dev_run) is int else _DEFAULT_FAST_DEV_RUN_ITEMS\n            workers_user_items = [w[:items_to_keep] for w in workers_user_items]\n            print(f\"Fast dev run is enabled. Limiting to {items_to_keep} items per process.\")\n\n        num_items = sum([len(items) for items in workers_user_items])\n\n        self._cleanup_cache()\n\n        print(f\"Starting {self.num_workers} workers with {num_items} items.\")\n\n        if self.input_dir is None and self.src_resolver is not None and self.input_dir:\n            self.input_dir = self.src_resolver(self.input_dir)\n            print(f\"The remote_dir is `{self.input_dir}`.\")\n\n        signal.signal(signal.SIGINT, self._signal_handler)\n\n        self._create_process_workers(data_recipe, workers_user_items)\n\n        print(\"Workers are ready ! Starting data processing...\")\n\n        current_total = 0\n        has_failed = False\n        pbar = _tqdm(\n            desc=\"Progress\",\n            total=num_items,\n            smoothing=0,\n            position=-1,\n            mininterval=1,\n            leave=True,\n            dynamic_ncols=True,\n        )\n\n        while True:\n            try:\n                error = self.error_queue.get(timeout=0.001)\n                self._exit_on_error(error)\n            except Empty:\n                assert self.progress_queue\n                try:\n                    index, counter = self.progress_queue.get(timeout=0.001)\n                except Empty:\n                    continue\n                self.workers_tracker[index] = counter\n                new_total = sum(self.workers_tracker.values())\n\n            pbar.update(new_total - current_total)\n\n            current_total = new_total\n            if current_total == num_items:\n                break\n\n            # Exit early if all the workers are done.\n            # This means there were some kinda of errors.\n            if all(not w.is_alive() for w in self.workers):\n                has_failed = True\n                break\n\n        pbar.close()\n\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        # TODO: Understand why it hangs.\n        if num_nodes == 1:\n            for w in self.workers:\n                w.join(0)\n\n        print(\"Workers are finished.\")\n        result = data_recipe._done(len(user_items), self.delete_cached_files, self.output_dir)\n\n        if num_nodes == node_rank + 1 and self.output_dir.url and _IS_IN_STUDIO:\n            assert self.output_dir.path\n            _create_dataset(\n                input_dir=self.input_dir.path,\n                storage_dir=self.output_dir.path,\n                dataset_type=V1DatasetType.CHUNKED\n                if isinstance(data_recipe, DataChunkRecipe)\n                else V1DatasetType.TRANSFORMED,\n                empty=False,\n                size=result.size,\n                num_bytes=result.num_bytes,\n                data_format=result.data_format,\n                compression=result.compression,\n                num_chunks=result.num_chunks,\n                num_bytes_per_chunk=result.num_bytes_per_chunk,\n            )\n\n        print(\"Finished data processing!\")\n\n        # TODO: Understand why it is required to avoid long shutdown.\n        if _get_num_nodes() > 1:\n            os._exit(int(has_failed))\n\n    def _exit_on_error(self, error: str) -> None:\n        for w in self.workers:\n            w.join(0)\n        raise RuntimeError(f\"We found the following error {error}.\")\n\n    def _create_process_workers(self, data_recipe: DataRecipe, workers_user_items: List[List[Any]]) -> None:\n        self.progress_queue = Queue()\n        workers: List[DataWorkerProcess] = []\n        stop_queues: List[Queue] = []\n        for worker_idx, worker_user_items in enumerate(workers_user_items):\n            stop_queues.append(Queue())\n            worker = DataWorkerProcess(\n                worker_idx,\n                self.num_workers,\n                _get_node_rank(),\n                data_recipe,\n                self.input_dir,\n                self.output_dir,\n                worker_user_items,\n                self.progress_queue,\n                self.error_queue,\n                stop_queues[-1],\n                self.num_downloaders,\n                self.num_uploaders,\n                self.delete_cached_files,\n                self.reader,\n            )\n            worker.start()\n            workers.append(worker)\n\n        # Note: Don't store within the loop as weakref aren't serializable\n        self.workers = workers\n        self.stop_queues = stop_queues\n\n    def _signal_handler(self, signal: Any, frame: Any) -> None:\n        \"\"\"On temrination, we stop all the processes to avoid leaking RAM.\"\"\"\n        for stop_queue in self.stop_queues:\n            stop_queue.put(None)\n        for w in self.workers:\n            w.join(0)\n        os._exit(0)\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n", "input_code": "def _upload_fn(upload_queue: Queue, remove_queue: Queue, cache_dir: str, output_dir: Dir) -> None:\n\n    \"\"\"\n    This function is responsible for uploading optimized chunks from a local directory to a remote dataset directory. It supports uploading to S3 cloud storage or moving files within the local filesystem based on the output directory's scheme. The function continuously processes items from the upload queue until a termination signal is received.\n\n    Input-Output Arguments\n    :param upload_queue: Queue. The queue containing data chunks to be uploaded. Each item can be a file path or a tuple containing a temporary directory and a file path.\n    :param remove_queue: Queue. The queue to which file paths are sent for removal after successful upload.\n    :param cache_dir: str. The local directory where files are cached before being uploaded. It is prepended to file paths that don't start with it.\n    :param output_dir: Dir. The target directory where files should be uploaded. It can be a local directory or an S3 bucket, determined by its URL scheme.\n    :return: None. This function does not return any value.\n    \"\"\"", "reference_steps": "1. Define a function `_upload_fn` that takes four parameters: `upload_queue`, `remove_queue`, `cache_dir`, and `output_dir`.\n2. Parse the URL or path from `output_dir` to determine the scheme (e.g., \"s3\") and initialize an S3 client if necessary.\n3. Enter an infinite loop to continuously process data from the `upload_queue`.\n4. Retrieve an item from the `upload_queue` and determine if it's a termination signal (None), a string representing a file path, or a tuple with a temporary directory and a file path.\n5. If a termination signal is received, exit the function.\n6. Adjust the local file path to be relative to the `cache_dir` if it doesn't already start with it.\n7. Depending on the scheme, upload the file to either S3 storage or a local directory:\n   - If the scheme is \"s3\", construct the remote file path and use the S3 client to upload the file.\n   - If uploading to a local directory, create the necessary directories and move the file to the target location.\n8. If the scheme is not recognized and there is no valid path, raise a `ValueError`.\n9. After successfully uploading the file, if the `remove_queue` is provided and the local file still exists, put the local file path into the `remove_queue` for deletion.\n10. Handle any exceptions during the upload process and print the error message.", "reference_code": "def _upload_fn(upload_queue: Queue, remove_queue: Queue, cache_dir: str, output_dir: Dir) -> None:\n    \"\"\"This function is used to upload optimised chunks from a local to remote dataset directory.\"\"\"\n    obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        tmpdir = None\n\n        if isinstance(data, str) or data is None:\n            local_filepath = data\n        else:\n            tmpdir, local_filepath = data\n\n        # Terminate the process if we received a termination signal\n        if local_filepath is None:\n            return\n\n        # Upload the file to the target cloud storage\n        if not local_filepath.startswith(cache_dir):\n            local_filepath = os.path.join(cache_dir, local_filepath)\n\n        if obj.scheme == \"s3\":\n            try:\n                if tmpdir is None:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n                else:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), local_filepath.replace(tmpdir, \"\")[1:])\n\n                s3.client.upload_file(\n                    local_filepath,\n                    obj.netloc,\n                    output_filepath,\n                )\n            except Exception as e:\n                print(e)\n\n        elif output_dir.path:\n            if tmpdir is None:\n                output_filepath = os.path.join(output_dir.path, os.path.basename(local_filepath))\n            else:\n                output_filepath = os.path.join(output_dir.path, local_filepath.replace(tmpdir, \"\")[1:])\n\n            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n            shutil.move(local_filepath, output_filepath)\n        else:\n            raise ValueError(f\"The provided {output_dir.path} isn't supported.\")\n\n        # Inform the remover to delete the file\n        if remove_queue and os.path.exists(local_filepath):\n            remove_queue.put([local_filepath])\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "type": "method", "class_name": "DataProcessor", "function_name": "_cleanup_cache", "dependency_all": "# Intra-file Dependency:\nlitdata.processing.data_processor._get_cache_data_dir\n    def _get_cache_data_dir(name: Optional[str] = None) -> str:\n        \"\"\"Returns the cache data directory used by the DataProcessor workers to download the files.\"\"\"\n\nlitdata.processing.data_processor._get_cache_dir\n    def _get_cache_dir(name: Optional[str] = None) -> str:\n        \"\"\"Returns the cache directory used by the Cache to store the chunks.\"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\nlitdata.processing.data_processor._get_cache_data_dir\n    def _get_cache_data_dir(name: Optional[str] = None) -> str:\n        \"\"\"Returns the cache data directory used by the DataProcessor workers to download the files.\"\"\"\n\n", "contexts_above": "import concurrent\nimport json\nimport logging\nimport os\nimport random\nimport shutil\nimport signal\nimport tempfile\nimport traceback\nimport types\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom multiprocessing import Process, Queue\nfrom pathlib import Path\nfrom queue import Empty\nfrom time import sleep, time\nfrom typing import Any, Dict, List, Optional, Tuple, TypeVar, Union\nfrom urllib import parse\n\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm as _tqdm\n\nfrom litdata.constants import (\n    _BOTO3_AVAILABLE,\n    _DEFAULT_FAST_DEV_RUN_ITEMS,\n    _INDEX_FILENAME,\n    _IS_IN_STUDIO,\n    _LIGHTNING_CLOUD_LATEST,\n    _TORCH_GREATER_EQUAL_2_1_0,\n)\nfrom litdata.processing.readers import BaseReader, StreamingDataLoaderReader\nfrom litdata.processing.utilities import _create_dataset\nfrom litdata.streaming import Cache\nfrom litdata.streaming.cache import Dir\nfrom litdata.streaming.client import S3Client\nfrom litdata.streaming.dataloader import StreamingDataLoader\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.broadcast import broadcast_object\nfrom litdata.utilities.packing import _pack_greedily\n\nif _TORCH_GREATER_EQUAL_2_1_0:\n    from torch.utils._pytree import tree_flatten, tree_unflatten, treespec_loads\n\nif _LIGHTNING_CLOUD_LATEST:\n    from lightning_cloud.openapi import V1DatasetType\n\n\nif _BOTO3_AVAILABLE:\n    import botocore\n\nlogger = logging.Logger(__name__)\n\n\ndef _get_num_nodes() -> int:\n    \"\"\"Returns the number of nodes.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 1))\n\n\ndef _get_node_rank() -> int:\n    \"\"\"Returns the current node rank of the instance.\"\"\"\n    return int(os.getenv(\"DATA_OPTIMIZER_NODE_RANK\", 0))\n\n\ndef _get_fast_dev_run() -> int:\n    \"\"\"Returns whether fast dev mode is enabled.\"\"\"\n    return bool(int(os.getenv(\"DATA_OPTIMIZER_FAST_DEV_RUN\", 1)))\n\n\ndef _get_default_cache() -> str:\n    return \"/cache\" if _IS_IN_STUDIO else tempfile.gettempdir()\n\n\ndef _get_cache_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache directory used by the Cache to store the chunks.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_FOLDER\", f\"{_get_default_cache()}/chunks\")\n    if name is None:\n        return cache_dir\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\ndef _get_cache_data_dir(name: Optional[str] = None) -> str:\n    \"\"\"Returns the cache data directory used by the DataProcessor workers to download the files.\"\"\"\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_DATA_CACHE_FOLDER\", f\"{_get_default_cache()}/data\")\n    if name is None:\n        return os.path.join(cache_dir)\n    return os.path.join(cache_dir, name.lstrip(\"/\"))\n\n\ndef _wait_for_file_to_exist(s3: S3Client, obj: parse.ParseResult, sleep_time: int = 2) -> Any:\n    \"\"\"This function check.\"\"\"\n    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if \"the HeadObject operation: Not Found\" in str(e):\n                sleep(sleep_time)\n            else:\n                raise e\n\n\ndef _wait_for_disk_usage_higher_than_threshold(input_dir: str, threshold_in_gb: int = 25, sleep_time: int = 3) -> None:\n    usage = shutil.disk_usage(input_dir)\n\n    while (usage.free / 1000 / 1000 / 1000) <= threshold_in_gb:\n        sleep(sleep_time)\n        usage = shutil.disk_usage(input_dir)\n\n    return\n\n\ndef _download_data_target(input_dir: Dir, cache_dir: str, queue_in: Queue, queue_out: Queue) -> None:\n    \"\"\"This function is used to download data from a remote directory to a cache directory to optimise reading.\"\"\"\n    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            queue_out.put(None)\n            return\n\n        # 4. Unpack\n        index, paths = r\n\n        # 5. Check whether all the files are already downloaded\n        if input_dir.path and all(\n            os.path.exists(p.replace(input_dir.path, cache_dir) if input_dir else p) for p in paths\n        ):\n            queue_out.put(index)\n            continue\n\n        if input_dir.url is not None or input_dir.path is not None:\n            if input_dir.url:\n                # 6. Wait for the removers to catch up when we are downloading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            # 7. Download all the required paths to unblock the current index\n            for path in paths:\n                if input_dir.path:\n                    local_path = path.replace(input_dir.path, cache_dir)\n\n                if input_dir.url and input_dir.path:\n                    path = path.replace(input_dir.path, input_dir.url)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    dirpath = os.path.dirname(local_path)\n\n                    os.makedirs(dirpath, exist_ok=True)\n\n                    with open(local_path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n                elif os.path.isfile(path):\n                    if not path.startswith(\"/teamspace/studios/this_studio\"):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copyfile(path, local_path)\n                else:\n                    raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n\n        # 7. Inform the worker the current files are available\n        queue_out.put(index)\n\n\ndef _remove_target(input_dir: Dir, cache_dir: str, queue_in: Queue) -> None:\n    \"\"\"This function is used to delete files from the cache directory to minimise disk space.\"\"\"\n    while True:\n        # 1. Collect paths\n        paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and delete them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    os.remove(path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.remove(path)\n\n\ndef _upload_fn(upload_queue: Queue, remove_queue: Queue, cache_dir: str, output_dir: Dir) -> None:\n    \"\"\"This function is used to upload optimised chunks from a local to remote dataset directory.\"\"\"\n    obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        tmpdir = None\n\n        if isinstance(data, str) or data is None:\n            local_filepath = data\n        else:\n            tmpdir, local_filepath = data\n\n        # Terminate the process if we received a termination signal\n        if local_filepath is None:\n            return\n\n        # Upload the file to the target cloud storage\n        if not local_filepath.startswith(cache_dir):\n            local_filepath = os.path.join(cache_dir, local_filepath)\n\n        if obj.scheme == \"s3\":\n            try:\n                if tmpdir is None:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n                else:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), local_filepath.replace(tmpdir, \"\")[1:])\n\n                s3.client.upload_file(\n                    local_filepath,\n                    obj.netloc,\n                    output_filepath,\n                )\n            except Exception as e:\n                print(e)\n\n        elif output_dir.path:\n            if tmpdir is None:\n                output_filepath = os.path.join(output_dir.path, os.path.basename(local_filepath))\n            else:\n                output_filepath = os.path.join(output_dir.path, local_filepath.replace(tmpdir, \"\")[1:])\n\n            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n            shutil.move(local_filepath, output_filepath)\n        else:\n            raise ValueError(f\"The provided {output_dir.path} isn't supported.\")\n\n        # Inform the remover to delete the file\n        if remove_queue and os.path.exists(local_filepath):\n            remove_queue.put([local_filepath])\n\n\ndef _map_items_to_workers_sequentially(num_workers: int, user_items: List[Any]) -> List[List[Any]]:\n\n    from typing import List, Any\n    import os\n    total_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = total_nodes * num_workers\n\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return result\n\n\ndef _map_items_to_workers_weighted(\n    num_workers: int,\n    user_items: List[Any],\n    weights: Optional[List[int]] = None,\n    file_size: bool = True,\n) -> List[List[Any]]:\n    # Associate the items to the workers based on number of nodes and node rank.\n    weights = [1] * len(user_items) if weights is None else weights\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    for worker_id, size in worker_weights.items():\n        if worker_id not in worker_ids_this_node:\n            continue\n\n        if file_size:\n            print(f\"Worker {worker_id} gets {size / 1e6:.1f} MB ({len(worker_items[worker_id])} files)\")\n        else:\n            print(f\"Worker {worker_id} gets ({len(worker_items[worker_id])}) items for a total weight of {size}.\")\n\n    return [np.random.permutation(worker_items[worker_id]).tolist() for worker_id in worker_ids_this_node]\n\n\ndef _get_num_bytes(item: Any, base_path: str) -> int:\n    flattened_item, _ = tree_flatten(item)\n\n    num_bytes = 0\n    for element in flattened_item:\n        if isinstance(element, str):\n            element = Path(element).resolve()\n            if not element.exists():\n                continue\n            file_bytes = os.path.getsize(element)\n            if file_bytes == 0:\n                raise RuntimeError(f\"The file {element} has 0 bytes!\")\n            num_bytes += file_bytes\n    return num_bytes\n\n\ndef _get_item_filesizes(items: List[Any], base_path: str = \"\") -> List[int]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        for future in futures:\n            item_sizes.append(future.result())\n    return item_sizes\n\n\ndef _to_path(element: str) -> str:\n    return element if _IS_IN_STUDIO and element.startswith(\"/teamspace\") else str(Path(element).resolve())\n\n\ndef _is_path(input_dir: Optional[str], element: Any) -> bool:\n    if not isinstance(element, str):\n        return False\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n        if element.startswith(input_dir):\n            return True\n\n    return os.path.exists(element)\n\n\nclass BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        data_recipe: \"DataRecipe\",\n        input_dir: Dir,\n        output_dir: Dir,\n        items: List[Any],\n        progress_queue: Queue,\n        error_queue: Queue,\n        stop_queue: Queue,\n        num_downloaders: int,\n        num_uploaders: int,\n        remove: bool,\n        reader: Optional[BaseReader] = None,\n    ) -> None:\n        \"\"\"The BaseWorker is responsible to process the user data.\"\"\"\n        self.worker_index = worker_index\n        self.num_workers = num_workers\n        self.node_rank = node_rank\n        self.data_recipe = data_recipe\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.items = items\n        self.num_items = len(self.items)\n        self.num_downloaders = num_downloaders\n        self.num_uploaders = num_uploaders\n        self.remove = remove\n        self.reader = reader\n        self.paths: List[List[str]] = []\n        self.remover: Optional[Process] = None\n        self.downloaders: List[Process] = []\n        self.uploaders: List[Process] = []\n        self.to_download_queues: List[Queue] = []\n        self.to_upload_queues: List[Queue] = []\n        self.stop_queue = stop_queue\n        self.ready_to_process_queue: Queue = Queue()\n        self.remove_queue: Queue = Queue()\n        self.progress_queue: Queue = progress_queue\n        self.error_queue: Queue = error_queue\n        self._counter = 0\n        self._last_time = time()\n        self._index_counter = 0\n\n    def run(self) -> None:\n        try:\n            self._setup()\n            self._loop()\n        except Exception:\n            traceback_format = traceback.format_exc()\n            print(traceback_format)\n            self.error_queue.put(traceback_format)\n        print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is done.\")\n\n    def _setup(self) -> None:\n        self._set_environ_variables()\n        self._create_cache()\n        self._collect_paths()\n        self._start_downloaders()\n        self._start_uploaders()\n        self._start_remover()\n\n    def _loop(self) -> None:\n        num_downloader_finished = 0\n\n        while True:\n            index = self.ready_to_process_queue.get()\n\n            if index is None:\n                num_downloader_finished += 1\n                if num_downloader_finished == self.num_downloaders:\n                    print(f\"Worker {str(_get_node_rank() * self.num_workers + self.worker_index)} is terminating.\")\n\n                    if isinstance(self.data_recipe, DataChunkRecipe):\n                        self._handle_data_chunk_recipe_end()\n\n                    if self.output_dir.url if self.output_dir.url else self.output_dir.path:\n                        # Inform the uploaders they are doing working\n                        for i in range(self.num_uploaders):\n                            self.to_upload_queues[i].put(None)\n\n                        # Wait for them all to be finished\n                        for uploader in self.uploaders:\n                            uploader.join()\n\n                    if self.remove:\n                        assert self.remover\n                        self.remove_queue.put(None)\n                        self.remover.join()\n\n                    if self.progress_queue:\n                        self.progress_queue.put((self.worker_index, self._counter))\n                    return\n                continue\n\n            if isinstance(self.data_recipe, DataChunkRecipe):\n                self._handle_data_chunk_recipe(index)\n            else:\n                self._handle_data_transform_recipe(index)\n\n            self._counter += 1\n\n            # Don't send the last progress update, so the main thread awaits for the uploader and remover\n            if self.progress_queue and (time() - self._last_time) > 1 and self._counter < (self.num_items - 2):\n                self.progress_queue.put((self.worker_index, self._counter))\n                self._last_time = time()\n\n            if self.remove and self.input_dir.path is not None and self.reader is None:\n                self.remove_queue.put(self.paths[index])\n\n            try:\n                self.stop_queue.get(timeout=0.0001)\n                return\n            except Empty:\n                pass\n\n    def _set_environ_variables(self) -> None:\n        # set the optimizer global rank and world_size\n        os.environ[\"DATA_OPTIMIZER_GLOBAL_RANK\"] = str(_get_node_rank() * self.num_workers + self.worker_index)\n        os.environ[\"DATA_OPTIMIZER_NUM_WORKERS\"] = str(self.num_workers)\n\n    def _create_cache(self) -> None:\n        self.cache_data_dir = _get_cache_data_dir()\n        os.makedirs(self.cache_data_dir, exist_ok=True)\n\n        self.cache_chunks_dir = _get_cache_dir()\n        os.makedirs(self.cache_chunks_dir, exist_ok=True)\n\n        if isinstance(self.data_recipe, DataTransformRecipe):\n            return\n\n        self.cache = Cache(\n            self.cache_chunks_dir,\n            chunk_bytes=self.data_recipe.chunk_bytes,\n            chunk_size=self.data_recipe.chunk_size,\n            compression=self.data_recipe.compression,\n        )\n        self.cache._reader._rank = _get_node_rank() * self.num_workers + self.worker_index\n\n    def _try_upload(self, data: Optional[Union[str, Tuple[str, str]]]) -> None:\n        if not data or (self.output_dir.url if self.output_dir.url else self.output_dir.path) is None:\n            return\n\n        if isinstance(data, str):\n            assert os.path.exists(data), data\n        else:\n            assert os.path.exists(data[-1]), data\n\n        self.to_upload_queues[self._counter % self.num_uploaders].put(data)\n\n    def _collect_paths(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            for index in range(len(self.items)):\n                self.ready_to_process_queue.put(index)\n            for _ in range(self.num_downloaders):\n                self.ready_to_process_queue.put(None)\n            return\n\n        items = []\n        for item in self.items:\n            flattened_item, spec = tree_flatten(item)\n\n            # For speed reasons, we assume starting with `self.input_dir` is enough to be a real file.\n            # Other alternative would be too slow.\n            # TODO: Try using dictionary for higher accurary.\n            indexed_paths = {\n                index: _to_path(element)\n                for index, element in enumerate(flattened_item)\n                if _is_path(self.input_dir.path, element)\n            }\n\n            if len(indexed_paths) == 0:\n                raise ValueError(\n                    f\"The provided item {item} didn't contain any filepaths. The input_dir is {self.input_dir.path}.\"\n                )\n\n            paths = []\n            for index, path in indexed_paths.items():\n                paths.append(path)\n                if self.input_dir and not self.input_dir.path.startswith(\"/teamspace/studios/this_studio\"):\n                    path = path.replace(self.input_dir.path, self.cache_data_dir)\n                flattened_item[index] = path\n\n            self.paths.append(paths)\n\n            items.append(tree_unflatten(flattened_item, spec))\n\n        self.items = items\n\n    def _start_downloaders(self) -> None:\n        if self.input_dir.path is None or self.reader is not None:\n            return\n\n        for _ in range(self.num_downloaders):\n            to_download_queue: Queue = Queue()\n            p = Process(\n                target=_download_data_target,\n                args=(\n                    self.input_dir,\n                    self.cache_data_dir,\n                    to_download_queue,\n                    self.ready_to_process_queue,\n                ),\n            )\n            p.start()\n            self.downloaders.append(p)\n            self.to_download_queues.append(to_download_queue)\n\n        for index, paths in enumerate(self.paths):\n            self.to_download_queues[index % self.num_downloaders].put((index, paths))\n\n        for downloader_index in range(self.num_downloaders):\n            self.to_download_queues[downloader_index].put(None)\n\n    def _start_remover(self) -> None:\n        if not self.remove:\n            return\n\n        self.remover = Process(\n            target=_remove_target,\n            args=(\n                self.input_dir,\n                self.cache_data_dir,\n                self.remove_queue,\n            ),\n        )\n        self.remover.start()\n\n    def _start_uploaders(self) -> None:\n        if self.output_dir.path is None and self.output_dir.url is None:\n            return\n\n        for _ in range(self.num_uploaders):\n            to_upload_queue: Queue = Queue()\n            p = Process(\n                target=_upload_fn,\n                args=(\n                    to_upload_queue,\n                    self.remove_queue,\n                    self.cache_chunks_dir,\n                    self.output_dir,\n                ),\n            )\n            p.start()\n            self.uploaders.append(p)\n            self.to_upload_queues.append(to_upload_queue)\n\n    def _handle_data_chunk_recipe(self, index: int) -> None:\n        try:\n            current_item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n            item_data_or_generator = self.data_recipe.prepare_item(current_item)\n            if isinstance(item_data_or_generator, types.GeneratorType):\n                for item_data in item_data_or_generator:\n                    if item_data is not None:\n                        chunk_filepath = self.cache._add_item(self._index_counter, item_data)\n                        self._try_upload(chunk_filepath)\n                        self._index_counter += 1\n            elif item_data_or_generator is not None:\n                chunk_filepath = self.cache._add_item(self._index_counter, item_data_or_generator)\n                self._try_upload(chunk_filepath)\n                self._index_counter += 1\n        except Exception as e:\n            raise RuntimeError(f\"Failed processing {self.items[index]}\") from e\n\n    def _handle_data_chunk_recipe_end(self) -> None:\n        chunks_filepaths = self.cache.done()\n\n        if chunks_filepaths and len(self.to_upload_queues):\n            for i, chunk_filepath in enumerate(chunks_filepaths):\n                if isinstance(chunk_filepath, str) and os.path.exists(chunk_filepath):\n                    self.to_upload_queues[i % self.num_uploaders].put(chunk_filepath)\n\n    def _handle_data_transform_recipe(self, index: int) -> None:\n        # Don't use a context manager to avoid deleting files that are being uploaded.\n        output_dir = tempfile.mkdtemp()\n        item = self.items[index] if self.reader is None else self.reader.read(self.items[index])\n        item_data = self.data_recipe.prepare_item(item, str(output_dir), len(self.items) - 1 == index)\n        if item_data is not None:\n            raise ValueError(\n                \"When using a `DataTransformRecipe`, the `prepare_item` shouldn't return anything.\"\n                \" Simply store your files under the output_dir.\"\n            )\n        filepaths = []\n        for directory, _, filenames in os.walk(output_dir):\n            for filename in filenames:\n                filepaths.append(os.path.join(directory, filename))\n\n        for filepath in filepaths:\n            self._try_upload((output_dir, filepath))\n\n\nclass DataWorkerProcess(BaseWorker, Process):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"The DataWorkerProcess is responsible to process the user data inside processes.\"\"\"\n        BaseWorker.__init__(self, *args, **kwargs)\n        Process.__init__(self)\n\n\n@dataclass\nclass _Result:\n    size: Optional[int] = None\n    num_bytes: Optional[str] = None\n    data_format: Optional[str] = None\n    compression: Optional[str] = None\n    num_chunks: Optional[int] = None\n    num_bytes_per_chunk: Optional[List[int]] = None\n\n\nT = TypeVar(\"T\")\n\n\nclass DataRecipe:\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        pass\n\n    @abstractmethod\n    def prepare_item(self, *args: Any, **kwargs: Any) -> Any:\n        pass\n\n    def __init__(self) -> None:\n        self._name: Optional[str] = None\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        return _Result(size=size)\n\n\nclass DataChunkRecipe(DataRecipe):\n    def __init__(\n        self,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n    ):\n        super().__init__()\n        if chunk_size is not None and chunk_bytes is not None:\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self.chunk_size = chunk_size\n        self.chunk_bytes = 1 << 26 if chunk_size is None else chunk_bytes\n        self.compression = compression\n\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T) -> Any:\n        \"\"\"The return of this `prepare_item` method is persisted in chunked binary files.\"\"\"\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        num_nodes = _get_num_nodes()\n        cache_dir = _get_cache_dir()\n\n        chunks = [file for file in os.listdir(cache_dir) if file.endswith(\".bin\")]\n        if chunks and delete_cached_files and output_dir.path is not None:\n            raise RuntimeError(f\"All the chunks should have been deleted. Found {chunks}\")\n\n        merge_cache = Cache(cache_dir, chunk_bytes=1)\n        node_rank = _get_node_rank()\n        merge_cache._merge_no_wait(node_rank if num_nodes > 1 else None)\n        self._upload_index(output_dir, cache_dir, num_nodes, node_rank)\n\n        if num_nodes == node_rank + 1:\n            with open(os.path.join(cache_dir, _INDEX_FILENAME)) as f:\n                config = json.load(f)\n\n            size = sum([c[\"dim\"] if c[\"dim\"] is not None else c[\"chunk_size\"] for c in config[\"chunks\"]])\n            num_bytes = sum([c[\"chunk_bytes\"] for c in config[\"chunks\"]])\n            if config[\"config\"] is not None:\n                data_format = tree_unflatten(\n                    config[\"config\"][\"data_format\"], treespec_loads(config[\"config\"][\"data_spec\"])\n                )\n            else:\n                data_format = None\n            num_chunks = len(config[\"chunks\"])\n\n            # The platform can't store more than 1024 entries.\n            # Note: This isn't really used right now, so it is fine to skip if too big.\n            num_bytes_per_chunk = [c[\"chunk_size\"] for c in config[\"chunks\"]] if num_chunks < 1024 else []\n\n            return _Result(\n                size=size,\n                num_bytes=num_bytes,\n                data_format=data_format,\n                compression=config[\"config\"][\"compression\"] if config[\"config\"] else None,\n                num_chunks=len(config[\"chunks\"]),\n                num_bytes_per_chunk=num_bytes_per_chunk,\n            )\n        return _Result(\n            size=size,\n        )\n\n    def _upload_index(self, output_dir: Dir, cache_dir: str, num_nodes: int, node_rank: Optional[int]) -> None:\n        \"\"\"This method upload the index file to the remote cloud directory.\"\"\"\n        if output_dir.path is None and output_dir.url is None:\n            return\n\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n        if num_nodes > 1:\n            local_filepath = os.path.join(cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")\n        else:\n            local_filepath = os.path.join(cache_dir, _INDEX_FILENAME)\n\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(\n                local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        if num_nodes == 1 or node_rank is None:\n            return\n\n        # Merge the index files generated by each node.\n        # Note: When using the Data Optimizer, they should be a single process on each node executing this section\n        # So no risk to get race conditon.\n        if num_nodes == node_rank + 1:\n            # Get the index file locally\n            for node_rank in range(num_nodes - 1):\n                output_dir_path = output_dir.url if output_dir.url else output_dir.path\n                assert output_dir_path\n                remote_filepath = os.path.join(output_dir_path, f\"{node_rank}-{_INDEX_FILENAME}\")\n                node_index_filepath = os.path.join(cache_dir, os.path.basename(remote_filepath))\n                if obj.scheme == \"s3\":\n                    obj = parse.urlparse(remote_filepath)\n                    _wait_for_file_to_exist(s3, obj)\n                    with open(node_index_filepath, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                elif output_dir.path and os.path.isdir(output_dir.path):\n                    shutil.copyfile(remote_filepath, node_index_filepath)\n\n            merge_cache = Cache(cache_dir, chunk_bytes=1)\n            merge_cache._merge_no_wait()\n            self._upload_index(output_dir, cache_dir, 1, None)\n\n\nclass DataTransformRecipe(DataRecipe):\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T, output_dir: str, is_last: bool) -> None:\n        \"\"\"Use your item metadata to process your files and save the file outputs into `output_dir`.\"\"\"\n\n\nclass DataProcessor:\n    def __init__(\n        self,\n        input_dir: Union[str, Dir],\n        output_dir: Optional[Union[str, Dir]] = None,\n        num_workers: Optional[int] = None,\n        num_downloaders: Optional[int] = None,\n        num_uploaders: Optional[int] = None,\n        delete_cached_files: bool = True,\n        fast_dev_run: Optional[Union[bool, int]] = None,\n        random_seed: Optional[int] = 42,\n        reorder_files: bool = True,\n        weights: Optional[List[int]] = None,\n        reader: Optional[BaseReader] = None,\n    ):\n        \"\"\"The `DatasetOptimiser` provides an efficient way to process data across multiple machine into chunks to make\n        training faster.\n\n        Arguments:\n            input_dir: The path to where the input data are stored.\n            output_dir: The path to where the output data are stored.\n            num_workers: The number of worker threads to use.\n            num_downloaders: The number of file downloaders to use.\n            num_uploaders: The number of file uploaders to use.\n            delete_cached_files: Whether to delete the cached files.\n            fast_dev_run: Whether to run a quick dev run.\n            random_seed: The random seed to be set before shuffling the data.\n            reorder_files: By default, reorders the files by file size to distribute work equally among all workers.\n                Set this to ``False`` if the order in which samples are processed should be preserved.\n            weights: Provide a list of weights associated to the inputs.\n                This is used to evenly split the work among the workers.\n            reader: Map the inputs to worker inputs and provides a read method to read a slice of the data.\n\n        \"\"\"\n        self.input_dir = _resolve_dir(input_dir)\n        self.output_dir = _resolve_dir(output_dir)\n        self.num_workers = num_workers or (1 if fast_dev_run else (os.cpu_count() or 1) * 4)\n        self.num_downloaders = num_downloaders or 2\n        self.num_uploaders = num_uploaders or 5\n        self.delete_cached_files = delete_cached_files\n        self.fast_dev_run = _get_fast_dev_run() if fast_dev_run is None else fast_dev_run\n        self.workers: Any = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.reorder_files = reorder_files\n        self.weights = weights\n        self.reader = reader\n\n        if self.reader is not None and self.weights is not None:\n            raise ValueError(\"Either the reader or the weights needs to be defined.\")\n\n        # Ensure the input dir is the same across all nodes\n        self.input_dir = broadcast_object(\"input_dir\", self.input_dir)\n\n        if self.output_dir:\n            # Ensure the output dir is the same across all nodes\n            self.output_dir = broadcast_object(\"output_dir\", self.output_dir)\n            print(f\"Storing the files under {self.output_dir.path}\")\n\n        self.random_seed = random_seed\n\n    def run(self, data_recipe: DataRecipe) -> None:\n        \"\"\"The `DataProcessor.run(...)` method triggers the data recipe processing over your dataset.\"\"\"\n        if not isinstance(data_recipe, DataRecipe):\n            raise ValueError(\"The provided value should be a data recipe.\")\n\n        t0 = time()\n        print(f\"Setup started with fast_dev_run={self.fast_dev_run}.\")\n\n        # Force random seed to be fixed\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n        torch.manual_seed(self.random_seed)\n\n        # Call the setup method of the user\n        user_items: List[Any] = data_recipe.prepare_structure(self.input_dir.path if self.input_dir else None)\n\n        if not isinstance(user_items, (list, StreamingDataLoader)):\n            raise ValueError(\"The `prepare_structure` should return a list of item metadata.\")\n\n        if isinstance(user_items, StreamingDataLoader):\n            self.reader = StreamingDataLoaderReader(user_items)\n\n        if self.reader:\n            user_items = self.reader.remap_items(user_items, self.num_workers)\n\n        if self.weights is not None:\n            if len(self.weights) != len(user_items):\n                raise ValueError(\"The provided weights length should match the inputs' length.\")\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=self.weights, file_size=False\n            )\n\n        elif self.reorder_files and self.input_dir.path:\n            # TODO: Only do this on node 0, and broadcast the item sizes to the other nodes.\n            item_sizes = _get_item_filesizes(user_items, base_path=self.input_dir.path)\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers=self.num_workers, user_items=user_items, weights=item_sizes\n            )\n        else:\n            workers_user_items = _map_items_to_workers_sequentially(num_workers=self.num_workers, user_items=user_items)\n\n        print(f\"Setup finished in {round(time() - t0, 3)} seconds. Found {len(user_items)} items to process.\")\n\n        if self.fast_dev_run:\n            items_to_keep = self.fast_dev_run if type(self.fast_dev_run) is int else _DEFAULT_FAST_DEV_RUN_ITEMS\n            workers_user_items = [w[:items_to_keep] for w in workers_user_items]\n            print(f\"Fast dev run is enabled. Limiting to {items_to_keep} items per process.\")\n\n        num_items = sum([len(items) for items in workers_user_items])\n\n        self._cleanup_cache()\n\n        print(f\"Starting {self.num_workers} workers with {num_items} items.\")\n\n        if self.input_dir is None and self.src_resolver is not None and self.input_dir:\n            self.input_dir = self.src_resolver(self.input_dir)\n            print(f\"The remote_dir is `{self.input_dir}`.\")\n\n        signal.signal(signal.SIGINT, self._signal_handler)\n\n        self._create_process_workers(data_recipe, workers_user_items)\n\n        print(\"Workers are ready ! Starting data processing...\")\n\n        current_total = 0\n        has_failed = False\n        pbar = _tqdm(\n            desc=\"Progress\",\n            total=num_items,\n            smoothing=0,\n            position=-1,\n            mininterval=1,\n            leave=True,\n            dynamic_ncols=True,\n        )\n\n        while True:\n            try:\n                error = self.error_queue.get(timeout=0.001)\n                self._exit_on_error(error)\n            except Empty:\n                assert self.progress_queue\n                try:\n                    index, counter = self.progress_queue.get(timeout=0.001)\n                except Empty:\n                    continue\n                self.workers_tracker[index] = counter\n                new_total = sum(self.workers_tracker.values())\n\n            pbar.update(new_total - current_total)\n\n            current_total = new_total\n            if current_total == num_items:\n                break\n\n            # Exit early if all the workers are done.\n            # This means there were some kinda of errors.\n            if all(not w.is_alive() for w in self.workers):\n                has_failed = True\n                break\n\n        pbar.close()\n\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        # TODO: Understand why it hangs.\n        if num_nodes == 1:\n            for w in self.workers:\n                w.join(0)\n\n        print(\"Workers are finished.\")\n        result = data_recipe._done(len(user_items), self.delete_cached_files, self.output_dir)\n\n        if num_nodes == node_rank + 1 and self.output_dir.url and _IS_IN_STUDIO:\n            assert self.output_dir.path\n            _create_dataset(\n                input_dir=self.input_dir.path,\n                storage_dir=self.output_dir.path,\n                dataset_type=V1DatasetType.CHUNKED\n                if isinstance(data_recipe, DataChunkRecipe)\n                else V1DatasetType.TRANSFORMED,\n                empty=False,\n                size=result.size,\n                num_bytes=result.num_bytes,\n                data_format=result.data_format,\n                compression=result.compression,\n                num_chunks=result.num_chunks,\n                num_bytes_per_chunk=result.num_bytes_per_chunk,\n            )\n\n        print(\"Finished data processing!\")\n\n        # TODO: Understand why it is required to avoid long shutdown.\n        if _get_num_nodes() > 1:\n            os._exit(int(has_failed))\n\n    def _exit_on_error(self, error: str) -> None:\n        for w in self.workers:\n            w.join(0)\n        raise RuntimeError(f\"We found the following error {error}.\")\n\n    def _create_process_workers(self, data_recipe: DataRecipe, workers_user_items: List[List[Any]]) -> None:\n        self.progress_queue = Queue()\n        workers: List[DataWorkerProcess] = []\n        stop_queues: List[Queue] = []\n        for worker_idx, worker_user_items in enumerate(workers_user_items):\n            stop_queues.append(Queue())\n            worker = DataWorkerProcess(\n                worker_idx,\n                self.num_workers,\n                _get_node_rank(),\n                data_recipe,\n                self.input_dir,\n                self.output_dir,\n                worker_user_items,\n                self.progress_queue,\n                self.error_queue,\n                stop_queues[-1],\n                self.num_downloaders,\n                self.num_uploaders,\n                self.delete_cached_files,\n                self.reader,\n            )\n            worker.start()\n            workers.append(worker)\n\n        # Note: Don't store within the loop as weakref aren't serializable\n        self.workers = workers\n        self.stop_queues = stop_queues\n\n    def _signal_handler(self, signal: Any, frame: Any) -> None:\n        \"\"\"On temrination, we stop all the processes to avoid leaking RAM.\"\"\"\n        for stop_queue in self.stop_queues:\n            stop_queue.put(None)\n        for w in self.workers:\n            w.join(0)\n        os._exit(0)\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n", "contexts_below": "", "input_code": "\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"", "reference_steps": "1. Define a function or variable `_get_cache_data_dir()` that returns the path to the cache data directory.\n2. Call `_get_cache_data_dir()` to get the path to the cache data directory and store it in the variable `cache_data_dir`.\n3. Check if the directory at the path `cache_data_dir` already exists.\n4. If the directory exists, delete it along with all its contents using `shutil.rmtree()`, ignoring any errors that occur during the deletion.\n5. Create the cache data directory at the path `cache_data_dir` using `os.makedirs()`.\n6. Use the `exist_ok=True` parameter to allow the creation of the directory if it already exists without raising an error.", "reference_code": "\ncache_data_dir = _get_cache_data_dir()\n\n# Cleanup the cache data folder to avoid corrupted files from previous run to be there.\nif os.path.exists(cache_data_dir):\n    shutil.rmtree(cache_data_dir, ignore_errors=True)\n\nos.makedirs(cache_data_dir, exist_ok=True)\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "type": "function", "class_name": null, "function_name": "hamming_distance", "dependency_all": "# Intra-file Dependency:\niris.io.dataclasses.IrisTemplate.iris_codes\n\niris.nodes.matcher.utils.count_nonmatchbits\n\niris.nodes.matcher.utils.count_sqrt_totalbits\n\niris.nodes.matcher.utils.normalized_HD\n\n# Cross-file Dependency:\niris.io.dataclasses.IrisTemplate\n\niris.io.dataclasses.IrisTemplate.mask_codes\n\niris.io.errors.MatcherError\n\n", "dependency_sampled": "# Intra-file Dependency:\niris.nodes.matcher.utils.count_sqrt_totalbits\n\niris.nodes.matcher.utils.normalized_HD\n\n# Cross-file Dependency:\niris.io.dataclasses.IrisTemplate.mask_codes\n\n", "contexts_above": "from typing import List, Optional, Tuple\n\nimport numpy as np\n\nfrom iris.io.dataclasses import IrisTemplate\nfrom iris.io.errors import MatcherError\n\n\ndef normalized_HD(irisbitcount: int, maskbitcount: int, sqrt_totalbitcount: float, nm_dist: float) -> float:\n    \"\"\"Perform normalized HD calculation.\n\n    Args:\n        irisbitcount (int): nonmatched iriscode bit count.\n        maskbitcount (int): common maskcode bit count.\n        sqrt_totalbitcount (float): square root of bit counts.\n        nm_dist (float): nonmatch distance used for normalized HD.\n\n    Returns:\n        float: normalized Hamming distance.\n    \"\"\"\n    norm_HD = max(\n        0, nm_dist - (nm_dist - irisbitcount / maskbitcount) * min(1.0, np.sqrt(maskbitcount) / sqrt_totalbitcount)\n    )\n    return norm_HD\n\n\ndef count_sqrt_totalbits(\n    toal_codesize: int,\n    half_width: List[int],\n    weights: Optional[List[np.ndarray]] = None,\n) -> Tuple[float, float, float]:\n    \"\"\"Count total amount of sqrt bits.\n\n    Args:\n        toal_codesizes (int): total size of iriscodes.\n        half_width (List[int]): half width of iriscodes.\n        weights (Optional[List[np.ndarray]] = None): list of weights table. Optional paremeter for weighted HD. Defaults to None.\n\n    Returns:\n        Tuple[float, float, float]: square root of bit counts from whole iris, top iris and bottom iris.\n    \"\"\"\n    sqrt_totalbitcount = np.sqrt(np.sum([np.sum(w) for w in weights])) if weights else np.sqrt(toal_codesize * 3 / 4)\n\n    sqrt_totalbitcount_bot = (\n        np.sqrt(np.sum([np.sum(w[:, :hw, ...]) for w, hw in zip(weights, half_width)]))\n        if weights\n        else sqrt_totalbitcount / np.sqrt(2)\n    )\n\n    sqrt_totalbitcount_top = (\n        np.sqrt(np.sum([np.sum(w[:, hw:, ...]) for w, hw in zip(weights, half_width)]))\n        if weights\n        else sqrt_totalbitcount / np.sqrt(2)\n    )\n\n    return sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot\n\n\ndef count_nonmatchbits(\n    irisbits: np.ndarray,\n    maskbits: np.ndarray,\n    half_width: List[int],\n    weights: Optional[List[np.ndarray]] = None,\n) -> Tuple[int, int, int, int]:\n    \"\"\"Count nonmatch bits for Hammming distance.\n\n    Args:\n        irisbits (np.ndarray): nonmatch irisbits.\n        maskbits (np.ndarray): common maskbits.\n        half_width (List[int]): list of half of code width.\n        weights (Optional[np.ndarray] = None): list of weights table. Optional paremeter for weighted HD. Defaults to None.\n\n    Returns:\n        Tuple[int, int, int, int]: nonmatch iriscode bit count and common maskcode bit count from top iris and bottom iris.\n    \"\"\"\n    if weights:\n        irisbitcount_top = np.sum(\n            [\n                np.sum(np.multiply(x[:, hw:, ...] & y[:, hw:, ...], z[:, hw:, ...]))\n                for x, y, hw, z in zip(irisbits, maskbits, half_width, weights)\n            ]\n        )\n        maskbitcount_top = np.sum(\n            [np.sum(np.multiply(x[:, hw:, ...], z[:, hw:, ...])) for x, hw, z in zip(maskbits, half_width, weights)]\n        )\n        irisbitcount_bot = np.sum(\n            [\n                np.sum(np.multiply(x[:, :hw, ...] & y[:, :hw, ...], z[:, :hw, ...]))\n                for x, y, hw, z in zip(irisbits, maskbits, half_width, weights)\n            ]\n        )\n        maskbitcount_bot = np.sum(\n            [np.sum(np.multiply(x[:, :hw, ...], z[:, :hw, ...])) for x, hw, z in zip(maskbits, half_width, weights)]\n        )\n    else:\n        irisbitcount_top = np.sum(\n            [np.sum(x[:, hw:, ...] & y[:, hw:, ...]) for x, y, hw in zip(irisbits, maskbits, half_width)]\n        )\n        maskbitcount_top = np.sum([np.sum(x[:, hw:, ...]) for x, hw in zip(maskbits, half_width)])\n        irisbitcount_bot = np.sum(\n            [np.sum(x[:, :hw, ...] & y[:, :hw, ...]) for x, y, hw in zip(irisbits, maskbits, half_width)]\n        )\n        maskbitcount_bot = np.sum([np.sum(x[:, :hw, ...]) for x, hw in zip(maskbits, half_width)])\n\n    return irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot\n\n\n", "contexts_below": "", "input_code": "def hamming_distance(\n    template_probe: IrisTemplate,\n    template_gallery: IrisTemplate,\n    rotation_shift: int,\n    nm_dist: Optional[float] = None,\n    weights: Optional[List[np.ndarray]] = None,\n) -> Tuple[float, int]:\n\n    \"\"\"\n    This function calculates the Hamming distance between two iris templates, considering an allowed rotation shift. It supports optional parameters for normalized and weighted Hamming distance calculations. The function returns the minimum Hamming distance and the corresponding rotation shift that achieves this minimum distance.\n\n    Input-Output Arguments\n    :param template_probe: IrisTemplate. The iris template from the probe, used as one of the inputs for the Hamming distance calculation.\n    :param template_gallery: IrisTemplate. The iris template from the gallery, used as the other input for the Hamming distance calculation.\n    :param rotation_shift: int. The amount of rotation allowed in the matching process, which is converted to columns for the calculation.\n    :param nm_dist: Optional[float] = None. The nonmatch distance, an optional parameter for calculating a normalized Hamming distance. Defaults to None.\n    :param weights: Optional[List[np.ndarray]] = None. A list of weights tables, an optional parameter for calculating a weighted Hamming distance. Defaults to None.\n    :return: Tuple[float, int]. The function returns a tuple containing the minimum Hamming distance and the corresponding rotation shift that achieves this minimum distance.\n    \"\"\"", "reference_steps": "1. Define a function `hamming_distance` that takes in two `IrisTemplate` objects (`template_probe` and `template_gallery`), an integer `rotation_shift`, an optional float `nm_dist`, and an optional list of numpy arrays `weights`.\n2. Initialize an empty list `half_codewidth` to store half the code width of the iris templates.\n3. Loop through each pair of probe and gallery iris codes, checking if they have the same shape and if the number of columns is even. Append the half-width of the iris code to `half_codewidth`.\n4. If weights are provided, verify that each weight table has the same shape as the corresponding iris code.\n5. If `nm_dist` is provided, calculate the square root of the total bit count (`sqrt_totalbitcount`) and the square root of the top and bottom half bit counts (`sqrt_totalbitcount_top` and `sqrt_totalbitcount_bot`) using the function `count_sqrt_totalbits`.\n6. Initialize variables `match_dist` to 1 and `match_rot` to 0 to store the minimum Hamming distance and corresponding rotation shift.\n7. Loop over the range of shifts from `-rotation_shift` to `rotation_shift` (inclusive), and for each shift:\n    a. Roll the probe iris codes by the shift amount and compare them with the gallery iris codes to get the differing bits (`irisbits`).\n    b. Roll the probe mask codes by the shift amount and perform a bitwise AND with the gallery mask codes to get the valid bits (`maskbits`).\n8. If weights are provided, count the non-matching bits (`irisbitcount_top`, `maskbitcount_top`, `irisbitcount_bot`, `maskbitcount_bot`) using the function `count_nonmatchbits` with weights.\n9. If `nm_dist` is provided, calculate the normalized Hamming distance (`Hdist`) for the top and bottom halves and combine them. Otherwise, calculate the simple Hamming distance by dividing the sum of non-matching bits by the total valid bits.\n10. Update `match_dist` and `match_rot` if the current Hamming distance is less than the previous minimum or if it is equal and the shift is zero. After looping through all shifts, return the minimum Hamming distance and the corresponding rotation shift as a tuple.", "reference_code": "def hamming_distance(\n    template_probe: IrisTemplate,\n    template_gallery: IrisTemplate,\n    rotation_shift: int,\n    nm_dist: Optional[float] = None,\n    weights: Optional[List[np.ndarray]] = None,\n) -> Tuple[float, int]:\n    \"\"\"Compute Hamming distance.\n\n    Args:\n        template_probe (IrisTemplate): Iris template from probe.\n        template_gallery (IrisTemplate): Iris template from gallery.\n        rotation_shift (int): rotation allowed in matching, converted to columns.\n        nm_dist (Optional[float] = None): nonmatch distance, Optional paremeter for normalized HD. Defaults to None.\n        weights (Optional[List[np.ndarray]]= None): list of weights table. Optional paremeter for weighted HD. Defaults to None.\n\n    Returns:\n        Tuple[float, int]: miminum Hamming distance and corresonding rotation shift.\n    \"\"\"\n    half_codewidth = []\n\n    for probe_code, gallery_code in zip(template_probe.iris_codes, template_gallery.iris_codes):\n        if probe_code.shape != gallery_code.shape:\n            raise MatcherError(\"probe and gallery iris codes are of different sizes\")\n        if (probe_code.shape[1] % 2) != 0:\n            raise MatcherError(\"number of columns of iris codes need to be even\")\n        half_codewidth.append(int(probe_code.shape[1] / 2))\n\n    if weights:\n        for probe_code, w in zip(template_probe.iris_codes, weights):\n            if probe_code.shape != w.shape:\n                raise MatcherError(\"weights table and iris codes are of different sizes\")\n\n    if nm_dist:\n        if weights:\n            sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n                np.sum([np.size(a) for a in template_probe.iris_codes]), half_codewidth, weights\n            )\n        else:\n            sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n                np.sum([np.size(a) for a in template_probe.iris_codes]), half_codewidth\n            )\n\n    # Calculate the Hamming distance between probe and gallery template.\n    match_dist = 1\n    match_rot = 0\n    for shiftby in range(-rotation_shift, rotation_shift + 1):\n        irisbits = [\n            np.roll(probe_code, shiftby, axis=1) != gallery_code\n            for probe_code, gallery_code in zip(template_probe.iris_codes, template_gallery.iris_codes)\n        ]\n        maskbits = [\n            np.roll(probe_code, shiftby, axis=1) & gallery_code\n            for probe_code, gallery_code in zip(template_probe.mask_codes, template_gallery.mask_codes)\n        ]\n\n        if weights:\n            irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n                irisbits, maskbits, half_codewidth, weights\n            )\n        else:\n            irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n                irisbits, maskbits, half_codewidth\n            )\n        maskbitcount = maskbitcount_top + maskbitcount_bot\n\n        if maskbitcount == 0:\n            continue\n\n        if nm_dist:\n            normdist_top = (\n                normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n                if maskbitcount_top > 0\n                else 1\n            )\n            normdist_bot = (\n                normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n                if maskbitcount_bot > 0\n                else 1\n            )\n            w_top = np.sqrt(maskbitcount_top)\n            w_bot = np.sqrt(maskbitcount_bot)\n            Hdist = (\n                normalized_HD((irisbitcount_top + irisbitcount_bot), maskbitcount, sqrt_totalbitcount, nm_dist) / 2\n                + (normdist_top * w_top + normdist_bot * w_bot) / (w_top + w_bot) / 2\n            )\n        else:\n            Hdist = (irisbitcount_top + irisbitcount_bot) / maskbitcount\n\n        if (Hdist < match_dist) or (Hdist == match_dist and shiftby == 0):\n            match_dist = Hdist\n            match_rot = shiftby\n\n    return match_dist, match_rot\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "type": "method", "class_name": "RotationTransform", "function_name": "apply_coords", "dependency_all": "# Intra-class Dependency:\ndetectron2.data.transforms.transform.RotationTransform.rm_coords\n\n", "dependency_sampled": "# Intra-class Dependency:\ndetectron2.data.transforms.transform.RotationTransform.rm_coords\n\n", "contexts_above": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\"\"\"\nSee \"Data Augmentation\" tutorial for an overview of the system:\nhttps://detectron2.readthedocs.io/tutorials/augmentation.html\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fvcore.transforms.transform import (\n    CropTransform,\n    HFlipTransform,\n    NoOpTransform,\n    Transform,\n    TransformList,\n)\nfrom PIL import Image\n\ntry:\n    import cv2  # noqa\nexcept ImportError:\n    # OpenCV is an optional dependency at the moment\n    pass\n\n__all__ = [\n    \"ExtentTransform\",\n    \"ResizeTransform\",\n    \"RotationTransform\",\n    \"ColorTransform\",\n    \"PILColorTransform\",\n]\n\n\nclass ExtentTransform(Transform):\n    \"\"\"\n    Extracts a subregion from the source image and scales it to the output size.\n\n    The fill color is used to map pixels from the source rect that fall outside\n    the source image.\n\n    See: https://pillow.readthedocs.io/en/latest/PIL.html#PIL.ImageTransform.ExtentTransform\n    \"\"\"\n\n    def __init__(self, src_rect, output_size, interp=Image.LINEAR, fill=0):\n        \"\"\"\n        Args:\n            src_rect (x0, y0, x1, y1): src coordinates\n            output_size (h, w): dst image size\n            interp: PIL interpolation methods\n            fill: Fill color used when src_rect extends outside image\n        \"\"\"\n        super().__init__()\n        self._set_attributes(locals())\n\n    def apply_image(self, img, interp=None):\n        h, w = self.output_size\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n        pil_image = pil_image.transform(\n            size=(w, h),\n            method=Image.EXTENT,\n            data=self.src_rect,\n            resample=interp if interp else self.interp,\n            fill=self.fill,\n        )\n        ret = np.asarray(pil_image)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret\n\n    def apply_coords(self, coords):\n        # Transform image center from source coordinates into output coordinates\n        # and then map the new origin to the corner of the output image.\n        h, w = self.output_size\n        x0, y0, x1, y1 = self.src_rect\n        new_coords = coords.astype(np.float32)\n        new_coords[:, 0] -= 0.5 * (x0 + x1)\n        new_coords[:, 1] -= 0.5 * (y0 + y1)\n        new_coords[:, 0] *= w / (x1 - x0)\n        new_coords[:, 1] *= h / (y1 - y0)\n        new_coords[:, 0] += 0.5 * w\n        new_coords[:, 1] += 0.5 * h\n        return new_coords\n\n    def apply_segmentation(self, segmentation):\n        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)\n        return segmentation\n\n\nclass ResizeTransform(Transform):\n    \"\"\"\n    Resize the image to a target size.\n    \"\"\"\n\n    def __init__(self, h, w, new_h, new_w, interp=None):\n        \"\"\"\n        Args:\n            h, w (int): original image size\n            new_h, new_w (int): new image size\n            interp: PIL interpolation methods, defaults to bilinear.\n        \"\"\"\n        # TODO decide on PIL vs opencv\n        super().__init__()\n        if interp is None:\n            interp = Image.BILINEAR\n        self._set_attributes(locals())\n\n    def apply_image(self, img, interp=None):\n        assert img.shape[:2] == (self.h, self.w)\n        assert len(img.shape) <= 4\n        interp_method = interp if interp is not None else self.interp\n\n        if img.dtype == np.uint8:\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n            else:\n                pil_image = Image.fromarray(img)\n            pil_image = pil_image.resize((self.new_w, self.new_h), interp_method)\n            ret = np.asarray(pil_image)\n            if len(img.shape) > 2 and img.shape[2] == 1:\n                ret = np.expand_dims(ret, -1)\n        else:\n            # PIL only supports uint8\n            if any(x < 0 for x in img.strides):\n                img = np.ascontiguousarray(img)\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n                Image.NEAREST: \"nearest\",\n                Image.BILINEAR: \"bilinear\",\n                Image.BICUBIC: \"bicubic\",\n            }\n            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n            align_corners = None if mode == \"nearest\" else False\n            img = F.interpolate(\n                img, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n            )\n            shape[:2] = (self.new_h, self.new_w)\n            ret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\n\n        return ret\n\n    def apply_coords(self, coords):\n        coords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)\n        coords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)\n        return coords\n\n    def apply_segmentation(self, segmentation):\n        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)\n        return segmentation\n\n    def inverse(self):\n        return ResizeTransform(self.new_h, self.new_w, self.h, self.w, self.interp)\n\n\nclass RotationTransform(Transform):\n    \"\"\"\n    This method returns a copy of this image, rotated the given\n    number of degrees counter clockwise around its center.\n    \"\"\"\n\n    def __init__(self, h, w, angle, expand=True, center=None, interp=None):\n        \"\"\"\n        Args:\n            h, w (int): original image size\n            angle (float): degrees for rotation\n            expand (bool): choose if the image should be resized to fit the whole\n                rotated image (default), or simply cropped\n            center (tuple (width, height)): coordinates of the rotation center\n                if left to None, the center will be fit to the center of each image\n                center has no effect if expand=True because it only affects shifting\n            interp: cv2 interpolation method, default cv2.INTER_LINEAR\n        \"\"\"\n        super().__init__()\n        image_center = np.array((w / 2, h / 2))\n        if center is None:\n            center = image_center\n        if interp is None:\n            interp = cv2.INTER_LINEAR\n        abs_cos, abs_sin = (abs(np.cos(np.deg2rad(angle))), abs(np.sin(np.deg2rad(angle))))\n        if expand:\n            # find the new width and height bounds\n            bound_w, bound_h = np.rint(\n                [h * abs_sin + w * abs_cos, h * abs_cos + w * abs_sin]\n            ).astype(int)\n        else:\n            bound_w, bound_h = w, h\n\n        self._set_attributes(locals())\n        self.rm_coords = self.create_rotation_matrix()\n        # Needed because of this problem https://github.com/opencv/opencv/issues/11784\n        self.rm_image = self.create_rotation_matrix(offset=-0.5)\n\n    def apply_image(self, img, interp=None):\n        \"\"\"\n        img should be a numpy array, formatted as Height * Width * Nchannels\n        \"\"\"\n        if len(img) == 0 or self.angle % 360 == 0:\n            return img\n        assert img.shape[:2] == (self.h, self.w)\n        interp = interp if interp is not None else self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n\n", "contexts_below": "\n    def apply_segmentation(self, segmentation):\n        segmentation = self.apply_image(segmentation, interp=cv2.INTER_NEAREST)\n        return segmentation\n\n    def create_rotation_matrix(self, offset=0):\n        center = (self.center[0] + offset, self.center[1] + offset)\n        rm = cv2.getRotationMatrix2D(tuple(center), self.angle, 1)\n        if self.expand:\n            # Find the coordinates of the center of rotation in the new image\n            # The only point for which we know the future coordinates is the center of the image\n            rot_im_center = cv2.transform(self.image_center[None, None, :] + offset, rm)[0, 0, :]\n            new_center = np.array([self.bound_w / 2, self.bound_h / 2]) + offset - rot_im_center\n            # shift the rotation center to the new coordinates\n            rm[:, 2] += new_center\n        return rm\n\n    def inverse(self):\n        \"\"\"\n        The inverse is to rotate it back with expand, and crop to get the original shape.\n        \"\"\"\n        if not self.expand:  # Not possible to inverse if a part of the image is lost\n            raise NotImplementedError()\n        rotation = RotationTransform(\n            self.bound_h, self.bound_w, -self.angle, True, None, self.interp\n        )\n        crop = CropTransform(\n            (rotation.bound_w - self.w) // 2, (rotation.bound_h - self.h) // 2, self.w, self.h\n        )\n        return TransformList([rotation, crop])\n\n\nclass ColorTransform(Transform):\n    \"\"\"\n    Generic wrapper for any photometric transforms.\n    These transformations should only affect the color space and\n        not the coordinate space of the image (e.g. annotation\n        coordinates such as bounding boxes should not be changed)\n    \"\"\"\n\n    def __init__(self, op):\n        \"\"\"\n        Args:\n            op (Callable): operation to be applied to the image,\n                which takes in an ndarray and returns an ndarray.\n        \"\"\"\n        if not callable(op):\n            raise ValueError(\"op parameter should be callable\")\n        super().__init__()\n        self._set_attributes(locals())\n\n    def apply_image(self, img):\n        return self.op(img)\n\n    def apply_coords(self, coords):\n        return coords\n\n    def inverse(self):\n        return NoOpTransform()\n\n    def apply_segmentation(self, segmentation):\n        return segmentation\n\n\nclass PILColorTransform(ColorTransform):\n    \"\"\"\n    Generic wrapper for PIL Photometric image transforms,\n        which affect the color space and not the coordinate\n        space of the image\n    \"\"\"\n\n    def __init__(self, op):\n        \"\"\"\n        Args:\n            op (Callable): operation to be applied to the image,\n                which takes in a PIL Image and returns a transformed\n                PIL Image.\n                For reference on possible operations see:\n                - https://pillow.readthedocs.io/en/stable/\n        \"\"\"\n        if not callable(op):\n            raise ValueError(\"op parameter should be callable\")\n        super().__init__(op)\n\n    def apply_image(self, img):\n        img = Image.fromarray(img)\n        return np.asarray(super().apply_image(img))\n\n\ndef HFlip_rotated_box(transform, rotated_boxes):\n    \"\"\"\n    Apply the horizontal flip transform on rotated boxes.\n\n    Args:\n        rotated_boxes (ndarray): Nx5 floating point array of\n            (x_center, y_center, width, height, angle_degrees) format\n            in absolute coordinates.\n    \"\"\"\n    # Transform x_center\n    rotated_boxes[:, 0] = transform.width - rotated_boxes[:, 0]\n    # Transform angle\n    rotated_boxes[:, 4] = -rotated_boxes[:, 4]\n    return rotated_boxes\n\n\ndef Resize_rotated_box(transform, rotated_boxes):\n    \"\"\"\n    Apply the resizing transform on rotated boxes. For details of how these (approximation)\n    formulas are derived, please refer to :meth:`RotatedBoxes.scale`.\n\n    Args:\n        rotated_boxes (ndarray): Nx5 floating point array of\n            (x_center, y_center, width, height, angle_degrees) format\n            in absolute coordinates.\n    \"\"\"\n    scale_factor_x = transform.new_w * 1.0 / transform.w\n    scale_factor_y = transform.new_h * 1.0 / transform.h\n    rotated_boxes[:, 0] *= scale_factor_x\n    rotated_boxes[:, 1] *= scale_factor_y\n    theta = rotated_boxes[:, 4] * np.pi / 180.0\n    c = np.cos(theta)\n    s = np.sin(theta)\n    rotated_boxes[:, 2] *= np.sqrt(np.square(scale_factor_x * c) + np.square(scale_factor_y * s))\n    rotated_boxes[:, 3] *= np.sqrt(np.square(scale_factor_x * s) + np.square(scale_factor_y * c))\n    rotated_boxes[:, 4] = np.arctan2(scale_factor_x * s, scale_factor_y * c) * 180 / np.pi\n\n    return rotated_boxes\n\n\nHFlipTransform.register_type(\"rotated_box\", HFlip_rotated_box)\nResizeTransform.register_type(\"rotated_box\", Resize_rotated_box)\n\n# not necessary any more with latest fvcore\nNoOpTransform.register_type(\"rotated_box\", lambda t, x: x)\n", "input_code": "    def apply_coords(self, coords):\n\n        \"\"\"\n        Applies a rotation transformation to a set of coordinates. This function transforms each coordinate in the input array according to a predefined rotation matrix associated with the instance. If the rotation angle is a multiple of 360 degrees or the input is empty, the original coordinates are returned without modification.\n\n        Input-Output Arguments\n        :param self: RotationTransform. An instance of the RotationTransform class, which should have a predefined rotation matrix (self.rm_coords) and an angle attribute.\n        :param coords: array-like, a N * 2 array containing N pairs of (x, y) points that represent the coordinates to be transformed. These coordinates are expected to undergo a rotation transformation based on the instance's rotation matrix.\n        :return: array-like, a N * 2 array of transformed coordinates. If the input coordinates are empty or the rotation angle is a multiple of 360 degrees, the original coordinates are returned.\n        \"\"\"", "reference_steps": "1. Define a function `apply_coords` that takes `self` and `coords` as arguments.\n2. The `coords` parameter is expected to be an array-like structure containing N pairs of (x, y) points.\n3. Convert `coords` into a NumPy array of type `float` to ensure compatibility with numerical operations.\n4. Check if `coords` is empty or if the `angle` attribute of `self` is a multiple of 360 degrees, which would imply no rotation is needed.\n5. If either of the above conditions is met, return the original `coords` array without any transformation.\n6. If the conditions are not met, reshape `coords` to add an extra dimension, preparing it for transformation. This is done by indexing with `[:, np.newaxis, :]`.\n7. Apply a transformation to the reshaped `coords` using OpenCV's `cv2.transform` function and a rotation matrix `self.rm_coords`.\n8. The result of the transformation is a 3-dimensional array; remove the extra dimension by indexing with `[:, 0, :]`.\n9. Return the transformed coordinates as a 2-dimensional array.\n10. The function applies a rotation transformation to the input coordinates based on the rotation matrix `self.rm_coords` unless the rotation angle is a multiple of 360 degrees or the input is empty.", "reference_code": "def apply_coords(self, coords):\n    \"\"\"\n    coords should be a N * 2 array-like, containing N couples of (x, y) points\n    \"\"\"\n    coords = np.asarray(coords, dtype=float)\n    if len(coords) == 0 or self.angle % 360 == 0:\n        return coords\n    return cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "type": "method", "class_name": "SearchEngine", "function_name": "search", "dependency_all": "# Intra-class Dependency:\nmicrosearch.engine.SearchEngine.bm25\n\n# Intra-file Dependency:\nmicrosearch.engine.normalize_string\n\nmicrosearch.engine.update_url_scores\n\n", "dependency_sampled": "# Intra-file Dependency:\nmicrosearch.engine.update_url_scores\n\n", "contexts_above": "from collections import defaultdict\nfrom math import log\nimport string\n\n\ndef update_url_scores(old: dict[str, float], new: dict[str, float]):\n    for url, score in new.items():\n        if url in old:\n            old[url] += score\n        else:\n            old[url] = score\n    return old\n\n\ndef normalize_string(input_string: str) -> str:\n    translation_table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n    string_without_punc = input_string.translate(translation_table)\n    string_without_double_spaces = \" \".join(string_without_punc.split())\n    return string_without_double_spaces.lower()\n\n\nclass SearchEngine:\n    def __init__(self, k1: float = 1.5, b: float = 0.75):\n        self._index: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        self._documents: dict[str, str] = {}\n        self.k1 = k1\n        self.b = b\n\n    @property\n    def posts(self) -> list[str]:\n        return list(self._documents.keys())\n\n    @property\n    def number_of_documents(self) -> int:\n        return len(self._documents)\n\n    @property\n    def avdl(self) -> float:\n        if not hasattr(self, \"_avdl\"):\n            self._avdl = sum(len(d) for d in self._documents.values()) / len(self._documents)\n        return self._avdl\n\n    def idf(self, kw: str) -> float:\n        N = self.number_of_documents\n        n_kw = len(self.get_urls(kw))\n        return log((N - n_kw + 0.5) / (n_kw + 0.5) + 1)\n\n    def bm25(self, kw: str) -> dict[str, float]:\n        result = {}\n        idf_score = self.idf(kw)\n        avdl = self.avdl\n        for url, freq in self.get_urls(kw).items():\n            numerator = freq * (self.k1 + 1)\n            denominator = freq + self.k1 * (\n                1 - self.b + self.b * len(self._documents[url]) / avdl\n            )\n            result[url] = idf_score * numerator / denominator\n        return result\n\n", "contexts_below": "\n    def index(self, url: str, content: str) -> None:\n        self._documents[url] = content\n        words = normalize_string(content).split(\" \")\n        for word in words:\n            self._index[word][url] += 1\n        if hasattr(self, \"_avdl\"):\n            del self._avdl\n\n    def bulk_index(self, documents: list[tuple[str, str]]):\n        for url, content in documents:\n            self.index(url, content)\n\n    def get_urls(self, keyword: str) -> dict[str, int]:\n        keyword = normalize_string(keyword)\n        return self._index[keyword]\n\n\nengine = SearchEngine()\n", "input_code": "    def search(self, query: str) -> dict[str, float]:\n\n        \"\"\"\n        The function performs a search based on the input query. It normalizes the query string, splits it into keywords, and then calculates the BM25 score for each keyword across URLs. The scores for URLs are aggregated and returned as a dictionary where each URL is a key and its aggregated score is the value.\n\n        Input-Output Arguments\n        :param self: SearchEngine. An instance of the SearchEngine class.\n        :param query: str, The search query input by the user. It is normalized and split into keywords for scoring.\n        :return: dict[str, float], A dictionary where each key is a URL and its value is the aggregated BM25 score based on the input query's keywords.\n        \"\"\"", "reference_steps": "1. Define a search function that accepts a query string as input and returns a dictionary with URLs as keys and their corresponding scores as float values.\n2. Normalize the query string to ensure uniformity (e.g., lowercasing, removing punctuation, etc.).\n3. Split the normalized query string into individual keywords.\n4. Initialize an empty dictionary to store the cumulative scores for each URL.\n5. Iterate over each keyword in the list of keywords.\n6. For each keyword, retrieve a dictionary of URLs and their associated scores using the BM25 algorithm (or a similar text-matching algorithm).\n7. Update the cumulative URL scores dictionary with the new scores obtained for the current keyword.\n8. Repeat steps 5 to 7 for each keyword in the query.\n9. After iterating through all keywords, finalize the URL scores dictionary, which now contains the aggregated scores for each URL based on all keywords.\n10. Return the final dictionary of URL scores.", "reference_code": "def search(self, query: str) -> dict[str, float]:\n    keywords = normalize_string(query).split(\" \")\n    url_scores: dict[str, float] = {}\n    for kw in keywords:\n        kw_urls_score = self.bm25(kw)\n        url_scores = update_url_scores(url_scores, kw_urls_score)\n    return url_scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "type": "method", "class_name": "SearchEngine", "function_name": "bulk_index", "dependency_all": "# Intra-class Dependency:\nmicrosearch.engine.SearchEngine.index\n\n", "dependency_sampled": "# Intra-class Dependency:\nmicrosearch.engine.SearchEngine.index\n\n", "contexts_above": "from collections import defaultdict\nfrom math import log\nimport string\n\n\ndef update_url_scores(old: dict[str, float], new: dict[str, float]):\n    for url, score in new.items():\n        if url in old:\n            old[url] += score\n        else:\n            old[url] = score\n    return old\n\n\ndef normalize_string(input_string: str) -> str:\n    translation_table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n    string_without_punc = input_string.translate(translation_table)\n    string_without_double_spaces = \" \".join(string_without_punc.split())\n    return string_without_double_spaces.lower()\n\n\nclass SearchEngine:\n    def __init__(self, k1: float = 1.5, b: float = 0.75):\n        self._index: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        self._documents: dict[str, str] = {}\n        self.k1 = k1\n        self.b = b\n\n    @property\n    def posts(self) -> list[str]:\n        return list(self._documents.keys())\n\n    @property\n    def number_of_documents(self) -> int:\n        return len(self._documents)\n\n    @property\n    def avdl(self) -> float:\n        if not hasattr(self, \"_avdl\"):\n            self._avdl = sum(len(d) for d in self._documents.values()) / len(self._documents)\n        return self._avdl\n\n    def idf(self, kw: str) -> float:\n        N = self.number_of_documents\n        n_kw = len(self.get_urls(kw))\n        return log((N - n_kw + 0.5) / (n_kw + 0.5) + 1)\n\n    def bm25(self, kw: str) -> dict[str, float]:\n        result = {}\n        idf_score = self.idf(kw)\n        avdl = self.avdl\n        for url, freq in self.get_urls(kw).items():\n            numerator = freq * (self.k1 + 1)\n            denominator = freq + self.k1 * (\n                1 - self.b + self.b * len(self._documents[url]) / avdl\n            )\n            result[url] = idf_score * numerator / denominator\n        return result\n\n    def search(self, query: str) -> dict[str, float]:\n        keywords = normalize_string(query).split(\" \")\n        url_scores: dict[str, float] = {}\n        for kw in keywords:\n            kw_urls_score = self.bm25(kw)\n            url_scores = update_url_scores(url_scores, kw_urls_score)\n        return url_scores\n\n    def index(self, url: str, content: str) -> None:\n        self._documents[url] = content\n        words = normalize_string(content).split(\" \")\n        for word in words:\n            self._index[word][url] += 1\n        if hasattr(self, \"_avdl\"):\n            del self._avdl\n\n", "contexts_below": "\n    def get_urls(self, keyword: str) -> dict[str, int]:\n        keyword = normalize_string(keyword)\n        return self._index[keyword]\n\n\nengine = SearchEngine()\n", "input_code": "    def bulk_index(self, documents: list[tuple[str, str]]):\n\n        \"\"\"\n        The bulk_index function takes a list of documents, where each document is represented as a tuple containing a URL and its corresponding content. It then indexes each document by calling the 'index' method with the URL and content of each document.\n\n        Input-Output Arguments\n        :param self: SearchEngine. An instance of the SearchEngine class.\n        :param documents: list[tuple[str, str]]. A list of tuples, where each tuple contains a URL (as a string) and its corresponding content (as a string). These documents are to be indexed.\n        :return: No return values.\n        \"\"\"", "reference_steps": "1. Define a method named `bulk_index` that takes `self` and a parameter named `documents`.\n2. The `documents` parameter is expected to be a list of tuples, where each tuple contains two strings representing a URL and its associated content.\n3. Iterate over each tuple in the `documents` list.\n4. Unpack the tuple into two variables, `url` and `content`.\n5. For each tuple, call the `index` method of the same class (`self`) with `url` and `content` as arguments.\n6. Repeat steps 4 and 5 for each tuple in the `documents` list until all documents have been processed.", "reference_code": "def bulk_index(self, documents: list[tuple[str, str]]):\n    for url, content in documents:\n        self.index(url, content)\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "type": "method", "class_name": "XinhuaHallucinations", "function_name": "statistics", "dependency_all": "# Intra-class Dependency:\nxinhua.XinhuaHallucinations.data\n\n", "dependency_sampled": "# Intra-class Dependency:\nxinhua.XinhuaHallucinations.data\n\n", "contexts_above": "# @Author : Shichao Song\n# @Email  : song.shichao@outlook.com\n\n\nimport json\nimport os\nimport random\n\n\nfrom uhgeval.dataset.base import BaseDataset\n\n\nclass XinhuaHallucinations(BaseDataset):\n    def __init__(self, path: str, shuffle: bool = False, seed: int = 22):\n        self.data = []\n        if os.path.isfile(path):\n            with open(path, encoding='utf-8') as f:\n                self.data = json.load(f)\n        if shuffle:\n            random.seed(seed)\n            random.shuffle(self.data)\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, key: int | slice) -> dict | list[dict]:\n            return self.data[key]\n\n    def load(self) -> list[dict]:\n        return self.data[:]\n\n", "contexts_below": "", "input_code": "    def statistics(self) -> dict:\n\n        \"\"\"\n        Calculates and returns the statistics of different types of objects contained in the XinhuaHallucinations instance's data. It counts the occurrences of each type ('doc', 'gen', 'kno', 'num') and updates the statistics accordingly.\n\n        Input-Output Arguments\n        :param self: XinhuaHallucinations. An instance of the XinhuaHallucinations class. It uses the 'data' attribute of the instance, which is expected to be a list of dictionaries, each with a 'type' key.\n        :return: dict. A dictionary with keys corresponding to the types ('doc', 'gen', 'kno', 'num') and values representing the count of each type found in the 'data' attribute of the instance.\n        \"\"\"", "reference_steps": "1. Define a method called `statistics` that returns a dictionary.\n2. Initialize a dictionary `stat` with keys `'doc'`, `'gen'`, `'kno'`, and `'num'`, all set to 0. These keys represent different types of objects.\n3. Iterate over the keys of the `stat` dictionary.\n4. For each key (representing a type), calculate the sum of objects in `self.data` that have a `'type'` attribute equal to the current key.\n5. Use a list comprehension to generate a list of boolean values, where each boolean indicates whether an object's `'type'` matches the current key.\n6. Sum the list of boolean values to get the count of objects of that type.\n7. Update the value associated with the current key in the `stat` dictionary with the count obtained in the previous step.\n8. Repeat steps 3 to 7 for each type key in the `stat` dictionary.\n9. After iterating through all the keys and calculating the counts, the `stat` dictionary will contain the total counts for each type.\n10. Return the `stat` dictionary containing the statistics.", "reference_code": "def statistics(self) -> dict:\n    stat = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n    for type_ in stat.keys():\n        stat[type_] = sum([obj['type']==type_ for obj in self.data])\n    return stat\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "type": "function", "class_name": null, "function_name": "build_segmentor", "dependency_all": "# Intra-file Dependency:\nmmdet3d.models.builder.SEGMENTORS\n\n", "dependency_sampled": "# Intra-file Dependency:\nmmdet3d.models.builder.SEGMENTORS\n\n", "contexts_above": "# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\n\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.utils import Registry\n\nfrom mmdet.models.builder import BACKBONES as MMDET_BACKBONES\nfrom mmdet.models.builder import DETECTORS as MMDET_DETECTORS\nfrom mmdet.models.builder import HEADS as MMDET_HEADS\nfrom mmdet.models.builder import LOSSES as MMDET_LOSSES\nfrom mmdet.models.builder import NECKS as MMDET_NECKS\nfrom mmdet.models.builder import ROI_EXTRACTORS as MMDET_ROI_EXTRACTORS\nfrom mmdet.models.builder import SHARED_HEADS as MMDET_SHARED_HEADS\nfrom mmseg.models.builder import LOSSES as MMSEG_LOSSES\n\nMODELS = Registry('models', parent=MMCV_MODELS)\n\nBACKBONES = MODELS\nNECKS = MODELS\nROI_EXTRACTORS = MODELS\nSHARED_HEADS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDETECTORS = MODELS\nVOXEL_ENCODERS = MODELS\nMIDDLE_ENCODERS = MODELS\nFUSION_LAYERS = MODELS\nSEGMENTORS = MODELS\n\n\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    if cfg['type'] in BACKBONES._module_dict.keys():\n        return BACKBONES.build(cfg)\n    else:\n        return MMDET_BACKBONES.build(cfg)\n\n\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n\ndef build_roi_extractor(cfg):\n    \"\"\"Build RoI feature extractor.\"\"\"\n    if cfg['type'] in ROI_EXTRACTORS._module_dict.keys():\n        return ROI_EXTRACTORS.build(cfg)\n    else:\n        return MMDET_ROI_EXTRACTORS.build(cfg)\n\n\ndef build_shared_head(cfg):\n    \"\"\"Build shared head of detector.\"\"\"\n    if cfg['type'] in SHARED_HEADS._module_dict.keys():\n        return SHARED_HEADS.build(cfg)\n    else:\n        return MMDET_SHARED_HEADS.build(cfg)\n\n\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n\ndef build_loss(cfg):\n    \"\"\"Build loss function.\"\"\"\n    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build detector.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n\n", "contexts_below": "\n\ndef build_model(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"A function warpper for building 3D detector or segmentor according to\n    cfg.\n\n    Should be deprecated in the future.\n    \"\"\"\n    if cfg.type in ['EncoderDecoder3D']:\n        return build_segmentor(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\n    else:\n        return build_detector(cfg, train_cfg=train_cfg, test_cfg=test_cfg)\n\n\ndef build_voxel_encoder(cfg):\n    \"\"\"Build voxel encoder.\"\"\"\n    return VOXEL_ENCODERS.build(cfg)\n\n\ndef build_middle_encoder(cfg):\n    \"\"\"Build middle level encoder.\"\"\"\n    return MIDDLE_ENCODERS.build(cfg)\n\n\ndef build_fusion_layer(cfg):\n    \"\"\"Build fusion layer.\"\"\"\n    return FUSION_LAYERS.build(cfg)\n", "input_code": "def build_segmentor(cfg, train_cfg=None, test_cfg=None):\n\n    \"\"\"\n    Builds a segmentor model based on the given configuration. It warns the user if the training or testing configurations are specified both in the function arguments and the model configuration, and ensures that these configurations are not duplicated.\n\n    Input-Output Arguments\n    :param cfg: Dict. The configuration dictionary for the segmentor model. It is used to configure the model.\n    :param train_cfg: Dict, optional. The training configuration. If specified, it should not be duplicated in the cfg dictionary.\n    :param test_cfg: Dict, optional. The testing configuration. If specified, it should not be duplicated in the cfg dictionary.\n    :return: A segmentor model instance. The segmentor model built based on the provided configurations.\n    \"\"\"", "reference_steps": "1. Define a function `build_segmentor` that accepts `cfg`, `train_cfg`, and `test_cfg` as arguments.\n2. Check if `train_cfg` or `test_cfg` is provided and issue a deprecation warning if either is present.\n3. Ensure that `train_cfg` is not specified in both the `cfg` dictionary and as a separate argument.\n4. Ensure that `test_cfg` is not specified in both the `cfg` dictionary and as a separate argument.\n5. If there is a conflict where both `train_cfg` and `test_cfg` are specified in the `cfg` and as arguments, assert an error.\n6. Build the segmentor using the `SEGMENTORS.build` method.\n7. Pass the `cfg` dictionary to the `SEGMENTORS.build` method.\n8. Include `train_cfg` and `test_cfg` as part of the `default_args` dictionary if they are not None.\n9. Return the constructed segmentor object.\n10. The function is designed to be a part of a larger system where the `SEGMENTORS` object is likely a factory for creating segmentor instances based on configurations.", "reference_code": "def build_segmentor(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build segmentor.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "type": "method", "class_name": "ChallengeManager", "function_name": "_type_check_with_pyright", "dependency_all": "# Intra-file Dependency:\nchallenge.TypeCheckResult\n\n", "dependency_sampled": "# Intra-file Dependency:\nchallenge.TypeCheckResult\n\n", "contexts_above": "import io\nimport random\nimport re\nimport subprocess\nimport tempfile\nimport tokenize\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom enum import StrEnum\nfrom pathlib import Path\nfrom typing import ClassVar, Optional, TypeAlias\n\nROOT_DIR = Path(__file__).parent.parent\n\n\nclass Level(StrEnum):\n    BASIC = \"basic\"\n    INTERMEDIATE = \"intermediate\"\n    ADVANCED = \"advanced\"\n    EXTREME = \"extreme\"\n\n    @classmethod\n    def is_valid_level(cls, level: str):\n        return level in cls._value2member_map_\n\n\nChallengeName: TypeAlias = str\n\n\n@dataclass(frozen=True)\nclass ChallengeKey:\n    level: Level\n    name: ChallengeName\n\n    @classmethod\n    def from_str(cls, key: str):\n        \"\"\"Create a key object from a string like \"basic-foo\".\"\"\"\n        level, name = key.split(\"-\", maxsplit=1)\n        return cls(Level(level), name)\n\n\n@dataclass\nclass Challenge:\n    \"\"\"A challenge object.\n\n    :param hints: An optional string of hints, use markdown syntax.\n    \"\"\"\n\n    CODE_SPLITTER: ClassVar[str] = \"\\n## End of your code ##\\n\"\n\n    name: ChallengeName\n    level: Level\n    code: str\n    user_code: str = field(default=\"\", init=False)\n    test_code: str = field(default=\"\", init=False)\n    hints: Optional[str] = None\n\n    def __post_init__(self):\n        self.parse_code()\n\n    def parse_code(self):\n        self.user_code, _, self.test_code = self.code.partition(self.CODE_SPLITTER)\n\n\n@dataclass(frozen=True, slots=True)\nclass TypeCheckResult:\n    message: str\n    passed: bool\n    debug_info: dict = field(default_factory=dict)  # For debugging purposes\n\n\nclass ChallengeManager:\n    \"\"\"The manager for challenges.\n\n    :param root_dir: The root directory that contains the files of challenges.\n    \"\"\"\n\n    def __init__(self, root_dir: Optional[Path] = None):\n        if not root_dir:\n            root_dir = ROOT_DIR / \"challenges\"\n        self.challenges: dict[ChallengeKey, Challenge] = self._load_challenges(root_dir)\n        self.challenges_groupby_level: dict[Level, list[ChallengeName]]\n        self.challenges_groupby_level = self._get_challenges_groupby_level()\n\n    def has_challenge(self, key: ChallengeKey) -> bool:\n        return key in self.challenges\n\n    def get_challenge(self, key: ChallengeKey) -> Challenge:\n        return self.challenges[key]\n\n    @property\n    def challenge_count(self) -> int:\n        \"\"\"The count of challenges.\"\"\"\n        return len(self.challenges)\n\n    def run_challenge(self, key: ChallengeKey, user_code: str) -> TypeCheckResult:\n        challenge = self.get_challenge(key)\n        # Make sure user code ends with a new line to avoid issue #63.\n        return self._type_check_with_pyright(user_code + \"\\n\", challenge.test_code)\n\n    def get_random_challenge(self) -> dict[str, str]:\n        level = random.choice(list(self.challenges_groupby_level.keys()))\n        name = random.choice(self.challenges_groupby_level[level])\n        return {\"level\": level, \"name\": name}\n\n    @staticmethod\n    def _load_challenges(root_dir: Path) -> dict[ChallengeKey, Challenge]:\n        challenges = {}\n        for challenge_folder in root_dir.iterdir():\n            question_source = challenge_folder / \"question.py\"\n            if not question_source.exists():\n                continue\n\n            # Try to read the optional hints file\n            hints_file = challenge_folder / \"hints.md\"\n            if hints_file.exists():\n                hints = hints_file.read_text(encoding=\"utf-8\").strip()\n            else:\n                hints = None\n\n            key = ChallengeKey.from_str(challenge_folder.name)\n            challenges[key] = Challenge(\n                name=key.name,\n                level=key.level,\n                code=question_source.read_text(encoding=\"utf-8\"),\n                hints=hints,\n            )\n        return challenges\n\n    def _get_challenges_groupby_level(self) -> dict[Level, list[ChallengeName]]:\n        groups: defaultdict[str, list[ChallengeName]] = defaultdict(list)\n\n        for challenge in self.challenges.values():\n            groups[challenge.level].append(challenge.name)\n\n        # Sort challenge by name alphabetically.\n        for challenge_names in groups.values():\n            challenge_names.sort()\n\n        # Make sure groups are ordered by level (from easy to hard)\n        return {level: groups[level] for level in Level}\n\n    EXPECT_ERROR_COMMENT = \"expect-type-error\"\n\n    # Pyright error messages look like:\n    # `<filename>:<line_no>:<col_no> - <error|warning|information>: <message>`\n    # Here we only capture the error messages and line numbers\n    PYRIGHT_MESSAGE_REGEX = r\"^(?:.+?):(\\d+):[\\s\\-\\d]+(error:.+)$\"\n\n    @classmethod\n", "contexts_below": "\n\nchallenge_manager = ChallengeManager()\n", "input_code": "    def _type_check_with_pyright(\n        cls, user_code: str, test_code: str\n    ) -> TypeCheckResult:\n\n        \"\"\"\n        This function performs a type check on a combination of user-provided code and test code using Pyright, identifies lines with expected type errors, and returns a result indicating whether the type check passed or failed along with relevant error messages.\n\n        Input-Output Arguments\n        :param cls: The class that this method belongs to, used to access class-specific attributes and methods.\n        :param user_code: str, The user-provided code that needs to be type-checked.\n        :param test_code: str, The test code that is combined with the user code for type checking.\n        :return: TypeCheckResult, An object containing the result of the type check, including a message detailing the outcome and a boolean indicating if the type check passed.\n\n        \"\"\"", "reference_steps": "1. Combine the user's code and test code into a single string.\n2. Use the `tokenize` module to generate a list of tokens from the combined code.\n3. Identify lines that have a comment with the specific text `# expect-type-error` and store their line numbers.\n4. Create a dictionary to track whether each expected error line has been reported by the type checker.\n5. Write the combined code to a temporary file and run `pyright` on it to perform type checking, capturing the output and errors.\n6. If there's an error in the `stderr`, return a `TypeCheckResult` indicating failure and the error message.\n7. Calculate the line number delta between the user code and the combined code to adjust error line numbers accordingly.\n8. Parse the `pyright` output to find type error messages and match them to the expected error lines, updating the tracking dictionary.\n9. Add any errors that were not expected or expected errors that were not reported to the list of error lines.\n10. Determine if the type checking passed based on the presence of error lines and return a `TypeCheckResult` with the appropriate message and pass status.", "reference_code": "def _type_check_with_pyright(\n    cls, user_code: str, test_code: str\n) -> TypeCheckResult:\n    code = f\"{user_code}{test_code}\"\n    buffer = io.StringIO(code)\n\n    # This produces a stream of TokenInfos, example:\n    # TokenInfo(type=4 (NEWLINE), string='\\n', start=(4, 3), end=(4, 4), line='\"\"\"\\n'),\n    # TokenInfo(type=62 (NL), string='\\n', start=(5, 0), end=(5, 1), line='\\n')\n    # See https://docs.python.org/3/library/tokenize.html#tokenize.tokenize for more details\n    tokens = list(tokenize.generate_tokens(buffer.readline))\n\n    # Find all lines that are followed by a comment # expect-type-error\n    expect_error_line_numbers = [\n        token.start[0]\n        for token in tokens\n        if token.type == tokenize.COMMENT\n        and token.string[1:].strip() == cls.EXPECT_ERROR_COMMENT\n    ]\n    # Tracks whether an expected error has been reported by type checker.\n    error_line_seen_in_err_msg: dict[int, bool] = {\n        lineno: False for lineno in expect_error_line_numbers\n    }\n\n    with tempfile.NamedTemporaryFile(suffix=\".py\") as temp:\n        temp.write(code.encode())\n        temp.flush()\n        # TODO: switch to json output to simplify output parsing.\n        # https://microsoft.github.io/pyright/#/command-line?id=json-output\n        raw_result = subprocess.run(\n            [\"pyright\", \"--pythonversion\", \"3.12\", temp.name],\n            capture_output=True,\n            text=True,\n        )\n        stdout, stderr = raw_result.stdout, raw_result.stderr\n        if stderr:\n            return TypeCheckResult(message=stderr, passed=False)\n    error_lines: list[str] = []\n\n    # Substract lineno in merged code by lineno_delta, so that the lineno in\n    # error message matches those in the test code editor. Fixed #20.\n    lineno_delta = len(user_code.splitlines())\n    for line in stdout.splitlines():\n        m = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n        if m is None:\n            continue\n        line_number, message = int(m.group(1)), m.group(2)\n        if line_number in error_line_seen_in_err_msg:\n            # Each reported error should be attached to a specific line,\n            # If it is commented with # expect-type-error, let it pass.\n            error_line_seen_in_err_msg[line_number] = True\n            continue\n        # Error could be thrown from user code too, in which case delta shouldn't be applied.\n        error_lines.append(\n            f\"{line_number if line_number <= lineno_delta else line_number - lineno_delta}:{message}\"\n        )\n\n    # If there are any lines that are expected to fail but not reported by pyright,\n    # they should be considered as errors.\n    for line_number, seen in error_line_seen_in_err_msg.items():\n        if not seen:\n            error_lines.append(\n                f\"{line_number - lineno_delta}: error: Expected type error but instead passed\"\n            )\n\n    passed = len(error_lines) == 0\n    if passed:\n        error_lines.append(\"\\nAll tests passed\")\n    else:\n        error_lines.append(f\"\\nFound {len(error_lines)} errors\")\n\n    return TypeCheckResult(message=\"\\n\".join(error_lines), passed=passed)\n"}
{"namespace": "autorag.deploy.extract_best_config", "type": "function", "class_name": null, "function_name": "extract_best_config", "dependency_all": "# Intra-file Dependency:\nautorag.deploy.summary_df_to_yaml\n    def summary_df_to_yaml(summary_df: pd.DataFrame, config_dict: Dict) -> Dict:\n        \"\"\"\n        Convert trial summary dataframe to config yaml file.\n\n        :param summary_df: The trial summary dataframe of the evaluated trial.\n        :param config_dict: The yaml configuration dict for the pipeline.\n            You can load this to access trail_folder/config.yaml.\n        :return: Dictionary of config yaml file.\n            You can save this dictionary to yaml file.\n        \"\"\"\n\n# Cross-file Dependency:\nautorag.utils.util.load_summary_file\n    def load_summary_file(summary_path: str,\n                          dict_columns: Optional[List[str]] = None) -> pd.DataFrame:\n        \"\"\"\n        Load summary file from summary_path.\n\n        :param summary_path: The path of the summary file.\n        :param dict_columns: The columns that are dictionary type.\n            You must fill this parameter if you want to load summary file properly.\n            Default is ['module_params'].\n        :return: The summary dataframe.\n        \"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\nautorag.deploy.summary_df_to_yaml\n    def summary_df_to_yaml(summary_df: pd.DataFrame, config_dict: Dict) -> Dict:\n        \"\"\"\n        Convert trial summary dataframe to config yaml file.\n\n        :param summary_df: The trial summary dataframe of the evaluated trial.\n        :param config_dict: The yaml configuration dict for the pipeline.\n            You can load this to access trail_folder/config.yaml.\n        :return: Dictionary of config yaml file.\n            You can save this dictionary to yaml file.\n        \"\"\"\n\n", "contexts_above": "import logging\nimport os\nimport uuid\nfrom copy import deepcopy\nfrom typing import Optional, Dict, List\n\nimport pandas as pd\nimport uvicorn\nimport yaml\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nfrom autorag.support import get_support_modules\nfrom autorag.utils.util import load_summary_file\n\nlogger = logging.getLogger(\"AutoRAG\")\n\n\ndef extract_node_line_names(config_dict: Dict) -> List[str]:\n    \"\"\"\n    Extract node line names with the given config dictionary order.\n\n    :param config_dict: The yaml configuration dict for the pipeline.\n        You can load this to access trail_folder/config.yaml.\n    :return: The list of node line names.\n        It is the order of the node line names in the pipeline.\n    \"\"\"\n    return [node_line['node_line_name'] for node_line in config_dict['node_lines']]\n\n\ndef extract_node_strategy(config_dict: Dict) -> Dict:\n    \"\"\"\n    Extract node strategies with the given config dictionary.\n    The return value is a dictionary of node type and its strategy.\n\n    :param config_dict: The yaml configuration dict for the pipeline.\n        You can load this to access trail_folder/config.yaml.\n    :return: Key is node_type and value is strategy dict.\n    \"\"\"\n    return {node['node_type']: node.get('strategy', {})\n            for node_line in config_dict['node_lines']\n            for node in node_line['nodes']}\n\n\ndef summary_df_to_yaml(summary_df: pd.DataFrame, config_dict: Dict) -> Dict:\n    \"\"\"\n    Convert trial summary dataframe to config yaml file.\n\n    :param summary_df: The trial summary dataframe of the evaluated trial.\n    :param config_dict: The yaml configuration dict for the pipeline.\n        You can load this to access trail_folder/config.yaml.\n    :return: Dictionary of config yaml file.\n        You can save this dictionary to yaml file.\n    \"\"\"\n\n    # summary_df columns : 'node_line_name', 'node_type', 'best_module_filename',\n    #                      'best_module_name', 'best_module_params', 'best_execution_time'\n    node_line_names = extract_node_line_names(config_dict)\n    node_strategies = extract_node_strategy(config_dict)\n    strategy_df = pd.DataFrame({\n        'node_type': list(node_strategies.keys()),\n        'strategy': list(node_strategies.values())\n    })\n    summary_df = summary_df.merge(strategy_df, on='node_type', how='left')\n    summary_df['categorical_node_line_name'] = pd.Categorical(summary_df['node_line_name'], categories=node_line_names,\n                                                              ordered=True)\n    summary_df = summary_df.sort_values(by='categorical_node_line_name')\n    grouped = summary_df.groupby('categorical_node_line_name')\n\n    node_lines = [\n        {\n            'node_line_name': node_line_name,\n            'nodes': [\n                {\n                    'node_type': row['node_type'],\n                    'strategy': row['strategy'],\n                    'modules': [{\n                        'module_type': row['best_module_name'],\n                        **row['best_module_params']\n                    }]\n                }\n                for _, row in node_line.iterrows()\n            ]\n        }\n        for node_line_name, node_line in grouped\n    ]\n    return {'node_lines': node_lines}\n\n\n", "contexts_below": "\n\nclass Runner:\n    def __init__(self, config: Dict, project_dir: Optional[str] = None):\n        self.config = config\n        self.project_dir = os.getcwd() if project_dir is None else project_dir\n        self.app = FastAPI()\n        self.__add_api_route()\n\n    @classmethod\n    def from_yaml(cls, yaml_path: str, project_dir: Optional[str] = None):\n        \"\"\"\n        Load Runner from yaml file.\n        Must be extracted yaml file from evaluated trial using extract_best_config method.\n\n        :param yaml_path: The path of the yaml file.\n        :param project_dir: The path of the project directory.\n            Default is the current directory.\n        :return: Initialized Runner.\n        \"\"\"\n        with open(yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=project_dir)\n\n    @classmethod\n    def from_trial_folder(cls, trial_path: str):\n        \"\"\"\n        Load Runner from evaluated trial folder.\n        Must already be evaluated using Evaluator class.\n        It sets the project_dir as the parent directory of the trial folder.\n\n        :param trial_path: The path of the trial folder.\n        :return: Initialized Runner.\n        \"\"\"\n        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    def run(self, query: str, result_column: str = \"generated_texts\"):\n        \"\"\"\n        Run the pipeline with query.\n        The loaded pipeline must start with a single query,\n        so the first module of the pipeline must be `query_expansion` or `retrieval` module.\n\n        :param query: The query of the user.\n        :param result_column: The result column name for the answer.\n            Default is `generated_texts`, which is the output of the `generation` module.\n        :return: The result of the pipeline.\n        \"\"\"\n        node_lines = deepcopy(self.config['node_lines'])\n        previous_result = pd.DataFrame({\n            'qid': str(uuid.uuid4()),\n            'query': [query],\n            'retrieval_gt': [[]],\n            'generation_gt': [''],\n        })  # pseudo qa data for execution\n        for node_line in node_lines:\n            for node in node_line['nodes']:\n                if len(node['modules']) != 1:\n                    raise ValueError(\"The number of modules in a node must be 1 for using runner.\"\n                                     \"Please use extract_best_config method for extracting yaml file from evaluated trial.\")\n                module = node['modules'][0]\n                module_type = module.pop('module_type')\n                module_params = module\n                new_result = get_support_modules(module_type)(\n                    project_dir=self.project_dir,\n                    previous_result=previous_result,\n                    **module_params\n                )\n                duplicated_columns = previous_result.columns.intersection(new_result.columns)\n                drop_previous_result = previous_result.drop(columns=duplicated_columns)\n                previous_result = pd.concat([drop_previous_result, new_result], axis=1)\n\n        return previous_result[result_column].tolist()[0]\n\n    def __add_api_route(self):\n\n        @self.app.post(\"/run\")\n        async def run_pipeline(runner_input: RunnerInput):\n            query = runner_input.query\n            result_column = runner_input.result_column\n            result = self.run(query, result_column)\n            return {result_column: result}\n\n    def run_api_server(self, host: str = '0.0.0.0', port: int = 8000, **kwargs):\n        \"\"\"\n        Run the pipeline as api server.\n        You can send POST request to `http://host:port/run` with json body like below:\n\n        .. Code:: json\n\n            {\n                \"Query\": \"your query\",\n                \"result_column\": \"answer\"\n            }\n\n        And it returns json response like below:\n\n        .. Code:: json\n            {\n                \"answer\": \"your answer\"\n            }\n\n        :param host: The host of the api server.\n        :param port: The port of the api server.\n        :param kwargs: Other arguments for uvicorn.run.\n        \"\"\"\n        logger.info(f\"Run api server at {host}:{port}\")\n        uvicorn.run(self.app, host=host, port=port, **kwargs)\n\n\nclass RunnerInput(BaseModel):\n    query: str\n    result_column: str = \"answer\"\n", "input_code": "def extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict:\n\n    \"\"\"\n    Extracts the optimal pipeline configuration from a given trial directory and optionally saves it to a YAML file. It reads the summary and configuration files from the trial directory to construct a dictionary of the best pipeline configuration.\n\n    Input-Output Arguments\n    :param trial_path: str, The path to the trial directory from which the optimal pipeline configuration is to be extracted. The directory must contain a summary.csv file with the evaluation results.\n    :param output_path: Optional[str], The file path where the extracted pipeline configuration should be saved in YAML format. If not specified or None, the function will not save the configuration to a file but will still return the configuration dictionary. The file extension must be .yaml or .yml if provided.\n    :return: Dict, The dictionary containing the extracted optimal pipeline configuration.\n    \"\"\"", "reference_steps": "1. Define a function `extract_best_config` that takes a `trial_path` and an optional `output_path` as arguments and returns a dictionary representing the optimal pipeline configuration.\n2. Construct the path to the `summary.csv` file by joining it with the `trial_path`.\n3. Check if the `summary.csv` file exists at the specified path; if not, raise a `ValueError`.\n4. Load the summary data from `summary.csv` into a DataFrame, ensuring that columns containing dictionaries (such as 'best_module_params') are correctly interpreted.\n5. Construct the path to the `config.yaml` file by joining it with the `trial_path`.\n6. Open the `config.yaml` file in read mode and load its contents into a dictionary using `yaml.safe_load`.\n7. Transform the summary DataFrame into a dictionary suitable for YAML output, using the loaded configuration dictionary as a reference.\n8. If an `output_path` is provided, open the specified file in write mode and dump the YAML dictionary into the file using `yaml.dump`.\n9. Return the YAML dictionary representing the optimal pipeline configuration.\n10. Ensure that the function handles any potential errors, such as missing files or incorrect formats, by providing informative error messages.", "reference_code": "def extract_best_config(trial_path: str, output_path: Optional[str] = None) -> Dict:\n    \"\"\"\n    Extract the optimal pipeline from evaluated trial.\n\n    :param trial_path: The path to the trial directory that you want to extract the pipeline from.\n        Must already be evaluated.\n    :param output_path: Output path that pipeline yaml file will be saved.\n        Must be .yaml or .yml file.\n        If None, it does not save yaml file and just return dict values.\n        Default is None.\n    :return: The dictionary of the extracted pipeline.\n    \"\"\"\n    summary_path = os.path.join(trial_path, 'summary.csv')\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n    trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n    config_yaml_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_yaml_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(yaml_dict, f)\n    return yaml_dict\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "type": "function", "class_name": null, "function_name": "run_query_expansion_node", "dependency_all": "# Intra-file Dependency:\nautorag.nodes.queryexpansion.run.evaluate_one_query_expansion_node\n    def evaluate_one_query_expansion_node(retrieval_funcs: List[Callable],\n                                          retrieval_params: List[Dict],\n                                          expanded_queries: List[List[str]],\n                                          retrieval_gt: List[List[str]],\n                                          metrics: List[str],\n                                          project_dir,\n                                          previous_result: pd.DataFrame) -> pd.DataFrame:\n\nautorag.nodes.queryexpansion.run.make_retrieval_callable_params\n    def make_retrieval_callable_params(strategy_dict: Dict):\n        \"\"\"\n            strategy_dict looks like this:\n\n            .. Code:: json\n\n                {\n                    \"metrics\": [\"retrieval_f1\", \"retrieval_recall\"],\n                    \"top_k\": 50,\n                    \"retrieval_modules\": [\n                      {\"module_type\": \"bm25\"},\n                      {\"module_type\": \"vectordb\", \"embedding_model\": [\"openai\", \"huggingface\"]}\n                    ]\n                  }\n\n        \"\"\"\n\n# Cross-file Dependency:\nautorag.strategy.filter_by_threshold\n    def filter_by_threshold(results, value, threshold, metadatas=None) -> Tuple[List, List]:\n        \"\"\"\n        Filter results by value's threshold.\n\n        :param results: The result list to be filtered.\n        :param value: The value list to be filtered.\n            It must have the same length with results.\n        :param threshold: The threshold value.\n        :param metadatas: The metadata of each result.\n        :return: Filtered list of results and filtered list of metadatas.\n            Metadatas will be returned even if you did not give input metadatas.\n        :rtype: Tuple[List, List]\n        \"\"\"\n\nautorag.strategy.measure_speed\n    def measure_speed(func, *args, **kwargs):\n        \"\"\"\n        Method for measuring execution speed of the function.\n        \"\"\"\n\nautorag.strategy.select_best_average\n    def select_best_average(results: List[pd.DataFrame], columns: Iterable[str],\n                            metadatas: Optional[List[Any]] = None) -> Tuple[pd.DataFrame, Any]:\n        \"\"\"\n        Select the best result by average value among given columns.\n\n        :param results: The list of results.\n            Each result must be pd.DataFrame.\n        :param columns: Column names to be averaged.\n            Standard to select the best result.\n        :param metadatas: The metadata of each result. \n            It will select one metadata with the best result.\n        :return: The best result and the best metadata.\n            The metadata will be returned even if you did not give input 'metadatas' parameter.\n        :rtype: Tuple[pd.DataFrame, Any]\n        \"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\nautorag.nodes.queryexpansion.run.evaluate_one_query_expansion_node\n    def evaluate_one_query_expansion_node(retrieval_funcs: List[Callable],\n                                          retrieval_params: List[Dict],\n                                          expanded_queries: List[List[str]],\n                                          retrieval_gt: List[List[str]],\n                                          metrics: List[str],\n                                          project_dir,\n                                          previous_result: pd.DataFrame) -> pd.DataFrame:\n\n# Cross-file Dependency:\nautorag.strategy.select_best_average\n    def select_best_average(results: List[pd.DataFrame], columns: Iterable[str],\n                            metadatas: Optional[List[Any]] = None) -> Tuple[pd.DataFrame, Any]:\n        \"\"\"\n        Select the best result by average value among given columns.\n\n        :param results: The list of results.\n            Each result must be pd.DataFrame.\n        :param columns: Column names to be averaged.\n            Standard to select the best result.\n        :param metadatas: The metadata of each result. \n            It will select one metadata with the best result.\n        :return: The best result and the best metadata.\n            The metadata will be returned even if you did not give input 'metadatas' parameter.\n        :rtype: Tuple[pd.DataFrame, Any]\n        \"\"\"\n\n", "contexts_above": "import logging\nimport os\nimport pathlib\nfrom typing import List, Callable, Dict, Optional\nfrom copy import deepcopy\n\nimport pandas as pd\n\nfrom autorag.nodes.retrieval.run import evaluate_retrieval_node\nfrom autorag.strategy import measure_speed, filter_by_threshold, select_best_average\nfrom autorag.utils.util import make_combinations, explode\nfrom autorag.support import get_support_modules\n\nlogger = logging.getLogger(\"AutoRAG\")\n\n\n", "contexts_below": "\n\ndef evaluate_one_query_expansion_node(retrieval_funcs: List[Callable],\n                                      retrieval_params: List[Dict],\n                                      expanded_queries: List[List[str]],\n                                      retrieval_gt: List[List[str]],\n                                      metrics: List[str],\n                                      project_dir,\n                                      previous_result: pd.DataFrame) -> pd.DataFrame:\n    previous_result['queries'] = expanded_queries\n    retrieval_results = list(map(lambda x: x[0](project_dir=project_dir, previous_result=previous_result, **x[1]),\n                                 zip(retrieval_funcs, retrieval_params)))\n    evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, retrieval_gt, metrics),\n                                  retrieval_results))\n    best_result, _ = select_best_average(evaluation_results, metrics)\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    return best_result\n\n\ndef make_retrieval_callable_params(strategy_dict: Dict):\n    \"\"\"\n        strategy_dict looks like this:\n\n        .. Code:: json\n\n            {\n                \"metrics\": [\"retrieval_f1\", \"retrieval_recall\"],\n                \"top_k\": 50,\n                \"retrieval_modules\": [\n                  {\"module_type\": \"bm25\"},\n                  {\"module_type\": \"vectordb\", \"embedding_model\": [\"openai\", \"huggingface\"]}\n                ]\n              }\n\n    \"\"\"\n    node_dict = deepcopy(strategy_dict)\n    retrieval_module_list: Optional[List[Dict]] = node_dict.pop('retrieval_modules', None)\n    if retrieval_module_list is None:\n        retrieval_module_list = [{\n            'module_type': 'bm25',\n        }]\n    node_params = node_dict\n    modules = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                       retrieval_module_list))\n    param_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **node_params}),\n                                  retrieval_module_list))\n    return explode(modules, param_combinations)\n", "input_code": "def run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame:\n\n    \"\"\"\n    This function evaluates and selects the best module among query expansion node results by running each module with given parameters, measuring their execution times, and evaluating their performance based on specified strategies. It saves the results and a summary, including execution times and evaluation metrics, to the specified directory. Finally, it selects and saves the best result based on the evaluation.\n\n    Input-Output Arguments\n    :param modules: List[Callable]. A list of query expansion modules to be executed.\n    :param module_params: List[Dict]. Parameters for each query expansion module in the modules list.\n    :param previous_result: pd.DataFrame. The dataframe containing the results from the previous step, used as input for query expansion.\n    :param node_line_dir: str. The directory path where the results and summaries will be saved.\n    :param strategies: Dict. A dictionary containing strategies for selecting the best query expansion module, including metrics, speed thresholds, and other evaluation criteria.\n    :return: pd.DataFrame. The dataframe containing the best result after evaluating all query expansion modules according to the specified strategies.\n    \"\"\"", "reference_steps": "1. Check and create necessary directories for storing query expansion node results and summaries.\n2. Run all provided query expansion modules with their respective parameters and measure their execution times.\n3. Save the results of each query expansion module to a Parquet file in the designated node directory.\n4. Create a summary DataFrame containing filenames, module names, parameters, and average execution times.\n5. If multiple modules are provided, filter the results based on a speed threshold if specified in the strategies.\n6. Validate the presence of required evaluation metrics in the strategies and set a default value for `top_k` if not provided.\n7. Prepare retrieval modules and parameters based on the extra strategies provided.\n8. Run evaluations for each query expansion result using the specified retrieval modules and calculate the mean of the metrics.\n9. Merge the evaluation results with the summary DataFrame and select the best result based on the average of the evaluation metrics.\n10. Add an 'is_best' column to the summary DataFrame, save the summary and the best result to files, and return the best result DataFrame.", "reference_code": "def run_query_expansion_node(modules: List[Callable],\n                             module_params: List[Dict],\n                             previous_result: pd.DataFrame,\n                             node_line_dir: str,\n                             strategies: Dict,\n                             ) -> pd.DataFrame:\n    \"\"\"\n    Run evaluation and select the best module among query expansion node results.\n    Initially, retrieval is run using expanded_queries, the result of the query_expansion module.\n    The retrieval module is run as a combination of the retrieval_modules in strategies.\n    If there are multiple retrieval_modules, run them all and choose the best result.\n    If there are no retrieval_modules, run them with the default of bm25.\n    In this way, the best result is selected for each module, and then the best result is selected.\n\n    :param modules: Query expansion modules to run.\n    :param module_params: Query expansion module parameters.\n    :param previous_result: Previous result dataframe.\n        In this case, it would be qa data.\n    :param node_line_dir: This node line's directory.\n    :param strategies: Strategies for query expansion node.\n    :return: The best result dataframe.\n    \"\"\"\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n\n    # run query expansion\n    results, execution_times = zip(*map(lambda task: measure_speed(\n        task[0], project_dir=project_dir, previous_result=previous_result, **task[1]), zip(modules, module_params)))\n    average_times = list(map(lambda x: x / len(results[0]), execution_times))\n\n    # save results to folder\n    pseudo_module_params = deepcopy(module_params)\n    for i, module_param in enumerate(pseudo_module_params):\n        if 'prompt' in module_params:\n            module_param['prompt'] = str(i)\n    filepaths = list(map(lambda x: os.path.join(node_dir, f'{x}.parquet'), range(len(modules))))\n    list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(results, filepaths)))  # execute save to parquet\n    filenames = list(map(lambda x: os.path.basename(x), filepaths))\n\n    # make summary file\n    summary_df = pd.DataFrame({\n        'filename': filenames,\n        'module_name': list(map(lambda module: module.__name__, modules)),\n        'module_params': module_params,\n        'execution_time': average_times,\n    })\n\n    # Run evaluation when there are more than one module.\n    if len(modules) > 1:\n        # pop general keys from strategies (e.g. metrics, speed_threshold)\n        general_key = ['metrics', 'speed_threshold']\n        general_strategy = dict(filter(lambda x: x[0] in general_key, strategies.items()))\n        extra_strategy = dict(filter(lambda x: x[0] not in general_key, strategies.items()))\n\n        # first, filter by threshold if it is enabled.\n        if general_strategy.get('speed_threshold') is not None:\n            results, filenames = filter_by_threshold(results, average_times, general_strategy['speed_threshold'],\n                                                     filenames)\n\n        # check metrics in strategy\n        if general_strategy.get('metrics') is None:\n            raise ValueError(\"You must at least one metrics for query expansion evaluation.\")\n\n        if extra_strategy.get('top_k') is None:\n            extra_strategy['top_k'] = 10  # default value\n\n        # get retrieval modules from strategy\n        retrieval_callables, retrieval_params = make_retrieval_callable_params(extra_strategy)\n\n        # get retrieval_gt\n        retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))['retrieval_gt'].tolist()\n\n        # run evaluation\n        evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(\n            retrieval_callables, retrieval_params, result['queries'].tolist(), retrieval_gt,\n            general_strategy['metrics'], project_dir, previous_result), results))\n\n        evaluation_df = pd.DataFrame({\n            'filename': filenames,\n            **{f'query_expansion_{metric_name}': list(map(lambda x: x[metric_name].mean(), evaluation_results))\n               for metric_name in general_strategy['metrics']}\n        })\n        summary_df = pd.merge(on='filename', left=summary_df, right=evaluation_df, how='left')\n\n        best_result, best_filename = select_best_average(evaluation_results, general_strategy['metrics'], filenames)\n        # change metric name columns to query_expansion_metric_name\n        best_result = best_result.rename(columns={\n            metric_name: f'query_expansion_{metric_name}' for metric_name in strategies['metrics']})\n        best_result = best_result.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    else:\n        best_result, best_filename = results[0], filenames[0]\n        best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # add 'is_best' column at summary file\n    summary_df['is_best'] = summary_df['filename'] == best_filename\n\n    # save files\n    summary_df.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n    best_result.to_parquet(os.path.join(node_dir, f\"best_{os.path.splitext(best_filename)[0]}.parquet\"), index=False)\n\n    return best_result\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "type": "function", "class_name": null, "function_name": "extract_values_from_nodes", "dependency_all": "# Intra-file Dependency:\nautorag.schema.node.Node\n    class Node:\n\nautorag.schema.node.extract_values\n    def extract_values(node: Node, key: str) -> List[str]:\n        \"\"\"\n        This function extract values from node's modules' module_param.\n\n        :param node: The node you want to extract values from.\n        :param key: The key of module_param that you want to extract.\n        :return: The list of extracted values.\n            It removes duplicated elements automatically.\n        \"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\nautorag.schema.node.Node\n    class Node:\n\n", "contexts_above": "import itertools\nimport logging\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Callable, Tuple\n\nimport pandas as pd\n\nfrom autorag.schema.module import Module\nfrom autorag.support import get_support_nodes\nfrom autorag.utils.util import make_combinations, explode\n\nlogger = logging.getLogger(\"AutoRAG\")\n\n\n@dataclass\nclass Node:\n    node_type: str\n    strategy: Dict\n    node_params: Dict\n    modules: List[Module]\n    run_node: Callable = field(init=False)\n\n    def __post_init__(self):\n        self.run_node = get_support_nodes(self.node_type)\n        if self.run_node is None:\n            raise ValueError(f\"Node type {self.node_type} is not supported.\")\n\n    def get_param_combinations(self) -> Tuple[List[Callable], List[Dict]]:\n        \"\"\"\n        This method returns a combination of module and node parameters, also corresponding modules.\n\n        :return: Each module and its module parameters.\n        :rtype: Tuple[List[Callable], List[Dict]]\n        \"\"\"\n\n        def make_single_combination(module: Module) -> List[Dict]:\n            input_dict = {**self.node_params, **module.module_param}\n            return make_combinations(input_dict)\n\n        combinations = list(map(make_single_combination, self.modules))\n        module_list, combination_list = explode(self.modules, combinations)\n        return list(map(lambda x: x.module, module_list)), combination_list\n\n    @classmethod\n    def from_dict(cls, node_dict: Dict) -> 'Node':\n        _node_dict = deepcopy(node_dict)\n        node_type = _node_dict.pop('node_type')\n        strategy = _node_dict.pop('strategy')\n        modules = list(map(lambda x: Module.from_dict(x), _node_dict.pop('modules')))\n        node_params = _node_dict\n        return cls(node_type, strategy, node_params, modules)\n\n    def run(self, previous_result: pd.DataFrame, node_line_dir: str) -> pd.DataFrame:\n        logger.info(f'Running node {self.node_type}...')\n        input_modules, input_params = self.get_param_combinations()\n        return self.run_node(modules=input_modules,\n                             module_params=input_params,\n                             previous_result=previous_result,\n                             node_line_dir=node_line_dir,\n                             strategies=self.strategy)\n\n\ndef extract_values(node: Node, key: str) -> List[str]:\n    \"\"\"\n    This function extract values from node's modules' module_param.\n\n    :param node: The node you want to extract values from.\n    :param key: The key of module_param that you want to extract.\n    :return: The list of extracted values.\n        It removes duplicated elements automatically.\n    \"\"\"\n\n    def extract_module_values(module: Module):\n        if key not in module.module_param:\n            return []\n        value = module.module_param[key]\n        if isinstance(value, str):\n            return [value]\n        elif isinstance(value, list):\n            return value\n        else:\n            raise ValueError(f\"{key} must be str or list, but got {type(value)}\")\n\n    values = list(map(extract_module_values, node.modules))\n    return list(set(list(itertools.chain.from_iterable(values))))\n\n\n", "contexts_below": "\n\ndef module_type_exists(nodes: List[Node], module_type: str) -> bool:\n    \"\"\"\n    This function check if the module type exists in the nodes.\n\n    :param nodes: The nodes you want to check.\n    :param module_type: The module type you want to check.\n    :return: True if the module type exists in the nodes.\n    \"\"\"\n    return any(list(map(lambda node: any(list(map(lambda module: module.module_type == module_type, node.modules))),\n                        nodes)))\n", "input_code": "def extract_values_from_nodes(nodes: List[Node], key: str) -> List[str]:\n\n    \"\"\"\n    Extracts values associated with a specified key from a list of nodes, ensuring each value is unique by removing duplicates.\n\n    Input-Output Arguments\n    :param nodes: List[Node]. The nodes from which you want to extract values. These nodes are expected to have a structure that includes modules and module_params from which values are extracted.\n    :param key: String. The key corresponding to the value you want to extract from each node's module_param.\n    :return: List[String]. A list of unique values extracted from the nodes based on the specified key.\n    \"\"\"", "reference_steps": "1. Define a function `extract_values_from_nodes` that takes a list of `nodes` and a `key` as parameters.\n2. The function aims to extract values associated with the specified `key` from the `module_param` of modules within the nodes.\n3. Use `map` to apply a function `extract_values` to each node in the `nodes` list.\n4. The `extract_values` function (not shown in the reference code) is assumed to extract the values from a single node's `module_param` based on the `key`.\n5. Collect the results from the `map` operation into a list, resulting in a list of lists where each sublist contains the extracted values from each node.\n6. Use `itertools.chain.from_iterable` to flatten the list of lists into a single list containing all the extracted values.\n7. Convert the flattened list into a set to remove any duplicated elements.\n8. Convert the set back into a list to get the final list of unique extracted values.\n9. Return the list of unique extracted values.\n10. The function is documented with a docstring explaining its purpose, parameters, and return value.", "reference_code": "def extract_values_from_nodes(nodes: List[Node], key: str) -> List[str]:\n    \"\"\"\n    This function extract values from nodes' modules' module_param.\n\n    :param nodes: The nodes you want to extract values from.\n    :param key: The key of module_param that you want to extract.\n    :return: The list of extracted values.\n        It removes duplicated elements automatically.\n    \"\"\"\n    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "type": "function", "class_name": null, "function_name": "gfpgan_fix_faces", "dependency_all": "# Intra-file Dependency:\ngfpgan_model.gfpgan_face_restorer\n\ngfpgan_model.logger\n\n", "dependency_sampled": "# Intra-file Dependency:\ngfpgan_model.gfpgan_face_restorer\n\n", "contexts_above": "from __future__ import annotations\n\nimport logging\nimport os\n\nimport torch\n\nfrom modules import (\n    devices,\n    errors,\n    face_restoration,\n    face_restoration_utils,\n    modelloader,\n    shared,\n)\n\nlogger = logging.getLogger(__name__)\nmodel_url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\nmodel_download_name = \"GFPGANv1.4.pth\"\ngfpgan_face_restorer: face_restoration.FaceRestoration | None = None\n\n\nclass FaceRestorerGFPGAN(face_restoration_utils.CommonFaceRestoration):\n    def name(self):\n        return \"GFPGAN\"\n\n    def get_device(self):\n        return devices.device_gfpgan\n\n    def load_net(self) -> torch.Module:\n        for model_path in modelloader.load_models(\n            model_path=self.model_path,\n            model_url=model_url,\n            command_path=self.model_path,\n            download_name=model_download_name,\n            ext_filter=['.pth'],\n        ):\n            if 'GFPGAN' in os.path.basename(model_path):\n                model = modelloader.load_spandrel_model(\n                    model_path,\n                    device=self.get_device(),\n                    expected_architecture='GFPGAN',\n                ).model\n                model.different_w = True  # see https://github.com/chaiNNer-org/spandrel/pull/81\n                return model\n        raise ValueError(\"No GFPGAN model found\")\n\n    def restore(self, np_image):\n        def restore_face(cropped_face_t):\n            assert self.net is not None\n            return self.net(cropped_face_t, return_rgb=False)[0]\n\n        return self.restore_with_helper(np_image, restore_face)\n\n\n", "contexts_below": "\n\ndef setup_model(dirname: str) -> None:\n    global gfpgan_face_restorer\n\n    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        shared.face_restorers.append(gfpgan_face_restorer)\n    except Exception:\n        errors.report(\"Error setting up GFPGAN\", exc_info=True)\n", "input_code": "def gfpgan_fix_faces(np_image):\n\n    \"\"\"\n    This function attempts to restore faces in an image using the GFPGAN face restorer. If the GFPGAN face restorer is not set up, it logs a warning and returns the original image.\n\n    Input-Output Arguments\n    :param np_image: The image as a NumPy array on which face restoration is attempted.\n    :return: The image as a NumPy array. It is either the original image or the restored image if the GFPGAN face restorer is set up and used successfully.\n    \"\"\"", "reference_steps": "1. Define a function called `gfpgan_fix_faces` that takes an image in NumPy array format as its argument.\n2. Check if the global variable `gfpgan_face_restorer` is set (i.e., not `None` or `False`).\n3. If `gfpgan_face_restorer` is set, call the `restore` method of the `gfpgan_face_restorer` object with the input image as the argument.\n4. Return the result of the `restore` method, which should be the enhanced image with faces fixed.\n5. If `gfpgan_face_restorer` is not set, log a warning message indicating that the GFPGAN face restorer is not configured.\n6. Return the original input image unchanged if the face restorer is not set up.", "reference_code": "def gfpgan_fix_faces(np_image):\n    if gfpgan_face_restorer:\n        return gfpgan_face_restorer.restore(np_image)\n    logger.warning(\"GFPGAN face restorer not set up\")\n    return np_image\n"}
{"namespace": "codeformer_model.setup_model", "type": "function", "class_name": null, "function_name": "setup_model", "dependency_all": "# Intra-file Dependency:\ncodeformer_model.FaceRestorerCodeFormer\n\n", "dependency_sampled": "# Intra-file Dependency:\ncodeformer_model.FaceRestorerCodeFormer\n\n", "contexts_above": "from __future__ import annotations\n\nimport logging\n\nimport torch\n\nfrom modules import (\n    devices,\n    errors,\n    face_restoration,\n    face_restoration_utils,\n    modelloader,\n    shared,\n)\n\nlogger = logging.getLogger(__name__)\n\nmodel_url = 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth'\nmodel_download_name = 'codeformer-v0.1.0.pth'\n\n# used by e.g. postprocessing_codeformer.py\ncodeformer: face_restoration.FaceRestoration | None = None\n\n\nclass FaceRestorerCodeFormer(face_restoration_utils.CommonFaceRestoration):\n    def name(self):\n        return \"CodeFormer\"\n\n    def load_net(self) -> torch.Module:\n        for model_path in modelloader.load_models(\n            model_path=self.model_path,\n            model_url=model_url,\n            command_path=self.model_path,\n            download_name=model_download_name,\n            ext_filter=['.pth'],\n        ):\n            return modelloader.load_spandrel_model(\n                model_path,\n                device=devices.device_codeformer,\n                expected_architecture='CodeFormer',\n            ).model\n        raise ValueError(\"No codeformer model found\")\n\n    def get_device(self):\n        return devices.device_codeformer\n\n    def restore(self, np_image, w: float | None = None):\n        if w is None:\n            w = getattr(shared.opts, \"code_former_weight\", 0.5)\n\n        def restore_face(cropped_face_t):\n            assert self.net is not None\n            return self.net(cropped_face_t, w=w, adain=True)[0]\n\n        return self.restore_with_helper(np_image, restore_face)\n\n\n", "contexts_below": "", "input_code": "def setup_model(dirname: str) -> None:\n\n    \"\"\"\n    This function attempts to set up a model for face restoration by initializing a FaceRestorerCodeFormer instance with the given directory name. It adds the initialized instance to a global list of face restorers. If an error occurs during setup, it reports the error.\n\n    Input-Output Arguments\n    :param dirname: str, The directory name where the model or necessary files for the FaceRestorerCodeFormer are located. It is used to initialize the FaceRestorerCodeFormer instance.\n    :return: No return values.\n    \"\"\"", "reference_steps": "1. Define a function `setup_model` that takes a single argument `dirname` which is a string representing a directory name.\n2. Declare a global variable `codeformer` to be used within the function.\n3. Inside the function, attempt to create an instance of `FaceRestorerCodeFormer` using `dirname` as an argument.\n4. Append the created instance `codeformer` to the `shared.face_restorers` list.\n5. Use a try-except block to handle any exceptions that may occur during the setup process.\n6. If an exception occurs, call the `errors.report` function with a message \"Error setting up CodeFormer\" and pass `exc_info=True` to include exception information.\n7. The function `setup_model` does not return any value (`None`).", "reference_code": "def setup_model(dirname: str) -> None:\n    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        shared.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(\"Error setting up CodeFormer\", exc_info=True)\n"}
{"namespace": "gfpgan_model.setup_model", "type": "function", "class_name": null, "function_name": "setup_model", "dependency_all": "# Intra-file Dependency:\ngfpgan_model.FaceRestorerGFPGAN\n\n", "dependency_sampled": "# Intra-file Dependency:\ngfpgan_model.FaceRestorerGFPGAN\n\n", "contexts_above": "from __future__ import annotations\n\nimport logging\nimport os\n\nimport torch\n\nfrom modules import (\n    devices,\n    errors,\n    face_restoration,\n    face_restoration_utils,\n    modelloader,\n    shared,\n)\n\nlogger = logging.getLogger(__name__)\nmodel_url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\nmodel_download_name = \"GFPGANv1.4.pth\"\ngfpgan_face_restorer: face_restoration.FaceRestoration | None = None\n\n\nclass FaceRestorerGFPGAN(face_restoration_utils.CommonFaceRestoration):\n    def name(self):\n        return \"GFPGAN\"\n\n    def get_device(self):\n        return devices.device_gfpgan\n\n    def load_net(self) -> torch.Module:\n        for model_path in modelloader.load_models(\n            model_path=self.model_path,\n            model_url=model_url,\n            command_path=self.model_path,\n            download_name=model_download_name,\n            ext_filter=['.pth'],\n        ):\n            if 'GFPGAN' in os.path.basename(model_path):\n                model = modelloader.load_spandrel_model(\n                    model_path,\n                    device=self.get_device(),\n                    expected_architecture='GFPGAN',\n                ).model\n                model.different_w = True  # see https://github.com/chaiNNer-org/spandrel/pull/81\n                return model\n        raise ValueError(\"No GFPGAN model found\")\n\n    def restore(self, np_image):\n        def restore_face(cropped_face_t):\n            assert self.net is not None\n            return self.net(cropped_face_t, return_rgb=False)[0]\n\n        return self.restore_with_helper(np_image, restore_face)\n\n\ndef gfpgan_fix_faces(np_image):\n    if gfpgan_face_restorer:\n        return gfpgan_face_restorer.restore(np_image)\n    logger.warning(\"GFPGAN face restorer not set up\")\n    return np_image\n\n\n", "contexts_below": "", "input_code": "def setup_model(dirname: str) -> None:\n\n    \"\"\"\n    Sets up the GFPGAN model for face restoration by patching the facexlib with the given directory and initializing the GFPGAN face restorer with the model located in the specified directory. It also handles any exceptions that occur during the setup process and reports them.\n\n    Input-Output Arguments\n    :param dirname: str, The directory path where the GFPGAN model is located. This directory is used to patch the facexlib and initialize the GFPGAN face restorer.\n    :return: No return values.\n    \"\"\"", "reference_steps": "1. Define a function `setup_model` that takes a directory name (`dirname`) as a string parameter.\n\n2. Declare a global variable `gfpgan_face_restorer` to hold the instance of the GFPGAN face restorer model.\n\n3. Inside the function, use a `try` block to attempt the setup process.\n\n4. Call the `patch_facexlib` function from the `face_restoration_utils` module, passing `dirname` as an argument to patch the FaceX library with necessary files from the specified directory.\n\n5. Create an instance of `FaceRestorerGFPGAN` from the `gfpgan_face_restorer` module, passing `model_path` as `dirname` to initialize the GFPGAN face restoration model.\n\n6. Append the newly created `gfpgan_face_restorer` instance to the `shared.face_restorers` list to keep track of it.\n\n7. Use an `except` block to catch any exceptions that occur during the setup process.\n\n8. If an exception is raised, call the `report` method from the `errors` module with the message \"Error setting up GFPGAN\" and use `exc_info=True` to include exception information in the report.\n\n9. The function does not return any value (`None` is implied by the absence of a `return` statement).\n\n10. The purpose of the function is to initialize and register a GFPGAN face restoration model for later use.", "reference_code": "def setup_model(dirname: str) -> None:\n    global gfpgan_face_restorer\n\n    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        shared.face_restorers.append(gfpgan_face_restorer)\n    except Exception:\n        errors.report(\"Error setting up GFPGAN\", exc_info=True)\n"}
{"namespace": "math.plus_eps", "type": "function", "class_name": null, "function_name": "plus_eps", "dependency_all": "# Intra-file Dependency:\nmath.tiny_val\n\n", "dependency_sampled": "# Intra-file Dependency:\nmath.tiny_val\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Mathy utility functions.\"\"\"\n\nimport functools\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ntiny_val = np.float32(np.finfo(np.float32).tiny)\nmin_val = np.float32(np.finfo(np.float32).min)\nmax_val = np.float32(np.finfo(np.float32).max)\n\n\ndef laplace_cdf(x, beta):\n  alpha = 1 / beta\n  return alpha * (0.5 + 0.5 * safe_sign(x) * (jnp.exp(-jnp.abs(x) / beta) - 1))\n\n\ndef scaled_softplus(x, scale=100.0):\n  return (1.0 / scale) * jax.nn.softplus(scale * x)\n\n\ndef matmul(a, b):\n  \"\"\"jnp.matmul defaults to bfloat16, but this helper function doesn't.\"\"\"\n  return jnp.matmul(a, b, precision=jax.lax.Precision.HIGHEST)\n\n\ndef unstack(x, axis=0):\n  return tuple(\n      jnp.squeeze(z, axis=axis) for z in jnp.split(x, x.shape[axis], axis=axis)\n  )\n\n\n@jax.custom_jvp\n", "contexts_below": "\n\n@jax.custom_jvp\ndef minus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n\n@plus_eps.defjvp\ndef plus_eps_jvp(primals, tangents):\n  \"\"\"Make plus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return plus_eps(*primals), tangents[0]\n\n\n@minus_eps.defjvp\ndef minus_eps_jvp(primals, tangents):\n  \"\"\"Make minus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return minus_eps(*primals), tangents[0]\n\n\n@jax.custom_jvp\ndef expm1(x):\n  \"\"\"jnp.expm1() has inaccurate gradients when x << 0, this doesn't.\"\"\"\n  return jnp.expm1(x)\n\n\n@expm1.defjvp\ndef expm1_jvp(primals, tangents):\n  return expm1(*primals), tangents[0] * jnp.exp(primals[0])\n\n\ndef safe_trig_helper(x, fn, t=100 * jnp.pi):\n  \"\"\"Helper function used by safe_cos/safe_sin: mods x before sin()/cos().\"\"\"\n  return fn(jnp.nan_to_num(jnp.where(jnp.abs(x) < t, x, x % t)))\n\n\ndef safe_cos(x):\n  \"\"\"jnp.cos() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.cos)\n\n\ndef safe_sin(x):\n  \"\"\"jnp.sin() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.sin)\n\n\n@jax.custom_vjp\ndef safe_arctan2(x1, x2):\n  return safe_arctan2_fwd(x1, x2)[0]\n\n\ndef safe_arctan2_fwd(x1, x2):\n  return jnp.arctan2(x1, x2), (x1, x2)\n\n\ndef safe_arctan2_bwd(res, g):\n  x1, x2 = res\n  denom = remove_zero(x1**2 + x2**2)\n  d1 = g * (x2 / denom)\n  d2 = g * (-x1 / denom)\n  return d1, d2\n\n\nsafe_arctan2.defvjp(safe_arctan2_fwd, safe_arctan2_bwd)\n\n\ndef generate_clip_nograd_fn(a_min, a_max):\n  \"\"\"Generates a function that clips to [a_min, a_max] with no grad effects.\"\"\"\n\n  @jax.custom_jvp\n  def clip_nograd(a):\n    \"\"\"Clamps `a` from above and below.\"\"\"\n    return jnp.clip(a, a_min, a_max)\n\n  @clip_nograd.defjvp\n  def clip_nograd_jvp(primals, tangents):\n    \"\"\"Override clips()'s gradient to be a no-op.\"\"\"\n    return clip_nograd(primals[0]), tangents[0]\n\n  return clip_nograd\n\n\nclip_finite_nograd = generate_clip_nograd_fn(min_val, max_val)\n\nclip_pos_finite_nograd = generate_clip_nograd_fn(tiny_val, max_val)\n\n\ndef clip_pos(x):\n  \"\"\"Clamps `x` from below to be positive.\"\"\"\n  return jnp.maximum(tiny_val, x)\n\n\ndef safe_sign(x):\n  \"\"\"jnp.sign(x) except x=0 is assumed to have a sign of +1, not 0.\"\"\"\n  return jnp.where(x < 0, -1, +1)\n\n\ndef remove_zero(x):\n  \"\"\"Shifts `x` away from 0.\"\"\"\n  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, x)\n\n\ndef clip_finite(x):\n  return jnp.clip(x, min_val, max_val)\n\n\n@jax.custom_vjp\ndef safe_div(n, d):\n  \"\"\"Divide `n` by `d` but the value and gradient never nan out.\"\"\"\n  return safe_div_fwd(n, d)[0]\n\n\ndef safe_div_fwd(n, d):\n  r = jnp.clip(n / remove_zero(d), min_val, max_val)\n  return jnp.where(jnp.abs(d) < tiny_val, 0, r), (d, r)\n\n\ndef safe_div_bwd(res, g):\n  d, r = res\n  dn = jnp.clip(g / remove_zero(d), min_val, max_val)\n  dd = jnp.clip(-g * r / remove_zero(d), min_val, max_val)\n  return dn, dd\n\n\nsafe_div.defvjp(safe_div_fwd, safe_div_bwd)\n\n\ndef generate_safe_fn(fn, grad_fn, x_range):\n  \"\"\"Generate's a `safe` fn() where inputs are clipped in fwd and bwd passes.\"\"\"\n\n  @jax.custom_jvp\n  def safe_fn(x):\n    \"\"\"fn() with clipped inputs.\"\"\"\n    return fn(jnp.clip(x, *x_range))\n\n  @safe_fn.defjvp\n  def safe_fn_jvp(primals, tangents):\n    \"\"\"Backpropagate using the gradient and clipped inputs.\"\"\"\n    (x,) = primals\n    (x_dot,) = tangents\n    y = safe_fn(x)\n    y_dot = grad_fn(jnp.clip(x, *x_range), y, x_dot)\n    return y, y_dot\n\n  return safe_fn\n\n\n# These safe_* functions need to be wrapped in no-op function definitions for\n# gin to recognize them, otherwise they could just be calls to generate_safe_fn.\n\n\ndef safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)\n\n\ndef safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda _, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: (x, None), lambda _, y: (grad_fn(y),))\n  return fn_out\n\n\ndef select(cond_pairs, default):\n  \"\"\"A helpful wrapper around jnp.select() that is easier to read.\"\"\"\n  return jnp.select(*zip(*cond_pairs), default)\n\n\ndef power_ladder_max_output(p):\n  \"\"\"The limit of power_ladder(x, p) as x goes to infinity.\"\"\"\n  return select(\n      [\n          (p == -jnp.inf, 1),\n          (p >= 0, jnp.inf),\n      ],\n      safe_div(p - 1, p),\n  )\n\n\ndef power_ladder(x, p, premult=None, postmult=None):\n  \"\"\"Tukey's power ladder, with a +1 on x, some scaling, and special cases.\"\"\"\n  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    x = x * premult\n  xp = jnp.abs(x)\n  xs = xp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((xs + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    y = y * postmult\n  return y\n\n\ndef inv_power_ladder(y, p, premult=None, postmult=None):\n  \"\"\"The inverse of `power_ladder()`.\"\"\"\n  if postmult is not None:\n    y /= postmult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  yp = override_gradient(jnp.clip(yp, -y_max, y_max), yp)  # Clip val, not grad.\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * yp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if premult is not None:\n    x /= premult\n  return x\n\n\ndef log_lerp(t, v0, v1):\n  \"\"\"Interpolate log-linearly from `v0` (t=0) to `v1` (t=1).\"\"\"\n  if v0 <= 0 or v1 <= 0:\n    raise ValueError(f'Interpolants {v0} and {v1} must be positive.')\n  lv0 = jnp.log(v0)\n  lv1 = jnp.log(v1)\n  return jnp.exp(jnp.clip(t, 0, 1) * (lv1 - lv0) + lv0)\n\n\ndef approx_erf(x):\n  \"\"\"An approximation of erf() that is accurate to within 0.007.\"\"\"\n  return jnp.sign(x) * jnp.sqrt(1 - jnp.exp(-(4 / jnp.pi) * x**2))\n\n\ndef create_learning_rate_decay(**kwargs):\n  \"\"\"A partial evaluation of learning rate decay that can be used with gin.\"\"\"\n  return functools.partial(learning_rate_decay, **kwargs)\n\n\ndef learning_rate_decay(\n    step, lr_init, lr_final, max_steps, lr_delay_steps=0, lr_delay_mult=1\n):\n  \"\"\"Continuous learning rate decay function.\n\n  The returned rate is lr_init when step=0 and lr_final when step=max_steps, and\n  is log-linearly interpolated elsewhere (equivalent to exponential decay).\n  If lr_delay_steps>0 then the learning rate will be scaled by some smooth\n  function of lr_delay_mult, such that the initial learning rate is\n  lr_init*lr_delay_mult at the beginning of optimization but will be eased back\n  to the normal learning rate when steps>lr_delay_steps.\n\n  Args:\n    step: int, the current optimization step.\n    lr_init: float, the initial learning rate.\n    lr_final: float, the final learning rate.\n    max_steps: int, the number of steps during optimization.\n    lr_delay_steps: int, the number of steps to delay the full learning rate.\n    lr_delay_mult: float, the multiplier on the rate when delaying it.\n\n  Returns:\n    lr: the learning for current step 'step'.\n  \"\"\"\n  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  return delay_rate * log_lerp(step / max_steps, lr_init, lr_final)\n\n\ndef sorted_lookup(x, xp, fps, device_is_tpu):\n  \"\"\"Lookup `x` into locations `xp` , return indices and each `[fp]` value.\"\"\"\n  if not isinstance(fps, tuple):\n    raise ValueError(f'Input `fps` must be a tuple, but is {type(fps)}.')\n\n  if device_is_tpu:\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[Ellipsis, None, :] >= xp[Ellipsis, :, None]\n\n    def find_interval(x):\n      # Grab the value where `mask` switches from True to False, and vice versa.\n      # This approach takes advantage of the fact that `x` is sorted.\n      x0 = jnp.max(jnp.where(mask, x[Ellipsis, None], x[Ellipsis, :1, None]), -2)\n      x1 = jnp.min(jnp.where(~mask, x[Ellipsis, None], x[Ellipsis, -1:, None]), -2)\n      return x0, x1\n\n    idx0, idx1 = find_interval(jnp.arange(xp.shape[-1]))\n    vals = [find_interval(fp) for fp in fps]\n  else:\n    # jnp.searchsorted() has slightly different conventions for boundary\n    # handling than the rest of this codebase.\n    idx = jnp.vectorize(\n        lambda a, v: jnp.searchsorted(a, v, side='right'),\n        signature='(n),(m)->(m)',\n    )(xp, x)\n    idx1 = jnp.minimum(idx, xp.shape[-1] - 1)\n    idx0 = jnp.maximum(idx - 1, 0)\n    vals = []\n    for fp in fps:\n      fp0 = jnp.take_along_axis(fp, idx0, axis=-1)\n      fp1 = jnp.take_along_axis(fp, idx1, axis=-1)\n      vals.append((fp0, fp1))\n  return (idx0, idx1), vals\n\n\ndef sorted_interp(\n    x, xp, fp, device_is_tpu, eps=jnp.finfo(jnp.float32).eps ** 2\n):\n  \"\"\"A version of interp() where xp and fp must be sorted.\"\"\"\n  (xp0, xp1), (fp0, fp1) = sorted_lookup(\n      x, xp, (xp, fp), device_is_tpu=device_is_tpu\n  )[1]\n  offset = jnp.clip((x - xp0) / jnp.maximum(eps, xp1 - xp0), 0, 1)\n  ret = fp0 + offset * (fp1 - fp0)\n  return ret\n\n\ndef searchsorted(a, v, device_is_tpu):\n  \"\"\"Behaves like jnp.searchsorted, excluding boundary conditions.\"\"\"\n  return sorted_lookup(v, a, (), device_is_tpu=device_is_tpu)[0]\n\n\ndef override_gradient(fval, bval):\n  \"\"\"Use `fval` in the forward pass but `bval` in the backward pass.\"\"\"\n  # Note that the parentheses are needed to avoid catastrophic cancellation.\n  return jax.lax.stop_gradient(fval) + (bval - jax.lax.stop_gradient(bval))\n\n\ndef average_across_multisamples(x):\n  \"\"\"Function that averages grid query results across the multisample dimension.\"\"\"\n  return jnp.mean(x, axis=-2)\n\n\ndef noop(x):\n  return x\n\n\n@jax.custom_jvp\ndef fake_clip(a, a_min, a_max):\n  \"\"\"jnp.clip() but the gradient doesn't get clipped on the backward pass.\"\"\"\n  return jnp.clip(a, a_min, a_max)\n\n\n@fake_clip.defjvp\ndef fake_clip_jvp(primals, tangents):\n  \"\"\"Override fake_clip()'s gradient so that it's a no-op.\"\"\"\n  return jnp.clip(*primals), tangents[0]\n\n\n@jax.jit\ndef general_lossfun(x, alpha, scale):\n  r\"\"\"This implements the rho(x, \\alpha, c) function described in \"A General and\n  Adaptive Robust Loss Function\", Jonathan T. Barron,\n  https://arxiv.org/abs/1701.03077.\n\n  Args:\n    x: The residual for which the loss is being computed. x can have any shape,\n      and alpha and scale will be broadcasted to match x's shape if necessary.\n    alpha: The shape parameter of the loss (\\alpha in the paper), where more\n      negative values produce a loss with more robust behavior (outliers \"cost\"\n      less), and more positive values produce a loss with less robust behavior\n      (outliers are penalized more heavily). Alpha can be any value in\n      [-infinity, infinity], but the gradient of the loss with respect to alpha\n      is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n      interpolation between several discrete robust losses:\n        alpha=-Infinity: Welsch/Leclerc Loss.\n        alpha=-2: Geman-McClure loss.\n        alpha=0: Cauchy/Lortentzian loss.\n        alpha=1: Charbonnier/pseudo-Huber loss.\n        alpha=2: L2 loss.\n    scale: The scale parameter of the loss. When |x| < scale, the loss is an\n      L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n      different shape according to alpha.\n\n  Returns:\n    The losses for each element of x, in the same shape as x.\n  \"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  maxval = 1e15\n\n  # A \"safe\" versions of expm1 that will not NaN-out on large inputs.\n  expm1_safe = lambda x: jnp.expm1(jnp.minimum(x, 43))\n\n  # `scale` must be > 0.\n  scale = jnp.maximum(eps, scale)\n\n  # Large values of |x| can cause non-finite gradients.\n  x = fake_clip(x, -maxval, maxval)\n\n  # The loss when alpha == 2. This will get reused repeatedly.\n  loss_two = 0.5 * (x / scale)**2\n\n  # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n  a = jnp.where(alpha >= 0, jnp.ones_like(alpha),\n                -jnp.ones_like(alpha)) * jnp.maximum(eps, jnp.abs(alpha))\n\n  # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n  b = jnp.maximum(eps, jnp.abs(a - 2))\n\n  # The loss when not in one of the special casess.\n  loss_ow = (b / a) * ((loss_two / (0.5 * b) + 1)**(0.5 * a) - 1)\n\n  # Select which of the cases of the loss to return as a function of alpha.\n  return jnp.where(\n      alpha == -jnp.inf, -expm1_safe(-loss_two),\n      jnp.where(\n          alpha == 0, jnp.log1p(loss_two),\n          jnp.where(alpha == 2, loss_two,\n                    jnp.where(alpha == jnp.inf, expm1_safe(loss_two),\n                              loss_ow))))\n", "input_code": "def plus_eps(x):\n\n  \"\"\"\n  The function adjusts very small values of x (close to zero) to a minimum threshold or calculates the next representable floating-point value towards positive infinity for x. This is useful for avoiding issues with values that are too small to be represented accurately in computations.\n\n  Input-Output Arguments\n  :param x: The input value that needs adjustment. It is used to check against a tiny threshold value and to compute the next representable floating-point value if necessary.\n  :return: The adjusted value of x. If x is smaller than a tiny threshold, the threshold value is returned. Otherwise, the next floating-point value towards positive infinity is returned.\n  \"\"\"", "reference_steps": "1. Define a function `plus_eps` that takes a single argument `x`.\n2. Set a threshold `tiny_val` for the minimum absolute value (not provided in the reference code, but it's implied that it exists).\n3. Use `jnp.where` to create a conditional operation that checks if the absolute value of `x` is less than `tiny_val`.\n4. If the absolute value of `x` is less than `tiny_val`, return `tiny_val`.\n5. If the absolute value of `x` is greater than or equal to `tiny_val`, convert `x` to a 32-bit float using `jnp.float32(x)`.\n6. Use `jnp.nextafter` to find the next representable floating-point value after the 32-bit float of `x` towards positive infinity.\n7. Return the result of the `jnp.where` conditional operation.\n8. The function `plus_eps` effectively increases a given `x` to the next representable value greater than `x` if `x` is not too small, otherwise, it sets it to a minimum threshold value `tiny_val`.", "reference_code": "def plus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )\n"}
{"namespace": "math.minus_eps", "type": "function", "class_name": null, "function_name": "minus_eps", "dependency_all": "# Intra-file Dependency:\nmath.tiny_val\n\n", "dependency_sampled": "# Intra-file Dependency:\nmath.tiny_val\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Mathy utility functions.\"\"\"\n\nimport functools\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ntiny_val = np.float32(np.finfo(np.float32).tiny)\nmin_val = np.float32(np.finfo(np.float32).min)\nmax_val = np.float32(np.finfo(np.float32).max)\n\n\ndef laplace_cdf(x, beta):\n  alpha = 1 / beta\n  return alpha * (0.5 + 0.5 * safe_sign(x) * (jnp.exp(-jnp.abs(x) / beta) - 1))\n\n\ndef scaled_softplus(x, scale=100.0):\n  return (1.0 / scale) * jax.nn.softplus(scale * x)\n\n\ndef matmul(a, b):\n  \"\"\"jnp.matmul defaults to bfloat16, but this helper function doesn't.\"\"\"\n  return jnp.matmul(a, b, precision=jax.lax.Precision.HIGHEST)\n\n\ndef unstack(x, axis=0):\n  return tuple(\n      jnp.squeeze(z, axis=axis) for z in jnp.split(x, x.shape[axis], axis=axis)\n  )\n\n\n@jax.custom_jvp\ndef plus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )\n\n\n@jax.custom_jvp\n", "contexts_below": "\n\n@plus_eps.defjvp\ndef plus_eps_jvp(primals, tangents):\n  \"\"\"Make plus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return plus_eps(*primals), tangents[0]\n\n\n@minus_eps.defjvp\ndef minus_eps_jvp(primals, tangents):\n  \"\"\"Make minus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return minus_eps(*primals), tangents[0]\n\n\n@jax.custom_jvp\ndef expm1(x):\n  \"\"\"jnp.expm1() has inaccurate gradients when x << 0, this doesn't.\"\"\"\n  return jnp.expm1(x)\n\n\n@expm1.defjvp\ndef expm1_jvp(primals, tangents):\n  return expm1(*primals), tangents[0] * jnp.exp(primals[0])\n\n\ndef safe_trig_helper(x, fn, t=100 * jnp.pi):\n  \"\"\"Helper function used by safe_cos/safe_sin: mods x before sin()/cos().\"\"\"\n  return fn(jnp.nan_to_num(jnp.where(jnp.abs(x) < t, x, x % t)))\n\n\ndef safe_cos(x):\n  \"\"\"jnp.cos() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.cos)\n\n\ndef safe_sin(x):\n  \"\"\"jnp.sin() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.sin)\n\n\n@jax.custom_vjp\ndef safe_arctan2(x1, x2):\n  return safe_arctan2_fwd(x1, x2)[0]\n\n\ndef safe_arctan2_fwd(x1, x2):\n  return jnp.arctan2(x1, x2), (x1, x2)\n\n\ndef safe_arctan2_bwd(res, g):\n  x1, x2 = res\n  denom = remove_zero(x1**2 + x2**2)\n  d1 = g * (x2 / denom)\n  d2 = g * (-x1 / denom)\n  return d1, d2\n\n\nsafe_arctan2.defvjp(safe_arctan2_fwd, safe_arctan2_bwd)\n\n\ndef generate_clip_nograd_fn(a_min, a_max):\n  \"\"\"Generates a function that clips to [a_min, a_max] with no grad effects.\"\"\"\n\n  @jax.custom_jvp\n  def clip_nograd(a):\n    \"\"\"Clamps `a` from above and below.\"\"\"\n    return jnp.clip(a, a_min, a_max)\n\n  @clip_nograd.defjvp\n  def clip_nograd_jvp(primals, tangents):\n    \"\"\"Override clips()'s gradient to be a no-op.\"\"\"\n    return clip_nograd(primals[0]), tangents[0]\n\n  return clip_nograd\n\n\nclip_finite_nograd = generate_clip_nograd_fn(min_val, max_val)\n\nclip_pos_finite_nograd = generate_clip_nograd_fn(tiny_val, max_val)\n\n\ndef clip_pos(x):\n  \"\"\"Clamps `x` from below to be positive.\"\"\"\n  return jnp.maximum(tiny_val, x)\n\n\ndef safe_sign(x):\n  \"\"\"jnp.sign(x) except x=0 is assumed to have a sign of +1, not 0.\"\"\"\n  return jnp.where(x < 0, -1, +1)\n\n\ndef remove_zero(x):\n  \"\"\"Shifts `x` away from 0.\"\"\"\n  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, x)\n\n\ndef clip_finite(x):\n  return jnp.clip(x, min_val, max_val)\n\n\n@jax.custom_vjp\ndef safe_div(n, d):\n  \"\"\"Divide `n` by `d` but the value and gradient never nan out.\"\"\"\n  return safe_div_fwd(n, d)[0]\n\n\ndef safe_div_fwd(n, d):\n  r = jnp.clip(n / remove_zero(d), min_val, max_val)\n  return jnp.where(jnp.abs(d) < tiny_val, 0, r), (d, r)\n\n\ndef safe_div_bwd(res, g):\n  d, r = res\n  dn = jnp.clip(g / remove_zero(d), min_val, max_val)\n  dd = jnp.clip(-g * r / remove_zero(d), min_val, max_val)\n  return dn, dd\n\n\nsafe_div.defvjp(safe_div_fwd, safe_div_bwd)\n\n\ndef generate_safe_fn(fn, grad_fn, x_range):\n  \"\"\"Generate's a `safe` fn() where inputs are clipped in fwd and bwd passes.\"\"\"\n\n  @jax.custom_jvp\n  def safe_fn(x):\n    \"\"\"fn() with clipped inputs.\"\"\"\n    return fn(jnp.clip(x, *x_range))\n\n  @safe_fn.defjvp\n  def safe_fn_jvp(primals, tangents):\n    \"\"\"Backpropagate using the gradient and clipped inputs.\"\"\"\n    (x,) = primals\n    (x_dot,) = tangents\n    y = safe_fn(x)\n    y_dot = grad_fn(jnp.clip(x, *x_range), y, x_dot)\n    return y, y_dot\n\n  return safe_fn\n\n\n# These safe_* functions need to be wrapped in no-op function definitions for\n# gin to recognize them, otherwise they could just be calls to generate_safe_fn.\n\n\ndef safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)\n\n\ndef safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda _, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: (x, None), lambda _, y: (grad_fn(y),))\n  return fn_out\n\n\ndef select(cond_pairs, default):\n  \"\"\"A helpful wrapper around jnp.select() that is easier to read.\"\"\"\n  return jnp.select(*zip(*cond_pairs), default)\n\n\ndef power_ladder_max_output(p):\n  \"\"\"The limit of power_ladder(x, p) as x goes to infinity.\"\"\"\n  return select(\n      [\n          (p == -jnp.inf, 1),\n          (p >= 0, jnp.inf),\n      ],\n      safe_div(p - 1, p),\n  )\n\n\ndef power_ladder(x, p, premult=None, postmult=None):\n  \"\"\"Tukey's power ladder, with a +1 on x, some scaling, and special cases.\"\"\"\n  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    x = x * premult\n  xp = jnp.abs(x)\n  xs = xp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((xs + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    y = y * postmult\n  return y\n\n\ndef inv_power_ladder(y, p, premult=None, postmult=None):\n  \"\"\"The inverse of `power_ladder()`.\"\"\"\n  if postmult is not None:\n    y /= postmult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  yp = override_gradient(jnp.clip(yp, -y_max, y_max), yp)  # Clip val, not grad.\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * yp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if premult is not None:\n    x /= premult\n  return x\n\n\ndef log_lerp(t, v0, v1):\n  \"\"\"Interpolate log-linearly from `v0` (t=0) to `v1` (t=1).\"\"\"\n  if v0 <= 0 or v1 <= 0:\n    raise ValueError(f'Interpolants {v0} and {v1} must be positive.')\n  lv0 = jnp.log(v0)\n  lv1 = jnp.log(v1)\n  return jnp.exp(jnp.clip(t, 0, 1) * (lv1 - lv0) + lv0)\n\n\ndef approx_erf(x):\n  \"\"\"An approximation of erf() that is accurate to within 0.007.\"\"\"\n  return jnp.sign(x) * jnp.sqrt(1 - jnp.exp(-(4 / jnp.pi) * x**2))\n\n\ndef create_learning_rate_decay(**kwargs):\n  \"\"\"A partial evaluation of learning rate decay that can be used with gin.\"\"\"\n  return functools.partial(learning_rate_decay, **kwargs)\n\n\ndef learning_rate_decay(\n    step, lr_init, lr_final, max_steps, lr_delay_steps=0, lr_delay_mult=1\n):\n  \"\"\"Continuous learning rate decay function.\n\n  The returned rate is lr_init when step=0 and lr_final when step=max_steps, and\n  is log-linearly interpolated elsewhere (equivalent to exponential decay).\n  If lr_delay_steps>0 then the learning rate will be scaled by some smooth\n  function of lr_delay_mult, such that the initial learning rate is\n  lr_init*lr_delay_mult at the beginning of optimization but will be eased back\n  to the normal learning rate when steps>lr_delay_steps.\n\n  Args:\n    step: int, the current optimization step.\n    lr_init: float, the initial learning rate.\n    lr_final: float, the final learning rate.\n    max_steps: int, the number of steps during optimization.\n    lr_delay_steps: int, the number of steps to delay the full learning rate.\n    lr_delay_mult: float, the multiplier on the rate when delaying it.\n\n  Returns:\n    lr: the learning for current step 'step'.\n  \"\"\"\n  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  return delay_rate * log_lerp(step / max_steps, lr_init, lr_final)\n\n\ndef sorted_lookup(x, xp, fps, device_is_tpu):\n  \"\"\"Lookup `x` into locations `xp` , return indices and each `[fp]` value.\"\"\"\n  if not isinstance(fps, tuple):\n    raise ValueError(f'Input `fps` must be a tuple, but is {type(fps)}.')\n\n  if device_is_tpu:\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[Ellipsis, None, :] >= xp[Ellipsis, :, None]\n\n    def find_interval(x):\n      # Grab the value where `mask` switches from True to False, and vice versa.\n      # This approach takes advantage of the fact that `x` is sorted.\n      x0 = jnp.max(jnp.where(mask, x[Ellipsis, None], x[Ellipsis, :1, None]), -2)\n      x1 = jnp.min(jnp.where(~mask, x[Ellipsis, None], x[Ellipsis, -1:, None]), -2)\n      return x0, x1\n\n    idx0, idx1 = find_interval(jnp.arange(xp.shape[-1]))\n    vals = [find_interval(fp) for fp in fps]\n  else:\n    # jnp.searchsorted() has slightly different conventions for boundary\n    # handling than the rest of this codebase.\n    idx = jnp.vectorize(\n        lambda a, v: jnp.searchsorted(a, v, side='right'),\n        signature='(n),(m)->(m)',\n    )(xp, x)\n    idx1 = jnp.minimum(idx, xp.shape[-1] - 1)\n    idx0 = jnp.maximum(idx - 1, 0)\n    vals = []\n    for fp in fps:\n      fp0 = jnp.take_along_axis(fp, idx0, axis=-1)\n      fp1 = jnp.take_along_axis(fp, idx1, axis=-1)\n      vals.append((fp0, fp1))\n  return (idx0, idx1), vals\n\n\ndef sorted_interp(\n    x, xp, fp, device_is_tpu, eps=jnp.finfo(jnp.float32).eps ** 2\n):\n  \"\"\"A version of interp() where xp and fp must be sorted.\"\"\"\n  (xp0, xp1), (fp0, fp1) = sorted_lookup(\n      x, xp, (xp, fp), device_is_tpu=device_is_tpu\n  )[1]\n  offset = jnp.clip((x - xp0) / jnp.maximum(eps, xp1 - xp0), 0, 1)\n  ret = fp0 + offset * (fp1 - fp0)\n  return ret\n\n\ndef searchsorted(a, v, device_is_tpu):\n  \"\"\"Behaves like jnp.searchsorted, excluding boundary conditions.\"\"\"\n  return sorted_lookup(v, a, (), device_is_tpu=device_is_tpu)[0]\n\n\ndef override_gradient(fval, bval):\n  \"\"\"Use `fval` in the forward pass but `bval` in the backward pass.\"\"\"\n  # Note that the parentheses are needed to avoid catastrophic cancellation.\n  return jax.lax.stop_gradient(fval) + (bval - jax.lax.stop_gradient(bval))\n\n\ndef average_across_multisamples(x):\n  \"\"\"Function that averages grid query results across the multisample dimension.\"\"\"\n  return jnp.mean(x, axis=-2)\n\n\ndef noop(x):\n  return x\n\n\n@jax.custom_jvp\ndef fake_clip(a, a_min, a_max):\n  \"\"\"jnp.clip() but the gradient doesn't get clipped on the backward pass.\"\"\"\n  return jnp.clip(a, a_min, a_max)\n\n\n@fake_clip.defjvp\ndef fake_clip_jvp(primals, tangents):\n  \"\"\"Override fake_clip()'s gradient so that it's a no-op.\"\"\"\n  return jnp.clip(*primals), tangents[0]\n\n\n@jax.jit\ndef general_lossfun(x, alpha, scale):\n  r\"\"\"This implements the rho(x, \\alpha, c) function described in \"A General and\n  Adaptive Robust Loss Function\", Jonathan T. Barron,\n  https://arxiv.org/abs/1701.03077.\n\n  Args:\n    x: The residual for which the loss is being computed. x can have any shape,\n      and alpha and scale will be broadcasted to match x's shape if necessary.\n    alpha: The shape parameter of the loss (\\alpha in the paper), where more\n      negative values produce a loss with more robust behavior (outliers \"cost\"\n      less), and more positive values produce a loss with less robust behavior\n      (outliers are penalized more heavily). Alpha can be any value in\n      [-infinity, infinity], but the gradient of the loss with respect to alpha\n      is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n      interpolation between several discrete robust losses:\n        alpha=-Infinity: Welsch/Leclerc Loss.\n        alpha=-2: Geman-McClure loss.\n        alpha=0: Cauchy/Lortentzian loss.\n        alpha=1: Charbonnier/pseudo-Huber loss.\n        alpha=2: L2 loss.\n    scale: The scale parameter of the loss. When |x| < scale, the loss is an\n      L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n      different shape according to alpha.\n\n  Returns:\n    The losses for each element of x, in the same shape as x.\n  \"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  maxval = 1e15\n\n  # A \"safe\" versions of expm1 that will not NaN-out on large inputs.\n  expm1_safe = lambda x: jnp.expm1(jnp.minimum(x, 43))\n\n  # `scale` must be > 0.\n  scale = jnp.maximum(eps, scale)\n\n  # Large values of |x| can cause non-finite gradients.\n  x = fake_clip(x, -maxval, maxval)\n\n  # The loss when alpha == 2. This will get reused repeatedly.\n  loss_two = 0.5 * (x / scale)**2\n\n  # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n  a = jnp.where(alpha >= 0, jnp.ones_like(alpha),\n                -jnp.ones_like(alpha)) * jnp.maximum(eps, jnp.abs(alpha))\n\n  # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n  b = jnp.maximum(eps, jnp.abs(a - 2))\n\n  # The loss when not in one of the special casess.\n  loss_ow = (b / a) * ((loss_two / (0.5 * b) + 1)**(0.5 * a) - 1)\n\n  # Select which of the cases of the loss to return as a function of alpha.\n  return jnp.where(\n      alpha == -jnp.inf, -expm1_safe(-loss_two),\n      jnp.where(\n          alpha == 0, jnp.log1p(loss_two),\n          jnp.where(alpha == 2, loss_two,\n                    jnp.where(alpha == jnp.inf, expm1_safe(loss_two),\n                              loss_ow))))\n", "input_code": "def minus_eps(x):\n\n  \"\"\"\n  This function adjusts the input value `x` by returning a slightly smaller value than `x` if `x` is not smaller than a very small value (`tiny_val`). If `x` is smaller than `tiny_val`, it returns `-tiny_val` instead. This is useful for numerical computations where a value slightly less than `x` is needed, but avoiding values too close to zero.\n\n  Input-Output Arguments\n  :param x: The input value to be adjusted. It is used to check against `tiny_val` and to compute the next smaller floating-point number towards negative infinity.\n  :return: A floating-point number that is slightly smaller than `x` if `x` is not smaller than `tiny_val`, otherwise `-tiny_val`. This ensures the returned value is always slightly less than the input, avoiding values too close to zero.\n  \"\"\"", "reference_steps": "1. Define a function `minus_eps` that takes a single argument `x`.\n2. Set a small threshold value `tiny_val` that will be used to determine if `x` is close to zero (this value is not shown in the code snippet but is assumed to be defined elsewhere).\n3. Use `jnp.abs()` to get the absolute value of `x`.\n4. Compare the absolute value of `x` with `tiny_val` to check if `x` is smaller than this threshold.\n5. Use `jnp.where()` to choose between two possible return values based on the comparison.\n6. If `x` is less than `tiny_val`, return `-tiny_val` as the output.\n7. If `x` is not less than `tiny_val`, convert `x` to a 32-bit floating-point number using `jnp.float32(x)`.\n8. Use `jnp.nextafter()` to get the next smallest floating-point number towards negative infinity from `jnp.float32(x)`.\n9. Return the result of the `jnp.where()` condition as the output of the function.\n10. The function ultimately returns a slightly smaller value than `x`, or `-tiny_val` if `x` is very small.", "reference_code": "def minus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n"}
{"namespace": "math.safe_exp", "type": "function", "class_name": null, "function_name": "safe_exp", "dependency_all": "# Intra-file Dependency:\nmath.generate_safe_fn\n\nmath.max_val\n\nmath.min_val\n\n", "dependency_sampled": "# Intra-file Dependency:\nmath.max_val\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Mathy utility functions.\"\"\"\n\nimport functools\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ntiny_val = np.float32(np.finfo(np.float32).tiny)\nmin_val = np.float32(np.finfo(np.float32).min)\nmax_val = np.float32(np.finfo(np.float32).max)\n\n\ndef laplace_cdf(x, beta):\n  alpha = 1 / beta\n  return alpha * (0.5 + 0.5 * safe_sign(x) * (jnp.exp(-jnp.abs(x) / beta) - 1))\n\n\ndef scaled_softplus(x, scale=100.0):\n  return (1.0 / scale) * jax.nn.softplus(scale * x)\n\n\ndef matmul(a, b):\n  \"\"\"jnp.matmul defaults to bfloat16, but this helper function doesn't.\"\"\"\n  return jnp.matmul(a, b, precision=jax.lax.Precision.HIGHEST)\n\n\ndef unstack(x, axis=0):\n  return tuple(\n      jnp.squeeze(z, axis=axis) for z in jnp.split(x, x.shape[axis], axis=axis)\n  )\n\n\n@jax.custom_jvp\ndef plus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )\n\n\n@jax.custom_jvp\ndef minus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n\n@plus_eps.defjvp\ndef plus_eps_jvp(primals, tangents):\n  \"\"\"Make plus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return plus_eps(*primals), tangents[0]\n\n\n@minus_eps.defjvp\ndef minus_eps_jvp(primals, tangents):\n  \"\"\"Make minus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return minus_eps(*primals), tangents[0]\n\n\n@jax.custom_jvp\ndef expm1(x):\n  \"\"\"jnp.expm1() has inaccurate gradients when x << 0, this doesn't.\"\"\"\n  return jnp.expm1(x)\n\n\n@expm1.defjvp\ndef expm1_jvp(primals, tangents):\n  return expm1(*primals), tangents[0] * jnp.exp(primals[0])\n\n\ndef safe_trig_helper(x, fn, t=100 * jnp.pi):\n  \"\"\"Helper function used by safe_cos/safe_sin: mods x before sin()/cos().\"\"\"\n  return fn(jnp.nan_to_num(jnp.where(jnp.abs(x) < t, x, x % t)))\n\n\ndef safe_cos(x):\n  \"\"\"jnp.cos() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.cos)\n\n\ndef safe_sin(x):\n  \"\"\"jnp.sin() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.sin)\n\n\n@jax.custom_vjp\ndef safe_arctan2(x1, x2):\n  return safe_arctan2_fwd(x1, x2)[0]\n\n\ndef safe_arctan2_fwd(x1, x2):\n  return jnp.arctan2(x1, x2), (x1, x2)\n\n\ndef safe_arctan2_bwd(res, g):\n  x1, x2 = res\n  denom = remove_zero(x1**2 + x2**2)\n  d1 = g * (x2 / denom)\n  d2 = g * (-x1 / denom)\n  return d1, d2\n\n\nsafe_arctan2.defvjp(safe_arctan2_fwd, safe_arctan2_bwd)\n\n\ndef generate_clip_nograd_fn(a_min, a_max):\n  \"\"\"Generates a function that clips to [a_min, a_max] with no grad effects.\"\"\"\n\n  @jax.custom_jvp\n  def clip_nograd(a):\n    \"\"\"Clamps `a` from above and below.\"\"\"\n    return jnp.clip(a, a_min, a_max)\n\n  @clip_nograd.defjvp\n  def clip_nograd_jvp(primals, tangents):\n    \"\"\"Override clips()'s gradient to be a no-op.\"\"\"\n    return clip_nograd(primals[0]), tangents[0]\n\n  return clip_nograd\n\n\nclip_finite_nograd = generate_clip_nograd_fn(min_val, max_val)\n\nclip_pos_finite_nograd = generate_clip_nograd_fn(tiny_val, max_val)\n\n\ndef clip_pos(x):\n  \"\"\"Clamps `x` from below to be positive.\"\"\"\n  return jnp.maximum(tiny_val, x)\n\n\ndef safe_sign(x):\n  \"\"\"jnp.sign(x) except x=0 is assumed to have a sign of +1, not 0.\"\"\"\n  return jnp.where(x < 0, -1, +1)\n\n\ndef remove_zero(x):\n  \"\"\"Shifts `x` away from 0.\"\"\"\n  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, x)\n\n\ndef clip_finite(x):\n  return jnp.clip(x, min_val, max_val)\n\n\n@jax.custom_vjp\ndef safe_div(n, d):\n  \"\"\"Divide `n` by `d` but the value and gradient never nan out.\"\"\"\n  return safe_div_fwd(n, d)[0]\n\n\ndef safe_div_fwd(n, d):\n  r = jnp.clip(n / remove_zero(d), min_val, max_val)\n  return jnp.where(jnp.abs(d) < tiny_val, 0, r), (d, r)\n\n\ndef safe_div_bwd(res, g):\n  d, r = res\n  dn = jnp.clip(g / remove_zero(d), min_val, max_val)\n  dd = jnp.clip(-g * r / remove_zero(d), min_val, max_val)\n  return dn, dd\n\n\nsafe_div.defvjp(safe_div_fwd, safe_div_bwd)\n\n\ndef generate_safe_fn(fn, grad_fn, x_range):\n  \"\"\"Generate's a `safe` fn() where inputs are clipped in fwd and bwd passes.\"\"\"\n\n  @jax.custom_jvp\n  def safe_fn(x):\n    \"\"\"fn() with clipped inputs.\"\"\"\n    return fn(jnp.clip(x, *x_range))\n\n  @safe_fn.defjvp\n  def safe_fn_jvp(primals, tangents):\n    \"\"\"Backpropagate using the gradient and clipped inputs.\"\"\"\n    (x,) = primals\n    (x_dot,) = tangents\n    y = safe_fn(x)\n    y_dot = grad_fn(jnp.clip(x, *x_range), y, x_dot)\n    return y, y_dot\n\n  return safe_fn\n\n\n# These safe_* functions need to be wrapped in no-op function definitions for\n# gin to recognize them, otherwise they could just be calls to generate_safe_fn.\n\n\ndef safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)\n\n\n", "contexts_below": "\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: (x, None), lambda _, y: (grad_fn(y),))\n  return fn_out\n\n\ndef select(cond_pairs, default):\n  \"\"\"A helpful wrapper around jnp.select() that is easier to read.\"\"\"\n  return jnp.select(*zip(*cond_pairs), default)\n\n\ndef power_ladder_max_output(p):\n  \"\"\"The limit of power_ladder(x, p) as x goes to infinity.\"\"\"\n  return select(\n      [\n          (p == -jnp.inf, 1),\n          (p >= 0, jnp.inf),\n      ],\n      safe_div(p - 1, p),\n  )\n\n\ndef power_ladder(x, p, premult=None, postmult=None):\n  \"\"\"Tukey's power ladder, with a +1 on x, some scaling, and special cases.\"\"\"\n  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    x = x * premult\n  xp = jnp.abs(x)\n  xs = xp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((xs + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    y = y * postmult\n  return y\n\n\ndef inv_power_ladder(y, p, premult=None, postmult=None):\n  \"\"\"The inverse of `power_ladder()`.\"\"\"\n  if postmult is not None:\n    y /= postmult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  yp = override_gradient(jnp.clip(yp, -y_max, y_max), yp)  # Clip val, not grad.\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * yp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if premult is not None:\n    x /= premult\n  return x\n\n\ndef log_lerp(t, v0, v1):\n  \"\"\"Interpolate log-linearly from `v0` (t=0) to `v1` (t=1).\"\"\"\n  if v0 <= 0 or v1 <= 0:\n    raise ValueError(f'Interpolants {v0} and {v1} must be positive.')\n  lv0 = jnp.log(v0)\n  lv1 = jnp.log(v1)\n  return jnp.exp(jnp.clip(t, 0, 1) * (lv1 - lv0) + lv0)\n\n\ndef approx_erf(x):\n  \"\"\"An approximation of erf() that is accurate to within 0.007.\"\"\"\n  return jnp.sign(x) * jnp.sqrt(1 - jnp.exp(-(4 / jnp.pi) * x**2))\n\n\ndef create_learning_rate_decay(**kwargs):\n  \"\"\"A partial evaluation of learning rate decay that can be used with gin.\"\"\"\n  return functools.partial(learning_rate_decay, **kwargs)\n\n\ndef learning_rate_decay(\n    step, lr_init, lr_final, max_steps, lr_delay_steps=0, lr_delay_mult=1\n):\n  \"\"\"Continuous learning rate decay function.\n\n  The returned rate is lr_init when step=0 and lr_final when step=max_steps, and\n  is log-linearly interpolated elsewhere (equivalent to exponential decay).\n  If lr_delay_steps>0 then the learning rate will be scaled by some smooth\n  function of lr_delay_mult, such that the initial learning rate is\n  lr_init*lr_delay_mult at the beginning of optimization but will be eased back\n  to the normal learning rate when steps>lr_delay_steps.\n\n  Args:\n    step: int, the current optimization step.\n    lr_init: float, the initial learning rate.\n    lr_final: float, the final learning rate.\n    max_steps: int, the number of steps during optimization.\n    lr_delay_steps: int, the number of steps to delay the full learning rate.\n    lr_delay_mult: float, the multiplier on the rate when delaying it.\n\n  Returns:\n    lr: the learning for current step 'step'.\n  \"\"\"\n  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  return delay_rate * log_lerp(step / max_steps, lr_init, lr_final)\n\n\ndef sorted_lookup(x, xp, fps, device_is_tpu):\n  \"\"\"Lookup `x` into locations `xp` , return indices and each `[fp]` value.\"\"\"\n  if not isinstance(fps, tuple):\n    raise ValueError(f'Input `fps` must be a tuple, but is {type(fps)}.')\n\n  if device_is_tpu:\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[Ellipsis, None, :] >= xp[Ellipsis, :, None]\n\n    def find_interval(x):\n      # Grab the value where `mask` switches from True to False, and vice versa.\n      # This approach takes advantage of the fact that `x` is sorted.\n      x0 = jnp.max(jnp.where(mask, x[Ellipsis, None], x[Ellipsis, :1, None]), -2)\n      x1 = jnp.min(jnp.where(~mask, x[Ellipsis, None], x[Ellipsis, -1:, None]), -2)\n      return x0, x1\n\n    idx0, idx1 = find_interval(jnp.arange(xp.shape[-1]))\n    vals = [find_interval(fp) for fp in fps]\n  else:\n    # jnp.searchsorted() has slightly different conventions for boundary\n    # handling than the rest of this codebase.\n    idx = jnp.vectorize(\n        lambda a, v: jnp.searchsorted(a, v, side='right'),\n        signature='(n),(m)->(m)',\n    )(xp, x)\n    idx1 = jnp.minimum(idx, xp.shape[-1] - 1)\n    idx0 = jnp.maximum(idx - 1, 0)\n    vals = []\n    for fp in fps:\n      fp0 = jnp.take_along_axis(fp, idx0, axis=-1)\n      fp1 = jnp.take_along_axis(fp, idx1, axis=-1)\n      vals.append((fp0, fp1))\n  return (idx0, idx1), vals\n\n\ndef sorted_interp(\n    x, xp, fp, device_is_tpu, eps=jnp.finfo(jnp.float32).eps ** 2\n):\n  \"\"\"A version of interp() where xp and fp must be sorted.\"\"\"\n  (xp0, xp1), (fp0, fp1) = sorted_lookup(\n      x, xp, (xp, fp), device_is_tpu=device_is_tpu\n  )[1]\n  offset = jnp.clip((x - xp0) / jnp.maximum(eps, xp1 - xp0), 0, 1)\n  ret = fp0 + offset * (fp1 - fp0)\n  return ret\n\n\ndef searchsorted(a, v, device_is_tpu):\n  \"\"\"Behaves like jnp.searchsorted, excluding boundary conditions.\"\"\"\n  return sorted_lookup(v, a, (), device_is_tpu=device_is_tpu)[0]\n\n\ndef override_gradient(fval, bval):\n  \"\"\"Use `fval` in the forward pass but `bval` in the backward pass.\"\"\"\n  # Note that the parentheses are needed to avoid catastrophic cancellation.\n  return jax.lax.stop_gradient(fval) + (bval - jax.lax.stop_gradient(bval))\n\n\ndef average_across_multisamples(x):\n  \"\"\"Function that averages grid query results across the multisample dimension.\"\"\"\n  return jnp.mean(x, axis=-2)\n\n\ndef noop(x):\n  return x\n\n\n@jax.custom_jvp\ndef fake_clip(a, a_min, a_max):\n  \"\"\"jnp.clip() but the gradient doesn't get clipped on the backward pass.\"\"\"\n  return jnp.clip(a, a_min, a_max)\n\n\n@fake_clip.defjvp\ndef fake_clip_jvp(primals, tangents):\n  \"\"\"Override fake_clip()'s gradient so that it's a no-op.\"\"\"\n  return jnp.clip(*primals), tangents[0]\n\n\n@jax.jit\ndef general_lossfun(x, alpha, scale):\n  r\"\"\"This implements the rho(x, \\alpha, c) function described in \"A General and\n  Adaptive Robust Loss Function\", Jonathan T. Barron,\n  https://arxiv.org/abs/1701.03077.\n\n  Args:\n    x: The residual for which the loss is being computed. x can have any shape,\n      and alpha and scale will be broadcasted to match x's shape if necessary.\n    alpha: The shape parameter of the loss (\\alpha in the paper), where more\n      negative values produce a loss with more robust behavior (outliers \"cost\"\n      less), and more positive values produce a loss with less robust behavior\n      (outliers are penalized more heavily). Alpha can be any value in\n      [-infinity, infinity], but the gradient of the loss with respect to alpha\n      is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n      interpolation between several discrete robust losses:\n        alpha=-Infinity: Welsch/Leclerc Loss.\n        alpha=-2: Geman-McClure loss.\n        alpha=0: Cauchy/Lortentzian loss.\n        alpha=1: Charbonnier/pseudo-Huber loss.\n        alpha=2: L2 loss.\n    scale: The scale parameter of the loss. When |x| < scale, the loss is an\n      L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n      different shape according to alpha.\n\n  Returns:\n    The losses for each element of x, in the same shape as x.\n  \"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  maxval = 1e15\n\n  # A \"safe\" versions of expm1 that will not NaN-out on large inputs.\n  expm1_safe = lambda x: jnp.expm1(jnp.minimum(x, 43))\n\n  # `scale` must be > 0.\n  scale = jnp.maximum(eps, scale)\n\n  # Large values of |x| can cause non-finite gradients.\n  x = fake_clip(x, -maxval, maxval)\n\n  # The loss when alpha == 2. This will get reused repeatedly.\n  loss_two = 0.5 * (x / scale)**2\n\n  # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n  a = jnp.where(alpha >= 0, jnp.ones_like(alpha),\n                -jnp.ones_like(alpha)) * jnp.maximum(eps, jnp.abs(alpha))\n\n  # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n  b = jnp.maximum(eps, jnp.abs(a - 2))\n\n  # The loss when not in one of the special casess.\n  loss_ow = (b / a) * ((loss_two / (0.5 * b) + 1)**(0.5 * a) - 1)\n\n  # Select which of the cases of the loss to return as a function of alpha.\n  return jnp.where(\n      alpha == -jnp.inf, -expm1_safe(-loss_two),\n      jnp.where(\n          alpha == 0, jnp.log1p(loss_two),\n          jnp.where(alpha == 2, loss_two,\n                    jnp.where(alpha == jnp.inf, expm1_safe(loss_two),\n                              loss_ow))))\n", "input_code": "def safe_exp(x):\n\n  \"\"\"\n  The function creates a safe exponential function that avoids overflow issues by limiting the input range. It uses a helper function to generate this safe version of the exponential function, applying a custom gradient function for backpropagation in automatic differentiation contexts.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n  :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  \"\"\"", "reference_steps": "1. Define a function `safe_exp` that takes a single argument `x`.\n2. Use `generate_safe_fn` function to wrap the exponential function `jnp.exp` with additional safety checks.\n3. Provide a custom gradient function as the second argument to `generate_safe_fn`. This function takes three arguments: the primal outputs `_`, the tangents `y`, and the cotangents `x_dot`, and returns the product of `y` and `x_dot`.\n4. Define a tuple containing `min_val` and the next floating-point number after `np.log(max_val)` towards zero, using `np.nextafter` and `np.float32(0)`.\n5. This tuple defines the safe range for the input `x` to the exponential function.\n6. Pass the tuple as the third argument to `generate_safe_fn` to specify the domain over which the function is considered safe.\n7. Call the resulting safe exponential function with the input `x`.\n8. Return the result of the safe exponential function applied to `x`.\n9. Ensure that `min_val` and `max_val` are defined somewhere in the code, as they are used to set the bounds for the safe range.\n10. The function `safe_exp` is now ready to be used, providing a version of the exponential function that avoids numerical issues for inputs outside the specified safe range.", "reference_code": "def safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda _, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n"}
{"namespace": "geopoly.generate_basis", "type": "function", "class_name": null, "function_name": "generate_basis", "dependency_all": "# Intra-file Dependency:\ngeopoly.compute_sq_dist\n\ngeopoly.tesselate_geodesic\n\n", "dependency_sampled": "# Intra-file Dependency:\ngeopoly.tesselate_geodesic\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tools for constructing geodesic polyhedron, which are used as a basis.\"\"\"\n\nimport itertools\nimport numpy as np\n\n\ndef compute_sq_dist(mat0, mat1=None):\n  \"\"\"Compute the squared Euclidean distance between all pairs of columns.\"\"\"\n  if mat1 is None:\n    mat1 = mat0\n  # Use the fact that ||x - y||^2 == ||x||^2 + ||y||^2 - 2 x^T y.\n  sq_norm0 = np.sum(mat0**2, 0)\n  sq_norm1 = np.sum(mat1**2, 0)\n  sq_dist = sq_norm0[:, None] + sq_norm1[None, :] - 2 * mat0.T @ mat1\n  sq_dist = np.maximum(0, sq_dist)  # Negative values must be numerical errors.\n  return sq_dist\n\n\ndef compute_tesselation_weights(v):\n  \"\"\"Tesselate the vertices of a triangle by a factor of `v`.\"\"\"\n  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  int_weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      int_weights.append((i, j, v - (i + j)))\n  int_weights = np.array(int_weights)\n  weights = int_weights / v  # Barycentric weights.\n  return weights\n\n\ndef tesselate_geodesic(base_verts, base_faces, v, eps=1e-4):\n  \"\"\"Tesselate the vertices of a geodesic polyhedron.\n\n  Args:\n    base_verts: tensor of floats, the vertex coordinates of the geodesic.\n    base_faces: tensor of ints, the indices of the vertices of base_verts that\n      constitute eachface of the polyhedra.\n    v: int, the factor of the tesselation (v==1 is a no-op).\n    eps: float, a small value used to determine if two vertices are the same.\n\n  Returns:\n    verts: a tensor of floats, the coordinates of the tesselated vertices.\n  \"\"\"\n  if not isinstance(v, int):\n    raise ValueError(f'v {v} must an integer')\n  tri_weights = compute_tesselation_weights(v)\n\n  verts = []\n  for base_face in base_faces:\n    new_verts = np.matmul(tri_weights, base_verts[base_face, :])\n    new_verts /= np.sqrt(np.sum(new_verts**2, 1, keepdims=True))\n    verts.append(new_verts)\n  verts = np.concatenate(verts, 0)\n\n  sq_dist = compute_sq_dist(verts.T)\n  assignment = np.array([np.min(np.argwhere(d <= eps)) for d in sq_dist])\n  unique = np.unique(assignment)\n  verts = verts[unique, :]\n\n  return verts\n\n\n", "contexts_below": "", "input_code": "def generate_basis(\n    base_shape, angular_tesselation, remove_symmetries=True, eps=1e-4\n):\n\n  \"\"\"\n  Generates a 3D basis by tessellating a geometric polyhedron, optionally removing symmetric basis columns to avoid redundant projections. The function supports tessellation of tetrahedron, icosahedron, or octahedron shapes and returns a matrix representing the 3D basis.\n\n  Input-Output Arguments\n  :param base_shape: string, the name of the starting polyhedron. It must be either 'tetrahedron', 'icosahedron', or 'octahedron'. This parameter determines the initial shape to be tessellated.\n  :param angular_tesselation: int, the number of times the polyhedron is tessellated. A value of 1 means no tessellation is applied. This parameter controls the complexity of the resulting basis.\n  :param remove_symmetries: bool, optional (default=True). If True, symmetric basis columns are removed to prevent redundant negative copies in projections. This parameter affects the uniqueness of the basis vectors.\n  :param eps: float, optional (default=1e-4). A small number used to determine when two vertices are considered symmetric. This parameter helps in identifying and removing symmetries with a tolerance defined by eps.\n  :return: A matrix with shape [3, n], where n is the number of vertices after tessellation (and symmetry removal, if applicable). This matrix represents the 3D basis generated from the specified polyhedron.\n  \"\"\"", "reference_steps": "1. Define a function `generate_basis` that creates a 3D basis by tessellating a given polyhedron shape.\n2. Accept parameters `base_shape`, `angular_tesselation`, `remove_symmetries`, and `eps` to control the basis generation.\n3. Check the `base_shape` parameter to determine which polyhedron to use (tetrahedron, icosahedron, or octahedron).\n4. For each polyhedron, define the vertices (`verts`) and faces (`faces`) representing the shape in 3D space.\n5. Use the `tesselate_geodesic` function to tessellate the polyhedron based on the `angular_tesselation` parameter.\n6. If `remove_symmetries` is `True`, remove symmetric basis columns to avoid redundant negative copies in the basis.\n7. Use a small number `eps` to determine if two vertices are symmetric (reflections of each other).\n8. Filter out symmetric vertices by comparing distances and using a threshold based on `eps`.\n9. Reverse the order of the vertices' coordinates to form the final basis matrix.\n10. Return the basis matrix with shape `[3, n]`, where `n` is the number of vertices in the tessellated polyhedron.", "reference_code": "def generate_basis(\n    base_shape, angular_tesselation, remove_symmetries=True, eps=1e-4\n):\n  \"\"\"Generates a 3D basis by tesselating a geometric polyhedron.\n\n  Args:\n    base_shape: string, the name of the starting polyhedron, must be either\n      'tetrahedron', 'icosahedron' or 'octahedron'.\n    angular_tesselation: int, the number of times to tesselate the polyhedron,\n      must be >= 1 (a value of 1 is a no-op to the polyhedron).\n    remove_symmetries: bool, if True then remove the symmetric basis columns,\n      which is usually a good idea because otherwise projections onto the basis\n      will have redundant negative copies of each other.\n    eps: float, a small number used to determine symmetries.\n\n  Returns:\n    basis: a matrix with shape [3, n].\n  \"\"\"\n\n  if base_shape == 'tetrahedron':\n    verts = np.array([\n        (np.sqrt(8 / 9), 0, -1 / 3),\n        (-np.sqrt(2 / 9), np.sqrt(2 / 3), -1 / 3),\n        (-np.sqrt(2 / 9), -np.sqrt(2 / 3), -1 / 3),\n        (0, 0, 1),\n    ])\n    faces = np.array([(0, 1, 2), (0, 2, 3), (0, 1, 3), (1, 2, 3)])\n  elif base_shape == 'icosahedron':\n    a = (np.sqrt(5) + 1) / 2\n    verts = np.array([\n        (-1, 0, a),\n        (1, 0, a),\n        (-1, 0, -a),\n        (1, 0, -a),\n        (0, a, 1),\n        (0, a, -1),\n        (0, -a, 1),\n        (0, -a, -1),\n        (a, 1, 0),\n        (-a, 1, 0),\n        (a, -1, 0),\n        (-a, -1, 0),\n    ]) / np.sqrt(a + 2)\n    faces = np.array([\n        (0, 4, 1),\n        (0, 9, 4),\n        (9, 5, 4),\n        (4, 5, 8),\n        (4, 8, 1),\n        (8, 10, 1),\n        (8, 3, 10),\n        (5, 3, 8),\n        (5, 2, 3),\n        (2, 7, 3),\n        (7, 10, 3),\n        (7, 6, 10),\n        (7, 11, 6),\n        (11, 0, 6),\n        (0, 1, 6),\n        (6, 1, 10),\n        (9, 0, 11),\n        (9, 11, 2),\n        (9, 2, 5),\n        (7, 2, 11),\n    ])\n  elif base_shape == 'octahedron':\n    verts = np.array(\n        [(0, 0, -1), (0, 0, 1), (0, -1, 0), (0, 1, 0), (-1, 0, 0), (1, 0, 0)]\n    )\n    corners = np.array(list(itertools.product([-1, 1], repeat=3)))\n    pairs = np.argwhere(compute_sq_dist(corners.T, verts.T) == 2)\n    faces = np.sort(np.reshape(pairs[:, 1], [3, -1]).T, 1)\n  else:\n    raise ValueError(f'base_shape {base_shape} not supported')\n  verts = tesselate_geodesic(verts, faces, angular_tesselation)\n\n  if remove_symmetries:\n    # Remove elements of `verts` that are reflections of each other.\n    match = compute_sq_dist(verts.T, -verts.T) < eps\n    verts = verts[~np.any(np.triu(match), axis=0), :]\n\n  basis = verts[:, ::-1]\n  return basis\n"}
{"namespace": "math.safe_log1p", "type": "function", "class_name": null, "function_name": "safe_log1p", "dependency_all": "# Intra-file Dependency:\nmath.generate_safe_fn\n\nmath.max_val\n\n", "dependency_sampled": "# Intra-file Dependency:\nmath.max_val\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Mathy utility functions.\"\"\"\n\nimport functools\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ntiny_val = np.float32(np.finfo(np.float32).tiny)\nmin_val = np.float32(np.finfo(np.float32).min)\nmax_val = np.float32(np.finfo(np.float32).max)\n\n\ndef laplace_cdf(x, beta):\n  alpha = 1 / beta\n  return alpha * (0.5 + 0.5 * safe_sign(x) * (jnp.exp(-jnp.abs(x) / beta) - 1))\n\n\ndef scaled_softplus(x, scale=100.0):\n  return (1.0 / scale) * jax.nn.softplus(scale * x)\n\n\ndef matmul(a, b):\n  \"\"\"jnp.matmul defaults to bfloat16, but this helper function doesn't.\"\"\"\n  return jnp.matmul(a, b, precision=jax.lax.Precision.HIGHEST)\n\n\ndef unstack(x, axis=0):\n  return tuple(\n      jnp.squeeze(z, axis=axis) for z in jnp.split(x, x.shape[axis], axis=axis)\n  )\n\n\n@jax.custom_jvp\ndef plus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )\n\n\n@jax.custom_jvp\ndef minus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n\n@plus_eps.defjvp\ndef plus_eps_jvp(primals, tangents):\n  \"\"\"Make plus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return plus_eps(*primals), tangents[0]\n\n\n@minus_eps.defjvp\ndef minus_eps_jvp(primals, tangents):\n  \"\"\"Make minus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return minus_eps(*primals), tangents[0]\n\n\n@jax.custom_jvp\ndef expm1(x):\n  \"\"\"jnp.expm1() has inaccurate gradients when x << 0, this doesn't.\"\"\"\n  return jnp.expm1(x)\n\n\n@expm1.defjvp\ndef expm1_jvp(primals, tangents):\n  return expm1(*primals), tangents[0] * jnp.exp(primals[0])\n\n\ndef safe_trig_helper(x, fn, t=100 * jnp.pi):\n  \"\"\"Helper function used by safe_cos/safe_sin: mods x before sin()/cos().\"\"\"\n  return fn(jnp.nan_to_num(jnp.where(jnp.abs(x) < t, x, x % t)))\n\n\ndef safe_cos(x):\n  \"\"\"jnp.cos() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.cos)\n\n\ndef safe_sin(x):\n  \"\"\"jnp.sin() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.sin)\n\n\n@jax.custom_vjp\ndef safe_arctan2(x1, x2):\n  return safe_arctan2_fwd(x1, x2)[0]\n\n\ndef safe_arctan2_fwd(x1, x2):\n  return jnp.arctan2(x1, x2), (x1, x2)\n\n\ndef safe_arctan2_bwd(res, g):\n  x1, x2 = res\n  denom = remove_zero(x1**2 + x2**2)\n  d1 = g * (x2 / denom)\n  d2 = g * (-x1 / denom)\n  return d1, d2\n\n\nsafe_arctan2.defvjp(safe_arctan2_fwd, safe_arctan2_bwd)\n\n\ndef generate_clip_nograd_fn(a_min, a_max):\n  \"\"\"Generates a function that clips to [a_min, a_max] with no grad effects.\"\"\"\n\n  @jax.custom_jvp\n  def clip_nograd(a):\n    \"\"\"Clamps `a` from above and below.\"\"\"\n    return jnp.clip(a, a_min, a_max)\n\n  @clip_nograd.defjvp\n  def clip_nograd_jvp(primals, tangents):\n    \"\"\"Override clips()'s gradient to be a no-op.\"\"\"\n    return clip_nograd(primals[0]), tangents[0]\n\n  return clip_nograd\n\n\nclip_finite_nograd = generate_clip_nograd_fn(min_val, max_val)\n\nclip_pos_finite_nograd = generate_clip_nograd_fn(tiny_val, max_val)\n\n\ndef clip_pos(x):\n  \"\"\"Clamps `x` from below to be positive.\"\"\"\n  return jnp.maximum(tiny_val, x)\n\n\ndef safe_sign(x):\n  \"\"\"jnp.sign(x) except x=0 is assumed to have a sign of +1, not 0.\"\"\"\n  return jnp.where(x < 0, -1, +1)\n\n\ndef remove_zero(x):\n  \"\"\"Shifts `x` away from 0.\"\"\"\n  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, x)\n\n\ndef clip_finite(x):\n  return jnp.clip(x, min_val, max_val)\n\n\n@jax.custom_vjp\ndef safe_div(n, d):\n  \"\"\"Divide `n` by `d` but the value and gradient never nan out.\"\"\"\n  return safe_div_fwd(n, d)[0]\n\n\ndef safe_div_fwd(n, d):\n  r = jnp.clip(n / remove_zero(d), min_val, max_val)\n  return jnp.where(jnp.abs(d) < tiny_val, 0, r), (d, r)\n\n\ndef safe_div_bwd(res, g):\n  d, r = res\n  dn = jnp.clip(g / remove_zero(d), min_val, max_val)\n  dd = jnp.clip(-g * r / remove_zero(d), min_val, max_val)\n  return dn, dd\n\n\nsafe_div.defvjp(safe_div_fwd, safe_div_bwd)\n\n\ndef generate_safe_fn(fn, grad_fn, x_range):\n  \"\"\"Generate's a `safe` fn() where inputs are clipped in fwd and bwd passes.\"\"\"\n\n  @jax.custom_jvp\n  def safe_fn(x):\n    \"\"\"fn() with clipped inputs.\"\"\"\n    return fn(jnp.clip(x, *x_range))\n\n  @safe_fn.defjvp\n  def safe_fn_jvp(primals, tangents):\n    \"\"\"Backpropagate using the gradient and clipped inputs.\"\"\"\n    (x,) = primals\n    (x_dot,) = tangents\n    y = safe_fn(x)\n    y_dot = grad_fn(jnp.clip(x, *x_range), y, x_dot)\n    return y, y_dot\n\n  return safe_fn\n\n\n# These safe_* functions need to be wrapped in no-op function definitions for\n# gin to recognize them, otherwise they could just be calls to generate_safe_fn.\n\n\ndef safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)\n\n\ndef safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda _, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\n", "contexts_below": "\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: (x, None), lambda _, y: (grad_fn(y),))\n  return fn_out\n\n\ndef select(cond_pairs, default):\n  \"\"\"A helpful wrapper around jnp.select() that is easier to read.\"\"\"\n  return jnp.select(*zip(*cond_pairs), default)\n\n\ndef power_ladder_max_output(p):\n  \"\"\"The limit of power_ladder(x, p) as x goes to infinity.\"\"\"\n  return select(\n      [\n          (p == -jnp.inf, 1),\n          (p >= 0, jnp.inf),\n      ],\n      safe_div(p - 1, p),\n  )\n\n\ndef power_ladder(x, p, premult=None, postmult=None):\n  \"\"\"Tukey's power ladder, with a +1 on x, some scaling, and special cases.\"\"\"\n  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    x = x * premult\n  xp = jnp.abs(x)\n  xs = xp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((xs + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    y = y * postmult\n  return y\n\n\ndef inv_power_ladder(y, p, premult=None, postmult=None):\n  \"\"\"The inverse of `power_ladder()`.\"\"\"\n  if postmult is not None:\n    y /= postmult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  yp = override_gradient(jnp.clip(yp, -y_max, y_max), yp)  # Clip val, not grad.\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * yp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if premult is not None:\n    x /= premult\n  return x\n\n\ndef log_lerp(t, v0, v1):\n  \"\"\"Interpolate log-linearly from `v0` (t=0) to `v1` (t=1).\"\"\"\n  if v0 <= 0 or v1 <= 0:\n    raise ValueError(f'Interpolants {v0} and {v1} must be positive.')\n  lv0 = jnp.log(v0)\n  lv1 = jnp.log(v1)\n  return jnp.exp(jnp.clip(t, 0, 1) * (lv1 - lv0) + lv0)\n\n\ndef approx_erf(x):\n  \"\"\"An approximation of erf() that is accurate to within 0.007.\"\"\"\n  return jnp.sign(x) * jnp.sqrt(1 - jnp.exp(-(4 / jnp.pi) * x**2))\n\n\ndef create_learning_rate_decay(**kwargs):\n  \"\"\"A partial evaluation of learning rate decay that can be used with gin.\"\"\"\n  return functools.partial(learning_rate_decay, **kwargs)\n\n\ndef learning_rate_decay(\n    step, lr_init, lr_final, max_steps, lr_delay_steps=0, lr_delay_mult=1\n):\n  \"\"\"Continuous learning rate decay function.\n\n  The returned rate is lr_init when step=0 and lr_final when step=max_steps, and\n  is log-linearly interpolated elsewhere (equivalent to exponential decay).\n  If lr_delay_steps>0 then the learning rate will be scaled by some smooth\n  function of lr_delay_mult, such that the initial learning rate is\n  lr_init*lr_delay_mult at the beginning of optimization but will be eased back\n  to the normal learning rate when steps>lr_delay_steps.\n\n  Args:\n    step: int, the current optimization step.\n    lr_init: float, the initial learning rate.\n    lr_final: float, the final learning rate.\n    max_steps: int, the number of steps during optimization.\n    lr_delay_steps: int, the number of steps to delay the full learning rate.\n    lr_delay_mult: float, the multiplier on the rate when delaying it.\n\n  Returns:\n    lr: the learning for current step 'step'.\n  \"\"\"\n  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  return delay_rate * log_lerp(step / max_steps, lr_init, lr_final)\n\n\ndef sorted_lookup(x, xp, fps, device_is_tpu):\n  \"\"\"Lookup `x` into locations `xp` , return indices and each `[fp]` value.\"\"\"\n  if not isinstance(fps, tuple):\n    raise ValueError(f'Input `fps` must be a tuple, but is {type(fps)}.')\n\n  if device_is_tpu:\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[Ellipsis, None, :] >= xp[Ellipsis, :, None]\n\n    def find_interval(x):\n      # Grab the value where `mask` switches from True to False, and vice versa.\n      # This approach takes advantage of the fact that `x` is sorted.\n      x0 = jnp.max(jnp.where(mask, x[Ellipsis, None], x[Ellipsis, :1, None]), -2)\n      x1 = jnp.min(jnp.where(~mask, x[Ellipsis, None], x[Ellipsis, -1:, None]), -2)\n      return x0, x1\n\n    idx0, idx1 = find_interval(jnp.arange(xp.shape[-1]))\n    vals = [find_interval(fp) for fp in fps]\n  else:\n    # jnp.searchsorted() has slightly different conventions for boundary\n    # handling than the rest of this codebase.\n    idx = jnp.vectorize(\n        lambda a, v: jnp.searchsorted(a, v, side='right'),\n        signature='(n),(m)->(m)',\n    )(xp, x)\n    idx1 = jnp.minimum(idx, xp.shape[-1] - 1)\n    idx0 = jnp.maximum(idx - 1, 0)\n    vals = []\n    for fp in fps:\n      fp0 = jnp.take_along_axis(fp, idx0, axis=-1)\n      fp1 = jnp.take_along_axis(fp, idx1, axis=-1)\n      vals.append((fp0, fp1))\n  return (idx0, idx1), vals\n\n\ndef sorted_interp(\n    x, xp, fp, device_is_tpu, eps=jnp.finfo(jnp.float32).eps ** 2\n):\n  \"\"\"A version of interp() where xp and fp must be sorted.\"\"\"\n  (xp0, xp1), (fp0, fp1) = sorted_lookup(\n      x, xp, (xp, fp), device_is_tpu=device_is_tpu\n  )[1]\n  offset = jnp.clip((x - xp0) / jnp.maximum(eps, xp1 - xp0), 0, 1)\n  ret = fp0 + offset * (fp1 - fp0)\n  return ret\n\n\ndef searchsorted(a, v, device_is_tpu):\n  \"\"\"Behaves like jnp.searchsorted, excluding boundary conditions.\"\"\"\n  return sorted_lookup(v, a, (), device_is_tpu=device_is_tpu)[0]\n\n\ndef override_gradient(fval, bval):\n  \"\"\"Use `fval` in the forward pass but `bval` in the backward pass.\"\"\"\n  # Note that the parentheses are needed to avoid catastrophic cancellation.\n  return jax.lax.stop_gradient(fval) + (bval - jax.lax.stop_gradient(bval))\n\n\ndef average_across_multisamples(x):\n  \"\"\"Function that averages grid query results across the multisample dimension.\"\"\"\n  return jnp.mean(x, axis=-2)\n\n\ndef noop(x):\n  return x\n\n\n@jax.custom_jvp\ndef fake_clip(a, a_min, a_max):\n  \"\"\"jnp.clip() but the gradient doesn't get clipped on the backward pass.\"\"\"\n  return jnp.clip(a, a_min, a_max)\n\n\n@fake_clip.defjvp\ndef fake_clip_jvp(primals, tangents):\n  \"\"\"Override fake_clip()'s gradient so that it's a no-op.\"\"\"\n  return jnp.clip(*primals), tangents[0]\n\n\n@jax.jit\ndef general_lossfun(x, alpha, scale):\n  r\"\"\"This implements the rho(x, \\alpha, c) function described in \"A General and\n  Adaptive Robust Loss Function\", Jonathan T. Barron,\n  https://arxiv.org/abs/1701.03077.\n\n  Args:\n    x: The residual for which the loss is being computed. x can have any shape,\n      and alpha and scale will be broadcasted to match x's shape if necessary.\n    alpha: The shape parameter of the loss (\\alpha in the paper), where more\n      negative values produce a loss with more robust behavior (outliers \"cost\"\n      less), and more positive values produce a loss with less robust behavior\n      (outliers are penalized more heavily). Alpha can be any value in\n      [-infinity, infinity], but the gradient of the loss with respect to alpha\n      is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n      interpolation between several discrete robust losses:\n        alpha=-Infinity: Welsch/Leclerc Loss.\n        alpha=-2: Geman-McClure loss.\n        alpha=0: Cauchy/Lortentzian loss.\n        alpha=1: Charbonnier/pseudo-Huber loss.\n        alpha=2: L2 loss.\n    scale: The scale parameter of the loss. When |x| < scale, the loss is an\n      L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n      different shape according to alpha.\n\n  Returns:\n    The losses for each element of x, in the same shape as x.\n  \"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  maxval = 1e15\n\n  # A \"safe\" versions of expm1 that will not NaN-out on large inputs.\n  expm1_safe = lambda x: jnp.expm1(jnp.minimum(x, 43))\n\n  # `scale` must be > 0.\n  scale = jnp.maximum(eps, scale)\n\n  # Large values of |x| can cause non-finite gradients.\n  x = fake_clip(x, -maxval, maxval)\n\n  # The loss when alpha == 2. This will get reused repeatedly.\n  loss_two = 0.5 * (x / scale)**2\n\n  # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n  a = jnp.where(alpha >= 0, jnp.ones_like(alpha),\n                -jnp.ones_like(alpha)) * jnp.maximum(eps, jnp.abs(alpha))\n\n  # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n  b = jnp.maximum(eps, jnp.abs(a - 2))\n\n  # The loss when not in one of the special casess.\n  loss_ow = (b / a) * ((loss_two / (0.5 * b) + 1)**(0.5 * a) - 1)\n\n  # Select which of the cases of the loss to return as a function of alpha.\n  return jnp.where(\n      alpha == -jnp.inf, -expm1_safe(-loss_two),\n      jnp.where(\n          alpha == 0, jnp.log1p(loss_two),\n          jnp.where(alpha == 2, loss_two,\n                    jnp.where(alpha == jnp.inf, expm1_safe(loss_two),\n                              loss_ow))))\n", "input_code": "def safe_log1p(x):\n\n  \"\"\"\n  The function creates a safe version of the log1p function, which calculates the natural logarithm of 1 plus the input value, x. It ensures that the input value is within a safe range to avoid numerical errors or undefined behavior. The function also specifies how the derivative of this operation should be computed for automatic differentiation.\n\n  Input-Output Arguments\n  :param x: The input value for which the natural logarithm of 1 plus x is computed. \n  :return: The result of the safe log1p operation on the input value x.\n  \"\"\"", "reference_steps": "1. Define a function `safe_log1p` that takes an input `x`.\n2. Inside the function, call another function `generate_safe_fn` with three arguments.\n3. The first argument to `generate_safe_fn` is `jnp.log1p`, which is the JAX implementation of the `log1p` function that computes `log(1+x)` in a numerically stable way.\n4. The second argument is a lambda function that takes three parameters `x`, `_`, and `x_dot`. This lambda represents the derivative of `log1p` and returns `x_dot / (1 + x)`.\n5. The third argument is a tuple defining the valid input range for `x`. The range starts from `np.nextafter(np.float32(-1), np.float32(0))`, which is the smallest float32 value greater than -1, ensuring numerical stability for the log operation.\n6. The upper bound of the range is `max_val`, which is not defined within the given code snippet but is assumed to be a previously defined variable representing the maximum valid input value.\n7. The `generate_safe_fn` function presumably returns a new function that is a \"safe\" version of `log1p`, which can handle edge cases and derivatives properly within the specified range.\n8. The returned function from `generate_safe_fn` is immediately called with the input `x`.\n9. The result of this call, which is the safe computation of `log1p(x)`, is returned by the `safe_log1p` function.\n10. The `safe_log1p` function can be used to compute the logarithm of `1+x` in a numerically stable way for inputs within the specified range.", "reference_code": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n"}
{"namespace": "math.learning_rate_decay", "type": "function", "class_name": null, "function_name": "learning_rate_decay", "dependency_all": "# Intra-file Dependency:\nmath.log_lerp\n\n", "dependency_sampled": "# Intra-file Dependency:\nmath.log_lerp\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Mathy utility functions.\"\"\"\n\nimport functools\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ntiny_val = np.float32(np.finfo(np.float32).tiny)\nmin_val = np.float32(np.finfo(np.float32).min)\nmax_val = np.float32(np.finfo(np.float32).max)\n\n\ndef laplace_cdf(x, beta):\n  alpha = 1 / beta\n  return alpha * (0.5 + 0.5 * safe_sign(x) * (jnp.exp(-jnp.abs(x) / beta) - 1))\n\n\ndef scaled_softplus(x, scale=100.0):\n  return (1.0 / scale) * jax.nn.softplus(scale * x)\n\n\ndef matmul(a, b):\n  \"\"\"jnp.matmul defaults to bfloat16, but this helper function doesn't.\"\"\"\n  return jnp.matmul(a, b, precision=jax.lax.Precision.HIGHEST)\n\n\ndef unstack(x, axis=0):\n  return tuple(\n      jnp.squeeze(z, axis=axis) for z in jnp.split(x, x.shape[axis], axis=axis)\n  )\n\n\n@jax.custom_jvp\ndef plus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )\n\n\n@jax.custom_jvp\ndef minus_eps(x):\n  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n\n@plus_eps.defjvp\ndef plus_eps_jvp(primals, tangents):\n  \"\"\"Make plus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return plus_eps(*primals), tangents[0]\n\n\n@minus_eps.defjvp\ndef minus_eps_jvp(primals, tangents):\n  \"\"\"Make minus_eps()'s gradient a no-op (nextafter's gradient is undefined).\"\"\"\n  return minus_eps(*primals), tangents[0]\n\n\n@jax.custom_jvp\ndef expm1(x):\n  \"\"\"jnp.expm1() has inaccurate gradients when x << 0, this doesn't.\"\"\"\n  return jnp.expm1(x)\n\n\n@expm1.defjvp\ndef expm1_jvp(primals, tangents):\n  return expm1(*primals), tangents[0] * jnp.exp(primals[0])\n\n\ndef safe_trig_helper(x, fn, t=100 * jnp.pi):\n  \"\"\"Helper function used by safe_cos/safe_sin: mods x before sin()/cos().\"\"\"\n  return fn(jnp.nan_to_num(jnp.where(jnp.abs(x) < t, x, x % t)))\n\n\ndef safe_cos(x):\n  \"\"\"jnp.cos() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.cos)\n\n\ndef safe_sin(x):\n  \"\"\"jnp.sin() on a TPU may NaN out for large values.\"\"\"\n  return safe_trig_helper(x, jnp.sin)\n\n\n@jax.custom_vjp\ndef safe_arctan2(x1, x2):\n  return safe_arctan2_fwd(x1, x2)[0]\n\n\ndef safe_arctan2_fwd(x1, x2):\n  return jnp.arctan2(x1, x2), (x1, x2)\n\n\ndef safe_arctan2_bwd(res, g):\n  x1, x2 = res\n  denom = remove_zero(x1**2 + x2**2)\n  d1 = g * (x2 / denom)\n  d2 = g * (-x1 / denom)\n  return d1, d2\n\n\nsafe_arctan2.defvjp(safe_arctan2_fwd, safe_arctan2_bwd)\n\n\ndef generate_clip_nograd_fn(a_min, a_max):\n  \"\"\"Generates a function that clips to [a_min, a_max] with no grad effects.\"\"\"\n\n  @jax.custom_jvp\n  def clip_nograd(a):\n    \"\"\"Clamps `a` from above and below.\"\"\"\n    return jnp.clip(a, a_min, a_max)\n\n  @clip_nograd.defjvp\n  def clip_nograd_jvp(primals, tangents):\n    \"\"\"Override clips()'s gradient to be a no-op.\"\"\"\n    return clip_nograd(primals[0]), tangents[0]\n\n  return clip_nograd\n\n\nclip_finite_nograd = generate_clip_nograd_fn(min_val, max_val)\n\nclip_pos_finite_nograd = generate_clip_nograd_fn(tiny_val, max_val)\n\n\ndef clip_pos(x):\n  \"\"\"Clamps `x` from below to be positive.\"\"\"\n  return jnp.maximum(tiny_val, x)\n\n\ndef safe_sign(x):\n  \"\"\"jnp.sign(x) except x=0 is assumed to have a sign of +1, not 0.\"\"\"\n  return jnp.where(x < 0, -1, +1)\n\n\ndef remove_zero(x):\n  \"\"\"Shifts `x` away from 0.\"\"\"\n  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, x)\n\n\ndef clip_finite(x):\n  return jnp.clip(x, min_val, max_val)\n\n\n@jax.custom_vjp\ndef safe_div(n, d):\n  \"\"\"Divide `n` by `d` but the value and gradient never nan out.\"\"\"\n  return safe_div_fwd(n, d)[0]\n\n\ndef safe_div_fwd(n, d):\n  r = jnp.clip(n / remove_zero(d), min_val, max_val)\n  return jnp.where(jnp.abs(d) < tiny_val, 0, r), (d, r)\n\n\ndef safe_div_bwd(res, g):\n  d, r = res\n  dn = jnp.clip(g / remove_zero(d), min_val, max_val)\n  dd = jnp.clip(-g * r / remove_zero(d), min_val, max_val)\n  return dn, dd\n\n\nsafe_div.defvjp(safe_div_fwd, safe_div_bwd)\n\n\ndef generate_safe_fn(fn, grad_fn, x_range):\n  \"\"\"Generate's a `safe` fn() where inputs are clipped in fwd and bwd passes.\"\"\"\n\n  @jax.custom_jvp\n  def safe_fn(x):\n    \"\"\"fn() with clipped inputs.\"\"\"\n    return fn(jnp.clip(x, *x_range))\n\n  @safe_fn.defjvp\n  def safe_fn_jvp(primals, tangents):\n    \"\"\"Backpropagate using the gradient and clipped inputs.\"\"\"\n    (x,) = primals\n    (x_dot,) = tangents\n    y = safe_fn(x)\n    y_dot = grad_fn(jnp.clip(x, *x_range), y, x_dot)\n    return y, y_dot\n\n  return safe_fn\n\n\n# These safe_* functions need to be wrapped in no-op function definitions for\n# gin to recognize them, otherwise they could just be calls to generate_safe_fn.\n\n\ndef safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)\n\n\ndef safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda _, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: (x, None), lambda _, y: (grad_fn(y),))\n  return fn_out\n\n\ndef select(cond_pairs, default):\n  \"\"\"A helpful wrapper around jnp.select() that is easier to read.\"\"\"\n  return jnp.select(*zip(*cond_pairs), default)\n\n\ndef power_ladder_max_output(p):\n  \"\"\"The limit of power_ladder(x, p) as x goes to infinity.\"\"\"\n  return select(\n      [\n          (p == -jnp.inf, 1),\n          (p >= 0, jnp.inf),\n      ],\n      safe_div(p - 1, p),\n  )\n\n\ndef power_ladder(x, p, premult=None, postmult=None):\n  \"\"\"Tukey's power ladder, with a +1 on x, some scaling, and special cases.\"\"\"\n  # Compute sign(x) * |p - 1|/p * ((|x|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    x = x * premult\n  xp = jnp.abs(x)\n  xs = xp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((xs + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    y = y * postmult\n  return y\n\n\ndef inv_power_ladder(y, p, premult=None, postmult=None):\n  \"\"\"The inverse of `power_ladder()`.\"\"\"\n  if postmult is not None:\n    y /= postmult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  yp = override_gradient(jnp.clip(yp, -y_max, y_max), yp)  # Clip val, not grad.\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * yp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if premult is not None:\n    x /= premult\n  return x\n\n\ndef log_lerp(t, v0, v1):\n  \"\"\"Interpolate log-linearly from `v0` (t=0) to `v1` (t=1).\"\"\"\n  if v0 <= 0 or v1 <= 0:\n    raise ValueError(f'Interpolants {v0} and {v1} must be positive.')\n  lv0 = jnp.log(v0)\n  lv1 = jnp.log(v1)\n  return jnp.exp(jnp.clip(t, 0, 1) * (lv1 - lv0) + lv0)\n\n\ndef approx_erf(x):\n  \"\"\"An approximation of erf() that is accurate to within 0.007.\"\"\"\n  return jnp.sign(x) * jnp.sqrt(1 - jnp.exp(-(4 / jnp.pi) * x**2))\n\n\ndef create_learning_rate_decay(**kwargs):\n  \"\"\"A partial evaluation of learning rate decay that can be used with gin.\"\"\"\n  return functools.partial(learning_rate_decay, **kwargs)\n\n\n", "contexts_below": "\n\ndef sorted_lookup(x, xp, fps, device_is_tpu):\n  \"\"\"Lookup `x` into locations `xp` , return indices and each `[fp]` value.\"\"\"\n  if not isinstance(fps, tuple):\n    raise ValueError(f'Input `fps` must be a tuple, but is {type(fps)}.')\n\n  if device_is_tpu:\n    # Identify the location in `xp` that corresponds to each `x`.\n    # The final `True` index in `mask` is the start of the matching interval.\n    mask = x[Ellipsis, None, :] >= xp[Ellipsis, :, None]\n\n    def find_interval(x):\n      # Grab the value where `mask` switches from True to False, and vice versa.\n      # This approach takes advantage of the fact that `x` is sorted.\n      x0 = jnp.max(jnp.where(mask, x[Ellipsis, None], x[Ellipsis, :1, None]), -2)\n      x1 = jnp.min(jnp.where(~mask, x[Ellipsis, None], x[Ellipsis, -1:, None]), -2)\n      return x0, x1\n\n    idx0, idx1 = find_interval(jnp.arange(xp.shape[-1]))\n    vals = [find_interval(fp) for fp in fps]\n  else:\n    # jnp.searchsorted() has slightly different conventions for boundary\n    # handling than the rest of this codebase.\n    idx = jnp.vectorize(\n        lambda a, v: jnp.searchsorted(a, v, side='right'),\n        signature='(n),(m)->(m)',\n    )(xp, x)\n    idx1 = jnp.minimum(idx, xp.shape[-1] - 1)\n    idx0 = jnp.maximum(idx - 1, 0)\n    vals = []\n    for fp in fps:\n      fp0 = jnp.take_along_axis(fp, idx0, axis=-1)\n      fp1 = jnp.take_along_axis(fp, idx1, axis=-1)\n      vals.append((fp0, fp1))\n  return (idx0, idx1), vals\n\n\ndef sorted_interp(\n    x, xp, fp, device_is_tpu, eps=jnp.finfo(jnp.float32).eps ** 2\n):\n  \"\"\"A version of interp() where xp and fp must be sorted.\"\"\"\n  (xp0, xp1), (fp0, fp1) = sorted_lookup(\n      x, xp, (xp, fp), device_is_tpu=device_is_tpu\n  )[1]\n  offset = jnp.clip((x - xp0) / jnp.maximum(eps, xp1 - xp0), 0, 1)\n  ret = fp0 + offset * (fp1 - fp0)\n  return ret\n\n\ndef searchsorted(a, v, device_is_tpu):\n  \"\"\"Behaves like jnp.searchsorted, excluding boundary conditions.\"\"\"\n  return sorted_lookup(v, a, (), device_is_tpu=device_is_tpu)[0]\n\n\ndef override_gradient(fval, bval):\n  \"\"\"Use `fval` in the forward pass but `bval` in the backward pass.\"\"\"\n  # Note that the parentheses are needed to avoid catastrophic cancellation.\n  return jax.lax.stop_gradient(fval) + (bval - jax.lax.stop_gradient(bval))\n\n\ndef average_across_multisamples(x):\n  \"\"\"Function that averages grid query results across the multisample dimension.\"\"\"\n  return jnp.mean(x, axis=-2)\n\n\ndef noop(x):\n  return x\n\n\n@jax.custom_jvp\ndef fake_clip(a, a_min, a_max):\n  \"\"\"jnp.clip() but the gradient doesn't get clipped on the backward pass.\"\"\"\n  return jnp.clip(a, a_min, a_max)\n\n\n@fake_clip.defjvp\ndef fake_clip_jvp(primals, tangents):\n  \"\"\"Override fake_clip()'s gradient so that it's a no-op.\"\"\"\n  return jnp.clip(*primals), tangents[0]\n\n\n@jax.jit\ndef general_lossfun(x, alpha, scale):\n  r\"\"\"This implements the rho(x, \\alpha, c) function described in \"A General and\n  Adaptive Robust Loss Function\", Jonathan T. Barron,\n  https://arxiv.org/abs/1701.03077.\n\n  Args:\n    x: The residual for which the loss is being computed. x can have any shape,\n      and alpha and scale will be broadcasted to match x's shape if necessary.\n    alpha: The shape parameter of the loss (\\alpha in the paper), where more\n      negative values produce a loss with more robust behavior (outliers \"cost\"\n      less), and more positive values produce a loss with less robust behavior\n      (outliers are penalized more heavily). Alpha can be any value in\n      [-infinity, infinity], but the gradient of the loss with respect to alpha\n      is 0 at -infinity, infinity, 0, and 2. Varying alpha allows for smooth\n      interpolation between several discrete robust losses:\n        alpha=-Infinity: Welsch/Leclerc Loss.\n        alpha=-2: Geman-McClure loss.\n        alpha=0: Cauchy/Lortentzian loss.\n        alpha=1: Charbonnier/pseudo-Huber loss.\n        alpha=2: L2 loss.\n    scale: The scale parameter of the loss. When |x| < scale, the loss is an\n      L2-like quadratic bowl, and when |x| > scale the loss function takes on a\n      different shape according to alpha.\n\n  Returns:\n    The losses for each element of x, in the same shape as x.\n  \"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  maxval = 1e15\n\n  # A \"safe\" versions of expm1 that will not NaN-out on large inputs.\n  expm1_safe = lambda x: jnp.expm1(jnp.minimum(x, 43))\n\n  # `scale` must be > 0.\n  scale = jnp.maximum(eps, scale)\n\n  # Large values of |x| can cause non-finite gradients.\n  x = fake_clip(x, -maxval, maxval)\n\n  # The loss when alpha == 2. This will get reused repeatedly.\n  loss_two = 0.5 * (x / scale)**2\n\n  # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n  a = jnp.where(alpha >= 0, jnp.ones_like(alpha),\n                -jnp.ones_like(alpha)) * jnp.maximum(eps, jnp.abs(alpha))\n\n  # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n  b = jnp.maximum(eps, jnp.abs(a - 2))\n\n  # The loss when not in one of the special casess.\n  loss_ow = (b / a) * ((loss_two / (0.5 * b) + 1)**(0.5 * a) - 1)\n\n  # Select which of the cases of the loss to return as a function of alpha.\n  return jnp.where(\n      alpha == -jnp.inf, -expm1_safe(-loss_two),\n      jnp.where(\n          alpha == 0, jnp.log1p(loss_two),\n          jnp.where(alpha == 2, loss_two,\n                    jnp.where(alpha == jnp.inf, expm1_safe(loss_two),\n                              loss_ow))))\n", "input_code": "def learning_rate_decay(\n    step, lr_init, lr_final, max_steps, lr_delay_steps=0, lr_delay_mult=1\n):\n\n  \"\"\"\n  This function calculates the learning rate at a given optimization step with the option to include a delay at the start. It uses a log-linear interpolation (or exponential decay) between an initial and final learning rate over a specified number of steps. If a delay is specified, the initial learning rate is scaled down by a multiplier and gradually returns to the normal rate after the delay period.\n\n  Input-Output Arguments\n  :param step: int, the current optimization step, used to determine the learning rate based on the progression of steps.\n  :param lr_init: float, the initial learning rate at the start of optimization.\n  :param lr_final: float, the final learning rate to be reached at the end of optimization.\n  :param max_steps: int, the total number of steps in the optimization process, used to calculate the progression and the learning rate decay.\n  :param lr_delay_steps: int, optional, the number of steps to delay before applying the full learning rate, allows for a gradual increase to the initial learning rate.\n  :param lr_delay_mult: float, optional, the multiplier applied to the learning rate during the delay period, affects the starting learning rate when a delay is applied.\n  :return: float, the calculated learning rate for the current step, adjusted for any specified delay.\n  \"\"\"", "reference_steps": "1. Define a function `learning_rate_decay` that takes in the current optimization step, initial and final learning rates, maximum number of steps, and optional parameters for learning rate delay steps and multiplier.\n2. Check if there is a delay in applying the full learning rate (`lr_delay_steps > 0`).\n3. If there is a delay, calculate `delay_rate` using a smooth function that scales the initial learning rate by `lr_delay_mult` and gradually increases to the normal rate using a reverse cosine decay function.\n4. If there is no delay, set `delay_rate` to 1.0.\n5. Calculate the fraction of the way through the training process (`step / max_steps`).\n6. Define a function `log_lerp` (not provided in the reference code) that performs log-linear interpolation between `lr_init` and `lr_final` based on the fraction calculated in step 5.\n7. Multiply `delay_rate` by the interpolated learning rate from `log_lerp` to get the adjusted learning rate for the current step.\n8. Return the adjusted learning rate as the output of the function `learning_rate_decay`.\n9. The function will return `lr_init` when `step=0` and `lr_final` when `step=max_steps`, with an exponential decay between these two points.\n10. If learning rate delay is enabled, the initial learning rate will be `lr_init * lr_delay_mult` and will transition to `lr_init` over `lr_delay_steps` steps.", "reference_code": "def learning_rate_decay(\n    step, lr_init, lr_final, max_steps, lr_delay_steps=0, lr_delay_mult=1\n):\n  \"\"\"Continuous learning rate decay function.\n\n  The returned rate is lr_init when step=0 and lr_final when step=max_steps, and\n  is log-linearly interpolated elsewhere (equivalent to exponential decay).\n  If lr_delay_steps>0 then the learning rate will be scaled by some smooth\n  function of lr_delay_mult, such that the initial learning rate is\n  lr_init*lr_delay_mult at the beginning of optimization but will be eased back\n  to the normal learning rate when steps>lr_delay_steps.\n\n  Args:\n    step: int, the current optimization step.\n    lr_init: float, the initial learning rate.\n    lr_final: float, the final learning rate.\n    max_steps: int, the number of steps during optimization.\n    lr_delay_steps: int, the number of steps to delay the full learning rate.\n    lr_delay_mult: float, the multiplier on the rate when delaying it.\n\n  Returns:\n    lr: the learning for current step 'step'.\n  \"\"\"\n  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  return delay_rate * log_lerp(step / max_steps, lr_init, lr_final)\n"}
{"namespace": "utils.dummy_rays", "type": "function", "class_name": null, "function_name": "dummy_rays", "dependency_all": "# Intra-file Dependency:\nutils.generate_random_rays\n\n", "dependency_sampled": "# Intra-file Dependency:\nutils.generate_random_rays\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utility functions.\"\"\"\n\nimport concurrent\nimport enum\nimport os\nimport queue\nimport threading\nimport time\nfrom typing import Any, Callable, Iterable, Optional, TypeVar, Union\n\nfrom absl import logging\nimport flax\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\n\n\n_Array = Union[np.ndarray, jnp.ndarray]\n\n\n@flax.struct.dataclass\nclass Rays:\n  \"\"\"All tensors must have the same num_dims and first n-1 dims must match.\n\n  This dataclass contains spatially meaningful quantities associated with\n  the ray that can be calculated by the function casting the ray, as well as\n  all metadata necessary for the ray to be rendered by the Model class.\n  \"\"\"\n\n  origins: Optional[_Array] = None\n  directions: Optional[_Array] = None\n  viewdirs: Optional[_Array] = None\n  radii: Optional[_Array] = None\n  imageplane: Optional[_Array] = None\n  pixels: Optional[_Array] = None\n  lossmult: Optional[_Array] = None\n  near: Optional[_Array] = None\n  far: Optional[_Array] = None\n  cam_idx: Optional[_Array] = None\n  exposure_idx: Optional[_Array] = None\n  exposure_values: Optional[_Array] = None\n  device_idx: Optional[_Array] = None\n\n\ndef generate_random_rays(\n    rng,\n    n,\n    origin_lo,\n    origin_hi,\n    radius_lo,\n    radius_hi,\n    near_lo,\n    near_hi,\n    far_lo,\n    far_hi,\n    include_exposure_idx = False,\n    include_exposure_values = False,\n    include_device_idx = False,\n):\n  \"\"\"Generate a random Rays datastructure.\"\"\"\n  key, rng = random.split(rng)\n  origins = random.uniform(\n      key, shape=[n, 3], minval=origin_lo, maxval=origin_hi\n  )\n\n  key, rng = random.split(rng)\n  directions = random.normal(key, shape=[n, 3])\n  directions /= jnp.sqrt(\n      jnp.maximum(\n          jnp.finfo(jnp.float32).tiny,\n          jnp.sum(directions**2, axis=-1, keepdims=True),\n      )\n  )\n\n  viewdirs = directions\n\n  key, rng = random.split(rng)\n  radii = random.uniform(key, shape=[n, 1], minval=radius_lo, maxval=radius_hi)\n\n  key, rng = random.split(rng)\n  near = random.uniform(key, shape=[n, 1], minval=near_lo, maxval=near_hi)\n\n  key, rng = random.split(rng)\n  far = random.uniform(key, shape=[n, 1], minval=far_lo, maxval=far_hi)\n\n  imageplane = jnp.zeros([n, 2])\n  lossmult = jnp.zeros([n, 1])\n\n  key, rng = random.split(rng)\n  pixels = random.randint(key, shape=[n, 2], minval=0, maxval=1024)\n\n  int_scalar = jnp.int32(jnp.zeros([n, 1]))\n\n  exposure_kwargs = {}\n  if include_exposure_idx:\n    exposure_kwargs['exposure_idx'] = int_scalar\n  if include_exposure_values:\n    exposure_kwargs['exposure_values'] = jnp.zeros([n, 1])\n  if include_device_idx:\n    exposure_kwargs['device_idx'] = int_scalar\n\n  random_rays = Rays(\n      origins=origins,\n      directions=directions,\n      viewdirs=viewdirs,\n      radii=radii,\n      imageplane=imageplane,\n      pixels=pixels,\n      lossmult=lossmult,\n      near=near,\n      far=far,\n      cam_idx=int_scalar,\n      **exposure_kwargs,\n  )\n  return random_rays\n\n\n# Dummy Rays object that can be used to initialize NeRF model.\n", "contexts_below": "\n\n@flax.struct.dataclass\nclass Batch:\n  \"\"\"Data batch for NeRF training or testing.\n\n  This dataclass contains rays and also per-pixel data that is necessary for\n  computing the loss term or evaluating metrics but NOT necessary for rendering.\n  \"\"\"\n\n  rays: Rays\n  rgb: Optional[_Array] = None\n  disps: Optional[_Array] = None\n  normals: Optional[_Array] = None\n  alphas: Optional[_Array] = None\n  masks: Optional[_Array] = None\n\n\nclass DataSplit(enum.Enum):\n  \"\"\"Dataset split.\"\"\"\n\n  TRAIN = 'train'\n  TEST = 'test'\n\n\nclass BatchingMethod(enum.Enum):\n  \"\"\"Draw rays randomly from a single image or all images, in each batch.\"\"\"\n\n  ALL_IMAGES = 'all_images'\n  SINGLE_IMAGE = 'single_image'\n\n\ndef open_file(pth, mode='r'):\n  return open(pth, mode=mode)\n\n\ndef file_exists(pth):\n  return os.path.exists(pth)\n\n\ndef listdir(pth):\n  return os.listdir(pth)\n\n\ndef isdir(pth):\n  return os.path.isdir(pth)\n\n\ndef makedirs(pth):\n  if not file_exists(pth):\n    os.makedirs(pth)\n\n\ndef device_is_tpu():\n  return jax.local_devices()[0].platform == 'tpu'\n\n\ndef shard(xs):\n  \"\"\"Split data into shards for multiple devices along the first dimension.\"\"\"\n  return jax.tree_util.tree_map(\n      lambda x: x.reshape((jax.local_device_count(), -1) + x.shape[1:]), xs\n  )\n\n\ndef unshard(x, padding=0):\n  \"\"\"Collect the sharded tensor to the shape before sharding.\"\"\"\n  y = x.reshape([x.shape[0] * x.shape[1]] + list(x.shape[2:]))\n  if padding > 0:\n    y = y[:-padding]\n  return y\n\n\ndef load_npy(pth):\n  \"\"\"Load an numpy array cast to float32.\"\"\"\n  with open_file(pth, 'rb') as f:\n    x = np.load(f).astype(np.float32)\n  return x\n\n\ndef assert_valid_stepfun(t, y):\n  \"\"\"Assert that step function (t, y) has a valid shape.\"\"\"\n  if t.shape[-1] != y.shape[-1] + 1:\n    raise ValueError(\n        f'Invalid shapes ({t.shape}, {y.shape}) for a step function.'\n    )\n\n\ndef assert_valid_linspline(t, y):\n  \"\"\"Assert that piecewise linear spline (t, y) has a valid shape.\"\"\"\n  if t.shape[-1] != y.shape[-1]:\n    raise ValueError(\n        f'Invalid shapes ({t.shape}, {y.shape}) for a linear spline.'\n    )\n\n\n_FnT = TypeVar('_FnT', bound=Callable[Ellipsis, Iterable[Any]])\n\n\ndef iterate_in_separate_thread(\n    queue_size = 3,\n):\n  \"\"\"Decorator factory that iterates a function in a separate thread.\n\n  Args:\n    queue_size: Keep at most queue_size elements in memory.\n\n  Returns:\n    Decorator that will iterate a function in a separate thread.\n  \"\"\"\n\n  def decorator(\n      fn,\n  ):\n    def result_fn(*args, **kwargs):\n      results_queue = queue.Queue(queue_size)\n      populating_data = True\n      populating_data_lock = threading.Lock()\n\n      def thread_fn():\n        # Mark has_data as a variable that's outside of thread_fn\n        # Otherwise, `populating_data = True` creates a local variable\n        nonlocal populating_data\n        try:\n          for item in fn(*args, **kwargs):\n            results_queue.put(item)\n        finally:\n          # Set populating_data to False regardless of exceptions to stop\n          # iterations\n          with populating_data_lock:\n            populating_data = False\n\n      # Use executor + futures instead of Thread to propagate exceptions\n      with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n        thread_fn_future = executor.submit(thread_fn)\n\n        while True:\n          with populating_data_lock:\n            if not populating_data and results_queue.empty():\n              break\n          get_start = time.time()\n          try:\n            # Set timeout to allow for exceptions to be propagated.\n            next_value = results_queue.get(timeout=1.0)\n          except queue.Empty:\n            continue\n          logging.info('Got data in %0.3fs', time.time() - get_start)\n          yield next_value\n\n        # Thread exception will be raised here\n        thread_fn_future.result()\n\n    return result_fn\n\n  return decorator\n", "input_code": "def dummy_rays(\n    include_exposure_idx = False,\n    include_exposure_values = False,\n    include_device_idx = False,\n):\n\n  \"\"\"\n  Generates a set of random rays with specified parameters and options for including exposure index, exposure values, and device index. It utilizes a predefined function `generate_random_rays` to create these rays based on the input conditions.\n\n  Input-Output Arguments\n  :param include_exposure_idx: Bool, optional. Determines whether to include the exposure index in the generated rays.\n  :param include_exposure_values: Bool, optional. Specifies if the exposure values should be included in the generated rays.\n  :param include_device_idx: Bool, optional. Indicates whether to include the device index in the generated rays.\n  :return: The result from `generate_random_rays`. The type and structure of the return value depend on the implementation of `generate_random_rays`, typically a collection of generated rays with specified properties and optional information.\n  \"\"\"", "reference_steps": "1. Define a function `dummy_rays` with optional boolean parameters `include_exposure_idx`, `include_exposure_values`, and `include_device_idx`.\n\n2. Inside the function, call another function `generate_random_rays` with a predefined set of parameters.\n\n3. Use `random.PRNGKey(0)` to initialize the pseudo-random number generator with a fixed seed of `0`.\n\n4. Specify the number of rays to generate with `n=100`.\n\n5. Define the range for the origin of rays with `origin_lo=-1.5` and `origin_hi=1.5`.\n\n6. Set the range for the radius of rays with `radius_lo=1e-5` and `radius_hi=1e-3`.\n\n7. Establish the range for the near clipping plane with `near_lo=0.0` and `near_hi=1.0`.\n\n8. Define the range for the far clipping plane with `far_lo=10` and `far_hi=10000`.\n\n9. Pass the optional boolean parameters received by `dummy_rays` (`include_exposure_idx`, `include_exposure_values`, `include_device_idx`) directly to the `generate_random_rays` function.\n\n10. Return the result of the `generate_random_rays` function call.", "reference_code": "def dummy_rays(\n    include_exposure_idx = False,\n    include_exposure_values = False,\n    include_device_idx = False,\n):\n  return generate_random_rays(\n      random.PRNGKey(0),\n      n=100,\n      origin_lo=-1.5,\n      origin_hi=1.5,\n      radius_lo=1e-5,\n      radius_hi=1e-3,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=10,\n      far_hi=10000,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n"}
{"namespace": "render.compute_alpha_weights", "type": "function", "class_name": null, "function_name": "compute_alpha_weights", "dependency_all": "# Intra-file Dependency:\nrender.compute_alpha_weights_helper\n\n", "dependency_sampled": "# Intra-file Dependency:\nrender.compute_alpha_weights_helper\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helper functions for shooting and rendering rays.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\nfrom internal import math\nfrom internal import stepfun\n\n\ndef lift_gaussian(d, t_mean, t_var, r_var, diag):\n  \"\"\"Lift a Gaussian defined along a ray to 3D coordinates.\"\"\"\n  mean = d[Ellipsis, None, :] * t_mean[Ellipsis, None]\n\n  d_mag_sq = jnp.maximum(1e-10, jnp.sum(d**2, axis=-1, keepdims=True))\n\n  if diag:\n    d_outer_diag = d**2\n    null_outer_diag = 1 - d_outer_diag / d_mag_sq\n    t_cov_diag = t_var[Ellipsis, None] * d_outer_diag[Ellipsis, None, :]\n    xy_cov_diag = r_var[Ellipsis, None] * null_outer_diag[Ellipsis, None, :]\n    cov_diag = t_cov_diag + xy_cov_diag\n    return mean, cov_diag\n  else:\n    d_outer = d[Ellipsis, :, None] * d[Ellipsis, None, :]\n    eye = jnp.eye(d.shape[-1])\n    null_outer = eye - d[Ellipsis, :, None] * (d / d_mag_sq)[Ellipsis, None, :]\n    t_cov = t_var[Ellipsis, None, None] * d_outer[Ellipsis, None, :, :]\n    xy_cov = r_var[Ellipsis, None, None] * null_outer[Ellipsis, None, :, :]\n    cov = t_cov + xy_cov\n    return mean, cov\n\n\ndef gaussianize_frustum(t0, t1):\n  \"\"\"Convert intervals along a conical frustum into means and variances.\"\"\"\n  # A more stable version of Equation 7 from https://arxiv.org/abs/2103.13415.\n  s = t0 + t1\n  d = t1 - t0\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  ratio = d**2 / jnp.maximum(eps, 3 * s**2 + d**2)\n  t_mean = s * (1 / 2 + ratio)\n  t_var = (1 / 12) * d**2 - (1 / 15) * ratio**2 * (12 * s**2 - d**2)\n  r_var = (1 / 16) * s**2 + d**2 * (5 / 48 - (1 / 15) * ratio)\n  return t_mean, t_var, r_var\n\n\ndef conical_frustum_to_gaussian(d, t0, t1, base_radius, diag):\n  \"\"\"Approximate a 3D conical frustum as a Gaussian distribution (mean+cov).\n\n  Assumes the ray is originating from the origin, and base_radius is the\n  radius at dist=1. Doesn't assume `d` is normalized.\n\n  Args:\n    d: jnp.float32 3-vector, the axis of the cone\n    t0: float, the starting distance of the frustum.\n    t1: float, the ending distance of the frustum.\n    base_radius: float, the scale of the radius as a function of distance.\n    diag: boolean, whether or the Gaussian will be diagonal or full-covariance.\n\n  Returns:\n    a Gaussian (mean and covariance).\n  \"\"\"\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n\ndef cylinder_to_gaussian(d, t0, t1, radius, diag):\n  \"\"\"Approximate a cylinder as a Gaussian distribution (mean+cov).\n\n  Assumes the ray is originating from the origin, and radius is the\n  radius. Does not renormalize `d`.\n\n  Args:\n    d: jnp.float32 3-vector, the axis of the cylinder\n    t0: float, the starting distance of the cylinder.\n    t1: float, the ending distance of the cylinder.\n    radius: float, the radius of the cylinder\n    diag: boolean, whether or the Gaussian will be diagonal or full-covariance.\n\n  Returns:\n    a Gaussian (mean and covariance).\n  \"\"\"\n  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0) ** 2 / 12\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n\ndef cast_rays(tdist, origins, directions, radii, ray_shape, diag=True):\n  \"\"\"Cast rays (cone- or cylinder-shaped) and featurize sections of it.\n\n  Args:\n    tdist: float array, the \"fencepost\" distances along the ray.\n    origins: float array, the ray origin coordinates.\n    directions: float array, the ray direction vectors.\n    radii: float array, the radii (base radii for cones) of the rays.\n    ray_shape: string, the shape of the ray, must be 'cone' or 'cylinder'.\n    diag: boolean, whether or not the covariance matrices should be diagonal.\n\n  Returns:\n    a tuple of arrays of means and covariances.\n  \"\"\"\n  t0 = tdist[Ellipsis, :-1]\n  t1 = tdist[Ellipsis, 1:]\n  if ray_shape == 'cone':\n    gaussian_fn = conical_frustum_to_gaussian\n  elif ray_shape == 'cylinder':\n    gaussian_fn = cylinder_to_gaussian\n  else:\n    raise ValueError(\"ray_shape must be 'cone' or 'cylinder'\")\n  means, covs = gaussian_fn(directions, t0, t1, radii, diag)\n  means = means + origins[Ellipsis, None, :]\n  return means, covs\n\n\ndef compute_alpha_weights_helper(density_delta):\n  \"\"\"Helper function for compute_alpha_weights.\"\"\"\n\n  log_trans = -jnp.concatenate(\n      [\n          jnp.zeros_like(density_delta[Ellipsis, :1]),\n          jnp.cumsum(density_delta[Ellipsis, :-1], axis=-1),\n      ],\n      axis=-1,\n  )\n\n  alpha = 1 - jnp.exp(-density_delta)\n  trans = jnp.exp(log_trans)\n  weights = alpha * trans\n\n  return weights\n\n\n", "contexts_below": "\n\ndef volumetric_rendering(\n    rgbs,\n    weights,\n    tdist,\n    bg_rgbs,\n    compute_extras,\n    extras=None,\n    percentiles = (5, 50, 95),\n):\n  \"\"\"Volumetric Rendering Function.\n\n  Args:\n    rgbs: jnp.ndarray(float32), color, [batch_size, num_samples, 3]\n    weights: jnp.ndarray(float32), weights, [batch_size, num_samples].\n    tdist: jnp.ndarray(float32), [batch_size, num_samples].\n    bg_rgbs: jnp.ndarray(float32), the color(s) to use for the background.\n    compute_extras: bool, if True, compute extra quantities besides color.\n    extras: dict, a set of values along rays to render by alpha compositing.\n    percentiles: depth will be returned for these percentiles.\n\n  Returns:\n    rendering: a dict containing an rgb image of size [batch_size, 3], and other\n      visualizations if compute_extras=True.\n  \"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  rendering = {}\n\n  acc = weights.sum(axis=-1)\n  bg_w = jnp.maximum(0, 1 - acc[Ellipsis, None])  # The weight of the background.\n  if rgbs is not None:\n    rgb = (weights[Ellipsis, None] * rgbs).sum(axis=-2) + bg_w * bg_rgbs\n  else:\n    rgb = None\n  rendering['rgb'] = rgb\n\n  if compute_extras:\n    rendering['acc'] = acc\n\n    if extras is not None:\n      for k, v in extras.items():\n        if v is not None:\n          rendering[k] = (weights[Ellipsis, None] * v).sum(axis=-2)\n\n    expectation = lambda x: (weights * x).sum(axis=-1) / jnp.maximum(eps, acc)\n    t_mids = 0.5 * (tdist[Ellipsis, :-1] + tdist[Ellipsis, 1:])\n    # For numerical stability this expectation is computing using log-distance.\n    rendering['distance_mean'] = jnp.clip(\n        jnp.nan_to_num(jnp.exp(expectation(jnp.log(t_mids))), jnp.inf),\n        tdist[Ellipsis, 0],\n        tdist[Ellipsis, -1],\n    )\n\n    # Normalize the weights to sum to 1.\n    weights_norm = weights / jnp.maximum(eps, acc[Ellipsis, None])\n\n    distance_percentiles = stepfun.weighted_percentile(\n        tdist, weights_norm, percentiles\n    )\n\n    for i, p in enumerate(percentiles):\n      s = 'median' if p == 50 else 'percentile_' + str(p)\n      rendering['distance_' + s] = distance_percentiles[Ellipsis, i]\n\n  return rendering\n", "input_code": "def compute_alpha_weights(\n    density,\n    tdist,\n    dirs,\n    **kwargs,\n):\n\n  \"\"\"\n  This function calculates the alpha compositing weights based on the given density, distance between points (tdist), and direction vectors (dirs). It computes the product of density and the adjusted distance between points to use as input for another helper function that computes the alpha weights.\n\n  Input-Output Arguments\n  :param density: Array-like. The density values for each point in the space, used to compute the weights.\n  :param tdist: Array-like. The distances between consecutive points along a path or direction, used to adjust the density values.\n  :param dirs: Array-like. The direction vectors for each point or segment, used to calculate the norm-adjusted distances between points.\n  :param **kwargs: Arbitrary keyword arguments. These are passed directly to the helper function for computing alpha weights.\n  :return: The return value from the helper function that computes the alpha weights. The data type and specifics depend on the implementation of the helper function.\n  \"\"\"", "reference_steps": "1. Define a function `compute_alpha_weights` that takes in `density`, `tdist`, `dirs`, and an arbitrary number of keyword arguments `**kwargs`.\n2. Calculate the differences between consecutive elements in the `tdist` array using `jnp.diff(tdist)` and store the result in `t_delta`.\n3. Compute the product of `t_delta` and the L2 norm (Euclidean distance) of the `dirs` array along the last axis using `jnp.linalg.norm(dirs[Ellipsis, None, :], axis=-1)` and store the result in `delta`.\n4. Multiply the `density` array by the `delta` array to get `density_delta`.\n5. Call another function `compute_alpha_weights_helper` with `density_delta` and the keyword arguments `**kwargs` passed through.\n6. Return the result of the `compute_alpha_weights_helper` function.", "reference_code": "def compute_alpha_weights(\n    density,\n    tdist,\n    dirs,\n    **kwargs,\n):\n  \"\"\"Helper function for computing alpha compositing weights.\"\"\"\n  t_delta = jnp.diff(tdist)\n  delta = t_delta * jnp.linalg.norm(dirs[Ellipsis, None, :], axis=-1)\n  density_delta = density * delta\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n"}
{"namespace": "stepfun.weighted_percentile", "type": "function", "class_name": null, "function_name": "weighted_percentile", "dependency_all": "# Intra-file Dependency:\nstepfun.integrate_weights\n\n", "dependency_sampled": "# Intra-file Dependency:\nstepfun.integrate_weights\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tools for manipulating step functions (piecewise-constant 1D functions).\n\nWe have a shared naming and dimension convention for these functions.\nAll input/output step functions are assumed to be aligned along the last axis.\n`t` always indicates the x coordinates of the *endpoints* of a step function.\n`y` indicates unconstrained values for the *bins* of a step function\n`w` indicates bin weights that sum to <= 1. `p` indicates non-negative bin\nvalues that *integrate* to <= 1.\n\"\"\"\n\nfrom internal import linspline\nfrom internal import math\nfrom internal import utils\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ndef query(tq, t, y, left=None, right=None):\n  \"\"\"Query step function (t, y) at locations tq. Edges repeat by default.\"\"\"\n  utils.assert_valid_stepfun(t, y)\n  # Query the step function to recover the interval value.\n  (i0, i1), ((yq, _),) = math.sorted_lookup(tq, t, (y,), utils.device_is_tpu())\n  # Apply boundary conditions.\n  left = y[Ellipsis, :1] if left is None else left\n  right = y[Ellipsis, -1:] if right is None else right\n  yq = math.select([(i1 == 0, left), (i0 == y.shape[-1], right)], yq)\n  return yq\n\n\ndef weight_to_pdf(t, w):\n  \"\"\"Turn a vector of weights that sums to 1 into a PDF that integrates to 1.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(w, td))\n\n\ndef pdf_to_weight(t, p):\n  \"\"\"Turn a PDF that integrates to 1 into a vector of weights that sums to 1.\"\"\"\n  utils.assert_valid_stepfun(t, p)\n  return p * jnp.diff(t)\n\n\ndef integrate_weights(w):\n  \"\"\"Compute the cumulative sum of w, assuming all weight vectors sum to 1.\n\n  The output's size on the last dimension is one greater than that of the input,\n  because we're computing the integral corresponding to the endpoints of a step\n  function, not the integral of the interior/bin values.\n\n  Args:\n    w: Tensor, which will be integrated along the last axis. This is assumed to\n      sum to 1 along the last axis, and this function will (silently) break if\n      that is not the case.\n\n  Returns:\n    cw0: Tensor, the integral of w, where cw0[..., 0] = 0 and cw0[..., -1] = 1\n  \"\"\"\n  cw = jnp.minimum(1, jnp.cumsum(w[Ellipsis, :-1], axis=-1))\n  shape = cw.shape[:-1] + (1,)\n  # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n  cw0 = jnp.concatenate([jnp.zeros(shape), cw, jnp.ones(shape)], axis=-1)\n  return cw0\n\n\ndef invert_cdf(u, t, w_logits):\n  \"\"\"Invert the CDF defined by (t, w) at the points specified by u in [0, 1).\"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new\n\n\ndef sample(\n    rng,\n    t,\n    w_logits,\n    num_samples,\n    single_jitter=False,\n    deterministic_center=False,\n    eps=jnp.finfo(jnp.float32).eps,\n):\n  \"\"\"Piecewise-Constant PDF sampling from a step function.\n\n  Args:\n    rng: random number generator (or None for `linspace` sampling).\n    t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n    w_logits: [..., num_bins], logits corresponding to bin weights\n    num_samples: int, the number of samples.\n    single_jitter: bool, if True, jitter every sample along each ray by the same\n      amount in the inverse CDF. Otherwise, jitter each sample independently.\n    deterministic_center: bool, if False, when `rng` is None return samples that\n      linspace the entire PDF. If True, skip the front and back of the linspace\n      so that the centers of each PDF interval are returned.\n    eps: float, something like numerical epsilon.\n\n  Returns:\n    t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n  \"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    if deterministic_center:\n      pad = 1 / (2 * num_samples)\n      u = jnp.linspace(pad, 1.0 - pad - eps, num_samples)\n    else:\n      u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  return invert_cdf(u, t, w_logits)\n\n\ndef sample_intervals(\n    rng,\n    t,\n    w_logits,\n    num_samples,\n    single_jitter=False,\n    domain=(-jnp.inf, jnp.inf),\n):\n  \"\"\"Sample *intervals* (rather than points) from a step function.\n\n  Args:\n    rng: random number generator (or None for `linspace` sampling).\n    t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n    w_logits: [..., num_bins], logits corresponding to bin weights\n    num_samples: int, the number of intervals to sample.\n    single_jitter: bool, if True, jitter every sample along each ray by the same\n      amount in the inverse CDF. Otherwise, jitter each sample independently.\n    domain: (minval, maxval), the range of valid values for `t`.\n\n  Returns:\n    t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n  \"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Sample a set of points from the step function.\n  centers = sample(\n      rng, t, w_logits, num_samples, single_jitter, deterministic_center=True\n  )\n\n  # The intervals we return will span the midpoints of each adjacent sample.\n  mid = (centers[Ellipsis, 1:] + centers[Ellipsis, :-1]) / 2\n\n  # Each first/last fencepost is the reflection of the first/last midpoint\n  # around the first/last sampled center.\n  first = 2 * centers[Ellipsis, :1] - mid[Ellipsis, :1]\n  last = 2 * centers[Ellipsis, -1:] - mid[Ellipsis, -1:]\n  samples = jnp.concatenate([first, mid, last], axis=-1)\n\n  # We clamp to the limits of the input domain, provided by the caller.\n  samples = jnp.clip(samples, *domain)\n  return samples\n\n\ndef lossfun_distortion(t, w):\n  \"\"\"Compute iint w[i] w[j] |t[i] - t[j]| di dj.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n\n  # The loss incurred between all pairs of intervals.\n  ut = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n  dut = jnp.abs(ut[Ellipsis, :, None] - ut[Ellipsis, None, :])\n  loss_inter = jnp.sum(w * jnp.sum(w[Ellipsis, None, :] * dut, axis=-1), axis=-1)\n\n  # The loss incurred within each individual interval with itself.\n  loss_intra = jnp.sum(w**2 * jnp.diff(t), axis=-1) / 3\n\n  return loss_inter + loss_intra\n\n\n", "contexts_below": "\n\ndef resample(t, tp, vp, use_avg=False):\n  \"\"\"Resample a step function defined by (tp, vp) into intervals t.\n\n  Notation roughly matches jnp.interp. Resamples by summation by default.\n\n  Args:\n    t: tensor with shape (..., n+1), the endpoints to resample into.\n    tp: tensor with shape (..., m+1), the endpoints of the step function being\n      resampled.\n    vp: tensor with shape (..., m), the values of the step function being\n      resampled.\n    use_avg: bool, if False, return the sum of the step function for each\n      interval in `t`. If True, return the average, weighted by the width of\n      each interval in `t`.\n\n  Returns:\n    v: tensor with shape (..., n), the values of the resampled step function.\n  \"\"\"\n  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    wp = jnp.diff(tp)\n    v_numer = resample(t, tp, vp * wp, use_avg=False)\n    v_denom = resample(t, tp, wp, use_avg=False)\n    v = math.safe_div(v_numer, v_denom)\n    return v\n\n  acc = jnp.cumsum(vp, axis=-1)\n  acc0 = jnp.concatenate([jnp.zeros(acc.shape[:-1] + (1,)), acc], axis=-1)\n  acc0_resampled = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      t, tp, acc0\n  )\n  v = jnp.diff(acc0_resampled, axis=-1)\n  return v\n\n\ndef blur_and_resample_weights(tq, t, w, blur_halfwidth):\n  \"\"\"Blur the (t, w) histogram by blur_halfwidth, then resample it into tq.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n\n  # Convert the histogram to a PDF.\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF step function into a piecewise linear spline PDF.\n  t_linspline, p_linspline = linspline.blur_stepfun(t, p, blur_halfwidth)\n\n  # Integrate the spline PDF, then query it to get integrated weights.\n  quad = linspline.compute_integral(t_linspline, p_linspline)\n  acc_wq = linspline.interpolate_integral(tq, t_linspline, *quad)\n\n  # Undo the integration to get weights.\n  wq = jnp.diff(acc_wq, axis=-1)\n\n  # Fix negative values to 0, as they should never happen but may due to\n  # numerical issues.\n  wq = jnp.maximum(0, wq)\n  return wq\n", "input_code": "def weighted_percentile(t, w, ps):\n\n  \"\"\"\n  Computes the weighted percentiles of a step function by interpolating into the integrated weights based on the given percentiles. It ensures that the weights sum to 1 and uses these weights to calculate the weighted percentiles.\n\n  Input-Output Arguments\n  :param t: Array-like. The values at which the step function changes. It is used as the x-coordinates for interpolation.\n  :param w: Array-like. The weights associated with the values in 't', which must sum to 1. These weights are integrated and used in the interpolation process.\n  :param ps: Array-like. The percentiles to compute, given as values between 0 and 100. These are the target y-values for which the corresponding x-values are found through interpolation.\n  :return: Array-like. The computed weighted percentiles corresponding to the input percentiles 'ps'.\n  \"\"\"", "reference_steps": "1. Define a function `weighted_percentile` that computes the weighted percentiles of a step function, given thresholds `t`, weights `w`, and percentile levels `ps`.\n2. Ensure that the weights `w` sum to 1 by calling `utils.assert_valid_stepfun(t, w)` to validate the step function.\n3. Integrate the weights `w` to create a cumulative weight array `cw` using a function like `integrate_weights(w)`.\n4. Normalize the percentile levels `ps` by dividing them by 100 to convert them to a proportion.\n5. Use `jnp.vectorize` to create a vectorized version of the `jnp.interp` function, which will be used to interpolate the weighted percentiles.\n6. Define the signature of the vectorized interpolation function as `'(n),(m),(m)->(n)'`, which indicates the expected dimensions of the input and output arrays.\n7. Perform the interpolation by passing the normalized percentile levels, the cumulative weights `cw`, and the thresholds `t` to the vectorized interpolation function.\n8. The interpolation will find the corresponding values in `t` that match the cumulative weight positions specified by the normalized percentile levels.\n9. Store the interpolated weighted percentiles in the variable `wprctile`.\n10. Return the computed weighted percentiles `wprctile` as the output of the function.", "reference_code": "def weighted_percentile(t, w, ps):\n  \"\"\"Compute the weighted percentiles of a step function. w's must sum to 1.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n  cw = integrate_weights(w)\n  # We want to interpolate into the integrated weights according to `ps`.\n  wprctile = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      jnp.array(ps) / 100, cw, t\n  )\n  return wprctile\n"}
{"namespace": "stepfun.blur_and_resample_weights", "type": "function", "class_name": null, "function_name": "blur_and_resample_weights", "dependency_all": "# Intra-file Dependency:\nstepfun.weight_to_pdf\n\n", "dependency_sampled": "# Intra-file Dependency:\nstepfun.weight_to_pdf\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tools for manipulating step functions (piecewise-constant 1D functions).\n\nWe have a shared naming and dimension convention for these functions.\nAll input/output step functions are assumed to be aligned along the last axis.\n`t` always indicates the x coordinates of the *endpoints* of a step function.\n`y` indicates unconstrained values for the *bins* of a step function\n`w` indicates bin weights that sum to <= 1. `p` indicates non-negative bin\nvalues that *integrate* to <= 1.\n\"\"\"\n\nfrom internal import linspline\nfrom internal import math\nfrom internal import utils\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ndef query(tq, t, y, left=None, right=None):\n  \"\"\"Query step function (t, y) at locations tq. Edges repeat by default.\"\"\"\n  utils.assert_valid_stepfun(t, y)\n  # Query the step function to recover the interval value.\n  (i0, i1), ((yq, _),) = math.sorted_lookup(tq, t, (y,), utils.device_is_tpu())\n  # Apply boundary conditions.\n  left = y[Ellipsis, :1] if left is None else left\n  right = y[Ellipsis, -1:] if right is None else right\n  yq = math.select([(i1 == 0, left), (i0 == y.shape[-1], right)], yq)\n  return yq\n\n\ndef weight_to_pdf(t, w):\n  \"\"\"Turn a vector of weights that sums to 1 into a PDF that integrates to 1.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(w, td))\n\n\ndef pdf_to_weight(t, p):\n  \"\"\"Turn a PDF that integrates to 1 into a vector of weights that sums to 1.\"\"\"\n  utils.assert_valid_stepfun(t, p)\n  return p * jnp.diff(t)\n\n\ndef integrate_weights(w):\n  \"\"\"Compute the cumulative sum of w, assuming all weight vectors sum to 1.\n\n  The output's size on the last dimension is one greater than that of the input,\n  because we're computing the integral corresponding to the endpoints of a step\n  function, not the integral of the interior/bin values.\n\n  Args:\n    w: Tensor, which will be integrated along the last axis. This is assumed to\n      sum to 1 along the last axis, and this function will (silently) break if\n      that is not the case.\n\n  Returns:\n    cw0: Tensor, the integral of w, where cw0[..., 0] = 0 and cw0[..., -1] = 1\n  \"\"\"\n  cw = jnp.minimum(1, jnp.cumsum(w[Ellipsis, :-1], axis=-1))\n  shape = cw.shape[:-1] + (1,)\n  # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n  cw0 = jnp.concatenate([jnp.zeros(shape), cw, jnp.ones(shape)], axis=-1)\n  return cw0\n\n\ndef invert_cdf(u, t, w_logits):\n  \"\"\"Invert the CDF defined by (t, w) at the points specified by u in [0, 1).\"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new\n\n\ndef sample(\n    rng,\n    t,\n    w_logits,\n    num_samples,\n    single_jitter=False,\n    deterministic_center=False,\n    eps=jnp.finfo(jnp.float32).eps,\n):\n  \"\"\"Piecewise-Constant PDF sampling from a step function.\n\n  Args:\n    rng: random number generator (or None for `linspace` sampling).\n    t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n    w_logits: [..., num_bins], logits corresponding to bin weights\n    num_samples: int, the number of samples.\n    single_jitter: bool, if True, jitter every sample along each ray by the same\n      amount in the inverse CDF. Otherwise, jitter each sample independently.\n    deterministic_center: bool, if False, when `rng` is None return samples that\n      linspace the entire PDF. If True, skip the front and back of the linspace\n      so that the centers of each PDF interval are returned.\n    eps: float, something like numerical epsilon.\n\n  Returns:\n    t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n  \"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    if deterministic_center:\n      pad = 1 / (2 * num_samples)\n      u = jnp.linspace(pad, 1.0 - pad - eps, num_samples)\n    else:\n      u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  return invert_cdf(u, t, w_logits)\n\n\ndef sample_intervals(\n    rng,\n    t,\n    w_logits,\n    num_samples,\n    single_jitter=False,\n    domain=(-jnp.inf, jnp.inf),\n):\n  \"\"\"Sample *intervals* (rather than points) from a step function.\n\n  Args:\n    rng: random number generator (or None for `linspace` sampling).\n    t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n    w_logits: [..., num_bins], logits corresponding to bin weights\n    num_samples: int, the number of intervals to sample.\n    single_jitter: bool, if True, jitter every sample along each ray by the same\n      amount in the inverse CDF. Otherwise, jitter each sample independently.\n    domain: (minval, maxval), the range of valid values for `t`.\n\n  Returns:\n    t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n  \"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Sample a set of points from the step function.\n  centers = sample(\n      rng, t, w_logits, num_samples, single_jitter, deterministic_center=True\n  )\n\n  # The intervals we return will span the midpoints of each adjacent sample.\n  mid = (centers[Ellipsis, 1:] + centers[Ellipsis, :-1]) / 2\n\n  # Each first/last fencepost is the reflection of the first/last midpoint\n  # around the first/last sampled center.\n  first = 2 * centers[Ellipsis, :1] - mid[Ellipsis, :1]\n  last = 2 * centers[Ellipsis, -1:] - mid[Ellipsis, -1:]\n  samples = jnp.concatenate([first, mid, last], axis=-1)\n\n  # We clamp to the limits of the input domain, provided by the caller.\n  samples = jnp.clip(samples, *domain)\n  return samples\n\n\ndef lossfun_distortion(t, w):\n  \"\"\"Compute iint w[i] w[j] |t[i] - t[j]| di dj.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n\n  # The loss incurred between all pairs of intervals.\n  ut = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n  dut = jnp.abs(ut[Ellipsis, :, None] - ut[Ellipsis, None, :])\n  loss_inter = jnp.sum(w * jnp.sum(w[Ellipsis, None, :] * dut, axis=-1), axis=-1)\n\n  # The loss incurred within each individual interval with itself.\n  loss_intra = jnp.sum(w**2 * jnp.diff(t), axis=-1) / 3\n\n  return loss_inter + loss_intra\n\n\ndef weighted_percentile(t, w, ps):\n  \"\"\"Compute the weighted percentiles of a step function. w's must sum to 1.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n  cw = integrate_weights(w)\n  # We want to interpolate into the integrated weights according to `ps`.\n  wprctile = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      jnp.array(ps) / 100, cw, t\n  )\n  return wprctile\n\n\ndef resample(t, tp, vp, use_avg=False):\n  \"\"\"Resample a step function defined by (tp, vp) into intervals t.\n\n  Notation roughly matches jnp.interp. Resamples by summation by default.\n\n  Args:\n    t: tensor with shape (..., n+1), the endpoints to resample into.\n    tp: tensor with shape (..., m+1), the endpoints of the step function being\n      resampled.\n    vp: tensor with shape (..., m), the values of the step function being\n      resampled.\n    use_avg: bool, if False, return the sum of the step function for each\n      interval in `t`. If True, return the average, weighted by the width of\n      each interval in `t`.\n\n  Returns:\n    v: tensor with shape (..., n), the values of the resampled step function.\n  \"\"\"\n  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    wp = jnp.diff(tp)\n    v_numer = resample(t, tp, vp * wp, use_avg=False)\n    v_denom = resample(t, tp, wp, use_avg=False)\n    v = math.safe_div(v_numer, v_denom)\n    return v\n\n  acc = jnp.cumsum(vp, axis=-1)\n  acc0 = jnp.concatenate([jnp.zeros(acc.shape[:-1] + (1,)), acc], axis=-1)\n  acc0_resampled = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      t, tp, acc0\n  )\n  v = jnp.diff(acc0_resampled, axis=-1)\n  return v\n\n\n", "contexts_below": "", "input_code": "def blur_and_resample_weights(tq, t, w, blur_halfwidth):\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"", "reference_steps": "1. Verify that the input histogram defined by `t` (time or position array) and `w` (weight array) is a valid step function using `utils.assert_valid_stepfun(t, w)`.\n\n2. Convert the histogram `(t, w)` into a probability density function (PDF) by calling `weight_to_pdf(t, w)`.\n\n3. Blur the step function PDF into a piecewise linear spline PDF using `linspline.blur_stepfun(t, p, blur_halfwidth)`, which takes the original time or position array `t`, the PDF `p`, and the `blur_halfwidth` as inputs.\n\n4. Compute the integral of the blurred piecewise linear spline PDF using `linspline.compute_integral(t_linspline, p_linspline)`.\n\n5. Query the integrated spline PDF at the new time or position points `tq` using `linspline.interpolate_integral(tq, t_linspline, *quad)` to get the accumulated weights `acc_wq`.\n\n6. Obtain the weights `wq` by taking the difference of the accumulated weights `acc_wq` along the last axis using `jnp.diff(acc_wq, axis=-1)`.\n\n7. Rectify any negative values in the weights array `wq` to zero using `jnp.maximum(0, wq)` to account for potential numerical inaccuracies.\n\n8. Return the resampled weights `wq` that correspond to the new time or position points `tq`.", "reference_code": "def blur_and_resample_weights(tq, t, w, blur_halfwidth):\n  \"\"\"Blur the (t, w) histogram by blur_halfwidth, then resample it into tq.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n\n  # Convert the histogram to a PDF.\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF step function into a piecewise linear spline PDF.\n  t_linspline, p_linspline = linspline.blur_stepfun(t, p, blur_halfwidth)\n\n  # Integrate the spline PDF, then query it to get integrated weights.\n  quad = linspline.compute_integral(t_linspline, p_linspline)\n  acc_wq = linspline.interpolate_integral(tq, t_linspline, *quad)\n\n  # Undo the integration to get weights.\n  wq = jnp.diff(acc_wq, axis=-1)\n\n  # Fix negative values to 0, as they should never happen but may due to\n  # numerical issues.\n  wq = jnp.maximum(0, wq)\n  return wq\n"}
{"namespace": "stepfun.resample", "type": "function", "class_name": null, "function_name": "resample", "dependency_all": "# Intra-file Dependency:\nstepfun.resample\n\n", "dependency_sampled": "# Intra-file Dependency:\nstepfun.resample\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tools for manipulating step functions (piecewise-constant 1D functions).\n\nWe have a shared naming and dimension convention for these functions.\nAll input/output step functions are assumed to be aligned along the last axis.\n`t` always indicates the x coordinates of the *endpoints* of a step function.\n`y` indicates unconstrained values for the *bins* of a step function\n`w` indicates bin weights that sum to <= 1. `p` indicates non-negative bin\nvalues that *integrate* to <= 1.\n\"\"\"\n\nfrom internal import linspline\nfrom internal import math\nfrom internal import utils\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\ndef query(tq, t, y, left=None, right=None):\n  \"\"\"Query step function (t, y) at locations tq. Edges repeat by default.\"\"\"\n  utils.assert_valid_stepfun(t, y)\n  # Query the step function to recover the interval value.\n  (i0, i1), ((yq, _),) = math.sorted_lookup(tq, t, (y,), utils.device_is_tpu())\n  # Apply boundary conditions.\n  left = y[Ellipsis, :1] if left is None else left\n  right = y[Ellipsis, -1:] if right is None else right\n  yq = math.select([(i1 == 0, left), (i0 == y.shape[-1], right)], yq)\n  return yq\n\n\ndef weight_to_pdf(t, w):\n  \"\"\"Turn a vector of weights that sums to 1 into a PDF that integrates to 1.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(w, td))\n\n\ndef pdf_to_weight(t, p):\n  \"\"\"Turn a PDF that integrates to 1 into a vector of weights that sums to 1.\"\"\"\n  utils.assert_valid_stepfun(t, p)\n  return p * jnp.diff(t)\n\n\ndef integrate_weights(w):\n  \"\"\"Compute the cumulative sum of w, assuming all weight vectors sum to 1.\n\n  The output's size on the last dimension is one greater than that of the input,\n  because we're computing the integral corresponding to the endpoints of a step\n  function, not the integral of the interior/bin values.\n\n  Args:\n    w: Tensor, which will be integrated along the last axis. This is assumed to\n      sum to 1 along the last axis, and this function will (silently) break if\n      that is not the case.\n\n  Returns:\n    cw0: Tensor, the integral of w, where cw0[..., 0] = 0 and cw0[..., -1] = 1\n  \"\"\"\n  cw = jnp.minimum(1, jnp.cumsum(w[Ellipsis, :-1], axis=-1))\n  shape = cw.shape[:-1] + (1,)\n  # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n  cw0 = jnp.concatenate([jnp.zeros(shape), cw, jnp.ones(shape)], axis=-1)\n  return cw0\n\n\ndef invert_cdf(u, t, w_logits):\n  \"\"\"Invert the CDF defined by (t, w) at the points specified by u in [0, 1).\"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new\n\n\ndef sample(\n    rng,\n    t,\n    w_logits,\n    num_samples,\n    single_jitter=False,\n    deterministic_center=False,\n    eps=jnp.finfo(jnp.float32).eps,\n):\n  \"\"\"Piecewise-Constant PDF sampling from a step function.\n\n  Args:\n    rng: random number generator (or None for `linspace` sampling).\n    t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n    w_logits: [..., num_bins], logits corresponding to bin weights\n    num_samples: int, the number of samples.\n    single_jitter: bool, if True, jitter every sample along each ray by the same\n      amount in the inverse CDF. Otherwise, jitter each sample independently.\n    deterministic_center: bool, if False, when `rng` is None return samples that\n      linspace the entire PDF. If True, skip the front and back of the linspace\n      so that the centers of each PDF interval are returned.\n    eps: float, something like numerical epsilon.\n\n  Returns:\n    t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n  \"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    if deterministic_center:\n      pad = 1 / (2 * num_samples)\n      u = jnp.linspace(pad, 1.0 - pad - eps, num_samples)\n    else:\n      u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  return invert_cdf(u, t, w_logits)\n\n\ndef sample_intervals(\n    rng,\n    t,\n    w_logits,\n    num_samples,\n    single_jitter=False,\n    domain=(-jnp.inf, jnp.inf),\n):\n  \"\"\"Sample *intervals* (rather than points) from a step function.\n\n  Args:\n    rng: random number generator (or None for `linspace` sampling).\n    t: [..., num_bins + 1], bin endpoint coordinates (must be sorted)\n    w_logits: [..., num_bins], logits corresponding to bin weights\n    num_samples: int, the number of intervals to sample.\n    single_jitter: bool, if True, jitter every sample along each ray by the same\n      amount in the inverse CDF. Otherwise, jitter each sample independently.\n    domain: (minval, maxval), the range of valid values for `t`.\n\n  Returns:\n    t_samples: jnp.ndarray(float32), [batch_size, num_samples].\n  \"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Sample a set of points from the step function.\n  centers = sample(\n      rng, t, w_logits, num_samples, single_jitter, deterministic_center=True\n  )\n\n  # The intervals we return will span the midpoints of each adjacent sample.\n  mid = (centers[Ellipsis, 1:] + centers[Ellipsis, :-1]) / 2\n\n  # Each first/last fencepost is the reflection of the first/last midpoint\n  # around the first/last sampled center.\n  first = 2 * centers[Ellipsis, :1] - mid[Ellipsis, :1]\n  last = 2 * centers[Ellipsis, -1:] - mid[Ellipsis, -1:]\n  samples = jnp.concatenate([first, mid, last], axis=-1)\n\n  # We clamp to the limits of the input domain, provided by the caller.\n  samples = jnp.clip(samples, *domain)\n  return samples\n\n\ndef lossfun_distortion(t, w):\n  \"\"\"Compute iint w[i] w[j] |t[i] - t[j]| di dj.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n\n  # The loss incurred between all pairs of intervals.\n  ut = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n  dut = jnp.abs(ut[Ellipsis, :, None] - ut[Ellipsis, None, :])\n  loss_inter = jnp.sum(w * jnp.sum(w[Ellipsis, None, :] * dut, axis=-1), axis=-1)\n\n  # The loss incurred within each individual interval with itself.\n  loss_intra = jnp.sum(w**2 * jnp.diff(t), axis=-1) / 3\n\n  return loss_inter + loss_intra\n\n\ndef weighted_percentile(t, w, ps):\n  \"\"\"Compute the weighted percentiles of a step function. w's must sum to 1.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n  cw = integrate_weights(w)\n  # We want to interpolate into the integrated weights according to `ps`.\n  wprctile = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      jnp.array(ps) / 100, cw, t\n  )\n  return wprctile\n\n\n", "contexts_below": "\n\ndef blur_and_resample_weights(tq, t, w, blur_halfwidth):\n  \"\"\"Blur the (t, w) histogram by blur_halfwidth, then resample it into tq.\"\"\"\n  utils.assert_valid_stepfun(t, w)\n\n  # Convert the histogram to a PDF.\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF step function into a piecewise linear spline PDF.\n  t_linspline, p_linspline = linspline.blur_stepfun(t, p, blur_halfwidth)\n\n  # Integrate the spline PDF, then query it to get integrated weights.\n  quad = linspline.compute_integral(t_linspline, p_linspline)\n  acc_wq = linspline.interpolate_integral(tq, t_linspline, *quad)\n\n  # Undo the integration to get weights.\n  wq = jnp.diff(acc_wq, axis=-1)\n\n  # Fix negative values to 0, as they should never happen but may due to\n  # numerical issues.\n  wq = jnp.maximum(0, wq)\n  return wq\n", "input_code": "def resample(t, tp, vp, use_avg=False):\n\n  \"\"\"\n  Resamples a step function defined by pairs of time points (tp) and their corresponding values (vp) into new intervals specified by t. It supports both summation and averaging methods for resampling.\n\n  Input-Output Arguments\n  :param t: Tensor. The endpoints into which the step function is resampled. It is used as the new time points for resampling.\n  :param tp: Tensor. The original time points of the step function being resampled. It defines the intervals of the original step function.\n  :param vp: Tensor. The values of the step function at the original time points (tp). These values are resampled according to the new intervals defined by t.\n  :param use_avg: Bool, optional. Determines the resampling method. If False, the function sums the values of the step function for each interval in `t`. If True, it returns the average value, weighted by the width of each interval in `t`.\n  :return: Tensor. The values of the resampled step function at the new intervals defined by t.\n  \"\"\"", "reference_steps": "1. Define a function `resample` that takes inputs `t`, `tp`, `vp`, and an optional argument `use_avg` to resample a step function into intervals defined by `t`.\n2. Ensure the input step function defined by `(tp, vp)` is valid using a utility function `utils.assert_valid_stepfun`.\n3. If `use_avg` is `True`, calculate the weighted average of the step function for each interval in `t`:\n   - Compute the width of each interval in `tp` using `jnp.diff(tp)`.\n   - Resample the product of the step function values `vp` and their corresponding widths into `t` without averaging.\n   - Resample the widths into `t` without averaging.\n   - Divide the resampled values by the resampled widths using `math.safe_div` to obtain the average.\n   - Return the average values `v`.\n4. If `use_avg` is `False`, proceed to resample by summation:\n   - Compute the cumulative sum of `vp` along the last axis using `jnp.cumsum`.\n   - Prepend a zero to the cumulative sum to create `acc0` for proper interval calculation.\n   - Use `jnp.vectorize` with `jnp.interp` to interpolate `acc0` at points defined by `t`, using `tp` as the x-coordinates and `acc0` as the y-coordinates.\n   - Compute the difference between adjacent interpolated values using `jnp.diff` to obtain the resampled values `v`.\n5. Return the resampled values `v` for the step function over the intervals defined by `t`.", "reference_code": "def resample(t, tp, vp, use_avg=False):\n  \"\"\"Resample a step function defined by (tp, vp) into intervals t.\n\n  Notation roughly matches jnp.interp. Resamples by summation by default.\n\n  Args:\n    t: tensor with shape (..., n+1), the endpoints to resample into.\n    tp: tensor with shape (..., m+1), the endpoints of the step function being\n      resampled.\n    vp: tensor with shape (..., m), the values of the step function being\n      resampled.\n    use_avg: bool, if False, return the sum of the step function for each\n      interval in `t`. If True, return the average, weighted by the width of\n      each interval in `t`.\n\n  Returns:\n    v: tensor with shape (..., n), the values of the resampled step function.\n  \"\"\"\n  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    wp = jnp.diff(tp)\n    v_numer = resample(t, tp, vp * wp, use_avg=False)\n    v_denom = resample(t, tp, wp, use_avg=False)\n    v = math.safe_div(v_numer, v_denom)\n    return v\n\n  acc = jnp.cumsum(vp, axis=-1)\n  acc0 = jnp.concatenate([jnp.zeros(acc.shape[:-1] + (1,)), acc], axis=-1)\n  acc0_resampled = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      t, tp, acc0\n  )\n  v = jnp.diff(acc0_resampled, axis=-1)\n  return v\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "type": "function", "class_name": null, "function_name": "generate_dir_enc_fn", "dependency_all": "# Intra-file Dependency:\nref_utils.generate_ide_fn\n\n", "dependency_sampled": "# Intra-file Dependency:\nref_utils.generate_ide_fn\n\n", "contexts_above": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Functions for reflection directions and directional encodings.\"\"\"\n\nimport math\n\nfrom internal import math as math_lib\nimport jax.numpy as jnp\nimport numpy as np\n\n\ndef reflect(viewdirs, normals):\n  \"\"\"Reflect view directions about normals.\n\n  The reflection of a vector v about a unit vector n is a vector u such that\n  dot(v, n) = dot(u, n), and dot(u, u) = dot(v, v). The solution to these two\n  equations is u = 2 dot(n, v) n - v.\n\n  Args:\n    viewdirs: [..., 3] array of view directions.\n    normals: [..., 3] array of normal directions (assumed to be unit vectors).\n\n  Returns:\n    [..., 3] array of reflection directions.\n  \"\"\"\n  return (\n      2.0 * jnp.sum(normals * viewdirs, axis=-1, keepdims=True) * normals\n      - viewdirs\n  )\n\n\ndef l2_normalize(x, grad_eps=jnp.finfo(jnp.float32).eps):\n  \"\"\"Normalize x to unit length along last axis.\n\n  Normalizing vectors is surprisingly tricky, because you have to address the\n  case where the denominator in the normalization is tiny or zero, in which case\n  gradients will explode. For this reason, we perform two normalizations: in the\n  forward pass, we clamp the denominator with ~1e-40, but in the backward pass\n  we clamp with `grad_eps`, which defaults to ~1e-7. This guarantees that the\n  output of this function is unit norm (unless x is very very small) while\n  preventing exploding gradients.\n\n  Args:\n    x: The array of values to normalize.\n    grad_eps: The value to clip the squared norm by before division in the\n      backward pass.\n\n  Returns:\n    A normalized array x / ||x||, normalized along the last axis.\n  \"\"\"\n  tiny = jnp.finfo(jnp.float32).tiny\n  grad_eps = jnp.maximum(tiny, grad_eps)\n  denom_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  normal_val = x / jnp.sqrt(jnp.maximum(tiny, denom_sq))\n  normal_grad = x / jnp.sqrt(jnp.maximum(grad_eps, denom_sq))\n  # Use `normal_val` in the forward pass but `normal_grad` in the backward pass.\n  normal = math_lib.override_gradient(normal_val, normal_grad)\n  return jnp.where(denom_sq < tiny, jnp.zeros_like(normal), normal)\n\n\ndef compute_weighted_mae(weights, normals, normals_gt):\n  \"\"\"Compute weighted mean angular error, assuming normals are unit length.\"\"\"\n  angles = math_lib.safe_arccos((normals * normals_gt).sum(axis=-1))\n  return (180.0 / jnp.pi) * ((weights * angles).sum() / weights.sum())\n\n\ndef generalized_binomial_coeff(a, k):\n  \"\"\"Compute generalized binomial coefficients.\"\"\"\n  return np.prod(a - np.arange(k)) / math.factorial(k)\n\n\ndef assoc_legendre_coeff(l, m, k):\n  \"\"\"Compute associated Legendre polynomial coefficients.\n\n  Returns the coefficient of the cos^k(theta)*sin^m(theta) term in the\n  (l, m)th associated Legendre polynomial, P_l^m(cos(theta)).\n\n  Args:\n    l: associated Legendre polynomial degree.\n    m: associated Legendre polynomial order.\n    k: power of cos(theta).\n\n  Returns:\n    A float, the coefficient of the term corresponding to the inputs.\n  \"\"\"\n  return (\n      (-1) ** m\n      * 2**l\n      * math.factorial(l)\n      / math.factorial(k)\n      / math.factorial(l - k - m)\n      * generalized_binomial_coeff(0.5 * (l + k + m - 1.0), l)\n  )\n\n\ndef sph_harm_coeff(l, m, k):\n  \"\"\"Compute spherical harmonic coefficients.\"\"\"\n  return np.sqrt(\n      (2.0 * l + 1.0)\n      * math.factorial(l - m)\n      / (4.0 * np.pi * math.factorial(l + m))\n  ) * assoc_legendre_coeff(l, m, k)\n\n\ndef get_ml_array(deg_view):\n  \"\"\"Create a list with all pairs of (l, m) values to use in the encoding.\"\"\"\n  ml_list = []\n  for i in range(deg_view):\n    l = 2**i\n    # Only use nonnegative m values, later splitting real and imaginary parts.\n    for m in range(l + 1):\n      ml_list.append((m, l))\n\n  # Convert list into a numpy array.\n  ml_array = np.array(ml_list).T\n  return ml_array\n\n\ndef generate_ide_fn(deg_view):\n  \"\"\"Generate integrated directional encoding (IDE) function.\n\n  This function returns a function that computes the integrated directional\n  encoding from Equations 6-8 of arxiv.org/abs/2112.03907.\n\n  Args:\n    deg_view: number of spherical harmonics degrees to use.\n\n  Returns:\n    A function for evaluating integrated directional encoding.\n\n  Raises:\n    ValueError: if deg_view is larger than 5.\n  \"\"\"\n  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning integrated directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting IDE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation function using the von Mises-Fisher distribution\n    # concentration parameter, kappa.\n    sigma = 0.5 * ml_array[1, :] * (ml_array[1, :] + 1)\n    ide = sph_harms * jnp.exp(-sigma * kappa_inv)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(ide), jnp.imag(ide)], axis=-1)\n\n  return integrated_dir_enc_fn\n\n\n", "contexts_below": "\n\ndef orientation_loss(w, n, v):\n  \"\"\"Orientation loss on weights `w`, normals `n`, and -view directions `v`.\"\"\"\n  n_dot_v = (n * v[Ellipsis, None, :]).sum(axis=-1)\n  return jnp.mean((w * jnp.minimum(0.0, n_dot_v) ** 2).sum(axis=-1))\n", "input_code": "def generate_dir_enc_fn(deg_view):\n\n  \"\"\"\n  Generates a directional encoding function based on the specified number of spherical harmonics degrees. This function internally creates another function that evaluates the directional encoding for given inputs.\n\n  Input-Output Arguments\n  :param deg_view: Int. The number of spherical harmonics degrees to use for generating the directional encoding function. It determines the complexity and accuracy of the encoding.\n  :return: Function. A function that takes a 3D point (or points) as input and returns its directional encoding. This returned function internally uses a generated integrated directional encoding function with the specified degree of spherical harmonics.\n\n  \"\"\"", "reference_steps": "1. Define a function `generate_dir_enc_fn` that takes an argument `deg_view` representing the number of spherical harmonics degrees to use.\n2. Within `generate_dir_enc_fn`, call another function `generate_ide_fn` with `deg_view` as an argument and assign the result to `integrated_dir_enc_fn`.\n3. Define an inner function `dir_enc_fn` within `generate_dir_enc_fn` that takes an argument `xyz`.\n4. Inside `dir_enc_fn`, call `integrated_dir_enc_fn` with `xyz` and a zero array of the same shape as `xyz` but with the last dimension truncated to 1 (using `jnp.zeros_like(xyz[Ellipsis, :1])`).\n5. Return the result of the `integrated_dir_enc_fn` call from the `dir_enc_fn`.\n6. Return the `dir_enc_fn` from the `generate_dir_enc_fn`.\n7. The returned `dir_enc_fn` serves as the directional encoding function that can be used to evaluate directional encoding based on the specified number of spherical harmonics degrees (`deg_view`).\n8. The `generate_dir_enc_fn` is a higher-order function that produces a custom directional encoding function tailored to the given `deg_view`.\n9. The inner `dir_enc_fn` captures the `integrated_dir_enc_fn` from its enclosing scope, making it available for later use when the `dir_enc_fn` is called.\n10. The `dir_enc_fn` is designed to be a callable function that takes a 3D point (or points) as input and produces its directional encoding.", "reference_code": "def generate_dir_enc_fn(deg_view):\n  \"\"\"Generate directional encoding (DE) function.\n\n  Args:\n    deg_view: number of spherical harmonics degrees to use.\n\n  Returns:\n    A function for evaluating directional encoding.\n  \"\"\"\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding (DE).\"\"\"\n    return integrated_dir_enc_fn(xyz, jnp.zeros_like(xyz[Ellipsis, :1]))\n\n  return dir_enc_fn\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "type": "function", "class_name": null, "function_name": "clean_lines", "dependency_all": "# Intra-file Dependency:\nnlm_ingestor.ingestor.processors.clean_line\n    def clean_line(line):\n\nnlm_ingestor.ingestor.processors.fix_spaced_characters\n    def fix_spaced_characters(line_text):\n\nnlm_ingestor.ingestor.processors.logger\n\nnlm_ingestor.ingestor.processors.should_skip\n    def should_skip(line, xml=False):\n\n# Cross-file Dependency:\nnlm_ingestor.ingestor.formatter\n\nnlm_ingestor.ingestor.formatter.connect\n    def connect(prev, curr):\n\nnlm_ingestor.ingestor.line_parser\n\nnlm_ingestor.ingestor.line_parser.Line\n    class Line:\n\nnlm_ingestor.ingestor.line_parser.Line.continuing_line\n\nnlm_ingestor.ingestor.line_parser.Line.ends_with_period\n\nnlm_ingestor.ingestor.line_parser.Line.has_spaced_characters\n\nnlm_ingestor.ingestor.line_parser.Line.incomplete_line\n\nnlm_ingestor.ingestor.line_parser.Line.is_list_or_row\n\nnlm_ingestor.ingestor.line_parser.Line.line_type\n\nnlm_ingestor.ingestor.line_parser.Line.text\n\n", "dependency_sampled": "# Intra-file Dependency:\nnlm_ingestor.ingestor.processors.should_skip\n    def should_skip(line, xml=False):\n\n# Cross-file Dependency:\nnlm_ingestor.ingestor.formatter\n\nnlm_ingestor.ingestor.line_parser.Line.ends_with_period\n\nnlm_ingestor.ingestor.line_parser.Line.incomplete_line\n\nnlm_ingestor.ingestor.formatter.connect\n    def connect(prev, curr):\n\nnlm_ingestor.ingestor.line_parser.Line.is_list_or_row\n\nnlm_ingestor.ingestor.line_parser.Line.text\n\n", "contexts_above": "import logging\nimport re\nfrom collections import Counter\nfrom collections import defaultdict\n\nfrom . import formatter\nfrom . import line_parser\nfrom . import patterns\nfrom nlm_ingestor.ingestor_utils import spell_utils\nfrom nlm_ingestor.ingestor_utils.utils import sent_tokenize\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\nsu = spell_utils.SpellUtil()\n\n\ndef stem(line):\n    line = line.replace(\"'s\", \"\")\n    line = line.replace(\"\u2019s\", \"\")\n    return line\n\n\ndef check_parentheses(text):\n    count = 0\n    for i in text:\n        if i == \"(\":\n            count += 1\n        elif i == \")\":\n            count -= 1\n    return count == 0\n\n\ndef nlm_tokenize(line):\n    # print(line)\n    tokens = []\n    if not line:\n        line = \"\"\n    line = line.lower()\n    trans_table = line.maketrans(\"-/\", \"  \")\n    line = line.translate(trans_table)\n    line = line.translate(str.maketrans(\"\", \"\", \"\ufffd\\\\(*,.?\u2022\\\\\u27a2\u0192\uf0b7\u2013\\\\)'\\\"\u2014\"))\n    # line = patterns.num_unit.sub(r\"100 \\1\", line)\n    line = patterns.num_unit.sub(r\"\", line)\n    line = stem(line)\n    words = line.split()\n\n    for word in words:\n        if (\n            not word.isdigit()\n            and not word.endswith(\"%\")\n            and not word.startswith(\"$\")\n            and not word.endswith(\"$\")\n        ):\n            tokens.append(word)\n    if len(tokens) == 0:\n        tokens.append(\"unknown\")\n    return tokens\n\n\n# make sure that there is at least one word which is greater than two characters\ndef find_floating_chars(line):\n    words = line.split(\" \")\n    for word in words:\n        if len(word) > 2:\n            return False\n    return True\n\n\ndef is_table_row(line):\n    line = line_parser.Line(line)\n    return line.is_table_row\n\n\ndef should_skip(line, xml=False):\n    return len(line) <= 2 if not xml else len(line) == 0\n\n\n", "contexts_below": "\n\ndef line_list_check(prev_line, curr_line, list_char):\n    # if prev_line is list_item and list_char matches curr_line\n    if list_char == curr_line.text[0] and list_char not in [\"\u201d\", \"'\", '\"', \"(\"]:\n        return True\n    # same char is alpha\n    if prev_line.text[0] == curr_line.text[0] and prev_line.text[0].isalpha():\n        if len(prev_line.text) >= 2 and prev_line.text[1].isupper():\n            # spell check first word\n            first_word = prev_line.text.split(\" \")[0]\n            first_word = first_word.replace(\"'\", \"\")\n            correct_word = su.segment(first_word)\n            if first_word[1:] == correct_word:\n                return True\n    # same char is not alpha but not digit\n    if prev_line.text[0] == curr_line.text[0] and not (\n        prev_line.text[0].isalpha()\n        or prev_line.text[0].isdigit()\n        or list_char not in [\"\u201d\", \"'\", '\"', \"(\"]\n    ):\n        return True\n    return False\n\n\ndef should_join_table(prev_line, curr_line, ents_aligned):\n    \"\"\"\n    Check if next line should be joined as a tr. This makes no assumption if the current line is a table\n    \"\"\"\n    # print()\n    # print(\"Checking to join tr\", prev_line.visual_line.text_list, \"\\n\", curr_line.visual_line.text_list)\n    # check list of spaced words\n    curr_line_ents = len(prev_line.visual_line.text_list)\n    next_line_ents = len(curr_line.visual_line.text_list)\n    ent_match = (\n        curr_line_ents == next_line_ents and curr_line_ents >= 2\n    )  # tr should have at least two elements\n\n    # print(\"tab check\", prev_line.visual_line.tab_count, curr_line.visual_line.tab_count)\n    tab_match = (\n        prev_line.visual_line.tab_count == curr_line.visual_line.tab_count\n        and curr_line.visual_line.tab_count > 0\n    )\n    # casing should also be the same\n    same_case = (\n        prev_line.text[0].islower() == curr_line.text[0].islower()\n        or prev_line.text[0].isupper() == curr_line.text[0].isupper()\n    )\n    colon_check = (\n        prev_line.hit_colon\n        and curr_line.hit_colon\n        and prev_line\n        and same_case\n        and not prev_line.incomplete_line\n    )\n\n    # if prev_line.hit_colon and curr_line.hit_colon:\n    # print()\n    # print(\"colon check\")\n    # print(prev_line.visual_line.text_list)\n    # print(curr_line.visual_line.text_list)\n    # col_check\n    # print(tab_match, ent_match, colon_check)\n    tab_check = prev_line.visual_line.tab_count or curr_line.visual_line.tab_count\n    return (\n        (tab_match and ent_match)\n        or colon_check\n        or (ents_aligned and ent_match and tab_check)\n    )\n\n\ndef check_page_spacing(prev_line, curr_line, spacing_dict):\n    #     print(\"^\"*50)\n    #     print(\"checking page stats\")\n    #     print(prev_line.visual_line.start_fs, prev_line.visual_line.end_fs, prev_line.text)\n    #     print(curr_line.visual_line.start_fs, curr_line.visual_line.end_fs, curr_line.text)\n    #     print()\n\n    diff_top = round(curr_line.visual_line.start_y - prev_line.visual_line.end_y)\n    # find best fs reference\n    prev_line_fs = {prev_line.visual_line.start_fs, prev_line.visual_line.end_fs}\n    curr_line_fs = {curr_line.visual_line.start_fs, curr_line.visual_line.end_fs}\n\n    same_fs = prev_line_fs.intersection(curr_line_fs)\n    fs = min(same_fs) if same_fs else curr_line.visual_line.start_fs\n\n    min_check = (\n        spacing_dict[(fs, diff_top - 1)] if (fs, diff_top - 1) in spacing_dict else None\n    )\n    max_check = (\n        spacing_dict[(fs, diff_top + 1)] if (fs, diff_top + 1) in spacing_dict else None\n    )\n    normal_check = (fs, diff_top) in spacing_dict and spacing_dict[(fs, diff_top)] > 3\n\n    if min_check or normal_check or max_check:\n        # get all fs in spacing dict\n        # see if the diff top is a min\n        # print(\"checking space dict\")\n        distance_list = []\n        for val in spacing_dict:\n            if val[0] == fs and val[1] > 0 and spacing_dict[val] > 2:\n                distance_list.append((val, val[1]))\n        # print(distance_list)\n        val = min(distance_list) if len(distance_list) else []\n        if len(val):\n            join_fs, join_top = val[0]\n        if len(val):\n            join_fs, join_top = val[0]\n\n            if val[0] == (fs, diff_top):  # or close\n                # print(\"SHOULDJOIN\")\n                return True\n            elif (\n                join_fs == fs\n                and ((diff_top - 1) == join_top)\n                or ((diff_top + 1) == join_top)\n            ):\n                return True\n    return False\n\n\ndef compute_overlap(\n    start_x0: float,\n    end_x0: float,\n    start_x1: float,\n    end_x1: float,\n    divide_by_min=True,\n) -> float:\n    \"\"\"\n    Computes the % of intersection (overlap) of two lines w.r.t. the shortest line\n    \"\"\"\n    width_x0 = abs(end_x0 - start_x0)\n    width_x1 = abs(end_x1 - start_x1)\n    if start_x0 <= start_x1 <= end_x0:\n        intersect = min(abs(end_x0 - start_x1), width_x1)\n    elif start_x0 <= end_x1 <= end_x0:\n        intersect = min(abs(end_x1 - start_x0), width_x1)\n    elif start_x1 <= start_x0 <= end_x0 <= end_x1:\n        intersect = abs(end_x0 - start_x0)\n    else:\n        intersect = 0.0\n    if divide_by_min:\n        intersect /= min(width_x0, width_x1) + 1e-5\n    else:\n        intersect /= max(width_x0, width_x1) + 1e-5\n    return intersect\n\n\ndef compute_overlap_top_bottom(\n    start_x0: float,\n    end_x0: float,\n    start_x1: float,\n    end_x1: float,\n) -> float:\n    \"\"\"\n    This is different from the above function.\n    Finds percentage overlap of top to bottom.\n    Score of 100% is possible doesn't reference the shortest line\n    \"\"\"\n    width_x1 = abs(end_x1 - start_x1)\n    if width_x1 == 0:\n        return 0.0\n\n    if start_x0 <= start_x1:\n        # measure from left to right\n        if end_x1 <= end_x0:\n            # if start and end both less, full in subset\n            return 1.0\n        return (end_x1 - start_x0) / width_x1\n    else:\n        # measure from bottom start\n        if end_x1 <= start_x0:\n            return 0.0\n        return (end_x1 - start_x0) / width_x1\n\n\ndef compute_bottom_top_overlap(start_x0, end_x0, start_x1, end_x1):\n    \"\"\"\n    This is different from the above function.\n    Finds percentage overlap of top to bottom.\n    Score of 100% is possible doesn't reference the shortest line\n    \"\"\"\n    # print(start_x0, end_x0)\n    # print(start_x1, end_x1)\n\n    if start_x0 == start_x1 and end_x0 != start_x0:  # aligned with bottom line\n        # print()\n        # print(\"bottom overlap\", (end_x1 - start_x1) / (end_x0 - start_x0))\n        return (end_x1 - start_x1) / (end_x0 - start_x0)\n    # other conditions\n    # elif start_x0 < start_x1 and end_x0 > end_x1: # to the left of bottom line\n    #    return\n    # else: #to the right of bottom line\n    return 1.0\n\n\n# header check for lines with similar font\n# header check for lines with similar font\ndef visual_header_check(prev_line, curr_line, same_font):\n    # check top overlap (small) if the font size is bigger\n    # print()\n    # print(\"visual_header check:\")\n    # print(\"prev\", prev_line.text)\n    # print(\"checking\", curr_line.text)\n    # top also has to be higher\n    # print(\"prev_line.visual_line.start_y, prev_line.visual_line.end_y\")\n    # print(prev_line.visual_line.start_y, prev_line.visual_line.end_y)\n    # print(prev_line.visual_line.start_y, curr_line.visual_line.start_y)\n    if prev_line.visual_line.wrapped_page:\n        return False\n\n    if prev_line.visual_line.start_y < curr_line.visual_line.start_y:\n        prev_line_width = prev_line.visual_line.max_x - prev_line.visual_line.min_x\n        curr_line_width = curr_line.visual_line.max_x - curr_line.visual_line.min_x\n        # print(\"prev_line.visual_line.min_x, prev_line.visual_line.max_x, prev_line.visual_line.end_x\")\n        # print(prev_line.visual_line.min_x, prev_line.visual_line.max_x, prev_line.visual_line.end_x)\n        # print(\"curr_line.visual_line.min_x, curr_line.visual_line.max_x\")\n        # print(curr_line.visual_line.min_x, curr_line.visual_line.max_x)\n        # print(\"prev_line_width / curr_line_width\")\n        # print(prev_line_width / curr_line_width)\n        # print(\"prev_line_width, curr_line_width\")\n        # print(prev_line_width, curr_line_width)\n        if curr_line_width == 0:\n            return False\n        # print(round(prev_line.visual_line.min_x), round(curr_line.visual_line.min_x))\n        if round(prev_line.visual_line.min_x) == round(curr_line.visual_line.min_x):\n            if round(prev_line_width) == round(curr_line_width):\n                # print()\n                # print(\"NOT A HEADER1\")\n                return False\n        offset = 0\n        # print(prev_line.visual_line.min_x, curr_line.visual_line.min_x)\n        # print(prev_line.visual_line.min_x <= curr_line.visual_line.min_x)\n        if prev_line.visual_line.min_x <= curr_line.visual_line.min_x:\n            offset = curr_line.visual_line.min_x - prev_line.visual_line.min_x  # offset\n\n        # print(\"(prev_line_width - offset) / curr_line_width\")\n        # print((prev_line_width - offset) / curr_line_width)\n        overlap_percentage = (prev_line_width - offset) / curr_line_width\n        different_font_style = (\n            prev_line.visual_line.fw != curr_line.visual_line.fw\n            or prev_line.visual_line[1] != curr_line.visual_line[1]\n            or prev_line.visual_line.fs > curr_line.visual_line.fs\n        )\n\n        if (\n            overlap_percentage < 0.3\n            or (different_font_style and overlap_percentage < 0.6)\n            or (prev_line.line_type == \"header\" and different_font_style)\n            # or (prev_line.is_header and different_font_style)\n        ):\n            # print(\"HEADER INDENT\", prev_line.is_header)\n            # print(\"overlap rule::\", (prev_line_width - offset) / curr_line_width)\n            # print(True)\n            return True\n        # print(False)\n    # print()\n    # print(\"NOT A HEADER\")\n    return False\n\n\ndef visual_header_from_stats(prev_line, curr_line, page_stats):\n    prev_fs = prev_line.visual_line.fs\n    curr_fs = curr_line.visual_line.fs\n\n    median_val = round(page_stats[\"median_fs\"])\n    max_val = round(max(page_stats[\"fs_list\"]))\n\n    max_val_diff = ((max_val - prev_fs) / max_val) < 0.2 if max_val != 0 else True\n\n    prev_fs_diff = round(prev_fs - median_val)\n    curr_fs_diff = (\n        round(curr_fs - median_val) if round(curr_fs - median_val) else 0.8\n    )  # curr_fs is the median\n    varied_set = len(set(page_stats[\"fs_list\"])) >= 4\n    rounded_fs_count = Counter([round(x, 3) for x in page_stats[\"fs_list\"]])\n    unique_text = rounded_fs_count[round(prev_fs, 3)] / len(page_stats[\"fs_list\"])\n    prev_curr_ratio_from_median = prev_fs_diff / curr_fs_diff\n\n    #     print(\"prev_fs, curr_fs\", prev_fs, curr_fs)\n    #     print(\"unique text\")\n    #     print(rounded_fs_count[round(prev_fs, 3)], len(page_stats[\"fs_list\"]) )\n    #     print(\"visual_header check\", len(set(page_stats[\"fs_list\"])))\n    #     print(\"varied_set\", varied_set, \"unique_text\", unique_text)\n    #     print(rounded_fs_count)\n    #     print()\n\n    # close from max or far enough from median\n    bigger_text = max_val_diff or (\n        prev_curr_ratio_from_median > 2\n    )  # TODO text must also be relatively uncommon\n\n    if varied_set and (unique_text <= 0.08):\n        if bigger_text and (prev_fs_diff > 1) and (prev_fs_diff - curr_fs_diff) > 0.3:\n            # print(max_val_diff)\n            # print(prev_fs, prev_line.text)\n            # print(curr_fs, curr_line.text)\n            # print()\n            return True\n\n        # header join\n        if bigger_text and curr_fs == prev_fs and (prev_fs_diff > 1):\n            # print(max_val_diff)\n            # print(prev_fs, prev_line.text)\n            # print(curr_fs, curr_line.text)\n            # print()\n            return True\n\n    return False\n\n\n# def visual_clean_lines(lines, page_stats={}, page_info_dict={}):\n# def visual_clean_lines(lines, page_stats={}, page_info_dict={}):\n# def visual_clean_lines(lines, page_stats={}, page_info_dict={}):\ndef check_tr_alignment(prev_line, curr_line):\n    #     print(\"-=\" * 50)\n    #     print(\"check_tr_alignment!\")\n    #     print(prev_line.text)\n    #     print(curr_line.text)\n    #     print()\n    prev_ents = len(prev_line.visual_line.text_list)\n    curr_ents = len(curr_line.visual_line.text_list)\n    prev_positions = prev_line.visual_line.start_x_list\n    curr_positions = curr_line.visual_line.start_x_list\n\n    prev_line_start_ents = prev_line.visual_line.start_x_list_single_ent\n    curr_line_start_ents = curr_line.visual_line.start_x_list_single_ent\n\n    #     print(prev_line_start_ents)\n    #     print(curr_line_start_ents)\n\n    same_ents = prev_ents > 1 and abs(prev_ents - curr_ents) <= 1\n\n    if len(prev_line_start_ents) == len(curr_line_start_ents):\n        prev_positions = prev_line_start_ents\n        curr_positions = curr_line_start_ents\n\n    if len(prev_line_start_ents) == len(curr_positions) and len(\n        prev_line_start_ents,\n    ) != len(\n        prev_positions,\n    ):  # joined p_tags\n        prev_positions = prev_line_start_ents\n\n    if not same_ents:\n        #         print(\"check_tr_alignment False1\")\n        #         print(prev_ents, curr_ents)\n        return False\n\n    #     print(\"CHECKING POSITIONS\")\n    #     print(prev_positions)\n    #     print(curr_positions)\n    for p_x, c_x in zip(prev_positions, curr_positions):\n        p_x = round(p_x)\n        c_x = round(c_x)\n        if abs(p_x - c_x) > 100:\n            #             print(\"False\")\n            #             print(\"check_tr_alignment False3\")\n            return False\n    #     print(\"check_tr_alignment True\")\n    return True\n\n\ndef check_layout(prev_line, curr_line, prev_above_curr):\n    prev_line_width = range(\n        int(prev_line.visual_line.min_x),\n        int(prev_line.visual_line.max_x),\n    )\n\n    # weird edge case\n    if not prev_line_width:\n        prev_line_width = range(\n            int(prev_line.visual_line.max_x),\n            int(prev_line.visual_line.min_x),\n        )\n\n    curr_line_width = range(\n        int(curr_line.visual_line.min_x),\n        int(curr_line.visual_line.max_x),\n    )\n\n    prev_line_width = set(prev_line_width)\n    prev_curr_overlap = prev_line_width.intersection(curr_line_width)\n\n    if prev_curr_overlap and not prev_above_curr:\n        # print(prev_line.text)\n        # print(curr_line.text)\n        # print(\"misplaced text group\")\n        # print()\n        return True\n    return False\n\n\ndef order_blocks(blocks):\n    block_group_dict = defaultdict(list)\n    for idx, block in enumerate(blocks):\n        # print(idx, \"block-group\", block[\"group_id\"], block[\"block_type\"], block['block_text'])\n        group_id = block[\"group_id\"]\n        block_group_dict[group_id].append(block)\n\n    block_group_list = []  # list that holds tuples (group_id, y_pos)\n    for block_group_id in block_group_dict:\n        block_group_list.append(\n            (block_group_id, block_group_dict[block_group_id][0][\"y\"]),\n        )  # append starting y position of group\n\n    block_group_list = sorted(\n        block_group_list,\n        key=lambda x: x[1],\n    )  # sort block groups by y position\n\n    # get list of ordered block group keys\n    ordered_blocks = []\n    for block_group_id, y in block_group_list:\n        ordered_blocks += block_group_dict[block_group_id]\n\n    # for b in original_blocks:\n    # re-index blocks and headers based off of new ordering\n    header_idx = 0\n    for idx, block in enumerate(ordered_blocks):\n        block[\"block_idx\"] = idx\n        if block[\"block_type\"] == \"header\":\n            header_idx = idx\n        ordered_blocks[idx][\"header_block_idx\"] = header_idx\n    return ordered_blocks\n\n\ndef visual_clean_lines(\n    lines,\n    page_stats={},\n    page_info_dict={},\n    page_idx=0,\n    line_set={},\n):\n    page_blocks = []\n    header_block_idx = -1\n    block_idx = 0\n    # block_idx = page_idx\n    style_dict = {}\n    join_font_spacing = False\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n\n    prev_line = None\n    next_line = None\n    # for idx, line in enumerate(lines[12:14]):\n    sentence_visual_end = False\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str):\n            continue\n\n        if line_str in line_set:\n            continue\n\n        if len(line_str.split()) > 8:\n            line_set.add(line_str)\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n            page_details=page_stats,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.visual_line.text_list],\n                \"list_char\": list_char,\n                \"fs\": curr_line.visual_line.start_fs,\n                \"text_group_start_idx\": text_group_start_idx,\n                \"block_list\": curr_line.visual_line.text_list,\n                \"line\": curr_line,\n                \"y\": curr_line.visual_line.start_y,\n                \"group_id\": group_id,\n            }\n\n            prev_line = curr_line\n            block_idx += 1\n            # if (idx <= 3) or (idx >= len(lines) - 3):\n            #     line_without_numbers = re.sub(r\"[^a-zA-Z]+\", \"\", line_str).strip()\n            #     if line_without_numbers:\n            #         # track block_idx for de-duplication\n            #         line_set[line_without_numbers].append((page_idx, block_idx))\n\n            page_blocks.append(block)\n            continue\n\n        # print(\"--\" * 50)\n        # print(prev_line.line_type, \"\\n\", prev_line.text)\n        # print(prev_ents)\n        #         print(prev_line.visual_line.fw_list)\n        # print(prev_line.visual_line.font_family)\n        # print(prev_line.visual_line.fs, prev_line.visual_line.fw, \"prev_line:\", prev_line.line_type, prev_line.text)\n        # print(prev_line.visual_line.mode_fs)\n        # print(curr_line.line_type, \"\\n\", curr_line.text)\n        # print(curr_ents)\n        # print()\n        # print(curr_line.visual_line.font_family)\n        # print(curr_line.visual_line.mode_fs)\n        # print(curr_line.visual_line.fs, curr_line.visual_line.fw, \"curr_line:\", curr_line.line_type, curr_line.text)\n\n        if (\n            len(prev_line.text) > 1\n            and len(curr_line.text) > 1\n            and prev_line.text[:2] == curr_line.text[:2]\n            and prev_line.text[1] == \" \"\n            and not (prev_line.text[0].isdigit() or curr_line.text[0].isdigit())\n            and not (prev_line.text[0].isalpha() or curr_line.text[0].isalpha())\n        ):\n            curr_line.line_type = \"list_item\"\n            curr_line.is_list_item = True\n            curr_line.is_list_or_row = True\n\n            if page_blocks[-1][\"block_type\"] != \"list_item\":\n                page_blocks[-1][\"block_type\"] = \"list_item\"\n                page_blocks[-1][\"list_char\"] = page_blocks[-1][\"block_text\"][0]\n                page_blocks[-1][\"block_text\"] = page_blocks[-1][\"block_text\"][\n                    1:\n                ].lstrip()\n\n        same_start_fs = (\n            abs(prev_line.visual_line.start_fs - curr_line.visual_line.start_fs) < 0.5\n        )\n        same_end_fs = (\n            abs(prev_line.visual_line.end_fs - curr_line.visual_line.end_fs) < 0.5\n        )\n\n        same_end_start_fs = (\n            abs(prev_line.visual_line.end_fs - curr_line.visual_line.start_fs) < 0.5\n        )\n\n        prev_above_curr = (\n            True\n            if prev_line.visual_line.end_y < curr_line.visual_line.start_y\n            else False\n        )\n\n        y_diff = curr_line.visual_line.start_y - prev_line.visual_line.start_y\n\n        top_overlap = compute_overlap_top_bottom(\n            start_x0=prev_line.visual_line.start_x,\n            end_x0=prev_line.visual_line.end_x,\n            start_x1=curr_line.visual_line.start_x,\n            end_x1=curr_line.visual_line.end_x,\n        )\n\n        bottom_overlap = compute_bottom_top_overlap(\n            start_x0=prev_line.visual_line.start_x,\n            end_x0=prev_line.visual_line.end_x,\n            start_x1=curr_line.visual_line.start_x,\n            end_x1=curr_line.visual_line.end_x,\n        )\n\n        prev_overlap_curr = True if bottom_overlap or top_overlap else False\n        use_visual_join = True if prev_above_curr and prev_overlap_curr else False\n        if not use_visual_join and prev_line.incomplete_line:\n            join_font_spacing = True\n\n        if not (prev_line.is_table_row or curr_line.is_table_row):\n\n            if page_stats[\"n_lines\"] <= 3:\n                join_font_spacing = True\n            else:\n                join_font_spacing = check_page_spacing(\n                    prev_line,\n                    curr_line,\n                    page_stats[\"fs_and_diff_next_y\"],\n                )\n\n        # if the font is different and font-family is different\n        different_font_family = (\n            curr_line.visual_line.font_family != prev_line.visual_line.font_family\n        )\n        different_common_fs = (\n            prev_line.visual_line.mode_fs != curr_line.visual_line.mode_fs\n            and prev_line.visual_line.start_fs != curr_line.visual_line.start_fs\n        )\n        different_font = (\n            different_font_family and different_common_fs and not join_font_spacing\n        )\n\n        # start and end characters are same font or the mode of fonts of both lines is the same\n        same_font = (\n            (prev_line.visual_line.fs == curr_line.visual_line.fs)\n            or (same_start_fs and same_end_fs)\n            or same_end_start_fs\n            or prev_line.visual_line.mode_fs == curr_line.visual_line.mode_fs\n        ) and not different_font\n\n        prev_ents = (\n            len(prev_line.visual_line.text_list)\n            if not prev_line.line_type == \"list_item\"\n            else 0\n        )\n        curr_ents = (\n            len(curr_line.visual_line.text_list) if not curr_line.is_list_item else 0\n        )\n\n        ents_aligned = check_tr_alignment(prev_line, curr_line)\n\n        is_incomplete_sent = (\n            prev_line.incomplete_line\n            and not prev_line.ends_with_period\n            or prev_line.ends_with_comma\n        )\n\n        # logic using line after curr\n        if idx + 1 < len(lines):\n            # this is inefficent as line_parser is called twice,\n            # once for next_line and once for curr_line.\n            next_line = lines[idx + 1]\n            # print(\"NEXT LINE\\n\", next_line['text'])\n            next_line_str, next_style_dict, next_text_list = (\n                next_line[\"text\"],\n                next_line[\"style\"],\n                next_line[\"text_list\"],\n            )\n            next_line = line_parser.Line(\n                line_str=next_line_str,\n                style_dict=next_style_dict,\n                text_list=next_text_list,\n                page_details=page_stats,\n            )\n            # if the last line was not a table, check if the next line is a table to avoid single tr\n            if prev_line.line_type != \"table_row\" and not ents_aligned:\n                # check if the next line is a table and matches curr_line\n                next_line_tr = next_line.line_type == \"table_row\" or should_join_table(\n                    curr_line,\n                    next_line,\n                    False,\n                )\n                if not next_line_tr and curr_line.line_type == \"table_row\":\n                    curr_line.line_type = \"para\"\n\n        # if the next line is joinable by visual stats but prev and curr are not\n        # don't join the line (only true by x-span check and y is below for prev cur)\n        # if this is not true ignore the rule\n        prev_not_above_next = (\n            next_line and prev_line.visual_line.start_y > next_line.visual_line.start_y\n        )\n        next_line_join = False\n        if next_line and check_layout(prev_line, next_line, prev_not_above_next):\n            next_line_join = check_page_spacing(\n                curr_line,\n                next_line,\n                page_stats[\"fs_and_diff_next_y\"],\n            )\n\n        # if the prev line is not visually joinable and the curr_next is\n        # make sure the prev_line doesn't join the curr_line\n        curr_next_visual_join = not join_font_spacing and next_line_join\n\n        # print()\n        # print(\"is_incomplete_sent, (join_font_spacing and not sentence_visual_end), curr_line.continuing_line\")\n        # print(is_incomplete_sent, (join_font_spacing and not sentence_visual_end), curr_line.continuing_line)\n        # print(\"join_font_spacing:,\", join_font_spacing)\n\n        is_incomplete = (\n            is_incomplete_sent\n            or (join_font_spacing and not sentence_visual_end)\n            or curr_line.continuing_line\n        )\n        # print(\"is_incomplete\", is_incomplete)\n        has_overlap_with_min = (\n            compute_overlap(\n                curr_line.visual_line.start_x,\n                curr_line.visual_line.end_x,\n                prev_line.visual_line.start_x,\n                prev_line.visual_line.end_x,\n                divide_by_min=True,\n            )\n            > 0.7\n        )\n\n        is_below = curr_line.visual_line.start_y - prev_line.visual_line.start_y > 0\n        is_visually_apart = (has_overlap_with_min and not is_below) or (\n            not has_overlap_with_min and is_below\n        )\n\n        above_bold_below_not = (\n            prev_line.visual_line.fw >= 600.0 and curr_line.visual_line.fw <= 400.0\n        )\n        has_overlap_with_max = (\n            compute_overlap(\n                curr_line.visual_line.start_x,\n                curr_line.visual_line.end_x,\n                prev_line.visual_line.start_x,\n                prev_line.visual_line.end_x,\n                divide_by_min=False,\n            )\n            > 0.3\n        )\n\n        is_not_header_over_para = True\n        if (\n            above_bold_below_not\n            and not has_overlap_with_max\n            and prev_line.line_type == \"header\"\n            and not prev_line.incomplete_line\n        ):\n            is_not_header_over_para = False\n\n        #         print(\"header over para check\")\n        #         print(\"\"\"above_bold_below_not\n        #             and not has_overlap_with_max\n        #             and prev_line.line_type == \"header\"\n        #         \"\"\")\n        #         print(above_bold_below_not)\n        #         print(has_overlap_with_max, j)\n        #         print(prev_line.line_type == \"header\")\n        #         print()\n        #         print(is_not_header_over_para)\n\n        ###########\n        # List item\n\n        if line_list_check(prev_line, curr_line, page_blocks[-1][\"list_char\"]):\n            prev_line.line_type = \"list_item\"\n            curr_line.line_type = \"list_item\"\n            curr_line.is_list_item = True\n            # change prev_line to list item\n            if page_blocks[-1][\"block_type\"] != \"list_item\":\n                page_blocks[-1][\"list_char\"] = page_blocks[-1][\"block_text\"][0]\n                page_blocks[-1][\"block_text\"] = page_blocks[-1][\"block_text\"][\n                    1:\n                ].lstrip()\n            page_blocks[-1][\"block_type\"] = \"list_item\"\n\n        close_text_y = (\n            curr_line.visual_line.start_y\n            - curr_line.visual_line.mode_fs\n            - prev_line.visual_line.start_y\n            - prev_line.visual_line.mode_fs\n        ) <= 0\n        aligned_text = curr_line.visual_line.start_x == prev_line.visual_line.start_x\n\n        title_text = False\n        if len(lines) < 10:\n            title_text = top_overlap == 1.0 and close_text_y and aligned_text\n\n        visual_header = visual_header_check(prev_line, curr_line, same_font)\n\n        list_item_rule = curr_line.has_list_char or (\n            curr_line.numbered_line\n            and not (\n                (prev_line.incomplete_line and curr_line.continuing_line)\n                or join_font_spacing\n            )\n        )\n        last_2_block_tr = False\n        if len(page_blocks) >= 2:\n            last_block_tr = (\n                page_blocks[-1][\"block_type\"] == \"table_row\"\n                and page_blocks[-2][\"block_type\"] == \"table_row\"\n            )\n            if not last_block_tr and curr_line.line_type == \"para\":\n                # check to join\n                if prev_line.incomplete_line and curr_line.continuing_line:\n                    last_2_block_tr = True\n\n        no_space_join = prev_line.ends_with_period and curr_line.text[0] != \" \"\n        visual_header_by_stats = visual_header_from_stats(\n            prev_line,\n            curr_line,\n            page_stats,\n        )\n        header_join = False\n        common_list = curr_line.has_list_char or prev_line.has_list_char\n        if (\n            visual_header_by_stats\n            and curr_line.incomplete_line\n            and same_font\n            and not (prev_line.is_table_row or curr_line.is_table_row or common_list)\n        ):\n            header_join = True\n\n        #         print(\"LINEJOIN CHECK\")\n        #         print(\"positive\\n\", \"*\" * 10)\n        #         print(f\"\\nsame_font:{same_font}\",\n        #               f\"\\nis_incomplete:{is_incomplete}\",\n        #               f\"\\nis_not_header_over_para:{is_not_header_over_para}\")\n        #         print(\"join_font_spacing\", join_font_spacing)\n        #         print(\"header join\", header_join)\n\n        #         print()\n        #         print(\"negative\\n\", \"*\" * 10)\n\n        #         print(f\"\\nis_visually_apart:{is_visually_apart}\",\n        #               f\"\\nshould_join_table(prev_line, curr_line): {should_join_table(prev_line, curr_line, ents_aligned)}\",\n        #               f\"\\ncurr_line.is_list_or_row:{curr_line.is_list_or_row}\",\n        #               f\"\\ncurr_line table {curr_line.line_type == 'table_row'}\",\n        #               f\"\\ncurr_line list {curr_line.is_list_item}\",\n        #               f\"\\nvisual_header {visual_header}\",\n        #               f'\\nprev_line.line_type == \"table_row\", {prev_line.line_type == \"table_row\"}')\n\n        if (\n            same_font\n            and not should_join_table(prev_line, curr_line, ents_aligned)\n            and not (curr_line.line_type == \"table_row\" or list_item_rule)\n            and not (prev_line.line_type == \"table_row\" and not last_2_block_tr)\n            and is_incomplete\n            and not curr_next_visual_join  # is_visually_apart\n            and not visual_header\n            or not check_parentheses(prev_line.text)\n            and is_not_header_over_para\n            and not no_space_join\n            or title_text\n            or header_join\n        ):\n\n            # print(\"JOIN\")\n\n            if not is_visually_apart and bottom_overlap < 0.5:\n                # this would signify end of paragraph\n                sentence_visual_end = True\n            else:\n                sentence_visual_end = False\n            if page_stats[\"n_lines\"] <= 3:\n                page_blocks[-1][\"block_type\"] = \"header\"\n            elif (\n                not prev_line.line_type == \"list_item\"\n            ):  # and not curr_line.visual_line.is_header:\n                page_blocks[-1][\"block_type\"] = \"para\"\n            new_text = formatter.connect(\n                prev_line.text.rstrip(),\n                curr_line.text.lstrip(),\n            )\n            new_text_list = (\n                prev_line.visual_line.text_list + curr_line.visual_line.text_list\n            )\n\n            # print(\"Max ex min ex assignment\")\n            max_x = max(prev_line.visual_line.max_x, prev_line.visual_line.max_x)\n            min_x = min(prev_line.visual_line.min_x, curr_line.visual_line.min_x)\n\n            prev_line_type = prev_line.line_type\n\n            page_blocks[-1][\"block_text\"] = new_text\n            prev_start_y = prev_line.visual_line.start_y\n\n            curr_start_y = curr_line.visual_line.start_y\n            prev_end_y = prev_line.visual_line.end_y\n            wrapped_page = prev_line.visual_line.wrapped_page\n\n            # pass the line parser attributes\n            prev_line = curr_line\n\n            # add appended text and text_list, preserve the line type\n            prev_line.text = new_text\n            prev_line.visual_line.start_y = prev_start_y\n            prev_line.visual_line.text_list = new_text_list\n            prev_line.line_type = prev_line_type\n            prev_line.visual_line.min_x = min_x\n            prev_line.visual_line.max_x = max_x\n            prev_line.visual_line.wrapped_page = wrapped_page\n\n            if curr_start_y < prev_end_y:\n                prev_line.visual_line.wrapped_page = True\n        #             print(prev_start_y)\n        #             print(\"Join\")\n        #             print()\n        #             print(\"-\" * 50)\n        #            print()\n        # new block\n        else:\n            # print(\"NEW block\")\n            # print(\"*\" * 50)\n\n            if not is_visually_apart and bottom_overlap < 0.5:\n                # this would signify end of paragraph\n                sentence_visual_end = True\n            else:\n                sentence_visual_end = False\n\n            # print(\"-\"*50)\n            colon_rule = (\n                prev_line.hit_colon and curr_line.hit_colon and prev_ents == curr_ents\n            )\n            # normal case\n            tab_check_join = {\n                prev_line.visual_line.tab_count_join,\n                prev_line.visual_line.tab_count,\n            } & {curr_line.visual_line.tab_count_join, curr_line.visual_line.tab_count}\n            tab_check = sum(tab_check_join) > 0\n            # print(\"-+\" * 50)\n            # print(\"TAB POSITIONS\")\n            # print(prev_line.text)\n            # print(prev_line.visual_line.start_x_list)\n            # print(prev_line.visual_line.start_x_list_single_ent)\n            # print(prev_line.visual_line.tab_count)\n            # print(prev_line.visual_line.tab_count_join)\n            #\n            # print(curr_line.text)\n            # print(curr_line.visual_line.start_x_list)\n            # print(curr_line.visual_line.start_x_list_single_ent)\n            # print(curr_line.visual_line.tab_count)\n            # print(curr_line.visual_line.tab_count_join)\n            # print(\"tabcheck\", tab_check)\n            # print(\"ents_aligned\", ents_aligned)\n            # print(prev_ents, curr_ents)\n            # print(curr_line.visual_line.text_list)\n            # print(\"-+\" * 50)\n\n            if visual_header_by_stats and prev_line.line_type != \"table_row\":\n                page_blocks[-1][\"block_type\"] = \"header\"\n\n            elif (\n                colon_rule\n                and prev_ents == 1\n                and prev_line.line_type != \"list_item\"\n                and not (prev_line.incomplete_line and curr_line.continuing_line)\n            ):\n                # print(\"Table Conversion\")\n                # print()\n                # print(\"colon check\")\n                # print(prev_line.text.split(\":\"))\n                # print(curr_line.text.split(\":\"))\n                # print(\"TR1\")\n                new_text_list = prev_line.text.split(\":\")\n                new_text_list = [new_text_list[0] + \":\", new_text_list[1:]]\n                page_blocks[-1][\"block_type\"] = \"table_row\"\n                page_blocks[-1][\"block_list\"]: new_text_list\n                if text_group_start:\n                    text_group_start = False\n                    text_group_start_idx = page_blocks[-1][\"block_idx\"]\n                    page_blocks[-1][\"text_group_start_idx\"] = text_group_start_idx\n                curr_line.line_type = \"table_row\"\n                curr_line.is_list_or_row = True\n            #                 print(\"Table Conversion!\")\n            #                 print(prev_ents, curr_ents)\n            #                 print(page_blocks[-1][\"block_text\"])\n            #                 print(\"TR3\")\n\n            elif (\n                tab_check and ents_aligned and prev_line.line_type != \"list_item\"\n            ) or (colon_rule and not prev_line.incomplete_line):\n                #                 print(\"Table Conversion\")\n                #                 print(prev_ents, curr_ents)\n                #                 print(page_blocks[-1][\"block_text\"])\n                #                 print(\"TR2\")\n                page_blocks[-1][\"block_type\"] = \"table_row\"\n                if text_group_start:\n                    text_group_start = False\n                    text_group_start_idx = page_blocks[-1][\"block_idx\"]\n                    page_blocks[-1][\"text_group_start_idx\"] = text_group_start_idx\n                curr_line.line_type = \"table_row\"\n            else:\n                text_group_start = True\n                text_group_start_idx = -1\n\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            if (visual_header or visual_header_by_stats) and not (\n                prev_line.line_type == \"list_item\"\n                or prev_line.line_type == \"numbered_list_item\"\n            ):\n                page_blocks[-1][\"block_type\"] = \"header\"\n            #             print()\n            #             print(\"*\" * 40)\n            #             print(\"NEW BLOCK\")\n            # print()\n            # print(\"*\" * 40)\n            # print(curr_line.line_type, curr_line.text)\n            # group attribute\n            if check_layout(prev_line, curr_line, prev_above_curr) or y_diff < 0:\n                group_id += 1\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.visual_line.text_list],\n                \"text_group_start_idx\": text_group_start_idx,\n                \"list_char\": list_char,\n                \"group_id\": group_id,\n                \"fs\": curr_line.visual_line.start_fs,\n                \"x\": curr_line.visual_line.start_x,\n                \"y\": curr_line.visual_line.start_y,\n                \"line\": curr_line,\n                \"block_list\": curr_line.visual_line.text_list,\n            }\n            # This is to account for when the headers get false positive #TODO improve header code\n            prev_text = page_blocks[-1][\"block_text\"]\n\n            if page_blocks[-1][\"block_type\"] == \"header\" and (\n                len(sent_tokenize(prev_text)) >= 2 or len(prev_text.split()) > 16\n            ):\n                page_blocks[-1][\"block_type\"] = \"para\"\n\n            prev_line = curr_line\n            block_idx += 1\n            page_blocks.append(block)\n\n    # not too many blocks there may be title text missed\n    if len(page_blocks) <= 2:\n        for idx, block in enumerate(page_blocks):\n            if \".\" not in block[\"block_text\"] and len(block[\"block_text\"].split()) < 10:\n                page_blocks[idx][\"block_type\"] = \"header\"\n\n    page_blocks = order_blocks(page_blocks)\n\n    return page_blocks, line_set\n\n\ndef clean_line(line):\n    line = line.replace(\"\\n\", \" \")\n    line = line.replace(\"\\t\", \" \")\n    line = line.strip()\n    return line\n\n\ndef fix_spaced_characters(line_text):\n    line_text = re.sub(r\"\\s+\", \"\", line_text)\n    return su.segment(line_text)\n\n\ndef connect(prev, curr):\n    has_space = prev.endswith(\" \")\n    result = prev + (\"\" if has_space else \" \") + curr\n    return result\n\n\ndef get_numbers(line):\n    # test = re.compile(r\"[0-9]+\\.?[0-9]?\")\n    regex = re.compile(r\"\\$?(\\d*(\\d\\.?|\\.\\d{1,2}))$\")\n    return regex.search(line)\n\n\ndef check_block_join(prev_block, block):\n    prev_text = prev_block[\"block_text\"]\n    curr_text = block[\"block_text\"]\n    blocks_are_paras = (\n        prev_block[\"block_type\"] == \"para\" and block[\"block_type\"] == \"para\"\n    )\n    if len(prev_text.strip()) and len(curr_text.strip()) and blocks_are_paras:\n        prev_line = line_parser.Line(prev_block[\"block_text\"])\n        curr_line = line_parser.Line(block[\"block_text\"])\n        if prev_line.incomplete_line or curr_line.continuing_line:\n            return True\n    return False\n\n\ndef join_blocks(page_blocks, blocks):\n    prev_last_block = page_blocks[-1][-1]\n    # update page blocks and blocks\n    # prev_blocks = page_blocks[-1]\n    # last_prev_block = prev_blocks[-1]\n    # check to join last_prev_block with first blocks[0]\n    # if it's a join, pop the block and join, subtract block indexes\n    prev_last_block[\"block_text\"] = (\n        prev_last_block[\"block_text\"].rstrip() + \" \" + blocks[0][\"block_text\"].lstrip()\n    )\n    prev_last_block[\"block_list\"].append(blocks[0][\"block_list\"])\n    # print(prev_block)\n    page_blocks[-1][-1] = prev_last_block\n    for block in blocks[1:]:\n        block[\"block_idx\"] -= 1\n    return page_blocks, blocks[1:]\n", "input_code": "def clean_lines(lines, xml=False):\n\n    \"\"\"\n    This function processes a list of text lines, cleans them, and organizes them into blocks based on their content and structure. It removes duplicate lines (ignoring numbers), fixes spaced characters, connects incomplete lines, and categorizes lines into paragraphs, headers, or list items. Each block of text is then appended to a result list with metadata about the block type, index, and related header block index if applicable.\n\n    Input-Output Arguments\n    :param lines: List of strings. The text lines to be processed and cleaned.\n    :param xml: Bool, optional. A flag indicating whether the processing should consider XML-specific formatting rules.\n    :return: List of dictionaries. Each dictionary represents a block of text with metadata including the block index, text, type, starting index of the text group, list of blocks if any, the index of the associated header block, and the level of indentation or list.\n    \"\"\"", "reference_steps": "1. Initialize variables: an empty list `result` to store the final output, `running_line` as an empty string to accumulate text, `line_buffer` as an empty list to hold lines, `line_type` as \"para\", `header_block_idx` as -1, `block_idx` as 0, and `line_set` as an empty set to track unique lines.\n\n2. Iterate over each line in the input `lines`, cleaning each line using `clean_line(line_str)`.\n\n3. Skip the current line if `should_skip(line_str, xml=xml)` returns True or if the line without numbers is already in `line_set`.\n\n4. Parse the current line using `line_parser.Line(line_str)` and fix spaced characters if `xml` is False and the line has spaced characters using `fix_spaced_characters(line_str)`.\n\n5. If `line_buffer` is not empty, check if the previous line was incomplete or if the current line is continuing from the previous line.\n\n6. If the current line is a continuation of the previous line or the previous line was incomplete and the current line is not a list or row, connect the current line to `running_line` and append the current line to `line_buffer`.\n\n7. If the current line is not a continuation and the previous line was not incomplete, create a new block dictionary with the accumulated text and metadata, append it to `result`, and reset `running_line`, `line_buffer`, and `line_type`.\n\n8. If `line_buffer` is empty, set `running_line`, `line_type`, and `line_buffer` to the current line's text, type, and a list containing the current line, respectively.\n\n9. After the loop, if the last line type is \"list_item\" and starts with a special character, remove the character from `running_line`.\n\n10. Create a final block dictionary with the remaining accumulated text and metadata, append it to `result`, and return `result`.", "reference_code": "def clean_lines(lines, xml=False):\n    result = []\n    running_line = \"\"\n    line_buffer = []\n    line_type = \"para\"\n    header_block_idx = -1\n    block_idx = 0\n    line_set = set()\n    for line_str in lines:\n        # print(line_str)\n        line_str = clean_line(line_str)\n\n        if should_skip(line_str, xml=xml):\n            continue\n        line_without_numbers = re.sub(r\"\\d+\", \"\", line_str)\n        if line_without_numbers in line_set:\n            continue\n        else:\n            line_set.add(line_without_numbers)\n\n        curr_line = line_parser.Line(line_str)\n\n        # this converst strings like 'e x e c u t i v e summary' to 'executive summary'\n        if not xml and curr_line.has_spaced_characters:\n            line_str = fix_spaced_characters(line_str)\n            curr_line = line_parser.Line(line_str)\n\n        if len(line_buffer) > 0:\n\n            # find out if previous line was a discontinous line\n            prev_line = line_buffer[-1]\n\n            logger.debug(\"========\")\n            logger.debug(f\"{prev_line.incomplete_line} >> {prev_line.text} \\n\")\n            logger.debug(f\"{curr_line.continuing_line} >> {curr_line.text} \\n\")\n            # keep connecting lines as long as they seem incomplete\n            is_incomplete = prev_line.incomplete_line or (\n                len(line_buffer) > 1 and not prev_line.ends_with_period\n            )\n            logger.debug(\n                f\"incomplete: {is_incomplete}, is_list_or_row: {curr_line.is_list_or_row}, continuing_line: {curr_line.continuing_line}\",\n            )\n            if (\n                is_incomplete\n                and not (curr_line.is_list_or_row or curr_line.line_type == \"list_item\")\n            ) or curr_line.continuing_line:\n                logger.debug(\"connecting..\")\n                running_line = formatter.connect(running_line, curr_line.text)\n                line_buffer.append(curr_line)\n                # if we are connecting lines, then this has to be a para unless it is a list_item, basically no headers\n                if not line_type == \"list_item\":\n                    line_type = \"para\"\n            else:  # commit the line and start a new line\n                # remove different types of bulletted list (for better formatting) but do not touch numbered line\n                logger.debug(\"starting new line..\")\n                # if line_type == \"list_item\":\n                #     running_line = running_line[1:].lstrip()\n\n                if line_type == \"header\":\n                    header_block_idx = block_idx\n\n                block = {\n                    \"block_idx\": block_idx,\n                    \"block_text\": running_line,\n                    \"block_type\": line_type,\n                    \"text_group_start_idx\": -1,\n                    \"block_list\": [],\n                    \"header_block_idx\": header_block_idx,\n                    \"level\": 0,\n                }\n\n                result.append(block)\n\n                block_idx = block_idx + 1\n\n                running_line = curr_line.text\n                line_buffer = [curr_line]\n                line_type = curr_line.line_type\n            logger.debug(\"========\")\n        else:\n            running_line = curr_line.text\n            line_type = curr_line.line_type\n            line_buffer = [curr_line]\n\n    if line_type == \"list_item\" and running_line[0] in \"\ufffd\\\\*,.?\u2022\\\\\u27a2\u0192\uf0b7\u2013\\\\'\\\"\u2014\":\n        running_line = running_line[1:].lstrip()\n\n    block = {\n        \"block_idx\": block_idx,\n        \"block_text\": running_line,\n        \"block_type\": line_type,\n        \"text_group_start_idx\": -1,\n        \"block_list\": [],\n        \"header_block_idx\": header_block_idx,\n        \"level\": 0,\n    }\n\n    result.append(block)\n    return result\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "type": "function", "class_name": null, "function_name": "sent_tokenize", "dependency_all": "# Intra-file Dependency:\nnlm_ingestor.ingestor_utils.utils.bracket_rule\n\nnlm_ingestor.ingestor_utils.utils.nltk_tokenzier\n\nnlm_ingestor.ingestor_utils.utils.quotation_pattern\n\nnlm_ingestor.ingestor_utils.utils.rules\n\nnlm_ingestor.ingestor_utils.utils.space_rule\n\n", "dependency_sampled": "# Intra-file Dependency:\nnlm_ingestor.ingestor_utils.utils.space_rule\n\nnlm_ingestor.ingestor_utils.utils.rules\n\n", "contexts_above": "import json\nimport re\n\nimport numpy as np\nfrom nltk import load\nfrom nltk import PunktSentenceTokenizer\n\n\nnltk_abbs = load(\"tokenizers/punkt/{}.pickle\".format(\"english\"))._params.abbrev_types\n\n\nclass NpEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return super(NpEncoder, self).default(obj)\n\n\nnlm_abbs = {\n    \"u.s\",\n    \"u.s.a\",\n    \"n.w\",\n    \"p.o\",\n    \"po\",\n    \"st\",\n    \"ave\",\n    \"blvd\",\n    \"ctr\",\n    \"cir\",\n    \"ct\",\n    \"dr\",\n    \"mtn\",\n    \"apt\",\n    \"hwy\",\n    \"esq\",\n    \"fig\",\n    \"no\",\n    \"sec\",\n    \"n.a\",\n    \"s.a.b\",\n    \"non-u.s\",\n    \"cap\",\n    'u.s.c',\n    \"ste\",\n}\n\nnlm_special_abbs = {\n    \"inc\",\n}\n\nabbs = nltk_abbs | nlm_abbs\n\nnltk_tokenzier = PunktSentenceTokenizer()\n\nrules = []\n\nfor abb in abbs:\n    # match start of the sentence\n    pattern = fr\"^{abb}.\\s\"\n    replaced = f\"{abb}_ \"\n\n    # case insensitive replacement for synonyms\n    rule = re.compile(pattern, re.IGNORECASE)\n    rules.append((rule, replaced))\n\n    # match token in sentence\n    pattern = fr\"\\s{abb}.\\s\"\n    replaced = f\" {abb}_ \"\n\n    # case insensitive replacement for synonyms\n    rule = re.compile(pattern, re.IGNORECASE)\n    rules.append((rule, replaced))\n\nfor abb in nlm_special_abbs:\n    pattern = fr\"{abb}\\.\"\n    replaced = f\"{abb}_\"\n    rule = re.compile(pattern, re.IGNORECASE)\n    rules.append((rule, replaced))\n\n# match content inside brackets\n# (?<=\\() ==> starts with \"(\"\n# ([^)]+) ==> repeat not \")\"\n# (?=\\))\") ==> ends with \")\"\nbracket_rule = re.compile(r\"(?<=\\()([^)]+)(?=\\))\")\nspace_rule = re.compile(r\"\\s([.'](?:\\s|$|\\D))\", re.IGNORECASE)  # Remove any space between punctuations (.')\nquotation_pattern = re.compile(r'[\u201d\u201c\"\u2018\u2019\\']')\n\n\n", "contexts_below": "\n\ndef divide_list_into_chunks(lst, n):\n    # looping till length l\n    for i in range(0, len(lst), n):\n        yield lst[i : i + n]\n\n\ndef normalize(X):\n    norms = np.einsum(\"ij,ij->i\", X, X)\n    np.sqrt(norms, norms)\n\n    X /= norms[:, np.newaxis]\n    return X\n\n\ndef detect_block_center_aligned(block, page_width):\n    center_location = block[\"box_style\"][1] + block[\"box_style\"][3] / 2\n    center_aligned = abs(center_location - page_width / 2) < page_width * 0.01\n    width_check = block[\"box_style\"][3] * 2 < page_width\n    return center_aligned and width_check\n\n\ndef detect_block_center_of_page(block, page_height):\n    bottom = block[\"box_style\"][0] + block[\"box_style\"][4]\n    center_of_page = (page_height / 3) <= bottom <= ((2 * page_height) / 3)\n    return center_of_page\n\n\ndef check_char_is_word_boundary(c):\n    if c.isalnum():\n        return False\n    if c in ['-', '_']:\n        return False\n    return True\n\ndef blocks_to_sents(blocks, flatten_merged_table=False, debug=False):\n    block_texts = []\n    block_info = []\n    header_block_idx = -1\n    header_match_idx = -1\n    header_match_idx_offset = -1\n    header_block_text = \"\"\n    is_rendering_table = False\n    is_rendering_merged_cells = False\n    table_idx = 0\n    levels = []\n    prev_header = None\n    block_idx = 0\n    for block_idx, block in enumerate(blocks):\n        block_type = block[\"block_type\"]\n        if block_type == \"header\":\n            if debug:\n                print(\"---\", block[\"level\"], block[\"block_text\"])\n            header_block_text = block[\"block_text\"]\n            header_block_idx = block[\"block_idx\"]\n            header_match_idx = header_match_idx_offset + 1\n            if prev_header and block[\"level\"] <= prev_header['level'] and len(levels) > 0:\n                while len(levels) > 0 and levels[-1][\"level\"] >= block[\"level\"]:\n                    if debug:\n                        print(\"<<\", levels[-1][\"level\"], levels[-1][\"block_text\"])\n                    levels.pop(-1)\n            if debug:\n                print(\">>\", block[\"block_text\"])\n            levels.append(block)\n            prev_header = block\n            if debug:\n                print(\"-\", [str(level['level']) + \"-\" + level['block_text'] for level in levels])\n        block[\"header_text\"] = header_block_text\n        block[\"header_block_idx\"] = header_block_idx\n        block[\"header_match_idx\"] = header_match_idx\n        block[\"block_idx\"] = block_idx\n\n        level_chain = []\n        for level in levels:\n            level_chain.append({\"block_idx\": level[\"block_idx\"], \"block_text\": level[\"block_text\"]})\n        # remove a level for header\n        if block_type == \"header\":\n            level_chain = level_chain[:-1]\n        level_chain.reverse()\n        block[\"level_chain\"] = level_chain\n\n        # if block_type == \"header\" or block_type == \"table_row\":\n        if (\n                block_type == \"header\"\n                and not is_rendering_table and 'is_table_start' not in block\n        ):\n            block_texts.append(block[\"block_text\"])\n            # append text from next block to header block\n            # TODO: something happened here, it messed up the match_text\n            # if block_type == \"header\" and block_idx + 1 < len(blocks):\n            #     block[\n            #         \"block_text\"\n            #     ] += blocks[block_idx+1]['block_text']\n\n            block_info.append(block)\n            header_match_idx_offset += 1\n        elif (\n                block_type == \"list_item\" or block_type == \"para\" or block_type == \"numbered_list_item\"\n        ) and not is_rendering_table:\n            block_sents = block[\"block_sents\"]\n            header_match_idx_offset += len(block_sents)\n            for sent in block_sents:\n                block_texts.append(sent)\n                block_info.append(block)\n        elif 'is_table_start' in block:\n            is_rendering_table = True\n            if 'has_merged_cells' in block:\n                is_rendering_merged_cells = True\n        elif 'is_table_start' not in block and not is_rendering_table and block_type == \"table_row\":\n            block_info.append(block)\n            block_texts.append(block[\"block_text\"])\n            header_match_idx_offset += 1\n\n        if is_rendering_table:\n            if is_rendering_merged_cells and \"effective_para\" in block and flatten_merged_table:\n                eff_header_block = block[\"effective_header\"]\n                eff_para_block = block[\"effective_para\"]\n\n                eff_header_block[\"header_text\"] = block[\"header_text\"]\n                eff_header_block[\"header_block_idx\"] = block[\"block_idx\"]\n                eff_header_block[\"header_match_idx\"] = header_match_idx_offset + 1\n                eff_header_block[\"level\"] = block[\"level\"] + 1\n                eff_header_block[\"level_chain\"] = block[\"level_chain\"]\n\n                eff_para_block[\"header_block_idx\"] = block[\"block_idx\"]\n                eff_para_block[\"header_match_idx\"] = header_match_idx_offset + 1\n                eff_para_block[\"level\"] = block[\"level\"] + 2\n                eff_para_block[\"level_chain\"] = [\n                                {\n                                    \"block_idx\": eff_header_block[\"block_idx\"],\n                                    \"block_text\": eff_header_block[\"block_text\"],\n                                },\n                ] + eff_header_block[\"level_chain\"]\n                header_match_idx_offset += 1\n                block_info.append(block[\"effective_header\"])\n                block_texts.append(block[\"effective_header\"][\"block_text\"])\n                for sent in block[\"effective_para\"][\"block_sents\"]:\n                    block_texts.append(sent)\n                    block_info.append(block[\"effective_para\"])\n                header_match_idx_offset += len(block[\"effective_para\"][\"block_sents\"])\n            else:\n                block[\"table_idx\"] = table_idx\n                block_info.append(block)\n                block_texts.append(block[\"block_text\"])\n                header_match_idx_offset += 1\n\n        if 'is_table_end' in block:\n            is_rendering_table = False\n            table_idx += 1\n\n    return block_texts, block_info\n\n\ndef get_block_texts(blocks):\n    block_texts = []\n    block_info = []\n    for block in blocks:\n        block_type = block[\"block_type\"]\n        if (\n            block_type == \"list_item\"\n            or block_type == \"para\"\n            or block_type == \"numbered_list_item\"\n            or block_type == \"header\"\n        ):\n            block_texts.append(block[\"block_text\"])\n            block_info.append(block)\n    return block_texts, block_info", "input_code": "def sent_tokenize(org_texts):\n\n    \"\"\"\n    The function tokenizes a given text into sentences, handling special cases such as paragraphs separated by new lines, punctuation at the beginning of the text, and ensuring that sentences within brackets are not broken. It also normalizes quotation marks within the text.\n\n    Input-Output Arguments\n    :param org_texts: String. The original text that needs to be tokenized into sentences. It is used as the input to apply various tokenization and normalization rules.\n    :return: List of Strings. The tokenized sentences from the original text. If the input text is empty or None, it returns the input as is.\n\n    Note: The function uses several predefined rules and patterns (such as `space_rule`, `bracket_rule`, `rules`, and `quotation_pattern`) and an instance of a tokenizer (`nltk_tokenzier`) to perform the tokenization and normalization. These components are assumed to be defined outside the function.\n    \"\"\"", "reference_steps": "1. Define a function `sent_tokenize` that takes a string `org_texts` as input.\n2. Return the input string as is if it is empty.\n3. Initialize an empty list `sents` to store the tokenized sentences.\n4. Split the input text into paragraphs using newline characters (`\\n`).\n5. For each paragraph, apply a space rule regex substitution to clean up spaces.\n6. Remove any leading punctuation that can cause issues with sentence tokenization.\n7. Apply a bracket rule to prevent sentence breaking inside brackets, replacing periods within brackets to avoid incorrect splits.\n8. Iterate over a set of predefined rules (`rules`), applying regex substitutions to the text to handle special cases for sentence tokenization.\n9. Normalize all quotation marks in the text to a consistent form.\n10. Use the `nltk_tokenizer` to tokenize the modified text into sentences and then map these back to the original text offsets, adding the original sentences to the `sents` list.\n11. If the first sentence is a single character followed by a period, merge it with the second sentence and adjust the list of sentences accordingly.\n12. Return the list of tokenized sentences `sents`.", "reference_code": "def sent_tokenize(org_texts):\n    if not org_texts:\n        return org_texts\n\n    sents = []\n\n    # in case org_texts has \\n, break it into multiple paragraph\n    # edge case for html and markdown\n    for org_text in org_texts.split(\"\\n\"):\n        org_text = space_rule.sub(r'\\1', org_text)\n        modified_text = re.sub(r'^([.,?!]\\s+)+', \"\", org_text)  # To handle bug https://github.com/nltk/nltk/issues/2925\n        orig_offset = abs(len(org_text) - len(modified_text))\n\n        # do not break bracket\n        for span_group in bracket_rule.finditer(modified_text):\n            start_byte, end_byte = span_group.span()\n            span = modified_text[start_byte:end_byte]\n            # skip this logic when span is too big? disabled for now\n            # if len(span.split()) >= 10:\n            #     continue\n            modified_text = modified_text.replace(\n                f\"({span})\", f\"_{span.replace('.','_')}_\",\n            )\n\n        for rule, replaced in rules:\n            modified_text = rule.sub(replaced, modified_text)\n        # Normalize all the quotation.\n        modified_text = quotation_pattern.sub(\"\\\"\", modified_text)\n\n        modified_sents = nltk_tokenzier.tokenize(modified_text)\n\n        offset = orig_offset\n        sent_idx = 0\n        while offset < len(modified_text) and sent_idx < len(modified_sents):\n            if modified_text[offset] == \" \":\n                offset += 1\n                continue\n\n            # cut org_text based on lengths of modified_sent\n            modified_sent = modified_sents[sent_idx]\n            sents.append(org_text[offset: offset + len(modified_sent)])\n\n            offset += len(modified_sent)\n            sent_idx += 1\n    if len(sents) >= 2 and re.match(r\"^.\\.$\", sents[0]):\n        sents[1] = sents[0] + \" \" + sents[1]\n        sents = sents[1:]\n\n    return sents\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "type": "method", "class_name": "SearchArray", "function_name": "positions", "dependency_all": "# Intra-class Dependency:\nsearcharray.postings.SearchArray.posns\n\nsearcharray.postings.SearchArray.term_dict\n\nsearcharray.postings.SearchArray.term_mat\n\n", "dependency_sampled": "# Intra-class Dependency:\nsearcharray.postings.SearchArray.term_dict\n\n", "contexts_above": "\"\"\"Tokenized, searchable text as a pandas dtype.\"\"\"\nimport pandas as pd\nimport numbers\nfrom pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype\nfrom pandas.api.types import is_list_like\nfrom pandas.api.extensions import take\nimport json\nfrom collections import Counter\nimport warnings\nimport logging\nfrom typing import List, Union, Optional, Iterable\n\n\nimport numpy as np\nfrom searcharray.phrase.scan_merge import scan_merge_ins\nfrom searcharray.phrase.posn_diffs import compute_phrase_freqs\nfrom searcharray.phrase.middle_out import PosnBitArray\nfrom searcharray.similarity import Similarity, default_bm25\nfrom searcharray.indexing import build_index_from_tokenizer, build_index_from_terms_list\nfrom searcharray.term_dict import TermMissingError\n\nlogger = logging.getLogger(__name__)\n\n# When running in pytest\nimport sys  # noqa\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(logging.ERROR)\nformatter = logging.Formatter(\"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\")\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.setLevel(logging.ERROR)\n\n\nclass Terms:\n    \"\"\"An indexed search doc - a single bag of tokenized words and positions.\"\"\"\n\n    def __init__(self,\n                 postings,\n                 doc_len: int = 0,\n                 posns: Optional[dict] = None,\n                 encoded=False):\n        self.postings = postings\n        self.posns = None\n        self.encoded = encoded\n        self.doc_len = doc_len\n        self.posns = posns\n\n    def _validate_posns(self):\n        # (For testing/assertions) - Confirm every term in positions also in postings\n        if self.posns is None:\n            return\n        for term in self.posns:\n            if term not in self.postings:\n                raise ValueError(f\"Term {term} in positions but not in postings. \")\n\n    def termfreq(self, token):\n        return self.postings[token]\n\n    def terms(self):\n        return self.postings.items()\n\n    def positions(self, term=None):\n        if self.posns is None:\n            return {}\n        if term is None:\n            posns = self.posns.items()\n        else:\n            posns = self.posns[term]\n        return posns\n\n    def raw_positions(self, term_dict, term=None):\n        if self.posns is None:\n            return {}\n        if term is None:\n            posns = [(term_dict.get_term_id(term), posns) for term, posns in self.posns.items()]\n        else:\n            posns = [(term_dict.get_term_id(term), self.posns[term])]\n        return posns\n\n    def tf_to_dense(self, term_dict):\n        \"\"\"Convert to a dense vector of term frequencies.\"\"\"\n        dense = np.zeros(len(term_dict))\n        for term, freq in self.terms():\n            dense[term_dict.get_term_id(term)] = freq\n        return dense\n\n    def __len__(self):\n        return len(self.postings)\n\n    def __repr__(self):\n        posting_keys = set(self.postings.keys())\n        rval = f\"Terms({posting_keys})\"\n        return rval\n\n    def __str__(self):\n        return repr(self)\n\n    def __eq__(self, other):\n        # Flip to the other implementation if we're comparing to a SearchArray\n        # to get a boolean array back\n        if isinstance(other, SearchArray):\n            return other == self\n        same_postings = isinstance(other, Terms) and self.postings == other.postings\n        if same_postings and self.doc_len == other.doc_len:\n            return True\n\n    def __lt__(self, other):\n        # return isinstance(other, Terms) and hash(self) < hash(other)\n        keys_both = set(self.postings.keys()).union(set(other.postings.keys()))\n        # Sort lexically\n        keys_both = sorted(keys_both)\n\n        # Iterate as if these are two vectors of the same large dimensional vector sparse\n        for key in keys_both:\n            lhs_val = 0\n            rhs_val = 0\n            try:\n                lhs_val = self.postings[key]\n            except KeyError:\n                pass\n\n            try:\n                rhs_val = other.postings[key]\n            except KeyError:\n                pass\n\n            if lhs_val < rhs_val:\n                return True\n            elif lhs_val > rhs_val:\n                return False\n            else:\n                continue\n        return False\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    def __gt__(self, other):\n        return not (self < other) and self != other\n\n    def __hash__(self):\n        return hash(json.dumps(self.postings, sort_keys=True))\n\n\nclass TermsDtype(ExtensionDtype):\n    \"\"\"Pandas dtype for terms.\"\"\"\n\n    name = 'tokenized_text'\n    type = Terms\n    kind = 'O'\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\n                \"'construct_from_string' expects a string, got {}\".format(type(string))\n            )\n        elif string == cls.name:\n            return cls()\n        else:\n            raise TypeError(\n                \"Cannot construct a '{}' from '{}'\".format(cls.__name__, string)\n            )\n\n    @classmethod\n    def construct_array_type(cls):\n        return SearchArray\n\n    def __repr__(self):\n        return 'TermsDtype()'\n\n    @property\n    def na_value(self):\n        return Terms({})\n\n    def valid_value(self, value):\n        return isinstance(value, dict) or pd.isna(value) or isinstance(value, Terms)\n\n\nregister_extension_dtype(TermsDtype)\n\n\ndef ws_tokenizer(string):\n    if pd.isna(string):\n        return []\n    if not isinstance(string, str):\n        raise ValueError(\"Expected a string\")\n    return string.split()\n\n\ndef _row_to_postings_row(doc_id, row, doc_len, term_dict, posns: PosnBitArray):\n    tfs = {}\n    labeled_posns = {}\n    for term_idx in row.cols:\n        term = term_dict.get_term(term_idx)\n        tfs[term] = 1\n        enc_term_posns = posns.doc_encoded_posns(term_idx, doc_id=doc_id)\n        labeled_posns[term] = enc_term_posns\n\n    result = Terms(tfs, posns=labeled_posns,\n                   doc_len=doc_len, encoded=True)\n    return result\n\n\nclass SearchArray(ExtensionArray):\n    \"\"\"An array of tokenized text (Termss).\"\"\"\n\n    dtype = TermsDtype()\n\n    def __init__(self, postings, tokenizer=ws_tokenizer, avoid_copies=True):\n        # Check dtype, raise TypeError\n        if not is_list_like(postings):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(postings)))\n\n        self.avoid_copies = avoid_copies\n        self.tokenizer = tokenizer\n        self.term_mat, self.posns, \\\n            self.term_dict, self.avg_doc_length, \\\n            self.doc_lens = build_index_from_terms_list(postings, Terms)\n\n    @classmethod\n    def index(cls, array: Iterable, tokenizer=ws_tokenizer,\n              truncate=False, batch_size=100000, avoid_copies=True) -> 'SearchArray':\n        \"\"\"Index an array of strings using tokenizer.\"\"\"\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens =\\\n            build_index_from_tokenizer(array, tokenizer, batch_size=batch_size,\n                                       truncate=truncate)\n\n        postings = cls([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n        postings.term_mat = term_mat\n        postings.posns = posns\n        postings.term_dict = term_dict\n        postings.avg_doc_length = avg_doc_length\n        postings.doc_lens = doc_lens\n        return postings\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        \"\"\"Construct a new SearchArray from a sequence of scalars (PostingRow or convertible into).\"\"\"\n        if dtype is not None:\n            if not isinstance(dtype, TermsDtype):\n                return scalars\n        if isinstance(scalars, np.ndarray) and scalars.dtype == TermsDtype():\n            return cls(scalars)\n        # String types\n        elif isinstance(scalars, np.ndarray) and scalars.dtype.kind in 'US':\n            return cls(scalars)\n        # Other objects\n        elif isinstance(scalars, np.ndarray) and scalars.dtype != object:\n            return scalars\n        return cls(scalars)\n\n    def memory_usage(self, deep=False):\n        \"\"\"Return memory usage of this array in bytes.\"\"\"\n        return self.nbytes\n\n    @property\n    def nbytes(self):\n        return self.term_mat.nbytes + self.posns.nbytes + self.doc_lens.nbytes + self.term_dict.nbytes\n\n    def __getitem__(self, key):\n        key = pd.api.indexers.check_array_indexer(self, key)\n        # Want to take rows of term freqs\n        if isinstance(key, numbers.Integral):\n            try:\n                rows = self.term_mat[key]\n                doc_len = self.doc_lens[key]\n                doc_id = key\n                if doc_id < 0:\n                    doc_id += len(self)\n                return _row_to_postings_row(doc_id, rows[0], doc_len,\n                                            self.term_dict, self.posns)\n            except IndexError:\n                raise IndexError(\"index out of bounds\")\n        else:\n            # Construct a sliced view of this array\n            sliced_tfs = self.term_mat.slice(key)\n            sliced_posns = self.posns.slice(sliced_tfs.rows) if not self.avoid_copies else self.posns\n            arr = SearchArray([], tokenizer=self.tokenizer)\n            arr.term_mat = sliced_tfs\n            arr.doc_lens = self.doc_lens[key]\n            arr.posns = sliced_posns\n            arr.term_dict = self.term_dict\n            arr.avg_doc_length = self.avg_doc_length\n            return arr\n\n    def __setitem__(self, key, value):\n        \"\"\"Set an item in the array.\"\"\"\n        key = pd.api.indexers.check_array_indexer(self, key)\n        if isinstance(value, pd.Series):\n            value = value.values\n        if isinstance(value, pd.DataFrame):\n            value = value.values.flatten()\n        if isinstance(value, SearchArray):\n            value = value.to_numpy()\n        if isinstance(value, list):\n            value = np.asarray(value, dtype=object)\n\n        if not isinstance(value, np.ndarray) and not self.dtype.valid_value(value):\n            raise ValueError(f\"Cannot set non-object array to SearchArray -- you passed type:{type(value)} -- {value}\")\n\n        # Cant set a single value to an array\n        if isinstance(key, numbers.Integral) and isinstance(value, np.ndarray):\n            raise ValueError(\"Cannot set a single value to an array\")\n\n        try:\n            is_encoded = False\n            posns = None\n            term_mat = np.asarray([])\n            doc_lens = np.asarray([])\n            if isinstance(value, float):\n                term_mat = np.asarray([value])\n                doc_lens = np.asarray([0])\n            elif isinstance(value, Terms):\n                term_mat = np.asarray([value.tf_to_dense(self.term_dict)])\n                doc_lens = np.asarray([value.doc_len])\n                is_encoded = value.encoded\n                posns = [value.raw_positions(self.term_dict)]\n            elif isinstance(value, np.ndarray):\n                term_mat = np.asarray([x.tf_to_dense(self.term_dict) for x in value])\n                doc_lens = np.asarray([x.doc_len for x in value])\n                is_encoded = value[0].encoded if len(value) > 0 else False\n                posns = [x.raw_positions(self.term_dict) for x in value]\n            np.nan_to_num(term_mat, copy=False, nan=0)\n            self.term_mat[key] = term_mat\n            self.doc_lens[key] = doc_lens\n\n            if posns is not None:\n                self.posns.insert(key, posns, is_encoded)\n\n            # Assume we have a positions for each term, doc pair. We can just update it.\n            # Otherwise we would have added new terms\n        except TermMissingError:\n            self._add_new_terms(key, value)\n\n    def _add_new_terms(self, key, value):\n        msg = \"\"\"Adding new terms! This might not be good if you tokenized this new text\n                 with a different tokenizer.\n\n                 Also. This is slow.\"\"\"\n        warnings.warn(msg)\n\n        scan_value = value\n        if isinstance(value, Terms):\n            scan_value = np.asarray([value])\n        for row in scan_value:\n            for term in row.terms():\n                self.term_dict.add_term(term[0])\n\n        self.term_mat.resize((self.term_mat.shape[0], len(self.term_dict)))\n        # Ensure posns_lookup has at least max self.posns\n        self[key] = value\n\n    def value_counts(\n        self,\n        dropna: bool = True,\n    ):\n        if dropna:\n            counts = Counter(self[:])\n            counts.pop(Terms({}), None)\n        else:\n            counts = Counter(self[:])\n        return pd.Series(counts)\n\n    def __len__(self):\n        len_rval = len(self.term_mat.rows)\n        return len_rval\n\n    def __ne__(self, other):\n        if isinstance(other, pd.DataFrame) or isinstance(other, pd.Series) or isinstance(other, pd.Index):\n            return NotImplemented\n\n        return ~(self == other)\n\n    def __eq__(self, other):\n        \"\"\"Return a boolean numpy array indicating elementwise equality.\"\"\"\n        # When other is a dataframe or series, not implemented\n        if isinstance(other, pd.DataFrame) or isinstance(other, pd.Series) or isinstance(other, pd.Index):\n            return NotImplemented\n\n        # When other is an ExtensionArray\n        if isinstance(other, SearchArray):\n            if len(self) != len(other):\n                return False\n            elif len(other) == 0:\n                return np.array([], dtype=bool)\n            else:\n                # Compatible term dicts, and same term freqs\n                # (not looking at positions, maybe we should?)\n                if self.term_dict.compatible(other.term_dict):\n                    return (self.term_mat == other.term_mat) & (self.doc_lens == other.doc_lens)\n                else:\n                    return np.zeros(len(self), dtype=bool)\n            # return np.array(self[:]) == np.array(other[:])\n\n        # When other is a scalar value\n        elif isinstance(other, Terms):\n            other = SearchArray([other], tokenizer=self.tokenizer)\n            warnings.warn(\"Comparing a scalar value to a SearchArray. This is slow.\")\n            return np.array(self[:]) == np.array(other[:])\n\n        # When other is a sequence but not an ExtensionArray\n        # its an array of dicts\n        elif is_list_like(other):\n            if len(self) != len(other):\n                return False\n            elif len(other) == 0:\n                return np.array([], dtype=bool)\n            # We actually don't know how it was tokenized\n            other = SearchArray(other, tokenizer=self.tokenizer)\n            return np.array(self[:]) == np.array(other[:])\n\n        # Return False where 'other' is neither the same length nor a scalar\n        else:\n            return np.full(len(self), False)\n\n    def isna(self):\n        # Every row with all 0s\n        empties = self.doc_lens == 0\n        return empties\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        # Want to take rows of term freqs\n        row_indices = np.arange(len(self.term_mat.rows))\n        # Take within the row indices themselves\n        result_indices = take(row_indices, indices, allow_fill=allow_fill, fill_value=-1)\n\n        if allow_fill and -1 in result_indices:\n            if fill_value is None or pd.isna(fill_value):\n                fill_value = Terms({}, encoded=True)\n\n            to_fill_mask = result_indices == -1\n            # This is slow as it rebuilds all the term dictionaries\n            # on the subsequent assignment lines\n            # However, this case tends to be the exception for\n            # most dataframe operations\n            taken = SearchArray([fill_value] * len(result_indices))\n            taken[~to_fill_mask] = self[result_indices[~to_fill_mask]].copy()\n\n            return taken\n        else:\n            taken = self[result_indices].copy()\n            return taken\n\n    def copy(self):\n        postings_arr = SearchArray([], tokenizer=self.tokenizer)\n        postings_arr.doc_lens = self.doc_lens.copy()\n        postings_arr.term_mat = self.term_mat.copy()\n        postings_arr.posns = self.posns\n        postings_arr.term_dict = self.term_dict\n        postings_arr.avg_doc_length = self.avg_doc_length\n\n        if not self.avoid_copies:\n            postings_arr.posns = self.posns.copy()\n            postings_arr.term_dict = self.term_dict.copy()\n        return postings_arr\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        concatenated_data = np.concatenate([ea[:] for ea in to_concat])\n        return SearchArray(concatenated_data, tokenizer=to_concat[0].tokenizer)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values)\n\n    def _values_for_factorize(self):\n        \"\"\"Return an array and missing value suitable for factorization (ie grouping).\"\"\"\n        arr = np.asarray(self[:], dtype=object)\n        return arr, Terms({})\n\n    def _check_token_arg(self, token):\n        if isinstance(token, str):\n            return token\n        elif isinstance(token, list) and len(token) == 1:\n            return token[0]\n        elif isinstance(token, list):\n            return token\n        else:\n            raise TypeError(\"Expected a string or list of strings for phrases\")\n\n    # ***********************************************************\n    # Search functionality\n    # ***********************************************************\n    def termfreqs(self, token: Union[List[str], str]) -> np.ndarray:\n        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            return self.phrase_freq(token)\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            matches = np.zeros(len(self), dtype=int)\n            slice_of_rows = None\n            if self.term_mat.subset:\n                slice_of_rows = self.term_mat.rows\n                doc_ids, termfreqs = self.posns.termfreqs(term_id,\n                                                          doc_ids=slice_of_rows)\n                mask = np.isin(self.term_mat.rows, doc_ids)\n                matches[mask] = termfreqs\n                return matches\n            else:\n                doc_ids, termfreqs = self.posns.termfreqs(term_id,\n                                                          doc_ids=slice_of_rows)\n                matches[doc_ids] = termfreqs\n                return matches\n        except TermMissingError:\n            return np.zeros(len(self), dtype=int)\n\n    def docfreq(self, token: str) -> int:\n        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        # Count number of rows where the term appears\n        try:\n            return self.posns.docfreq(self.term_dict.get_term_id(token))\n        except TermMissingError:\n            return 0\n\n    def doclengths(self) -> np.ndarray:\n        return self.doc_lens\n\n    def match(self, token: Union[List[str], str], slop: int = 1) -> np.ndarray:\n        \"\"\"Return a boolean numpy array indicating which elements contain the given term.\"\"\"\n        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            term_freq = self.phrase_freq(token)\n        else:\n            term_freq = self.termfreqs(token)\n        return term_freq > 0\n\n    def score(self, token: Union[str, List[str]], similarity: Similarity = default_bm25) -> np.ndarray:\n        \"\"\"Score each doc using a similarity function.\n\n        Parameters\n        ----------\n        token : str or list of str of what to search (already tokenized)\n        similarity : How to score the documents. Default is BM25.\n        \"\"\"\n        # Get term freqs per token\n        token = self._check_token_arg(token)\n\n        # For expensive toknes, we compute doc freq first, so we\n        # cache them in the DF cache, to let TF cache know it should be cached\n        tokens_l = [token] if isinstance(token, str) else token\n        all_dfs = np.asarray([self.docfreq(token) for token in tokens_l])\n\n        tfs = self.termfreqs(token)\n        token = self._check_token_arg(token)\n        doc_lens = self.doclengths()\n        scores = similarity(term_freqs=tfs, doc_freqs=all_dfs,\n                            doc_lens=doc_lens, avg_doc_lens=self.avg_doc_length,\n                            num_docs=len(self))\n        return scores\n\n", "contexts_below": "\n    def and_query(self, tokens: Union[List[str], List[List[str]]]) -> np.ndarray:\n        \"\"\"Return a mask on the postings array indicating which elements contain all terms.\"\"\"\n        masks = [self.match(term) for term in tokens]\n        mask = np.ones(len(self), dtype=bool)\n        for curr_mask in masks:\n            mask = mask & curr_mask\n        return mask\n\n    def or_query(self, tokens: Union[List[str], List[List[str]]], min_should_match: int = 1) -> np.ndarray:\n        \"\"\"Return a mask on the postings array indicating which elements contain all terms.\"\"\"\n        masks = [self.match(term) for term in tokens]\n        mask = np.sum(masks, axis=0) >= min_should_match\n        return mask\n\n    def phrase_freq(self, tokens: List[str], slop=1) -> np.ndarray:\n        if slop == 1 and len(tokens) == len(set(tokens)):\n            phrase_freqs = np.zeros(len(self))\n            try:\n                doc_ids = self.term_mat.rows\n                term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n                return self.posns.phrase_freqs(term_ids, doc_ids=doc_ids,\n                                               phrase_freqs=phrase_freqs)\n            except TermMissingError:\n                return phrase_freqs\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n\n    def phrase_freq_scan(self, tokens: List[str], mask=None, slop=1) -> np.ndarray:\n        if mask is None:\n            mask = self.and_query(tokens)\n\n        if np.sum(mask) == 0:\n            return mask\n\n        # Gather positions\n        posns = [self.positions(token, mask) for token in tokens]\n        phrase_freqs = np.zeros(len(self))\n\n        phrase_freqs[mask] = scan_merge_ins(posns, phrase_freqs[mask], slop=slop)\n        return phrase_freqs\n\n    def phrase_freq_every_diff(self, tokens: List[str], slop=1) -> np.ndarray:\n        phrase_freqs = -np.ones(len(self))\n\n        mask = self.and_query(tokens)\n        phrase_freqs[~mask] = 0\n        if np.sum(mask) == 0:\n            return phrase_freqs\n\n        term_posns = [self.positions(term, mask) for term in tokens]\n        for width in [10, 20, 30, 40]:\n            phrase_freqs[mask] = compute_phrase_freqs(term_posns,\n                                                      phrase_freqs[mask],\n                                                      slop=slop,\n                                                      width=width)\n\n        remaining_mask = phrase_freqs == -1\n        if np.any(remaining_mask):\n            remainder_freqs = self.phrase_freq_scan(tokens, mask=remaining_mask, slop=slop)\n            phrase_freqs[remaining_mask] = remainder_freqs[remaining_mask]\n        return phrase_freqs\n", "input_code": "    def positions(self, token: str, key=None) -> List[np.ndarray]:\n\n        \"\"\"\n        This function retrieves and returns a list of numpy arrays representing the positions of a given term within documents. If a specific document key is provided, it returns the positions for that document only; otherwise, it returns positions across all documents.\n\n        Input-Output Arguments\n        :param self: SearchArray. An instance of the SearchArray class.\n        :param token: str, The term for which positions are to be found.\n        :param key: The specific document key to search within. If None, positions are searched across all documents. Defaults to None.\n        :return: List[np.ndarray], A list of numpy arrays where each array contains the positions of the given term. The positions are specific to a document if a key is provided, or across all documents if no key is provided.\n        \"\"\"", "reference_steps": "1. Define a function named `positions` that takes `self`, a string `token`, and an optional parameter `key`.\n2. Retrieve the term ID for the given `token` using the method `self.term_dict.get_term_id`.\n3. Check if the `key` parameter is provided.\n4. If `key` is not `None`, use it to index into `self.term_mat.rows` to get a specific row; otherwise, use all rows in `self.term_mat.rows`.\n5. Call the `positions` method of `self.posns` with the term ID and the selected document IDs (`key`).\n6. Store the result of the `positions` method call in a variable `posns`.\n7. Return the variable `posns`, which contains a list of numpy arrays representing the positions of the given term.", "reference_code": "def positions(self, token: str, key=None) -> List[np.ndarray]:\n    \"\"\"Return a list of lists of positions of the given term.\"\"\"\n    term_id = self.term_dict.get_term_id(token)\n    key = self.term_mat.rows[key] if key is not None else self.term_mat.rows\n    posns = self.posns.positions(term_id, doc_ids=key)\n    return posns\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "type": "function", "class_name": null, "function_name": "parse_min_should_match", "dependency_all": "# Intra-file Dependency:\nsearcharray.solr.parse_min_should_match\n    def parse_min_should_match(num_clauses: int, spec: str) -> int:\n        \"\"\"Parse Solr's min should match (ie mm) spec.\n\n        See this ChatGPT translation of mm code from Solr's Java code for parsing this\n        https://chat.openai.com/share/76642aec-7e05-420f-a53a-83b8e2eea8fb\n\n        Parameters\n        ----------\n        num_clauses : int\n        spec : str\n\n        Returns\n        -------\n        int : the number of clauses that must match\n        \"\"\"\n\n", "dependency_sampled": "# Intra-file Dependency:\nsearcharray.solr.parse_min_should_match\n    def parse_min_should_match(num_clauses: int, spec: str) -> int:\n        \"\"\"Parse Solr's min should match (ie mm) spec.\n\n        See this ChatGPT translation of mm code from Solr's Java code for parsing this\n        https://chat.openai.com/share/76642aec-7e05-420f-a53a-83b8e2eea8fb\n\n        Parameters\n        ----------\n        num_clauses : int\n        spec : str\n\n        Returns\n        -------\n        int : the number of clauses that must match\n        \"\"\"\n\n", "contexts_above": "\"\"\"Utility functions for Solr users of searcharray.\"\"\"\nimport re\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Optional, Dict, Tuple\nfrom searcharray.postings import SearchArray\nfrom searcharray.similarity import Similarity, default_bm25\n\n\n", "contexts_below": "\n\ndef parse_field_boosts(field_lists: List[str]) -> dict:\n    \"\"\"Parse Solr's qf, pf, pf2, pf3 field boosts.\"\"\"\n    if not field_lists:\n        return {}\n\n    out = {}\n    carat_pattern = re.compile(r'\\^')\n\n    for field in field_lists:\n        parts = carat_pattern.split(field)\n        out[parts[0]] = None if len(parts) == 1 else float(parts[1])\n\n    return out\n\n\ndef get_field(frame, field) -> SearchArray:\n    if field not in frame.columns:\n        raise ValueError(f\"Field {field} not in dataframe\")\n    if not isinstance(frame[field].array, SearchArray):\n        raise ValueError(f\"Field {field} is not a searcharray field\")\n    return frame[field].array\n\n\ndef parse_query_terms(frame: pd.DataFrame,\n                      query: str,\n                      query_fields: List[str]):\n\n    search_terms: Dict[str, List[str]] = {}\n    num_search_terms = 0\n    term_centric = True\n\n    for field in query_fields:\n        arr = get_field(frame, field)\n\n        tokenizer = arr.tokenizer\n        search_terms[field] = []\n        field_num_search_terms = 0\n        for posn, term in enumerate(tokenizer(query)):\n            search_terms[field].append(term)\n            field_num_search_terms += 1\n        if num_search_terms == 0:\n            num_search_terms = field_num_search_terms\n        elif field_num_search_terms != num_search_terms:\n            term_centric = False\n\n    return num_search_terms, search_terms, term_centric\n\n\ndef _edismax_term_centric(frame: pd.DataFrame,\n                          query_fields: Dict[str, float],\n                          num_search_terms: int,\n                          search_terms: Dict[str, List[str]],\n                          mm: str,\n                          similarity: Similarity) -> Tuple[np.ndarray, str]:\n    explain = []\n    term_scores = []\n    for term_posn in range(num_search_terms):\n        max_scores = np.zeros(len(frame))\n        term_explain = []\n        for field, boost in query_fields.items():\n            term = search_terms[field][term_posn]\n            post_arr = get_field(frame, field)\n            field_term_score = post_arr.score(term, similarity=similarity) * (1 if boost is None else boost)\n            boost_exp = f\"{boost}\" if boost is not None else \"1\"\n            term_explain.append(f\"{field}:{term}^{boost_exp}\")\n            max_scores = np.maximum(max_scores, field_term_score)\n        term_scores.append(max_scores)\n        explain.append(\"(\" + \" | \".join(term_explain) + \")\")\n\n    min_should_match = parse_min_should_match(num_search_terms, spec=mm)\n    qf_scores = np.asarray(term_scores)\n    matches_gt_mm = np.sum(qf_scores > 0, axis=0) >= min_should_match\n    qf_scores = np.sum(term_scores, axis=0)\n    qf_scores[~matches_gt_mm] = 0\n    return qf_scores, \"(\" + \" \".join(explain) + f\")~{min_should_match}\"\n\n\ndef _edismax_field_centric(frame: pd.DataFrame,\n                           query_fields: Dict[str, float],\n                           num_search_terms: int,\n                           search_terms: Dict[str, List[str]],\n                           mm: str,\n                           similarity: Similarity = default_bm25) -> Tuple[np.ndarray, str]:\n    field_scores = []\n    explain = []\n    for field, boost in query_fields.items():\n        post_arr = get_field(frame, field)\n        term_scores = np.array([post_arr.score(term, similarity=similarity)\n                                for term in search_terms[field]])\n        min_should_match = parse_min_should_match(len(search_terms[field]), spec=mm)\n        exp = \" \".join([f\"{field}:{term}\" for term in search_terms[field]])\n        boost_exp = f\"{boost}\" if boost is not None else \"1\"\n        exp = \"(\" + exp + f\")~{min(min_should_match, len(search_terms[field]))}\"\n        exp = \"(\" + exp + f\")^{boost_exp}\"\n\n        matches_gt_mm = np.sum(term_scores > 0, axis=0) >= min(min_should_match, len(search_terms[field]))\n        sum_terms_bm25 = np.sum(term_scores, axis=0)\n        sum_terms_bm25[~matches_gt_mm] = 0\n        field_scores.append(sum_terms_bm25 * (1 if boost is None else boost))\n        explain.append(exp)\n    # Take maximum field scores as qf\n    qf_scores = np.asarray(field_scores)\n    qf_scores = np.max(qf_scores, axis=0)\n    return qf_scores, \" | \".join(explain)\n\n\ndef edismax(frame: pd.DataFrame,\n            q: str,\n            qf: List[str],\n            mm: Optional[str] = None,\n            pf: Optional[List[str]] = None,\n            pf2: Optional[List[str]] = None,\n            pf3: Optional[List[str]] = None,\n            q_op: str = \"OR\",\n            similarity: Similarity = default_bm25) -> Tuple[np.ndarray, str]:\n    \"\"\"Run edismax search over dataframe with searcharray fields.\n\n    Parameters\n    ----------\n    q : str\n        The query string\n    mm : str\n        The minimum should match spec\n    qf : list\n        The fields to search\n    pf : list\n        The fields to search for phrase matches\n    pf2 : list\n        The fields to search for bigram matches\n    pf3 : list\n        The fields to search for trigram matches\n    q_op : str, optional\n        The default operator, by default \"OR\"\n\n    Returns\n    -------\n    np.ndarray\n        The search results\n    \"\"\"\n    def listify(x):\n        return x if isinstance(x, list) else [x]\n\n    query_fields = parse_field_boosts(listify(qf))\n    phrase_fields = parse_field_boosts(listify(pf)) if pf else {}\n    if mm is None:\n        mm = \"1\"\n    if q_op == \"AND\":\n        mm = \"100%\"\n\n    # bigram_fields = parse_field_boosts(pf2) if pf2 else {}\n    # trigram_fields = parse_field_boosts(pf3) if pf3 else {}\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, list(query_fields.keys()))\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields,\n                                                   num_search_terms, search_terms, mm,\n                                                   similarity=similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields,\n                                                    num_search_terms, search_terms, mm,\n                                                    similarity=similarity)\n\n    phrase_scores = []\n    for field, boost in phrase_fields.items():\n        arr = get_field(frame, field)\n        terms = search_terms[field]\n        field_phrase_score = arr.score(terms, similarity=similarity) * (1 if boost is None else boost)\n        boost_exp = f\"{boost}\" if boost is not None else \"1\"\n        explain += f\" ({field}:\\\"{' '.join(terms)}\\\")^{boost_exp}\"\n        phrase_scores.append(field_phrase_score)\n\n    if len(phrase_scores) > 0:\n        phrase_scores = np.sum(phrase_scores, axis=0)\n        # Add where term_scores > 0\n        term_match_idx = np.where(qf_scores)[0]\n\n        qf_scores[term_match_idx] += phrase_scores[term_match_idx]\n    return qf_scores, explain\n", "input_code": "def parse_min_should_match(num_clauses: int, spec: str) -> int:\n\n    \"\"\"\n    This function parses the \"min should match\" specification (commonly referred to as 'mm' spec) from Solr's search engine configuration and calculates the minimum number of clauses that must match based on the given specification and the total number of clauses. It supports both absolute and percentage-based specifications, as well as conditional specifications using '<'.\n\n    Input-Output Arguments\n    :param num_clauses: int, the total number of clauses in the query.\n    :param spec: str, the 'min should match' specification as a string, which can include absolute numbers, percentages, and conditional expressions.\n    :return: int, the calculated minimum number of clauses that must match according to the 'mm' spec.\n    \"\"\"", "reference_steps": "1. Define a function `parse_min_should_match` that takes two parameters: `num_clauses` (an integer representing the number of clauses) and `spec` (a string representing the specification for the minimum number of clauses that should match).\n\n2. Define a nested helper function `checked_parse_int` within `parse_min_should_match` to safely convert a string to an integer, raising a `ValueError` with a custom error message if the conversion fails.\n\n3. Trim whitespace from the `spec` string using the `strip` method.\n\n4. Check if the `spec` contains the '<' character, indicating conditional specifications. If so, process each condition:\n   - Replace any whitespace around '<' with no space using a regular expression.\n   - Split the `spec` on spaces and iterate over each conditional spec.\n   - Split each conditional spec on '<' and validate that there are two parts.\n   - Parse the integer before the '<' and compare it with `num_clauses`. If `num_clauses` is less than or equal to this integer, return the current result.\n   - Otherwise, recursively call `parse_min_should_match` with the second part of the conditional spec and update the result.\n\n5. If the `spec` does not contain '<', check if it contains a '%' character, indicating a percentage specification:\n   - Remove the '%' character from the end of the `spec`.\n   - Convert the remaining string to an integer to get the percentage.\n   - Calculate the number of clauses to match based on the percentage of `num_clauses`.\n\n6. If the `spec` does not contain a '%' character, assume it is a simple integer specification:\n   - Convert the `spec` directly to an integer.\n\n7. Add the calculated value to `num_clauses` if the calculated value is negative; otherwise, use the calculated value as the result.\n\n8. Ensure the result is not greater than `num_clauses` and not less than 0 by using the `min` and `max` functions.\n\n9. Return the final result, which is the number of clauses that must match according to the parsed specification.\n\n10. Raise a `ValueError` with an appropriate error message at any point if the `spec` cannot be parsed correctly, ensuring input validation and error handling.", "reference_code": "def parse_min_should_match(num_clauses: int, spec: str) -> int:\n    \"\"\"Parse Solr's min should match (ie mm) spec.\n\n    See this ChatGPT translation of mm code from Solr's Java code for parsing this\n    https://chat.openai.com/share/76642aec-7e05-420f-a53a-83b8e2eea8fb\n\n    Parameters\n    ----------\n    num_clauses : int\n    spec : str\n\n    Returns\n    -------\n    int : the number of clauses that must match\n    \"\"\"\n    def checked_parse_int(value, error_message):\n        try:\n            return int(value)\n        except ValueError:\n            raise ValueError(error_message)\n\n    result = num_clauses\n    spec = spec.strip()\n\n    if '<' in spec:\n        # we have conditional spec(s)\n        space_around_less_than_pattern = re.compile(r'\\s*<\\s*')\n        spec = space_around_less_than_pattern.sub('<', spec)\n        for s in spec.split():\n            parts = s.split('<', 1)\n            if len(parts) < 2:\n                raise ValueError(\"Invalid 'mm' spec: '\" + s + \"'. Expecting values before and after '<'\")\n            upper_bound = checked_parse_int(parts[0], \"Invalid 'mm' spec. Expecting an integer.\")\n            if num_clauses <= upper_bound:\n                return result\n            else:\n                result = parse_min_should_match(num_clauses, parts[1])\n        return result\n\n    # otherwise, simple expression\n    if '%' in spec:\n        # percentage - assume the % was the last char. If not, let int() fail.\n        spec = spec[:-1]\n        percent = checked_parse_int(spec, \"Invalid 'mm' spec. Expecting an integer.\")\n        calc = (result * percent) * (1 / 100)\n        result = result + int(calc) if calc < 0 else int(calc)\n    else:\n        calc = checked_parse_int(spec, \"Invalid 'mm' spec. Expecting an integer.\")\n        result = result + calc if calc < 0 else calc\n\n    return min(num_clauses, max(result, 0))\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "type": "method", "class_name": "SearchArray", "function_name": "phrase_freq", "dependency_all": "# Intra-class Dependency:\nsearcharray.postings.SearchArray.phrase_freq_every_diff\n    def phrase_freq_every_diff(self, tokens: List[str], slop=1) -> np.ndarray:\n\nsearcharray.postings.SearchArray.posns\n\nsearcharray.postings.SearchArray.term_dict\n\nsearcharray.postings.SearchArray.term_mat\n\n# Cross-file Dependency:\nsearcharray.term_dict.TermMissingError\n    class TermMissingError(KeyError):\n\n", "dependency_sampled": "# Intra-class Dependency:\nsearcharray.postings.SearchArray.phrase_freq_every_diff\n    def phrase_freq_every_diff(self, tokens: List[str], slop=1) -> np.ndarray:\n\nsearcharray.postings.SearchArray.term_mat\n\n", "contexts_above": "\"\"\"Tokenized, searchable text as a pandas dtype.\"\"\"\nimport pandas as pd\nimport numbers\nfrom pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype\nfrom pandas.api.types import is_list_like\nfrom pandas.api.extensions import take\nimport json\nfrom collections import Counter\nimport warnings\nimport logging\nfrom typing import List, Union, Optional, Iterable\n\n\nimport numpy as np\nfrom searcharray.phrase.scan_merge import scan_merge_ins\nfrom searcharray.phrase.posn_diffs import compute_phrase_freqs\nfrom searcharray.phrase.middle_out import PosnBitArray\nfrom searcharray.similarity import Similarity, default_bm25\nfrom searcharray.indexing import build_index_from_tokenizer, build_index_from_terms_list\nfrom searcharray.term_dict import TermMissingError\n\nlogger = logging.getLogger(__name__)\n\n# When running in pytest\nimport sys  # noqa\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(logging.ERROR)\nformatter = logging.Formatter(\"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\")\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.setLevel(logging.ERROR)\n\n\nclass Terms:\n    \"\"\"An indexed search doc - a single bag of tokenized words and positions.\"\"\"\n\n    def __init__(self,\n                 postings,\n                 doc_len: int = 0,\n                 posns: Optional[dict] = None,\n                 encoded=False):\n        self.postings = postings\n        self.posns = None\n        self.encoded = encoded\n        self.doc_len = doc_len\n        self.posns = posns\n\n    def _validate_posns(self):\n        # (For testing/assertions) - Confirm every term in positions also in postings\n        if self.posns is None:\n            return\n        for term in self.posns:\n            if term not in self.postings:\n                raise ValueError(f\"Term {term} in positions but not in postings. \")\n\n    def termfreq(self, token):\n        return self.postings[token]\n\n    def terms(self):\n        return self.postings.items()\n\n    def positions(self, term=None):\n        if self.posns is None:\n            return {}\n        if term is None:\n            posns = self.posns.items()\n        else:\n            posns = self.posns[term]\n        return posns\n\n    def raw_positions(self, term_dict, term=None):\n        if self.posns is None:\n            return {}\n        if term is None:\n            posns = [(term_dict.get_term_id(term), posns) for term, posns in self.posns.items()]\n        else:\n            posns = [(term_dict.get_term_id(term), self.posns[term])]\n        return posns\n\n    def tf_to_dense(self, term_dict):\n        \"\"\"Convert to a dense vector of term frequencies.\"\"\"\n        dense = np.zeros(len(term_dict))\n        for term, freq in self.terms():\n            dense[term_dict.get_term_id(term)] = freq\n        return dense\n\n    def __len__(self):\n        return len(self.postings)\n\n    def __repr__(self):\n        posting_keys = set(self.postings.keys())\n        rval = f\"Terms({posting_keys})\"\n        return rval\n\n    def __str__(self):\n        return repr(self)\n\n    def __eq__(self, other):\n        # Flip to the other implementation if we're comparing to a SearchArray\n        # to get a boolean array back\n        if isinstance(other, SearchArray):\n            return other == self\n        same_postings = isinstance(other, Terms) and self.postings == other.postings\n        if same_postings and self.doc_len == other.doc_len:\n            return True\n\n    def __lt__(self, other):\n        # return isinstance(other, Terms) and hash(self) < hash(other)\n        keys_both = set(self.postings.keys()).union(set(other.postings.keys()))\n        # Sort lexically\n        keys_both = sorted(keys_both)\n\n        # Iterate as if these are two vectors of the same large dimensional vector sparse\n        for key in keys_both:\n            lhs_val = 0\n            rhs_val = 0\n            try:\n                lhs_val = self.postings[key]\n            except KeyError:\n                pass\n\n            try:\n                rhs_val = other.postings[key]\n            except KeyError:\n                pass\n\n            if lhs_val < rhs_val:\n                return True\n            elif lhs_val > rhs_val:\n                return False\n            else:\n                continue\n        return False\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    def __gt__(self, other):\n        return not (self < other) and self != other\n\n    def __hash__(self):\n        return hash(json.dumps(self.postings, sort_keys=True))\n\n\nclass TermsDtype(ExtensionDtype):\n    \"\"\"Pandas dtype for terms.\"\"\"\n\n    name = 'tokenized_text'\n    type = Terms\n    kind = 'O'\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\n                \"'construct_from_string' expects a string, got {}\".format(type(string))\n            )\n        elif string == cls.name:\n            return cls()\n        else:\n            raise TypeError(\n                \"Cannot construct a '{}' from '{}'\".format(cls.__name__, string)\n            )\n\n    @classmethod\n    def construct_array_type(cls):\n        return SearchArray\n\n    def __repr__(self):\n        return 'TermsDtype()'\n\n    @property\n    def na_value(self):\n        return Terms({})\n\n    def valid_value(self, value):\n        return isinstance(value, dict) or pd.isna(value) or isinstance(value, Terms)\n\n\nregister_extension_dtype(TermsDtype)\n\n\ndef ws_tokenizer(string):\n    if pd.isna(string):\n        return []\n    if not isinstance(string, str):\n        raise ValueError(\"Expected a string\")\n    return string.split()\n\n\ndef _row_to_postings_row(doc_id, row, doc_len, term_dict, posns: PosnBitArray):\n    tfs = {}\n    labeled_posns = {}\n    for term_idx in row.cols:\n        term = term_dict.get_term(term_idx)\n        tfs[term] = 1\n        enc_term_posns = posns.doc_encoded_posns(term_idx, doc_id=doc_id)\n        labeled_posns[term] = enc_term_posns\n\n    result = Terms(tfs, posns=labeled_posns,\n                   doc_len=doc_len, encoded=True)\n    return result\n\n\nclass SearchArray(ExtensionArray):\n    \"\"\"An array of tokenized text (Termss).\"\"\"\n\n    dtype = TermsDtype()\n\n    def __init__(self, postings, tokenizer=ws_tokenizer, avoid_copies=True):\n        # Check dtype, raise TypeError\n        if not is_list_like(postings):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(postings)))\n\n        self.avoid_copies = avoid_copies\n        self.tokenizer = tokenizer\n        self.term_mat, self.posns, \\\n            self.term_dict, self.avg_doc_length, \\\n            self.doc_lens = build_index_from_terms_list(postings, Terms)\n\n    @classmethod\n    def index(cls, array: Iterable, tokenizer=ws_tokenizer,\n              truncate=False, batch_size=100000, avoid_copies=True) -> 'SearchArray':\n        \"\"\"Index an array of strings using tokenizer.\"\"\"\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens =\\\n            build_index_from_tokenizer(array, tokenizer, batch_size=batch_size,\n                                       truncate=truncate)\n\n        postings = cls([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n        postings.term_mat = term_mat\n        postings.posns = posns\n        postings.term_dict = term_dict\n        postings.avg_doc_length = avg_doc_length\n        postings.doc_lens = doc_lens\n        return postings\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        \"\"\"Construct a new SearchArray from a sequence of scalars (PostingRow or convertible into).\"\"\"\n        if dtype is not None:\n            if not isinstance(dtype, TermsDtype):\n                return scalars\n        if isinstance(scalars, np.ndarray) and scalars.dtype == TermsDtype():\n            return cls(scalars)\n        # String types\n        elif isinstance(scalars, np.ndarray) and scalars.dtype.kind in 'US':\n            return cls(scalars)\n        # Other objects\n        elif isinstance(scalars, np.ndarray) and scalars.dtype != object:\n            return scalars\n        return cls(scalars)\n\n    def memory_usage(self, deep=False):\n        \"\"\"Return memory usage of this array in bytes.\"\"\"\n        return self.nbytes\n\n    @property\n    def nbytes(self):\n        return self.term_mat.nbytes + self.posns.nbytes + self.doc_lens.nbytes + self.term_dict.nbytes\n\n    def __getitem__(self, key):\n        key = pd.api.indexers.check_array_indexer(self, key)\n        # Want to take rows of term freqs\n        if isinstance(key, numbers.Integral):\n            try:\n                rows = self.term_mat[key]\n                doc_len = self.doc_lens[key]\n                doc_id = key\n                if doc_id < 0:\n                    doc_id += len(self)\n                return _row_to_postings_row(doc_id, rows[0], doc_len,\n                                            self.term_dict, self.posns)\n            except IndexError:\n                raise IndexError(\"index out of bounds\")\n        else:\n            # Construct a sliced view of this array\n            sliced_tfs = self.term_mat.slice(key)\n            sliced_posns = self.posns.slice(sliced_tfs.rows) if not self.avoid_copies else self.posns\n            arr = SearchArray([], tokenizer=self.tokenizer)\n            arr.term_mat = sliced_tfs\n            arr.doc_lens = self.doc_lens[key]\n            arr.posns = sliced_posns\n            arr.term_dict = self.term_dict\n            arr.avg_doc_length = self.avg_doc_length\n            return arr\n\n    def __setitem__(self, key, value):\n        \"\"\"Set an item in the array.\"\"\"\n        key = pd.api.indexers.check_array_indexer(self, key)\n        if isinstance(value, pd.Series):\n            value = value.values\n        if isinstance(value, pd.DataFrame):\n            value = value.values.flatten()\n        if isinstance(value, SearchArray):\n            value = value.to_numpy()\n        if isinstance(value, list):\n            value = np.asarray(value, dtype=object)\n\n        if not isinstance(value, np.ndarray) and not self.dtype.valid_value(value):\n            raise ValueError(f\"Cannot set non-object array to SearchArray -- you passed type:{type(value)} -- {value}\")\n\n        # Cant set a single value to an array\n        if isinstance(key, numbers.Integral) and isinstance(value, np.ndarray):\n            raise ValueError(\"Cannot set a single value to an array\")\n\n        try:\n            is_encoded = False\n            posns = None\n            term_mat = np.asarray([])\n            doc_lens = np.asarray([])\n            if isinstance(value, float):\n                term_mat = np.asarray([value])\n                doc_lens = np.asarray([0])\n            elif isinstance(value, Terms):\n                term_mat = np.asarray([value.tf_to_dense(self.term_dict)])\n                doc_lens = np.asarray([value.doc_len])\n                is_encoded = value.encoded\n                posns = [value.raw_positions(self.term_dict)]\n            elif isinstance(value, np.ndarray):\n                term_mat = np.asarray([x.tf_to_dense(self.term_dict) for x in value])\n                doc_lens = np.asarray([x.doc_len for x in value])\n                is_encoded = value[0].encoded if len(value) > 0 else False\n                posns = [x.raw_positions(self.term_dict) for x in value]\n            np.nan_to_num(term_mat, copy=False, nan=0)\n            self.term_mat[key] = term_mat\n            self.doc_lens[key] = doc_lens\n\n            if posns is not None:\n                self.posns.insert(key, posns, is_encoded)\n\n            # Assume we have a positions for each term, doc pair. We can just update it.\n            # Otherwise we would have added new terms\n        except TermMissingError:\n            self._add_new_terms(key, value)\n\n    def _add_new_terms(self, key, value):\n        msg = \"\"\"Adding new terms! This might not be good if you tokenized this new text\n                 with a different tokenizer.\n\n                 Also. This is slow.\"\"\"\n        warnings.warn(msg)\n\n        scan_value = value\n        if isinstance(value, Terms):\n            scan_value = np.asarray([value])\n        for row in scan_value:\n            for term in row.terms():\n                self.term_dict.add_term(term[0])\n\n        self.term_mat.resize((self.term_mat.shape[0], len(self.term_dict)))\n        # Ensure posns_lookup has at least max self.posns\n        self[key] = value\n\n    def value_counts(\n        self,\n        dropna: bool = True,\n    ):\n        if dropna:\n            counts = Counter(self[:])\n            counts.pop(Terms({}), None)\n        else:\n            counts = Counter(self[:])\n        return pd.Series(counts)\n\n    def __len__(self):\n        len_rval = len(self.term_mat.rows)\n        return len_rval\n\n    def __ne__(self, other):\n        if isinstance(other, pd.DataFrame) or isinstance(other, pd.Series) or isinstance(other, pd.Index):\n            return NotImplemented\n\n        return ~(self == other)\n\n    def __eq__(self, other):\n        \"\"\"Return a boolean numpy array indicating elementwise equality.\"\"\"\n        # When other is a dataframe or series, not implemented\n        if isinstance(other, pd.DataFrame) or isinstance(other, pd.Series) or isinstance(other, pd.Index):\n            return NotImplemented\n\n        # When other is an ExtensionArray\n        if isinstance(other, SearchArray):\n            if len(self) != len(other):\n                return False\n            elif len(other) == 0:\n                return np.array([], dtype=bool)\n            else:\n                # Compatible term dicts, and same term freqs\n                # (not looking at positions, maybe we should?)\n                if self.term_dict.compatible(other.term_dict):\n                    return (self.term_mat == other.term_mat) & (self.doc_lens == other.doc_lens)\n                else:\n                    return np.zeros(len(self), dtype=bool)\n            # return np.array(self[:]) == np.array(other[:])\n\n        # When other is a scalar value\n        elif isinstance(other, Terms):\n            other = SearchArray([other], tokenizer=self.tokenizer)\n            warnings.warn(\"Comparing a scalar value to a SearchArray. This is slow.\")\n            return np.array(self[:]) == np.array(other[:])\n\n        # When other is a sequence but not an ExtensionArray\n        # its an array of dicts\n        elif is_list_like(other):\n            if len(self) != len(other):\n                return False\n            elif len(other) == 0:\n                return np.array([], dtype=bool)\n            # We actually don't know how it was tokenized\n            other = SearchArray(other, tokenizer=self.tokenizer)\n            return np.array(self[:]) == np.array(other[:])\n\n        # Return False where 'other' is neither the same length nor a scalar\n        else:\n            return np.full(len(self), False)\n\n    def isna(self):\n        # Every row with all 0s\n        empties = self.doc_lens == 0\n        return empties\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        # Want to take rows of term freqs\n        row_indices = np.arange(len(self.term_mat.rows))\n        # Take within the row indices themselves\n        result_indices = take(row_indices, indices, allow_fill=allow_fill, fill_value=-1)\n\n        if allow_fill and -1 in result_indices:\n            if fill_value is None or pd.isna(fill_value):\n                fill_value = Terms({}, encoded=True)\n\n            to_fill_mask = result_indices == -1\n            # This is slow as it rebuilds all the term dictionaries\n            # on the subsequent assignment lines\n            # However, this case tends to be the exception for\n            # most dataframe operations\n            taken = SearchArray([fill_value] * len(result_indices))\n            taken[~to_fill_mask] = self[result_indices[~to_fill_mask]].copy()\n\n            return taken\n        else:\n            taken = self[result_indices].copy()\n            return taken\n\n    def copy(self):\n        postings_arr = SearchArray([], tokenizer=self.tokenizer)\n        postings_arr.doc_lens = self.doc_lens.copy()\n        postings_arr.term_mat = self.term_mat.copy()\n        postings_arr.posns = self.posns\n        postings_arr.term_dict = self.term_dict\n        postings_arr.avg_doc_length = self.avg_doc_length\n\n        if not self.avoid_copies:\n            postings_arr.posns = self.posns.copy()\n            postings_arr.term_dict = self.term_dict.copy()\n        return postings_arr\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        concatenated_data = np.concatenate([ea[:] for ea in to_concat])\n        return SearchArray(concatenated_data, tokenizer=to_concat[0].tokenizer)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values)\n\n    def _values_for_factorize(self):\n        \"\"\"Return an array and missing value suitable for factorization (ie grouping).\"\"\"\n        arr = np.asarray(self[:], dtype=object)\n        return arr, Terms({})\n\n    def _check_token_arg(self, token):\n        if isinstance(token, str):\n            return token\n        elif isinstance(token, list) and len(token) == 1:\n            return token[0]\n        elif isinstance(token, list):\n            return token\n        else:\n            raise TypeError(\"Expected a string or list of strings for phrases\")\n\n    # ***********************************************************\n    # Search functionality\n    # ***********************************************************\n    def termfreqs(self, token: Union[List[str], str]) -> np.ndarray:\n        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            return self.phrase_freq(token)\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            matches = np.zeros(len(self), dtype=int)\n            slice_of_rows = None\n            if self.term_mat.subset:\n                slice_of_rows = self.term_mat.rows\n                doc_ids, termfreqs = self.posns.termfreqs(term_id,\n                                                          doc_ids=slice_of_rows)\n                mask = np.isin(self.term_mat.rows, doc_ids)\n                matches[mask] = termfreqs\n                return matches\n            else:\n                doc_ids, termfreqs = self.posns.termfreqs(term_id,\n                                                          doc_ids=slice_of_rows)\n                matches[doc_ids] = termfreqs\n                return matches\n        except TermMissingError:\n            return np.zeros(len(self), dtype=int)\n\n    def docfreq(self, token: str) -> int:\n        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        # Count number of rows where the term appears\n        try:\n            return self.posns.docfreq(self.term_dict.get_term_id(token))\n        except TermMissingError:\n            return 0\n\n    def doclengths(self) -> np.ndarray:\n        return self.doc_lens\n\n    def match(self, token: Union[List[str], str], slop: int = 1) -> np.ndarray:\n        \"\"\"Return a boolean numpy array indicating which elements contain the given term.\"\"\"\n        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            term_freq = self.phrase_freq(token)\n        else:\n            term_freq = self.termfreqs(token)\n        return term_freq > 0\n\n    def score(self, token: Union[str, List[str]], similarity: Similarity = default_bm25) -> np.ndarray:\n        \"\"\"Score each doc using a similarity function.\n\n        Parameters\n        ----------\n        token : str or list of str of what to search (already tokenized)\n        similarity : How to score the documents. Default is BM25.\n        \"\"\"\n        # Get term freqs per token\n        token = self._check_token_arg(token)\n\n        # For expensive toknes, we compute doc freq first, so we\n        # cache them in the DF cache, to let TF cache know it should be cached\n        tokens_l = [token] if isinstance(token, str) else token\n        all_dfs = np.asarray([self.docfreq(token) for token in tokens_l])\n\n        tfs = self.termfreqs(token)\n        token = self._check_token_arg(token)\n        doc_lens = self.doclengths()\n        scores = similarity(term_freqs=tfs, doc_freqs=all_dfs,\n                            doc_lens=doc_lens, avg_doc_lens=self.avg_doc_length,\n                            num_docs=len(self))\n        return scores\n\n    def positions(self, token: str, key=None) -> List[np.ndarray]:\n        \"\"\"Return a list of lists of positions of the given term.\"\"\"\n        term_id = self.term_dict.get_term_id(token)\n        key = self.term_mat.rows[key] if key is not None else self.term_mat.rows\n        posns = self.posns.positions(term_id, doc_ids=key)\n        return posns\n\n    def and_query(self, tokens: Union[List[str], List[List[str]]]) -> np.ndarray:\n        \"\"\"Return a mask on the postings array indicating which elements contain all terms.\"\"\"\n        masks = [self.match(term) for term in tokens]\n        mask = np.ones(len(self), dtype=bool)\n        for curr_mask in masks:\n            mask = mask & curr_mask\n        return mask\n\n    def or_query(self, tokens: Union[List[str], List[List[str]]], min_should_match: int = 1) -> np.ndarray:\n        \"\"\"Return a mask on the postings array indicating which elements contain all terms.\"\"\"\n        masks = [self.match(term) for term in tokens]\n        mask = np.sum(masks, axis=0) >= min_should_match\n        return mask\n\n", "contexts_below": "\n    def phrase_freq_scan(self, tokens: List[str], mask=None, slop=1) -> np.ndarray:\n        if mask is None:\n            mask = self.and_query(tokens)\n\n        if np.sum(mask) == 0:\n            return mask\n\n        # Gather positions\n        posns = [self.positions(token, mask) for token in tokens]\n        phrase_freqs = np.zeros(len(self))\n\n        phrase_freqs[mask] = scan_merge_ins(posns, phrase_freqs[mask], slop=slop)\n        return phrase_freqs\n\n    def phrase_freq_every_diff(self, tokens: List[str], slop=1) -> np.ndarray:\n        phrase_freqs = -np.ones(len(self))\n\n        mask = self.and_query(tokens)\n        phrase_freqs[~mask] = 0\n        if np.sum(mask) == 0:\n            return phrase_freqs\n\n        term_posns = [self.positions(term, mask) for term in tokens]\n        for width in [10, 20, 30, 40]:\n            phrase_freqs[mask] = compute_phrase_freqs(term_posns,\n                                                      phrase_freqs[mask],\n                                                      slop=slop,\n                                                      width=width)\n\n        remaining_mask = phrase_freqs == -1\n        if np.any(remaining_mask):\n            remainder_freqs = self.phrase_freq_scan(tokens, mask=remaining_mask, slop=slop)\n            phrase_freqs[remaining_mask] = remainder_freqs[remaining_mask]\n        return phrase_freqs\n", "input_code": "    def phrase_freq(self, tokens: List[str], slop=1) -> np.ndarray:\n\n        \"\"\"\n        This function calculates the frequency of a phrase within a SearchArray instance based on the provided tokens and slop. If the slop is 1 and all tokens are unique, it attempts to directly calculate the phrase frequencies using the positions of terms. If the slop is not 1 or tokens are not unique, it delegates the calculation to another method that handles different slops or non-unique tokens.\n\n        Input-Output Arguments\n        :param self: SearchArray. An instance of the SearchArray class.\n        :param tokens: List[str]. A list of tokens (words) for which the phrase frequency is to be calculated.\n        :param slop: int, optional. The maximum distance between tokens in the phrase. Defaults to 1, meaning tokens must be adjacent.\n        :return: np.ndarray. An array of phrase frequencies corresponding to the tokens in the SearchArray instance.\n        \"\"\"", "reference_steps": "1. Define a function `phrase_freq` that takes a list of tokens and an optional argument `slop` with a default value of 1.\n\n2. Check if `slop` equals 1 and all tokens in the list are unique (i.e., the length of the list is equal to the length of the set of tokens).\n\n3. If the above condition is met, initialize a zero-filled NumPy array `phrase_freqs` with a length equal to the length of `self`.\n\n4. Attempt to execute the following steps within a `try` block:\n   - Retrieve document IDs from `self.term_mat.rows`.\n   - Convert each token into a corresponding term ID using `self.term_dict.get_term_id(token)`.\n   - Call `self.posns.phrase_freqs` with the term IDs, document IDs, and the zero-initialized `phrase_freqs` array to calculate the phrase frequencies.\n\n5. If a `TermMissingError` is encountered during the try block execution, return the zero-initialized `phrase_freqs` array.\n\n6. If the initial condition (slop equals 1 and all tokens are unique) is not met, call the method `self.phrase_freq_every_diff` with the tokens and the given `slop` value.\n\n7. Return the result of `self.phrase_freq_every_diff` if called, or the result from the try block if no error occurred.\n\n8. The function ultimately returns a NumPy array containing the frequencies of the given phrase (sequence of tokens) across documents, either by exact matching (when `slop` is 1 and tokens are unique) or with a specified `slop` (allowing for some variation in the phrase matching).", "reference_code": "def phrase_freq(self, tokens: List[str], slop=1) -> np.ndarray:\n    if slop == 1 and len(tokens) == len(set(tokens)):\n        phrase_freqs = np.zeros(len(self))\n        try:\n            doc_ids = self.term_mat.rows\n            term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n            return self.posns.phrase_freqs(term_ids, doc_ids=doc_ids,\n                                           phrase_freqs=phrase_freqs)\n        except TermMissingError:\n            return phrase_freqs\n    else:\n        return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "type": "method", "class_name": "SearchArray", "function_name": "index", "dependency_all": "# Intra-class Dependency:\nsearcharray.postings.SearchArray.__init__\n    def __init__(self,\n                 postings,\n                 doc_len: int = 0,\n                 posns: Optional[dict] = None,\n                 encoded=False):\n\nsearcharray.postings.SearchArray.avg_doc_length\n\nsearcharray.postings.SearchArray.doc_lens\n\nsearcharray.postings.SearchArray.posns\n\nsearcharray.postings.SearchArray.term_dict\n\nsearcharray.postings.SearchArray.term_mat\n\n# Intra-file Dependency:\nsearcharray.postings.ws_tokenizer\n    def ws_tokenizer(string):\n\n# Cross-file Dependency:\nsearcharray.indexing.build_index_from_tokenizer\n    def build_index_from_tokenizer(array: Iterable, tokenizer, batch_size=10000, truncate=False):\n        \"\"\"Build index directly from tokenizing docs (array of string).\"\"\"\n\n", "dependency_sampled": "# Intra-class Dependency:\nsearcharray.postings.SearchArray.term_dict\n\nsearcharray.postings.SearchArray.__init__\n    def __init__(self,\n                 postings,\n                 doc_len: int = 0,\n                 posns: Optional[dict] = None,\n                 encoded=False):\n\nsearcharray.postings.SearchArray.doc_lens\n\n# Intra-file Dependency:\nsearcharray.postings.ws_tokenizer\n    def ws_tokenizer(string):\n\n", "contexts_above": "\"\"\"Tokenized, searchable text as a pandas dtype.\"\"\"\nimport pandas as pd\nimport numbers\nfrom pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype\nfrom pandas.api.types import is_list_like\nfrom pandas.api.extensions import take\nimport json\nfrom collections import Counter\nimport warnings\nimport logging\nfrom typing import List, Union, Optional, Iterable\n\n\nimport numpy as np\nfrom searcharray.phrase.scan_merge import scan_merge_ins\nfrom searcharray.phrase.posn_diffs import compute_phrase_freqs\nfrom searcharray.phrase.middle_out import PosnBitArray\nfrom searcharray.similarity import Similarity, default_bm25\nfrom searcharray.indexing import build_index_from_tokenizer, build_index_from_terms_list\nfrom searcharray.term_dict import TermMissingError\n\nlogger = logging.getLogger(__name__)\n\n# When running in pytest\nimport sys  # noqa\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(logging.ERROR)\nformatter = logging.Formatter(\"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\")\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.setLevel(logging.ERROR)\n\n\nclass Terms:\n    \"\"\"An indexed search doc - a single bag of tokenized words and positions.\"\"\"\n\n    def __init__(self,\n                 postings,\n                 doc_len: int = 0,\n                 posns: Optional[dict] = None,\n                 encoded=False):\n        self.postings = postings\n        self.posns = None\n        self.encoded = encoded\n        self.doc_len = doc_len\n        self.posns = posns\n\n    def _validate_posns(self):\n        # (For testing/assertions) - Confirm every term in positions also in postings\n        if self.posns is None:\n            return\n        for term in self.posns:\n            if term not in self.postings:\n                raise ValueError(f\"Term {term} in positions but not in postings. \")\n\n    def termfreq(self, token):\n        return self.postings[token]\n\n    def terms(self):\n        return self.postings.items()\n\n    def positions(self, term=None):\n        if self.posns is None:\n            return {}\n        if term is None:\n            posns = self.posns.items()\n        else:\n            posns = self.posns[term]\n        return posns\n\n    def raw_positions(self, term_dict, term=None):\n        if self.posns is None:\n            return {}\n        if term is None:\n            posns = [(term_dict.get_term_id(term), posns) for term, posns in self.posns.items()]\n        else:\n            posns = [(term_dict.get_term_id(term), self.posns[term])]\n        return posns\n\n    def tf_to_dense(self, term_dict):\n        \"\"\"Convert to a dense vector of term frequencies.\"\"\"\n        dense = np.zeros(len(term_dict))\n        for term, freq in self.terms():\n            dense[term_dict.get_term_id(term)] = freq\n        return dense\n\n    def __len__(self):\n        return len(self.postings)\n\n    def __repr__(self):\n        posting_keys = set(self.postings.keys())\n        rval = f\"Terms({posting_keys})\"\n        return rval\n\n    def __str__(self):\n        return repr(self)\n\n    def __eq__(self, other):\n        # Flip to the other implementation if we're comparing to a SearchArray\n        # to get a boolean array back\n        if isinstance(other, SearchArray):\n            return other == self\n        same_postings = isinstance(other, Terms) and self.postings == other.postings\n        if same_postings and self.doc_len == other.doc_len:\n            return True\n\n    def __lt__(self, other):\n        # return isinstance(other, Terms) and hash(self) < hash(other)\n        keys_both = set(self.postings.keys()).union(set(other.postings.keys()))\n        # Sort lexically\n        keys_both = sorted(keys_both)\n\n        # Iterate as if these are two vectors of the same large dimensional vector sparse\n        for key in keys_both:\n            lhs_val = 0\n            rhs_val = 0\n            try:\n                lhs_val = self.postings[key]\n            except KeyError:\n                pass\n\n            try:\n                rhs_val = other.postings[key]\n            except KeyError:\n                pass\n\n            if lhs_val < rhs_val:\n                return True\n            elif lhs_val > rhs_val:\n                return False\n            else:\n                continue\n        return False\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    def __gt__(self, other):\n        return not (self < other) and self != other\n\n    def __hash__(self):\n        return hash(json.dumps(self.postings, sort_keys=True))\n\n\nclass TermsDtype(ExtensionDtype):\n    \"\"\"Pandas dtype for terms.\"\"\"\n\n    name = 'tokenized_text'\n    type = Terms\n    kind = 'O'\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\n                \"'construct_from_string' expects a string, got {}\".format(type(string))\n            )\n        elif string == cls.name:\n            return cls()\n        else:\n            raise TypeError(\n                \"Cannot construct a '{}' from '{}'\".format(cls.__name__, string)\n            )\n\n    @classmethod\n    def construct_array_type(cls):\n        return SearchArray\n\n    def __repr__(self):\n        return 'TermsDtype()'\n\n    @property\n    def na_value(self):\n        return Terms({})\n\n    def valid_value(self, value):\n        return isinstance(value, dict) or pd.isna(value) or isinstance(value, Terms)\n\n\nregister_extension_dtype(TermsDtype)\n\n\ndef ws_tokenizer(string):\n    if pd.isna(string):\n        return []\n    if not isinstance(string, str):\n        raise ValueError(\"Expected a string\")\n    return string.split()\n\n\ndef _row_to_postings_row(doc_id, row, doc_len, term_dict, posns: PosnBitArray):\n    tfs = {}\n    labeled_posns = {}\n    for term_idx in row.cols:\n        term = term_dict.get_term(term_idx)\n        tfs[term] = 1\n        enc_term_posns = posns.doc_encoded_posns(term_idx, doc_id=doc_id)\n        labeled_posns[term] = enc_term_posns\n\n    result = Terms(tfs, posns=labeled_posns,\n                   doc_len=doc_len, encoded=True)\n    return result\n\n\nclass SearchArray(ExtensionArray):\n    \"\"\"An array of tokenized text (Termss).\"\"\"\n\n    dtype = TermsDtype()\n\n    def __init__(self, postings, tokenizer=ws_tokenizer, avoid_copies=True):\n        # Check dtype, raise TypeError\n        if not is_list_like(postings):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(postings)))\n\n        self.avoid_copies = avoid_copies\n        self.tokenizer = tokenizer\n        self.term_mat, self.posns, \\\n            self.term_dict, self.avg_doc_length, \\\n            self.doc_lens = build_index_from_terms_list(postings, Terms)\n\n    @classmethod\n", "contexts_below": "\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        \"\"\"Construct a new SearchArray from a sequence of scalars (PostingRow or convertible into).\"\"\"\n        if dtype is not None:\n            if not isinstance(dtype, TermsDtype):\n                return scalars\n        if isinstance(scalars, np.ndarray) and scalars.dtype == TermsDtype():\n            return cls(scalars)\n        # String types\n        elif isinstance(scalars, np.ndarray) and scalars.dtype.kind in 'US':\n            return cls(scalars)\n        # Other objects\n        elif isinstance(scalars, np.ndarray) and scalars.dtype != object:\n            return scalars\n        return cls(scalars)\n\n    def memory_usage(self, deep=False):\n        \"\"\"Return memory usage of this array in bytes.\"\"\"\n        return self.nbytes\n\n    @property\n    def nbytes(self):\n        return self.term_mat.nbytes + self.posns.nbytes + self.doc_lens.nbytes + self.term_dict.nbytes\n\n    def __getitem__(self, key):\n        key = pd.api.indexers.check_array_indexer(self, key)\n        # Want to take rows of term freqs\n        if isinstance(key, numbers.Integral):\n            try:\n                rows = self.term_mat[key]\n                doc_len = self.doc_lens[key]\n                doc_id = key\n                if doc_id < 0:\n                    doc_id += len(self)\n                return _row_to_postings_row(doc_id, rows[0], doc_len,\n                                            self.term_dict, self.posns)\n            except IndexError:\n                raise IndexError(\"index out of bounds\")\n        else:\n            # Construct a sliced view of this array\n            sliced_tfs = self.term_mat.slice(key)\n            sliced_posns = self.posns.slice(sliced_tfs.rows) if not self.avoid_copies else self.posns\n            arr = SearchArray([], tokenizer=self.tokenizer)\n            arr.term_mat = sliced_tfs\n            arr.doc_lens = self.doc_lens[key]\n            arr.posns = sliced_posns\n            arr.term_dict = self.term_dict\n            arr.avg_doc_length = self.avg_doc_length\n            return arr\n\n    def __setitem__(self, key, value):\n        \"\"\"Set an item in the array.\"\"\"\n        key = pd.api.indexers.check_array_indexer(self, key)\n        if isinstance(value, pd.Series):\n            value = value.values\n        if isinstance(value, pd.DataFrame):\n            value = value.values.flatten()\n        if isinstance(value, SearchArray):\n            value = value.to_numpy()\n        if isinstance(value, list):\n            value = np.asarray(value, dtype=object)\n\n        if not isinstance(value, np.ndarray) and not self.dtype.valid_value(value):\n            raise ValueError(f\"Cannot set non-object array to SearchArray -- you passed type:{type(value)} -- {value}\")\n\n        # Cant set a single value to an array\n        if isinstance(key, numbers.Integral) and isinstance(value, np.ndarray):\n            raise ValueError(\"Cannot set a single value to an array\")\n\n        try:\n            is_encoded = False\n            posns = None\n            term_mat = np.asarray([])\n            doc_lens = np.asarray([])\n            if isinstance(value, float):\n                term_mat = np.asarray([value])\n                doc_lens = np.asarray([0])\n            elif isinstance(value, Terms):\n                term_mat = np.asarray([value.tf_to_dense(self.term_dict)])\n                doc_lens = np.asarray([value.doc_len])\n                is_encoded = value.encoded\n                posns = [value.raw_positions(self.term_dict)]\n            elif isinstance(value, np.ndarray):\n                term_mat = np.asarray([x.tf_to_dense(self.term_dict) for x in value])\n                doc_lens = np.asarray([x.doc_len for x in value])\n                is_encoded = value[0].encoded if len(value) > 0 else False\n                posns = [x.raw_positions(self.term_dict) for x in value]\n            np.nan_to_num(term_mat, copy=False, nan=0)\n            self.term_mat[key] = term_mat\n            self.doc_lens[key] = doc_lens\n\n            if posns is not None:\n                self.posns.insert(key, posns, is_encoded)\n\n            # Assume we have a positions for each term, doc pair. We can just update it.\n            # Otherwise we would have added new terms\n        except TermMissingError:\n            self._add_new_terms(key, value)\n\n    def _add_new_terms(self, key, value):\n        msg = \"\"\"Adding new terms! This might not be good if you tokenized this new text\n                 with a different tokenizer.\n\n                 Also. This is slow.\"\"\"\n        warnings.warn(msg)\n\n        scan_value = value\n        if isinstance(value, Terms):\n            scan_value = np.asarray([value])\n        for row in scan_value:\n            for term in row.terms():\n                self.term_dict.add_term(term[0])\n\n        self.term_mat.resize((self.term_mat.shape[0], len(self.term_dict)))\n        # Ensure posns_lookup has at least max self.posns\n        self[key] = value\n\n    def value_counts(\n        self,\n        dropna: bool = True,\n    ):\n        if dropna:\n            counts = Counter(self[:])\n            counts.pop(Terms({}), None)\n        else:\n            counts = Counter(self[:])\n        return pd.Series(counts)\n\n    def __len__(self):\n        len_rval = len(self.term_mat.rows)\n        return len_rval\n\n    def __ne__(self, other):\n        if isinstance(other, pd.DataFrame) or isinstance(other, pd.Series) or isinstance(other, pd.Index):\n            return NotImplemented\n\n        return ~(self == other)\n\n    def __eq__(self, other):\n        \"\"\"Return a boolean numpy array indicating elementwise equality.\"\"\"\n        # When other is a dataframe or series, not implemented\n        if isinstance(other, pd.DataFrame) or isinstance(other, pd.Series) or isinstance(other, pd.Index):\n            return NotImplemented\n\n        # When other is an ExtensionArray\n        if isinstance(other, SearchArray):\n            if len(self) != len(other):\n                return False\n            elif len(other) == 0:\n                return np.array([], dtype=bool)\n            else:\n                # Compatible term dicts, and same term freqs\n                # (not looking at positions, maybe we should?)\n                if self.term_dict.compatible(other.term_dict):\n                    return (self.term_mat == other.term_mat) & (self.doc_lens == other.doc_lens)\n                else:\n                    return np.zeros(len(self), dtype=bool)\n            # return np.array(self[:]) == np.array(other[:])\n\n        # When other is a scalar value\n        elif isinstance(other, Terms):\n            other = SearchArray([other], tokenizer=self.tokenizer)\n            warnings.warn(\"Comparing a scalar value to a SearchArray. This is slow.\")\n            return np.array(self[:]) == np.array(other[:])\n\n        # When other is a sequence but not an ExtensionArray\n        # its an array of dicts\n        elif is_list_like(other):\n            if len(self) != len(other):\n                return False\n            elif len(other) == 0:\n                return np.array([], dtype=bool)\n            # We actually don't know how it was tokenized\n            other = SearchArray(other, tokenizer=self.tokenizer)\n            return np.array(self[:]) == np.array(other[:])\n\n        # Return False where 'other' is neither the same length nor a scalar\n        else:\n            return np.full(len(self), False)\n\n    def isna(self):\n        # Every row with all 0s\n        empties = self.doc_lens == 0\n        return empties\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        # Want to take rows of term freqs\n        row_indices = np.arange(len(self.term_mat.rows))\n        # Take within the row indices themselves\n        result_indices = take(row_indices, indices, allow_fill=allow_fill, fill_value=-1)\n\n        if allow_fill and -1 in result_indices:\n            if fill_value is None or pd.isna(fill_value):\n                fill_value = Terms({}, encoded=True)\n\n            to_fill_mask = result_indices == -1\n            # This is slow as it rebuilds all the term dictionaries\n            # on the subsequent assignment lines\n            # However, this case tends to be the exception for\n            # most dataframe operations\n            taken = SearchArray([fill_value] * len(result_indices))\n            taken[~to_fill_mask] = self[result_indices[~to_fill_mask]].copy()\n\n            return taken\n        else:\n            taken = self[result_indices].copy()\n            return taken\n\n    def copy(self):\n        postings_arr = SearchArray([], tokenizer=self.tokenizer)\n        postings_arr.doc_lens = self.doc_lens.copy()\n        postings_arr.term_mat = self.term_mat.copy()\n        postings_arr.posns = self.posns\n        postings_arr.term_dict = self.term_dict\n        postings_arr.avg_doc_length = self.avg_doc_length\n\n        if not self.avoid_copies:\n            postings_arr.posns = self.posns.copy()\n            postings_arr.term_dict = self.term_dict.copy()\n        return postings_arr\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        concatenated_data = np.concatenate([ea[:] for ea in to_concat])\n        return SearchArray(concatenated_data, tokenizer=to_concat[0].tokenizer)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values)\n\n    def _values_for_factorize(self):\n        \"\"\"Return an array and missing value suitable for factorization (ie grouping).\"\"\"\n        arr = np.asarray(self[:], dtype=object)\n        return arr, Terms({})\n\n    def _check_token_arg(self, token):\n        if isinstance(token, str):\n            return token\n        elif isinstance(token, list) and len(token) == 1:\n            return token[0]\n        elif isinstance(token, list):\n            return token\n        else:\n            raise TypeError(\"Expected a string or list of strings for phrases\")\n\n    # ***********************************************************\n    # Search functionality\n    # ***********************************************************\n    def termfreqs(self, token: Union[List[str], str]) -> np.ndarray:\n        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            return self.phrase_freq(token)\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            matches = np.zeros(len(self), dtype=int)\n            slice_of_rows = None\n            if self.term_mat.subset:\n                slice_of_rows = self.term_mat.rows\n                doc_ids, termfreqs = self.posns.termfreqs(term_id,\n                                                          doc_ids=slice_of_rows)\n                mask = np.isin(self.term_mat.rows, doc_ids)\n                matches[mask] = termfreqs\n                return matches\n            else:\n                doc_ids, termfreqs = self.posns.termfreqs(term_id,\n                                                          doc_ids=slice_of_rows)\n                matches[doc_ids] = termfreqs\n                return matches\n        except TermMissingError:\n            return np.zeros(len(self), dtype=int)\n\n    def docfreq(self, token: str) -> int:\n        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        # Count number of rows where the term appears\n        try:\n            return self.posns.docfreq(self.term_dict.get_term_id(token))\n        except TermMissingError:\n            return 0\n\n    def doclengths(self) -> np.ndarray:\n        return self.doc_lens\n\n    def match(self, token: Union[List[str], str], slop: int = 1) -> np.ndarray:\n        \"\"\"Return a boolean numpy array indicating which elements contain the given term.\"\"\"\n        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            term_freq = self.phrase_freq(token)\n        else:\n            term_freq = self.termfreqs(token)\n        return term_freq > 0\n\n    def score(self, token: Union[str, List[str]], similarity: Similarity = default_bm25) -> np.ndarray:\n        \"\"\"Score each doc using a similarity function.\n\n        Parameters\n        ----------\n        token : str or list of str of what to search (already tokenized)\n        similarity : How to score the documents. Default is BM25.\n        \"\"\"\n        # Get term freqs per token\n        token = self._check_token_arg(token)\n\n        # For expensive toknes, we compute doc freq first, so we\n        # cache them in the DF cache, to let TF cache know it should be cached\n        tokens_l = [token] if isinstance(token, str) else token\n        all_dfs = np.asarray([self.docfreq(token) for token in tokens_l])\n\n        tfs = self.termfreqs(token)\n        token = self._check_token_arg(token)\n        doc_lens = self.doclengths()\n        scores = similarity(term_freqs=tfs, doc_freqs=all_dfs,\n                            doc_lens=doc_lens, avg_doc_lens=self.avg_doc_length,\n                            num_docs=len(self))\n        return scores\n\n    def positions(self, token: str, key=None) -> List[np.ndarray]:\n        \"\"\"Return a list of lists of positions of the given term.\"\"\"\n        term_id = self.term_dict.get_term_id(token)\n        key = self.term_mat.rows[key] if key is not None else self.term_mat.rows\n        posns = self.posns.positions(term_id, doc_ids=key)\n        return posns\n\n    def and_query(self, tokens: Union[List[str], List[List[str]]]) -> np.ndarray:\n        \"\"\"Return a mask on the postings array indicating which elements contain all terms.\"\"\"\n        masks = [self.match(term) for term in tokens]\n        mask = np.ones(len(self), dtype=bool)\n        for curr_mask in masks:\n            mask = mask & curr_mask\n        return mask\n\n    def or_query(self, tokens: Union[List[str], List[List[str]]], min_should_match: int = 1) -> np.ndarray:\n        \"\"\"Return a mask on the postings array indicating which elements contain all terms.\"\"\"\n        masks = [self.match(term) for term in tokens]\n        mask = np.sum(masks, axis=0) >= min_should_match\n        return mask\n\n    def phrase_freq(self, tokens: List[str], slop=1) -> np.ndarray:\n        if slop == 1 and len(tokens) == len(set(tokens)):\n            phrase_freqs = np.zeros(len(self))\n            try:\n                doc_ids = self.term_mat.rows\n                term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n                return self.posns.phrase_freqs(term_ids, doc_ids=doc_ids,\n                                               phrase_freqs=phrase_freqs)\n            except TermMissingError:\n                return phrase_freqs\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n\n    def phrase_freq_scan(self, tokens: List[str], mask=None, slop=1) -> np.ndarray:\n        if mask is None:\n            mask = self.and_query(tokens)\n\n        if np.sum(mask) == 0:\n            return mask\n\n        # Gather positions\n        posns = [self.positions(token, mask) for token in tokens]\n        phrase_freqs = np.zeros(len(self))\n\n        phrase_freqs[mask] = scan_merge_ins(posns, phrase_freqs[mask], slop=slop)\n        return phrase_freqs\n\n    def phrase_freq_every_diff(self, tokens: List[str], slop=1) -> np.ndarray:\n        phrase_freqs = -np.ones(len(self))\n\n        mask = self.and_query(tokens)\n        phrase_freqs[~mask] = 0\n        if np.sum(mask) == 0:\n            return phrase_freqs\n\n        term_posns = [self.positions(term, mask) for term in tokens]\n        for width in [10, 20, 30, 40]:\n            phrase_freqs[mask] = compute_phrase_freqs(term_posns,\n                                                      phrase_freqs[mask],\n                                                      slop=slop,\n                                                      width=width)\n\n        remaining_mask = phrase_freqs == -1\n        if np.any(remaining_mask):\n            remainder_freqs = self.phrase_freq_scan(tokens, mask=remaining_mask, slop=slop)\n            phrase_freqs[remaining_mask] = remainder_freqs[remaining_mask]\n        return phrase_freqs\n", "input_code": "    def index(cls, array: Iterable, tokenizer=ws_tokenizer,\n              truncate=False, batch_size=100000, avoid_copies=True) -> 'SearchArray':\n\n        \"\"\"\n        Indexes an array of strings using a specified tokenizer and returns an instance of SearchArray containing the indexed data. It builds an index from the given array and tokenizer, handling large arrays by processing them in batches if necessary.\n\n        Input-Output Arguments\n        :param cls: The class SearchArray itself. It is used to create a new instance of SearchArray that will hold the indexed data.\n        :param array: Iterable, the array of strings to be indexed. It is the input data that needs to be processed and indexed.\n        :param tokenizer: A function used for tokenizing the strings in the array. It defaults to ws_tokenizer if not specified. This tokenizer is applied to each string in the array to generate tokens for indexing.\n        :param truncate: bool, indicates whether to truncate the data to fit within memory constraints. It defaults to False, meaning the data will not be truncated unless specified.\n        :param batch_size: int, the size of batches to process the array in. This is useful for processing large arrays without loading everything into memory at once. Defaults to 100000.\n        :param avoid_copies: bool, indicates whether to avoid making copies of the data to save memory. Defaults to True, optimizing memory usage during the indexing process.\n        :return: An instance of SearchArray, which contains the indexed data including term matrix, positions, term dictionary, average document length, and document lengths.\n        \"\"\"", "reference_steps": "1. Define a class method `index` that takes an iterable `array` and optional parameters `tokenizer`, `truncate`, `batch_size`, and `avoid_copies`.\n2. Check if the input `array` is list-like; raise a `TypeError` if it is not.\n3. Call the function `build_index_from_tokenizer` with the `array`, `tokenizer`, and other optional parameters to create an index.\n4. The `build_index_from_tokenizer` function returns five values: `term_mat`, `posns`, `term_dict`, `avg_doc_length`, and `doc_lens`.\n5. Instantiate an object `postings` of the class `cls` with an empty list and the given `tokenizer` and `avoid_copies` settings.\n6. Assign the `term_mat` from the index to the `postings.term_mat` attribute.\n7. Assign the `posns` (positions) from the index to the `postings.posns` attribute.\n8. Assign the `term_dict` (term dictionary) from the index to the `postings.term_dict` attribute.\n9. Assign the `avg_doc_length` (average document length) from the index to the `postings.avg_doc_length` attribute.\n10. Assign the `doc_lens` (document lengths) from the index to the `postings.doc_lens` attribute and return the `postings` object.", "reference_code": "def index(cls, array: Iterable, tokenizer=ws_tokenizer,\n          truncate=False, batch_size=100000, avoid_copies=True) -> 'SearchArray':\n    \"\"\"Index an array of strings using tokenizer.\"\"\"\n    if not is_list_like(array):\n        raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n    term_mat, posns, term_dict, avg_doc_length, doc_lens =\\\n        build_index_from_tokenizer(array, tokenizer, batch_size=batch_size,\n                                   truncate=truncate)\n\n    postings = cls([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n    postings.term_mat = term_mat\n    postings.posns = posns\n    postings.term_dict = term_dict\n    postings.avg_doc_length = avg_doc_length\n    postings.doc_lens = doc_lens\n    return postings\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "type": "function", "class_name": null, "function_name": "bit_count64", "dependency_all": "# Intra-file Dependency:\nsearcharray.utils.bitcount._1\n\nsearcharray.utils.bitcount._2\n\nsearcharray.utils.bitcount._4\n\nsearcharray.utils.bitcount.all_but_one_bit\n\nsearcharray.utils.bitcount.s01\n\nsearcharray.utils.bitcount.s0F\n\nsearcharray.utils.bitcount.s33\n\nsearcharray.utils.bitcount.s55\n\n", "dependency_sampled": "# Intra-file Dependency:\nsearcharray.utils.bitcount._4\n\nsearcharray.utils.bitcount.s0F\n\nsearcharray.utils.bitcount.all_but_one_bit\n\nsearcharray.utils.bitcount.s01\n\n", "contexts_above": "\"\"\"Naive popcount implementation until such time that's exposed in numpy (SOON!).\"\"\"\nimport numpy as np\n\n\nm1 = np.uint64(0x5555555555555555)\nm2 = np.uint64(0x3333333333333333)\nm3 = np.uint64(0x0F0F0F0F0F0F0F0F)\nm4 = np.uint64(0x0101010101010101)\n\n\nmask = np.uint64(-1)\ns55 = np.uint64(m1 & mask)  # Add more digits for 128bit support\ns33 = np.uint64(m2 & mask)\ns0F = np.uint64(m3 & mask)\ns01 = np.uint64(m4 & mask)\nnum_bytes_64 = 8\nall_but_one_bit = np.uint64(8 * (num_bytes_64 - 1))\n\n_1 = np.uint64(1)\n_2 = np.uint64(2)\n_4 = np.uint64(4)\n\n\n", "contexts_below": "", "input_code": "def bit_count64(arr):\n\n    \"\"\"\n    Counts the number of bits set to 1 (also known as the Hamming weight or the population count) in each element of a 64-bit integer array using bitwise operations. This is a step in the process of efficiently calculating the bit count for each element in the array.\n    Input-Output Arguments\n    :param arr: Array of 64-bit integers. The array whose elements' bit counts are to be calculated. It is modified in-place to contain the bit counts as its elements.\n    :return: Array of integers. The same array passed as input, but now each element has been replaced with its corresponding bit count.\n    \"\"\"", "reference_steps": "1. Define a function `bit_count64` that takes an array `arr` as input, with the purpose of counting the number of set bits (1s) in each element of the array.\n\n2. Subtract from each element in the array half of itself bitwise ANDed with a mask `s55`. This step partially sums bits in pairs, effectively counting the number of set bits in each pair.\n\n3. Apply a mask `s33` to the array and add it to itself right-shifted by 2 bits ANDed with the same mask. This step combines the partial sums from pairs to 4-bit nibbles.\n\n4. Add to each element in the array its version right-shifted by 4 bits. This step aggregates the counts from 4-bit nibbles to 8-bit bytes.\n\n5. Apply a mask `s0F` to the array to keep only the lower 4 bits of each byte. This step ensures that the count doesn't overflow into the next byte.\n\n6. Multiply each element in the array by a mask `s01`. This step is likely a placeholder for a specific constant that would distribute the bit count across a specific set of bits in the element.\n\n7. Right-shift each element in the array by `all_but_one_bit`. This step is likely to move the bit count to a specific position in the element, possibly to the least significant bits.\n\n8. Return the modified array, which now contains the count of set bits for each original element in the input array.\n\nNote: The actual behavior of the function depends on the values of `_1`, `_2`, `_4`, `s55`, `s33`, `s0F`, `s01`, and `all_but_one_bit`, which are not provided in the reference code. These are likely constants that define specific bit masks and shift amounts used in the bit-counting algorithm. Without these values, the exact operation of the function is ambiguous.", "reference_code": "def bit_count64(arr):\n    \"\"\"Count the number of bits set in each element in the array.\"\"\"\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n\n    arr += (arr >> _4)\n    arr &= s0F\n    arr *= s01\n    arr >>= all_but_one_bit\n\n    return arr\n"}
{"namespace": "searcharray.solr.edismax", "type": "function", "class_name": null, "function_name": "edismax", "dependency_all": "# Intra-file Dependency:\nsearcharray.solr._edismax_field_centric\n    def _edismax_field_centric(frame: pd.DataFrame,\n                               query_fields: Dict[str, float],\n                               num_search_terms: int,\n                               search_terms: Dict[str, List[str]],\n                               mm: str,\n                               similarity: Similarity = default_bm25) -> Tuple[np.ndarray, str]:\n\nsearcharray.solr._edismax_term_centric\n    def _edismax_term_centric(frame: pd.DataFrame,\n                              query_fields: Dict[str, float],\n                              num_search_terms: int,\n                              search_terms: Dict[str, List[str]],\n                              mm: str,\n                              similarity: Similarity) -> Tuple[np.ndarray, str]:\n\nsearcharray.solr.get_field\n    def get_field(frame, field) -> SearchArray:\n\nsearcharray.solr.parse_field_boosts\n    def parse_field_boosts(field_lists: List[str]) -> dict:\n        \"\"\"Parse Solr's qf, pf, pf2, pf3 field boosts.\"\"\"\n\nsearcharray.solr.parse_query_terms\n    def parse_query_terms(frame: pd.DataFrame,\n                          query: str,\n                          query_fields: List[str]):\n\n# Cross-file Dependency:\nsearcharray.postings.SearchArray.score\n    def score(self, token: Union[str, List[str]], similarity: Similarity = default_bm25) -> np.ndarray:\n        \"\"\"Score each doc using a similarity function.\n\n        Parameters\n        ----------\n        token : str or list of str of what to search (already tokenized)\n        similarity : How to score the documents. Default is BM25.\n        \"\"\"\n\nsearcharray.similarity.Similarity\n    class Similarity(Protocol):\n        \"\"\"Similarity function protocol.\"\"\"\n\nsearcharray.similarity.default_bm25\n\n", "dependency_sampled": "# Intra-file Dependency:\nsearcharray.solr.parse_query_terms\n    def parse_query_terms(frame: pd.DataFrame,\n                          query: str,\n                          query_fields: List[str]):\n\nsearcharray.solr._edismax_field_centric\n    def _edismax_field_centric(frame: pd.DataFrame,\n                               query_fields: Dict[str, float],\n                               num_search_terms: int,\n                               search_terms: Dict[str, List[str]],\n                               mm: str,\n                               similarity: Similarity = default_bm25) -> Tuple[np.ndarray, str]:\n\n# Cross-file Dependency:\nsearcharray.similarity.Similarity\n    class Similarity(Protocol):\n        \"\"\"Similarity function protocol.\"\"\"\n\nsearcharray.postings.SearchArray.score\n    def score(self, token: Union[str, List[str]], similarity: Similarity = default_bm25) -> np.ndarray:\n        \"\"\"Score each doc using a similarity function.\n\n        Parameters\n        ----------\n        token : str or list of str of what to search (already tokenized)\n        similarity : How to score the documents. Default is BM25.\n        \"\"\"\n\n", "contexts_above": "\"\"\"Utility functions for Solr users of searcharray.\"\"\"\nimport re\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Optional, Dict, Tuple\nfrom searcharray.postings import SearchArray\nfrom searcharray.similarity import Similarity, default_bm25\n\n\ndef parse_min_should_match(num_clauses: int, spec: str) -> int:\n    \"\"\"Parse Solr's min should match (ie mm) spec.\n\n    See this ChatGPT translation of mm code from Solr's Java code for parsing this\n    https://chat.openai.com/share/76642aec-7e05-420f-a53a-83b8e2eea8fb\n\n    Parameters\n    ----------\n    num_clauses : int\n    spec : str\n\n    Returns\n    -------\n    int : the number of clauses that must match\n    \"\"\"\n    def checked_parse_int(value, error_message):\n        try:\n            return int(value)\n        except ValueError:\n            raise ValueError(error_message)\n\n    result = num_clauses\n    spec = spec.strip()\n\n    if '<' in spec:\n        # we have conditional spec(s)\n        space_around_less_than_pattern = re.compile(r'\\s*<\\s*')\n        spec = space_around_less_than_pattern.sub('<', spec)\n        for s in spec.split():\n            parts = s.split('<', 1)\n            if len(parts) < 2:\n                raise ValueError(\"Invalid 'mm' spec: '\" + s + \"'. Expecting values before and after '<'\")\n            upper_bound = checked_parse_int(parts[0], \"Invalid 'mm' spec. Expecting an integer.\")\n            if num_clauses <= upper_bound:\n                return result\n            else:\n                result = parse_min_should_match(num_clauses, parts[1])\n        return result\n\n    # otherwise, simple expression\n    if '%' in spec:\n        # percentage - assume the % was the last char. If not, let int() fail.\n        spec = spec[:-1]\n        percent = checked_parse_int(spec, \"Invalid 'mm' spec. Expecting an integer.\")\n        calc = (result * percent) * (1 / 100)\n        result = result + int(calc) if calc < 0 else int(calc)\n    else:\n        calc = checked_parse_int(spec, \"Invalid 'mm' spec. Expecting an integer.\")\n        result = result + calc if calc < 0 else calc\n\n    return min(num_clauses, max(result, 0))\n\n\ndef parse_field_boosts(field_lists: List[str]) -> dict:\n    \"\"\"Parse Solr's qf, pf, pf2, pf3 field boosts.\"\"\"\n    if not field_lists:\n        return {}\n\n    out = {}\n    carat_pattern = re.compile(r'\\^')\n\n    for field in field_lists:\n        parts = carat_pattern.split(field)\n        out[parts[0]] = None if len(parts) == 1 else float(parts[1])\n\n    return out\n\n\ndef get_field(frame, field) -> SearchArray:\n    if field not in frame.columns:\n        raise ValueError(f\"Field {field} not in dataframe\")\n    if not isinstance(frame[field].array, SearchArray):\n        raise ValueError(f\"Field {field} is not a searcharray field\")\n    return frame[field].array\n\n\ndef parse_query_terms(frame: pd.DataFrame,\n                      query: str,\n                      query_fields: List[str]):\n\n    search_terms: Dict[str, List[str]] = {}\n    num_search_terms = 0\n    term_centric = True\n\n    for field in query_fields:\n        arr = get_field(frame, field)\n\n        tokenizer = arr.tokenizer\n        search_terms[field] = []\n        field_num_search_terms = 0\n        for posn, term in enumerate(tokenizer(query)):\n            search_terms[field].append(term)\n            field_num_search_terms += 1\n        if num_search_terms == 0:\n            num_search_terms = field_num_search_terms\n        elif field_num_search_terms != num_search_terms:\n            term_centric = False\n\n    return num_search_terms, search_terms, term_centric\n\n\ndef _edismax_term_centric(frame: pd.DataFrame,\n                          query_fields: Dict[str, float],\n                          num_search_terms: int,\n                          search_terms: Dict[str, List[str]],\n                          mm: str,\n                          similarity: Similarity) -> Tuple[np.ndarray, str]:\n    explain = []\n    term_scores = []\n    for term_posn in range(num_search_terms):\n        max_scores = np.zeros(len(frame))\n        term_explain = []\n        for field, boost in query_fields.items():\n            term = search_terms[field][term_posn]\n            post_arr = get_field(frame, field)\n            field_term_score = post_arr.score(term, similarity=similarity) * (1 if boost is None else boost)\n            boost_exp = f\"{boost}\" if boost is not None else \"1\"\n            term_explain.append(f\"{field}:{term}^{boost_exp}\")\n            max_scores = np.maximum(max_scores, field_term_score)\n        term_scores.append(max_scores)\n        explain.append(\"(\" + \" | \".join(term_explain) + \")\")\n\n    min_should_match = parse_min_should_match(num_search_terms, spec=mm)\n    qf_scores = np.asarray(term_scores)\n    matches_gt_mm = np.sum(qf_scores > 0, axis=0) >= min_should_match\n    qf_scores = np.sum(term_scores, axis=0)\n    qf_scores[~matches_gt_mm] = 0\n    return qf_scores, \"(\" + \" \".join(explain) + f\")~{min_should_match}\"\n\n\ndef _edismax_field_centric(frame: pd.DataFrame,\n                           query_fields: Dict[str, float],\n                           num_search_terms: int,\n                           search_terms: Dict[str, List[str]],\n                           mm: str,\n                           similarity: Similarity = default_bm25) -> Tuple[np.ndarray, str]:\n    field_scores = []\n    explain = []\n    for field, boost in query_fields.items():\n        post_arr = get_field(frame, field)\n        term_scores = np.array([post_arr.score(term, similarity=similarity)\n                                for term in search_terms[field]])\n        min_should_match = parse_min_should_match(len(search_terms[field]), spec=mm)\n        exp = \" \".join([f\"{field}:{term}\" for term in search_terms[field]])\n        boost_exp = f\"{boost}\" if boost is not None else \"1\"\n        exp = \"(\" + exp + f\")~{min(min_should_match, len(search_terms[field]))}\"\n        exp = \"(\" + exp + f\")^{boost_exp}\"\n\n        matches_gt_mm = np.sum(term_scores > 0, axis=0) >= min(min_should_match, len(search_terms[field]))\n        sum_terms_bm25 = np.sum(term_scores, axis=0)\n        sum_terms_bm25[~matches_gt_mm] = 0\n        field_scores.append(sum_terms_bm25 * (1 if boost is None else boost))\n        explain.append(exp)\n    # Take maximum field scores as qf\n    qf_scores = np.asarray(field_scores)\n    qf_scores = np.max(qf_scores, axis=0)\n    return qf_scores, \" | \".join(explain)\n\n\n", "contexts_below": "", "input_code": "def edismax(frame: pd.DataFrame,\n            q: str,\n            qf: List[str],\n            mm: Optional[str] = None,\n            pf: Optional[List[str]] = None,\n            pf2: Optional[List[str]] = None,\n            pf3: Optional[List[str]] = None,\n            q_op: str = \"OR\",\n            similarity: Similarity = default_bm25) -> Tuple[np.ndarray, str]:\n\n    \"\"\"\n    This function performs an Extended Disjunction Max Query (edismax) search over a given DataFrame, considering various search parameters such as query fields, minimum match specifications, and phrase, bigram, and trigram matches. It supports both term-centric and field-centric approaches to calculate the relevance scores of documents based on the input query and returns the scores along with an explanation of how they were computed.\n\n    Input-Output Arguments\n    :param frame: pd.DataFrame. The DataFrame over which the search is performed.\n    :param q: str. The query string used for the search.\n    :param qf: List[str]. The fields in the DataFrame to search against.\n    :param mm: Optional[str]. The minimum should match specification, which dictates the minimum number of query terms that must match. Defaults to None.\n    :param pf: Optional[List[str]]. The fields to search for phrase matches. Defaults to None.\n    :param pf2: Optional[List[str]]. The fields to search for bigram matches. Defaults to None.\n    :param pf3: Optional[List[str]]. The fields to search for trigram matches. Defaults to None.\n    :param q_op: str. The default operator for the query, which can be \"OR\" or \"AND\". Defaults to \"OR\".\n    :param similarity: Similarity. The similarity measure to use for scoring documents. Defaults to a BM25 similarity measure.\n    :return: Tuple[np.ndarray, str]. A tuple containing an array of search results scores and a string explaining the scoring.\n    \"\"\"", "reference_steps": "1. Define a function `edismax` that takes a pandas DataFrame and various parameters to perform an Extended Disjunction Max (eDisMax) search query.\n\n2. Accept a query string `q`, a list of query fields `qf`, and optional parameters for minimum should match `mm`, phrase fields `pf`, bigram fields `pf2`, trigram fields `pf3`, and a default operator `q_op`.\n\n3. Convert query fields, phrase fields, bigram fields, and trigram fields into lists if they are not already lists using the `listify` helper function.\n\n4. Parse the query fields and their associated boosts using the `parse_field_boosts` function.\n\n5. Determine the default value for `mm` based on the presence of the parameter and the `q_op` value.\n\n6. Parse the query terms from the input query string `q` using the `parse_query_terms` function.\n\n7. Depending on whether the query is term-centric or field-centric, calculate the scores for the query fields using either `_edismax_term_centric` or `_edismax_field_centric` functions.\n\n8. If phrase fields are provided, calculate the phrase scores for each field, taking into account the boost factors.\n\n9. Sum the phrase scores across all fields and add them to the term scores where term scores are greater than zero.\n\n10. Return the final scores `qf_scores` and the explanation `explain` for the search results.", "reference_code": "def edismax(frame: pd.DataFrame,\n            q: str,\n            qf: List[str],\n            mm: Optional[str] = None,\n            pf: Optional[List[str]] = None,\n            pf2: Optional[List[str]] = None,\n            pf3: Optional[List[str]] = None,\n            q_op: str = \"OR\",\n            similarity: Similarity = default_bm25) -> Tuple[np.ndarray, str]:\n    \"\"\"Run edismax search over dataframe with searcharray fields.\n\n    Parameters\n    ----------\n    q : str\n        The query string\n    mm : str\n        The minimum should match spec\n    qf : list\n        The fields to search\n    pf : list\n        The fields to search for phrase matches\n    pf2 : list\n        The fields to search for bigram matches\n    pf3 : list\n        The fields to search for trigram matches\n    q_op : str, optional\n        The default operator, by default \"OR\"\n\n    Returns\n    -------\n    np.ndarray\n        The search results\n    \"\"\"\n    def listify(x):\n        return x if isinstance(x, list) else [x]\n\n    query_fields = parse_field_boosts(listify(qf))\n    phrase_fields = parse_field_boosts(listify(pf)) if pf else {}\n    if mm is None:\n        mm = \"1\"\n    if q_op == \"AND\":\n        mm = \"100%\"\n\n    # bigram_fields = parse_field_boosts(pf2) if pf2 else {}\n    # trigram_fields = parse_field_boosts(pf3) if pf3 else {}\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, list(query_fields.keys()))\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields,\n                                                   num_search_terms, search_terms, mm,\n                                                   similarity=similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields,\n                                                    num_search_terms, search_terms, mm,\n                                                    similarity=similarity)\n\n    phrase_scores = []\n    for field, boost in phrase_fields.items():\n        arr = get_field(frame, field)\n        terms = search_terms[field]\n        field_phrase_score = arr.score(terms, similarity=similarity) * (1 if boost is None else boost)\n        boost_exp = f\"{boost}\" if boost is not None else \"1\"\n        explain += f\" ({field}:\\\"{' '.join(terms)}\\\")^{boost_exp}\"\n        phrase_scores.append(field_phrase_score)\n\n    if len(phrase_scores) > 0:\n        phrase_scores = np.sum(phrase_scores, axis=0)\n        # Add where term_scores > 0\n        term_match_idx = np.where(qf_scores)[0]\n\n        qf_scores[term_match_idx] += phrase_scores[term_match_idx]\n    return qf_scores, explain\n"}
