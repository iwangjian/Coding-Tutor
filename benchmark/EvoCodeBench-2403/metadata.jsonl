{"namespace": "chat.utils.run_cmd", "type": "function", "completion_path": "Test-Agent/chat/utils.py", "signature_position": [265, 265], "body_position": [267, 268], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_cli.py::test_single_gpu", "tests/test_cli.py::test_multi_gpu", "tests/test_cli.py::test_8bit", "tests/test_cli.py::test_hf_api"], "requirement": {"Functionality": "Executes a given bash command by printing it first and then using the system's command line interface to run it.", "Arguments": ":param cmd: str, The bash command to be executed.\n:return: int, The exit status of the executed command. A value of 0 typically indicates that the command was executed successfully, while any other value indicates an error."}, "indent": 4}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "type": "function", "completion_path": "skfolio/src/skfolio/utils/stats.py", "signature_position": [165, 165], "body_position": [181, 185], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_stats.py::test_cov_nearest_cov_non_psd", "tests/test_utils/test_stats.py::TestIsDefPos::test_definite_positive_2x2", "tests/test_utils/test_stats.py::TestIsDefPos::test_negative_definite_3x3"], "requirement": {"Functionality": "Determines if a Cholesky decomposition can be performed on a given matrix. It checks if the matrix is suitable for Cholesky decomposition without verifying its Hermitian property but relies on the operation's success or failure.\n", "Arguments": ":param x: np.ndarray. The matrix to be checked for Cholesky decomposition suitability. It should have a shape of (n, m).\n:return: bool. Indicates whether the Cholesky decomposition can be applied to the matrix (True) or not (False)."}, "indent": 4}
{"namespace": "coord.inv_contract", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [35, 35], "body_position": [38, 41], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_inv_contract_inverts_contract"], "requirement": {"Functionality": "Computes the inverse of the contract function for a given input. It scales the input vector in a specific way, ensuring the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain.\n", "Arguments": ":param z: Array-like. The input vector for which the inverse operation of contract is to be computed. \n:return: Array-like. The scaled vector after applying the inverse operation of the contract function."}, "indent": 2}
{"namespace": "memoize.memoize_to_sqlite", "type": "function", "completion_path": "microagents/integrations/memoize.py", "signature_position": [8, 8], "body_position": [13, 19], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_sqlite_memoization.py::TestSQLiteMemoization::test_memoization_decorator_caches_function_output"], "requirement": {"Functionality": "This function acts as a decorator for memoization, caching the output of a function in a SQLite database to avoid redundant computations. It checks if the result of a function call is already stored in the database; if so, it retrieves the result from the database, otherwise, it computes the result, stores it, and then returns it.\n", "Arguments": ":param func_name: str, The name of the function to be memoized. It is used as a key in the SQLite database to store and retrieve the function's output.\n:param filename: str, The name of the SQLite database file where the function outputs are cached. Defaults to \"cache.db\".\n:return: A decorator function that wraps the original function, adding memoization functionality to it. No direct return value from this function itself."}, "indent": 4}
{"namespace": "iris.io.validators.is_valid_bbox", "type": "function", "completion_path": "open-iris/src/iris/io/validators.py", "signature_position": [149, 149], "body_position": [151, 157], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_validators.py::test_is_valid_bbox_raises_error"], "requirement": {"Functionality": "Checks if the provided bounding box values are valid by ensuring that the minimum x and y values are less than the maximum x and y values, respectively. If the bounding box is invalid, it raises a ValueError.", "Arguments": ":param cls: type. The class type that is calling this function, used for error messaging.\n:param values: Dict[str, float]. A dictionary containing the bounding box values with keys \"x_min\", \"x_max\", \"y_min\", and \"y_max\".\n:return: Dict[str, float]. The same dictionary of bounding box values if they are found to be valid."}, "indent": 4}
{"namespace": "geopoly.compute_sq_dist", "type": "function", "completion_path": "camp_zipnerf/internal/geopoly.py", "signature_position": [22, 22], "body_position": [24, 31], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/geopoly_test.py::GeopolyTest::test_compute_sq_dist_reference", "tests/geopoly_test.py::GeopolyTest::test_compute_sq_dist_single_input"], "requirement": {"Functionality": "This function computes the squared Euclidean distance between all pairs of columns in one or two matrices. It uses the mathematical property that the squared distance between two points x and y is equal to the sum of the squares of their norms minus twice their dot product. This is used to efficiently calculate distances in a vectorized manner, ensuring compatibility with numerical errors by setting negative distances to zero.\n", "Arguments": ":param mat0: numpy.ndarray, the first matrix whose columns are considered as individual vectors among which distances are computed.\n:param mat1: numpy.ndarray [optional], the second matrix whose columns are considered as individual vectors for distance computation. If not provided, `mat0` is used for both sets of vectors, computing all pairwise distances within the same set.\n:return: numpy.ndarray. A matrix where each element (i, j) represents the squared Euclidean distance between the i-th column of `mat0` and the j-th column of `mat1`. If `mat1` is not provided, it returns the squared distances among columns of `mat0`."}, "indent": 2}
{"namespace": "litdata.streaming.dataset._should_replace_path", "type": "function", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [415, 415], "body_position": [417, 420], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_dataset.py::test_should_replace_path"], "requirement": {"Functionality": "Determines if the given path is a special path that needs to be replaced based on predefined criteria. It checks if the path is either None, an empty string, or starts with specific prefixes.", "Arguments": ":param path: Optional[str]. The file or directory path to be checked. It is used to determine if the path meets certain conditions that classify it as a special path.\n:return: Bool. Whether the path is considered a special path that should be replaced."}, "indent": 4}
{"namespace": "skfolio.utils.tools.input_to_array", "type": "function", "completion_path": "skfolio/src/skfolio/utils/tools.py", "signature_position": [211, 218], "body_position": [250, 296], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_tools.py::TestInputToArray::test_raise_value_error_no_assets_names"], "requirement": {"Functionality": "Converts a collection of items, which can be array-like or a dictionary, into a numpy array while ensuring it matches the specified dimensions and shape. It also fills missing values in the dictionary case and verifies the final array's dimensions and shape against expected values.\n", "Arguments": ":param items: dict | npt.ArrayLike. The items to be verified and converted into an array. It can be a dictionary, numpy array, or any array-like structure.\n:param n_assets: int. The expected number of assets, used to verify the shape of the converted array.\n:param fill_value: any. The value used to fill in for missing elements when 'items' is a dictionary and some assets are not present.\n:param dim: int. The dimension of the final array, which can be either 1 or 2.\n:param assets_names: np.ndarray | None. Optional. The names of the assets, used when 'items' is a dictionary to match keys and fill missing values.\n:param name: str. The name of the items, used in error messages for clarity.\n:return: np.ndarray. The converted array with shape (n_assets,) for dim=1 or (n_groups, n_assets) for dim=2, after verifying and converting the input items."}, "indent": 4}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "type": "method", "completion_path": "microagents/agents/agent_serializer.py", "signature_position": [32, 32], "body_position": [36, 58], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_agent_serializer.py::TestAgentSerializer::test_deserialization"], "requirement": {"Functionality": "Deserialize a dictionary into a MicroAgent object, setting its attributes based on the dictionary's key-value pairs and handling optional keys gracefully.", "Arguments": ":param data: dict, the dictionary containing the MicroAgent's data.\n:param agent_lifecycle: The lifecycle state of the agent, used to initialize the MicroAgent.\n:param openai_wrapper: An instance or interface used for OpenAI operations, required to initialize the MicroAgent.\n:return: MicroAgent, the deserialized MicroAgent object with its attributes set according to the input dictionary."}, "indent": 8}
{"namespace": "image_utils.srgb_to_linear", "type": "function", "completion_path": "camp_zipnerf/internal/image_utils.py", "signature_position": [64, 66], "body_position": [68, 72], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/image_utils_test.py::ImageTest::test_srgb_linearize", "tests/image_utils_test.py::ImageTest::test_srgb_to_linear_golden"], "requirement": {"Functionality": "Converts sRGB color values to linear color space values. It uses a piecewise function as defined in the sRGB color space specification to perform the conversion, ensuring accurate color representation in linear space.\n", "Arguments": ":param srgb: Array-like. The sRGB color values expected to be in the range [0, 1]. These values are converted to linear space.\n:param eps: Float, optional. A small epsilon value to avoid division by zero or logarithm of zero in calculations. If not provided, it defaults to the machine epsilon for float32 data type.\n:param xnp: Module, optional. The numerical processing module used for calculations (e.g., NumPy, JAX NumPy). It defaults to JAX NumPy (jnp) if not specified.\n:return: Array-like. The converted color values in linear space."}, "indent": 2}
{"namespace": "camera_utils.safe_interpolate_1d", "type": "function", "completion_path": "camp_zipnerf/internal/camera_utils.py", "signature_position": [536, 542], "body_position": [547, 555], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/camera_utils_test.py::CameraUtilsTest::test_safe_interpolate_1d", "tests/camera_utils_test.py::CameraUtilsTest::test_safe_interpolate_1d_too_few_points", "tests/camera_utils_test.py::CameraUtilsTest::test_safe_interpolate_1d_empty_input"], "requirement": {"Functionality": "Interpolates a 1-dimensional signal defined at certain input times and queries it at specified output times using spline interpolation of a given degree and smoothness.", "Arguments": ":param x: Array-like. The 1-dimensional signal to be interpolated.\n:param spline_degree: Integer. The degree of the spline used for interpolation. It is adjusted to be at most one less than the number of points in x.\n:param smoothness: Float. A parameter controlling the smoothness of the spline fit. \n:param t_input: Array-like. The times at which the signal x is defined.\n:param t_output: Array-like. The times at which the interpolated signal is queried.\n:return: Array-like. The interpolated values of the signal at t_output times."}, "indent": 2}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "type": "function", "completion_path": "nlm-ingestor/nlm_ingestor/ingestor/formatter.py", "signature_position": [12, 14], "body_position": [15, 23], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_processor.py::MyTest::test_mixed_cased_words"], "requirement": {"Functionality": "This function checks a given word and corrects its casing based on specific rules. If the word is entirely in uppercase or lowercase, it returns the word as is. If the word is mixed case, it applies different rules to correct its casing, such as converting it to lowercase, uppercase, or capitalizing it based on the casing of the first two letters.\n", "Arguments": ":param word: String. The word to be checked and potentially corrected for mixed casing.\n:return: String. The word after applying casing corrections based on the function's logic."}, "indent": 4}
{"namespace": "iris.io.validators.is_binary", "type": "function", "completion_path": "open-iris/src/iris/io/validators.py", "signature_position": [29, 29], "body_position": [43, 46], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_validators.py::test_is_binary_raises_an_exception"], "requirement": {"Functionality": "Checks if the given numpy array contains only boolean values, and raises an exception if it does not. It ensures that the array is binary (i.e., contains only True or False values) before it is sent for further processing.\n", "Arguments": ":param cls: type. The class type that is calling this function. It is used for error messaging to specify which class encountered the non-binary array.\n:param v: np.ndarray. The numpy array to be checked for binary values. It is the main subject of the function's validation process.\n:param field: fields.ModelField. A field descriptor that provides context about the field being checked, used in error messaging to specify which field failed the binary check.\n:raises ValueError: If the array `v` does not contain boolean data types, a ValueError is raised with a message indicating the class and field name, along with the incorrect data type found.\n:return: np.ndarray. The input numpy array `v` is returned unchanged if it passes the binary check, allowing for further processing elsewhere."}, "indent": 4}
{"namespace": "coord.contract3_isoscale", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [110, 110], "body_position": [112, 116], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_contract3_isoscale"], "requirement": {"Functionality": "Performs a specific isotropic scaling operation on 3D inputs. It calculates a scaled version of the input based on its norm, designed for fast execution in the context of isotropic contraction operations.", "Arguments": ":param x: ndarray. The input array which is expected to be 3-dimensional. It is used to compute the norm and then apply the scaling operation.\n:return: ndarray. The scaled version of the input array after applying the isotropic scaling operation."}, "indent": 2}
{"namespace": "autorag.utils.util.load_summary_file", "type": "function", "completion_path": "AutoRAG/autorag/utils/util.py", "signature_position": [51, 52], "body_position": [62, 75], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/autorag/nodes/passagereranker/test_passage_reranker_run.py::test_run_passage_reranker_node", "tests/autorag/nodes/retrieval/test_run_retrieval_node.py::test_run_retrieval_node_only_hybrid", "tests/autorag/nodes/generator/test_run_generator_node.py::test_run_generator_node", "tests/autorag/nodes/queryexpansion/test_query_expansion_run.py::test_run_query_expansion_one_module"], "requirement": {"Functionality": "Loads a summary file from a specified path into a pandas DataFrame, converting specified columns that contain dictionary-like strings into actual dictionary objects.\n", "Arguments": ":param summary_path: str, The path of the summary file to be loaded. It is used to locate and read the file.\n:param dict_columns: Optional[List[str]], The names of the columns within the summary file that contain dictionary-like strings. These columns will be converted into actual dictionaries. Defaults to ['module_params'] if not provided.\n:return: pd.DataFrame, The loaded summary DataFrame with specified columns converted to dictionary objects."}, "indent": 4}
{"namespace": "coord.isotropize", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [220, 220], "body_position": [222, 239], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_track_isotropic"], "requirement": {"Functionality": "This function converts covariance matrices into isotropic covariance matrices with the same determinant. It supports two modes of operation, 'fast' and 'accurate', to compute the isotropic covariances, and includes a check to handle invalid determinants or logarithms of determinants.\n", "Arguments": ":param cov: ndarray. The covariance matrix or matrices to be isotropized. It is used to compute the isotropic covariance matrices.\n:param mode: String. The mode of computation, either 'fast' or 'accurate'. The 'fast' mode uses the determinant directly, while the 'accurate' mode uses the logarithm of the determinant for stability. Defaults to 'accurate'.\n:return: ndarray. The isotropic covariance matrix or matrices with the same determinant as the input covariance matrix or matrices."}, "indent": 2}
{"namespace": "run.parse_args", "type": "function", "completion_path": "XAgent/run.py", "signature_position": [9, 9], "body_position": [16, 38], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_run.py::test_execute_command_line_process_quiet_mode", "tests/test_run.py::test_start_command_line"], "requirement": {"Functionality": "Parses command line arguments provided by the user and returns them encapsulated in an argparse.Namespace object for easy access to each argument's value.\n", "Arguments": ":param --task: str, required. The task description, specifying what task should be performed.\n:param --upload-files: list of str, optional. List of files to upload, allowing multiple files to be specified.\n:param --model: str, optional. Model identifier for the task, specifying which model to use.\n:param --record-dir: str, optional. Directory to record task execution logs, specifying where to save the logs.\n:param --mode: str, optional, defaults to \"auto\". Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\n:param --quiet: bool, optional, defaults to False. If set, the program runs in quiet mode with minimal output.\n:param --max-subtask-chain-length: int, optional. Maximum length of subtask chain, specifying how long a subtask chain can be.\n:param --enable-ask-human-for-help: bool, optional. Flag to enable asking for human assistance during task execution.\n:param --max-plan-refine-chain-length: int, optional. Maximum length of plan refinement chain, specifying the limit for refining plans.\n:param --max-plan-tree-depth: int, optional. Maximum depth of the plan tree, specifying how deep the plan tree can be.\n:param --max-plan-tree-width: int, optional. Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\n:param --max-retry-times: int, optional. Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\n:param --config-file: str, optional, defaults to the value of the 'CONFIG_FILE' environment variable or 'assets/config.yml'. Path to the configuration file, specifying where to find the configuration settings.\n\n:return: argparse.Namespace. An object containing the parsed command line arguments and their values."}, "indent": 4}
{"namespace": "iris.io.validators.is_list_of_points", "type": "function", "completion_path": "open-iris/src/iris/io/validators.py", "signature_position": [49, 49], "body_position": [63, 66], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_validators.py::test_is_list_of_points_raises_an_exception"], "requirement": {"Functionality": "This function checks if the given numpy array has a shape that corresponds to a list of 2D points, specifically having a shape of (_, 2). If the array does not meet this criteria, it raises a ValueError.\n", "Arguments": ":param cls: type. The class type, used for error messaging to indicate the source of the error.\n:param v: np.ndarray. The numpy array to be checked. It is expected to represent a list of 2D points.\n:param field: fields.ModelField. A field descriptor, used in the error message to specify which field failed the validation.\n:raises ValueError: If the numpy array `v` does not have a shape of (_, 2), indicating it does not represent a list of 2D points.\n:return: np.ndarray. The original numpy array `v` is returned for further processing if it passes the shape check."}, "indent": 4}
{"namespace": "tanuki.utils.encode_int", "type": "function", "completion_path": "tanuki_py/src/tanuki/utils.py", "signature_position": [158, 159], "body_position": [160, 161], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_finetune_hash.py::test_encode_decode_hash"], "requirement": {"Functionality": "Encodes an integer into a single character based on a predefined character set. The character set consists of lowercase letters, digits, and an underscore.\n", "Arguments": ":param n: Integer. The integer to be encoded. It is used as an index to select a character from the character set.\n:return: String. The encoded character corresponding to the input integer."}, "indent": 4}
{"namespace": "spin_math.safe_log", "type": "function", "completion_path": "camp_zipnerf/internal/spin_math.py", "signature_position": [59, 62], "body_position": [74, 75], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/spin_math_test.py::SpinMathTest::test_safe_log_grad"], "requirement": {"Functionality": "Computes a logarithm of the input array in a safe manner, avoiding NaNs by clamping values at or near zero to a specified small value before taking the logarithm.\n", "Arguments": ":param x: Input array, the array for which the logarithm is to be computed.\n:param eps: Float, a small number to prevent NaNs by acting as a threshold below which values are considered too close to zero. It defaults to the smallest positive representable float32.\n:param value_at_zero: Float, the value to which inputs at or near zero are clamped before computing the logarithm. It defaults to the smallest positive representable float32. The return value for these inputs will be log(value_at_zero).\n:return: An array of the same shape as x, containing the logarithm of each element in x, with adjustments near zero to avoid NaNs."}, "indent": 2}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "type": "function", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [476, 478], "body_position": [479, 491], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_dataset.py::test_replay_chunks_sampling"], "requirement": {"Functionality": "This function calculates and updates the indexes for replaying chunks of data based on the intervals provided for each worker. It iterates through each worker's intervals, updating the current index and the chunk index based on the size of each interval.\n", "Arguments": ":param workers_intervals: Dict[int, List[Any]]. A dictionary where keys are worker indices and values are lists of intervals (each interval is represented as a list or tuple with at least two elements, where the difference between the last and first elements indicates the size of the interval). It is used to determine the chunk sizes for each worker.\n:param indexes: Dict[int, int]. A dictionary where keys are worker indices and values are the current indexes within those workers' data chunks. It is updated based on the intervals' sizes.\n:return: Tuple[Dict[int, int], Dict[int, int]]. The first dictionary in the tuple represents the updated chunk index for each worker, indicating which chunk should be replayed next. The second dictionary represents the updated indexes within those chunks for each worker."}, "indent": 4}
{"namespace": "grid_utils.trilerp", "type": "function", "completion_path": "camp_zipnerf/internal/grid_utils.py", "signature_position": [43, 47], "body_position": [68, 93], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/grid_utils_test.py::GridUtilsTest::test_trilerp_hash_all_same", "tests/grid_utils_test.py::GridUtilsTest::test_trilerp_invalid_datastructure"], "requirement": {"Functionality": "Performs trilinear interpolation on a given set of values using specified coordinates. It supports sampling from either a 3D voxel grid or a hash data structure based on the provided parameters. The function adjusts the coordinates and chooses the appropriate sampling method based on the type of data structure specified.\n", "Arguments": ":param values: A numpy array. If datastructure is 'grid', it expects a 4D array with shape (D,H,W,C) representing a 3D voxel grid with channels. If 'hash', it expects a 2D array with shape (N,C) representing hashed values with channels.\n:param coordinates: A numpy array. A 2D or higher array with shape (..., 3) containing the coordinates for sampling. Coordinates should be within the bounds of the dimensions they refer to.\n:param datastructure: String. Specifies the type of data structure from which to sample. Can be either 'grid' for a 3D voxel grid or 'hash' for a hashed data structure.\n:return: A numpy array. A 2D or higher array with shape (..., C) containing the interpolated values at the specified coordinates. The shape of the output array matches the input coordinates array, with an additional dimension for channels.\n\nRaises:\n    ValueError: If an invalid datastructure is passed, indicating that the function only supports 'grid' or 'hash' as valid data structures."}, "indent": 2}
{"namespace": "geopoly.compute_tesselation_weights", "type": "function", "completion_path": "camp_zipnerf/internal/geopoly.py", "signature_position": [34, 34], "body_position": [36, 44], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/geopoly_test.py::GeopolyTest::test_compute_tesselation_weights_reference"], "requirement": {"Functionality": "Computes the barycentric weights for tessellating the vertices of a triangle by a given factor. This involves generating integer weights for each vertex of the triangle and then normalizing these weights to get the barycentric coordinates.\n", "Arguments": ":param v: Int, the tessellation factor, which determines how finely the triangle is subdivided. It must be greater than or equal to 1.\n:return: Numpy array, the barycentric weights for each point in the tessellated triangle. These weights are used to interpolate values across the triangle."}, "indent": 2}
{"namespace": "linspline.query", "type": "function", "completion_path": "camp_zipnerf/internal/linspline.py", "signature_position": [32, 32], "body_position": [34, 36], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/linspline_test.py::LinsplineTest::test_clamp", "tests/linspline_test.py::LinsplineTest::test_blur_stepfun_matches_convolution"], "requirement": {"Functionality": "This function queries a linear spline defined by time points (t) and corresponding values (v) at given query points (tq). It ensures the spline is valid, then uses interpolation to find the values at the query points, with extrapolated values set to 0 outside the original range.\n", "Arguments": ":param tq: Array-like. The query points where the spline should be evaluated.\n:param t: Array-like. The time points that define the knots of the linear spline.\n:param v: Array-like. The values corresponding to each time point in 't', defining the spline.\n:return: Array-like. The interpolated values at each query point in 'tq'."}, "indent": 2}
{"namespace": "iris.io.validators.are_all_positive", "type": "function", "completion_path": "open-iris/src/iris/io/validators.py", "signature_position": [109, 109], "body_position": [123, 129], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_validators.py::test_are_all_positive_raises_an_exception"], "requirement": {"Functionality": "This function checks if all values in the given input are positive. It supports both single values and iterables (like lists). If any value is not positive, it raises a ValueError indicating that all values must be positive.\n", "Arguments": ":param cls: type. The class type that is calling this function. It is used for error messaging to indicate which class encountered the error.\n:param v: Any. The value or values to be checked for positivity. It can be a single value or an iterable of values.\n:param field: fields.ModelField. A field descriptor that provides context about the field being checked. It is used in error messaging to specify which field must contain only positive values.\n:raises ValueError: If any value in `v` is not positive, a ValueError is raised with a message indicating that all values must be positive, including the class name and field name.\n:return: Any. The original value `v` is returned for further processing if all values are positive."}, "indent": 4}
{"namespace": "camera_utils.convert_to_ndc", "type": "function", "completion_path": "camp_zipnerf/internal/camera_utils.py", "signature_position": [46, 52], "body_position": [91, 115], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/camera_utils_test.py::CameraUtilsTest::test_convert_to_ndc"], "requirement": {"Functionality": "Converts a set of rays from world space to normalized device coordinates (NDC) using a perspective projection model. This is useful for rendering or processing 3D scenes in a standardized coordinate system.\n", "Arguments": ":param origins: ndarray(float32), The origins of the rays in world space, used to determine their starting points in NDC.\n:param directions: ndarray(float32), The directions of the rays in world space, used to calculate their orientation in NDC.\n:param pixtocam: ndarray(float32), The inverse intrinsic matrix of the camera, used for the perspective projection calculation.\n:param near: float, The distance to the near plane along the negative z-axis, used to define the depth range of the projection.\n:param xnp: module, Either numpy or jax.numpy, specifies the numerical library to use for calculations.\n:return: Tuple of ndarray(float32), The origins and directions of the rays in normalized device coordinates.\n\nThis function performs a perspective projection of rays defined in world space into a normalized device coordinate system, assuming an identity extrinsic matrix (camera pose) and intrinsic parameters defined by the pixtocam matrix. The function adjusts ray origins to the near plane and calculates the corresponding directions in NDC, facilitating the rendering or analysis of 3D scenes from a standardized viewpoint."}, "indent": 2}
{"namespace": "geometry.are_lines_parallel", "type": "function", "completion_path": "camp_zipnerf/internal/geometry.py", "signature_position": [173, 173], "body_position": [174, 177], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/geometry_test.py::GeometryTest::test_are_lines_parallel_not_parallel"], "requirement": {"Functionality": "Determines if two lines are parallel by comparing the dot product of their normalized direction vectors, considering a small epsilon to account for numerical precision issues.\n", "Arguments": ":param dir1: Array-like. The direction vector of the first line, which is normalized within the function.\n:param dir2: Array-like. The direction vector of the second line, which is also normalized within the function.\n:return: Bool. True if the lines are parallel (considering numerical precision), otherwise False."}, "indent": 2}
{"namespace": "common.bleu4_score", "type": "function", "completion_path": "UHGEval/uhgeval/metric/common.py", "signature_position": [24, 28], "body_position": [29, 37], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/metric/test_common.py::TestEvaluateFunctions::test_bleu4_score"], "requirement": {"Functionality": "Calculates the BLEU-4 score for a given continuation and reference text, optionally including the brevity penalty in the score calculation. It tokenizes the input texts using a custom tokenizer function before computing the BLEU score.\n", "Arguments": ":param continuation: str, the generated text whose quality is being evaluated.\n:param reference: str, the reference text against which the continuation is compared.\n:param with_penalty: Bool, indicates whether to include the brevity penalty in the final score calculation. Defaults to False.\n:return: float, the calculated BLEU-4 score, optionally adjusted for brevity penalty."}, "indent": 4}
{"namespace": "spin_math.safe_sqrt", "type": "function", "completion_path": "camp_zipnerf/internal/spin_math.py", "signature_position": [32, 35], "body_position": [49, 50], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/spin_math_test.py::SpinMathTest::test_safe_sqrt"], "requirement": {"Functionality": "This function calculates the square root of a given number in a safe manner, avoiding the evaluation at zero by using a small epsilon value. If the input is less than this epsilon, it uses a predefined value near zero instead to prevent NaN results.\n", "Arguments": ":param x: The operand for which the square root is to be calculated.\n:param eps: A small number to prevent NaN results by avoiding division by zero or square root of zero. It defaults to the machine epsilon for float32 data type.\n:param value_at_zero: The value to use instead of zero to prevent NaN results. The function will return the square root of this value if x is less than eps.\n:return: The safe square root of x, or the square root of value_at_zero if x is less than eps."}, "indent": 2}
{"namespace": "stepfun.weight_to_pdf", "type": "function", "completion_path": "camp_zipnerf/internal/stepfun.py", "signature_position": [46, 46], "body_position": [48, 50], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/stepfun_test.py::StepFunTest::test_weight_pdf_conversion_is_accurate"], "requirement": {"Functionality": "Converts a vector of weights that sum to 1 into a probability density function (PDF) that integrates to 1, by dividing the weights by the difference between consecutive elements in a given vector.\n", "Arguments": ":param t: Array-like. A vector of thresholds or time points, used along with weights to generate the PDF.\n:param w: Array-like. A vector of weights that sum to 1, representing the distribution to be converted into a PDF.\n:return: Array-like. The resulting PDF that integrates to 1, obtained by dividing the weights by the difference between consecutive elements in the input vector t."}, "indent": 2}
{"namespace": "litdata.streaming.reader._get_folder_size", "type": "function", "completion_path": "litdata/litdata/streaming/reader.py", "signature_position": [293, 293], "body_position": [299, 304], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_reader.py::test_get_folder_size"], "requirement": {"Functionality": "Calculates the total size of all files within a specified folder, including files in its subdirectories. It is designed to handle situations where files may be deleted during the size calculation process by ignoring FileNotFoundError exceptions.\n", "Arguments": ":param path: str, The path to the folder whose total file size is to be calculated. \n:return: int, The total size of all files within the specified folder in bytes."}, "indent": 4}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/core/bbox/structures/utils.py", "signature_position": [11, 11], "body_position": [24, 25], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_box3d.py::test_limit_period"], "requirement": {"Functionality": "Limits the input value within a specific period range for periodic functions, adjusting it based on the given offset and period parameters.\n", "Arguments": ":param val: torch.Tensor or np.ndarray, the value to be adjusted to fit within the specified period range.\n:param offset: float, optional, the offset used to define the value range, with a default of 0.5. It determines the starting point of the period range.\n:param period: the period of the value, with a default of np.pi. It defines the length of the period in which the value should be limited.\n:return: torch.Tensor or np.ndarray, the adjusted value that fits within the range of [-offset * period, (1-offset) * period]."}, "indent": 4}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "type": "method", "completion_path": "microagents/agents/agent_serializer.py", "signature_position": [7, 7], "body_position": [11, 29], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_agent_serializer.py::TestAgentSerializer::test_full_serialization"], "requirement": {"Functionality": "Serializes the MicroAgent object into a dictionary format for the purpose of persistence. This includes converting the purpose_embedding from a numpy array to a list if necessary, to ensure compatibility with serialization formats.\n", "Arguments": ":param agent: MicroAgent. The MicroAgent instance to be serialized.\n:return: Dictionary. A dictionary representation of the MicroAgent instance, including all relevant attributes such as dynamic_prompt, purpose, purpose_embedding, depth, max_depth, usage_count, id, parent_id, working_agent, is_prime, evolve_count, number_of_code_executions, and last_input."}, "indent": 8}
{"namespace": "litdata.utilities.packing._pack_greedily", "type": "function", "completion_path": "litdata/litdata/utilities/packing.py", "signature_position": [5, 5], "body_position": [9, 23], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/utilities/test_packing.py::test_pack_greedily"], "requirement": {"Functionality": "This function attempts to distribute a list of items with associated weights into a specified number of bins in a way that the total weight is as evenly distributed as possible across the bins. It does so greedily by sorting the items by weight in descending order and then placing each item into the bin with the current lowest total weight.\n", "Arguments": ":param items: List[Any]. A list of items to be distributed into bins. Each item can be of any type.\n:param weights: List[int]. A list of integers representing the weights of the corresponding items in the 'items' list. Each weight must be positive.\n:param num_bins: int. The number of bins into which the items are to be distributed.\n:return: Tuple[Dict[int, List[Any]], Dict[int, int]]. The function returns a tuple containing two dictionaries. The first dictionary maps each bin index to a list of items that have been placed in that bin. The second dictionary maps each bin index to the total weight of the items in that bin."}, "indent": 4}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "type": "method", "completion_path": "microagents/integrations/memoize.py", "signature_position": [52, 52], "body_position": [53, 54], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_sqlite_memoization.py::TestSQLiteMemoization::test_hash_computation_returns_string", "tests/test_sqlite_memoization.py::TestSQLiteMemoization::test_caching_mechanism_stores_and_retrieves_data_correctly"], "requirement": {"Functionality": "Computes a SHA-256 hash of the function name, its arguments, and keyword arguments. This is typically used for creating a unique identifier for caching or memoization purposes in the context of SQLite database operations.\n", "Arguments": ":param self: SQLiteMemoization. An instance of the SQLiteMemoization class.\n:param func_name: str, The name of the function for which the hash is being computed. It is used as part of the data to generate the hash.\n:param args: tuple, The positional arguments of the function. These are used as part of the data to generate the hash.\n:param kwargs: dict, The keyword arguments of the function. These are used as part of the data to generate the hash.\n:return: str, The hexadecimal digest of the SHA-256 hash of the input data."}, "indent": 8}
{"namespace": "iris.utils.math.polygon_length", "type": "function", "completion_path": "open-iris/src/iris/utils/math.py", "signature_position": [176, 176], "body_position": [194, 200], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/utils/test_math.py::test_polygon_length_fails"], "requirement": {"Functionality": "Computes the total length of a polygon by summing the distances between consecutive points, excluding distances that exceed a specified maximum, to avoid counting large gaps between disjoint arcs as part of the polygon's length.\n", "Arguments": ":param polygon: np.ndarray. A 2-dimensional numpy array representing a polygon, where each row corresponds to a point in 2D space.\n:param max_point_distance: int, optional. The maximum distance between two consecutive points that can be considered part of the same arc. Points further apart than this threshold are not counted in the total length. Defaults to 20.\n:return: float. The total length of the polygon, considering only the distances between consecutive points that are below the specified maximum distance."}, "indent": 4}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "type": "function", "completion_path": "open-iris/src/iris/nodes/vectorization/contouring.py", "signature_position": [13, 15], "body_position": [26, 35], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/nodes/vectorization/test_contouring.py::test_eliminate_tiny_contours"], "requirement": {"Functionality": "The function filters out polygons based on their area. It removes polygons whose area is below either an absolute threshold or a relative threshold compared to the largest polygon's area in the list.\n", "Arguments": ":param polygons: List of numpy.ndarray. The list of polygons (each represented as a numpy array) to be filtered.\n:param rel_tr: NonNegativeFloat, optional. The relative threshold as a fraction of the largest polygon's area. Polygons with an area smaller than this fraction are filtered out. Defaults to 0.03.\n:param abs_tr: NonNegativeFloat, optional. The absolute threshold for the area. Polygons with an area smaller than this threshold are filtered out. Defaults to 0.0.\n:return: List of numpy.ndarray. The list of polygons that meet the area criteria and are not filtered out."}, "indent": 4}
{"namespace": "litdata.streaming.dataset._replay_sampling", "type": "function", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [453, 453], "body_position": [455, 473], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_dataset.py::test_replay_sampling"], "requirement": {"Functionality": "This function calculates and returns the number of samples each worker has processed, given the total number of samples yielded, the batch size, and the number of workers. It ensures that the distribution of samples among workers is as even as possible, taking into account the division of the total samples by the number of workers and the batch size, and then distributing any remaining samples.\n", "Arguments": ":param num_samples_yielded: Int. The total number of samples that have been yielded.\n:param batch_size: Int. The size of each batch of samples.\n:param num_workers: Int. The number of workers involved in processing the samples.\n:return: Dict[Int, Int]. A dictionary where each key is a worker index (starting from 0) and its value is the number of samples that worker has processed.\n"}, "indent": 4}
{"namespace": "autorag.strategy.filter_by_threshold", "type": "function", "completion_path": "AutoRAG/autorag/strategy.py", "signature_position": [51, 51], "body_position": [64, 72], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/autorag/test_strategy.py::test_avoid_empty_result"], "requirement": {"Functionality": "Filters the given lists of results and values by a specified threshold, returning the filtered results and their corresponding metadata. It ensures that only those results (and their metadata) where the associated value is less than or equal to the threshold are returned.\n", "Arguments": ":param results: List. The list of results that need to be filtered based on the threshold.\n:param value: List. The list of values corresponding to each result. This list is used to determine whether a result should be filtered based on the threshold. It must have the same length as the results list.\n:param threshold: Numeric. The threshold value used to filter the results and values. Only items with a value less than or equal to this threshold will be included in the output.\n:param metadatas: List, optional. A list containing metadata for each result. If not provided, a list of None values will be used instead. This list should have the same length as the results list.\n:return: A tuple containing two lists: the filtered list of results and the filtered list of metadatas. If no results meet the threshold criteria, empty lists are returned.\n:rtype: Tuple[List, List]. The first list in the tuple contains the filtered results, and the second list contains the corresponding filtered metadata."}, "indent": 4}
{"namespace": "iris.utils.math.area", "type": "function", "completion_path": "open-iris/src/iris/utils/math.py", "signature_position": [7, 7], "body_position": [26, 32], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/utils/test_math.py::test_area_fails"], "requirement": {"Functionality": "Calculates the area of a simple polygon using the Shoelace formula. The function takes an array of points that define the polygon and returns the calculated area. It raises an error if the input does not meet the expected shape.\n", "Arguments": ":param array: np.ndarray. An array representing a polygon as a list of points, where each point is a pair of coordinates (x, y). The shape of the array must be (_, 2), where _ can be any number of points.\n:raises ValueError: If the input array does not have the shape (_, 2), indicating it does not represent a valid list of polygon points.\n:return: float. The calculated area of the polygon.\n\nReferences:\n[1] https://en.wikipedia.org/wiki/Shoelace_formula\n[2] https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates"}, "indent": 4}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/prop_utils.py", "signature_position": [99, 99], "body_position": [115, 119], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/mipnerf360_tests.py::test_searchsorted_out_of_bounds", "tests/mipnerf360_tests.py::test_searchsorted", "tests/mipnerf360_tests.py::test_searchsorted_out_of_bounds", "tests/mipnerf360_tests.py::test_searchsorted"], "requirement": {"Functionality": "This function finds the indices where elements of the tensor v should be inserted into the tensor a to maintain order. It is designed to be faster than similar functions by using additional memory. It returns two tensors, idx_lo and idx_hi, indicating the lower and upper bounds where each element of v could be inserted into a.\n", "Arguments": ":param a: torch.Tensor. A sorted tensor that serves as the reference points for insertion.\n:param v: torch.Tensor. A tensor containing the query points to be inserted into a. The query points do not need to be sorted, and their shape should be compatible with that of a, except for the last dimension which can differ.\n:return: Tuple[torch.Tensor, torch.Tensor]. A tuple containing two tensors, idx_lo and idx_hi. For each element in v, idx_lo and idx_hi represent the indices in a where the element could be inserted while maintaining the sorted order. If an element in v is out of the range of a, both idx_lo and idx_hi will point to either the first or last index of a."}, "indent": 4}
{"namespace": "camera_utils.intrinsic_matrix", "type": "function", "completion_path": "camp_zipnerf/internal/camera_utils.py", "signature_position": [768, 774], "body_position": [776, 780], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/camera_utils_test.py::CameraUtilsTest::test_convert_to_ndc"], "requirement": {"Functionality": "Generates the intrinsic matrix for a pinhole camera model using the OpenCV coordinate system. This matrix is crucial for camera calibration and 3D reconstruction tasks.\n", "Arguments": ":param fx: Numeric. The focal length of the camera along the x-axis. It influences how wide or narrow the view is captured.\n:param fy: Numeric. The focal length of the camera along the y-axis. Similar to fx, it affects the vertical field of view.\n:param cx: Numeric. The x-coordinate of the optical center of the camera. It's used to align the center of the image with the principal point.\n:param cy: Numeric. The y-coordinate of the optical center of the camera. Works alongside cx for proper image centering.\n:param xnp: Module, optional. The numerical Python module used for matrix operations, defaulting to numpy. It allows for flexibility in case a different module or a mock object is needed for testing.\n:return: Array. The 3x3 intrinsic matrix representing the camera's internal parameters."}, "indent": 2}
{"namespace": "coord.contract", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [26, 26], "body_position": [29, 32], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_contract_matches_special_case", "tests/coord_test.py::CoordTest::test_contract_is_bounded", "tests/coord_test.py::CoordTest::test_contract_is_noop_when_norm_is_leq_one", "tests/coord_test.py::CoordTest::test_inv_contract_inverts_contract", "tests/coord_test.py::CoordTest::test_contract3_isoscale"], "requirement": {"Functionality": "Contracts points towards the origin based on a specific mathematical formula (Equation 10 from the paper available at arxiv.org/abs/2111.12077). This involves scaling the magnitude of the input vector(s) in a way that points within a unit distance from the origin are scaled correctly.\n", "Arguments": ":param x: Array. The input array of points to be contracted towards the origin. It is used to calculate the magnitude squared of the points and then scale them accordingly.\n:return: Array. The array of points after being scaled towards the origin."}, "indent": 2}
{"namespace": "litdata.utilities.format._human_readable_bytes", "type": "function", "completion_path": "litdata/litdata/utilities/format.py", "signature_position": [24, 24], "body_position": [25, 29], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/utilities/test_format.py::test_human_readable_bytes"], "requirement": {"Functionality": "Converts a number representing bytes into a human-readable format with appropriate units (B, KB, MB, GB, TB, PB). It scales the number down by 1000 and changes units from bytes to petabytes as necessary until a human-friendly format is achieved.", "Arguments": ":param num_bytes: Float. The number of bytes to be converted into a human-readable format.\n:return: String. The number of bytes converted into a human-readable format with the appropriate unit."}, "indent": 4}
{"namespace": "iris.io.validators.is_array_n_dimensions", "type": "function", "completion_path": "open-iris/src/iris/io/validators.py", "signature_position": [163, 163], "body_position": [173, 183], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_validators.py::test_is_array_n_dimensions_raises_an_exception"], "requirement": {"Functionality": "This function creates a Pydantic validator that checks if a given array has a specific number of dimensions. It is designed to be used within Pydantic models to validate the dimensions of array fields.\n", "Arguments": ":param nb_dimensions: Int. The number of dimensions the array must have for the validation to pass.\n:return: Callable. A validator function that can be used in Pydantic models to validate the dimensionality of array fields.\n\nThe validator function itself has the following parameters and return type:\n:param cls: Type. The class of the model being validated.\n:param v: np.ndarray. The array being validated.\n:param field: fields.ModelField. The field of the model that is being validated.\n:return: np.ndarray. The validated array if it meets the specified number of dimensions."}, "indent": 4}
{"namespace": "geometry.cartesian_to_spherical", "type": "function", "completion_path": "camp_zipnerf/internal/geometry.py", "signature_position": [231, 234], "body_position": [252, 259], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/geometry_test.py::GeometryTest::test_coordinate_transform"], "requirement": {"Functionality": "Converts cartesian coordinates to spherical coordinates using a right-handed coordinate system. It calculates the radius, inclination (theta), and azimuth (phi) of a point given its cartesian coordinates (x, y, z).\n", "Arguments": ":param cartesian_vector: Array-like, the cartesian coordinates of a point or set of points, expected to have a shape of (..., 3) where the last dimension represents the x, y, and z coordinates.\n:param eps: Float, a small epsilon value used to prevent division by zero in the calculation of the inclination angle (theta). It defaults to the smallest positive float32 value.\n:return: Tuple of arrays. The spherical coordinates as a tuple consisting of radius (r), inclination (theta), and azimuth (phi). Each element in the tuple is an array with the same shape as the input array's leading dimensions."}, "indent": 2}
{"namespace": "common.rougeL_score", "type": "function", "completion_path": "UHGEval/uhgeval/metric/common.py", "signature_position": [41, 44], "body_position": [45, 49], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/metric/test_common.py::TestEvaluateFunctions::test_rougeL_score"], "requirement": {"Functionality": "Calculates the ROUGE-L score between a generated text continuation and a reference text. It tokenizes both texts using a custom tokenizer function based on jieba, then computes the ROUGE-L score to evaluate the quality of the generated text in comparison to the reference.\n", "Arguments": ":param continuation: str, The generated text whose quality is to be evaluated.\n:param reference: str, The reference text against which the generated text is compared.\n:return: float. The ROUGE-L score indicating the quality of the generated text in relation to the reference text."}, "indent": 4}
{"namespace": "detectron2.utils.registry.locate", "type": "function", "completion_path": "UniRef/detectron2/utils/registry.py", "signature_position": [40, 40], "body_position": [47, 60], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_registry.py::TestLocate::test_compress_target"], "requirement": {"Functionality": "Locates and returns a Python object based on its fully qualified name given as a string. If the object cannot be found using the standard method, it attempts to locate the object using a fallback method. An exception is raised if the object cannot be located by either method.\n", "Arguments": ":param name: str, The fully qualified name of the object to locate, in the format \"module.submodule.class_name\".\n:return: Any. The located Python object."}, "indent": 4}
{"namespace": "detectron2.utils.testing.reload_script_model", "type": "function", "completion_path": "UniRef/detectron2/utils/testing.py", "signature_position": [133, 133], "body_position": [138, 141], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/structures/test_rotated_boxes.py::TestRotatedBoxesStructure::test_scriptability"], "requirement": {"Functionality": "This function saves a PyTorch JIT module to an in-memory buffer and then loads it back from this buffer. This process is useful for testing or ensuring that a module can be successfully serialized and deserialized without loss of information or functionality.\n", "Arguments": ":param module: torch.jit.ScriptModule. The JIT module that needs to be reloaded. It is used to first save the module into a buffer and then load it back from that buffer.\n:return: torch.jit.ScriptModule. The reloaded JIT module after it has been saved to and loaded from an in-memory buffer."}, "indent": 4}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "type": "function", "completion_path": "AutoRAG/autorag/nodes/retrieval/hybrid_cc.py", "signature_position": [9, 13], "body_position": [37, 53], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/autorag/nodes/retrieval/test_hybrid_cc.py::test_hybrid_cc_node"], "requirement": {"Functionality": "The hybrid_cc function fuses multiple retrieval results into a single set of results using a convex combination method. It normalizes and combines the scores of each retrieval result based on the provided weights, and selects the top_k results. This function is unique because it does not perform retrieval itself but fuses results from other retrieval functions.\n", "Arguments": ":param ids: Tuple of Lists. Each list within the tuple contains the ids from a different retrieval result. The length of this tuple must match the length of the scores tuple.\n:param scores: Tuple of Lists. Each list within the tuple contains the scores from a different retrieval result. The length of this tuple must match the length of the ids tuple.\n:param top_k: Int. The number of top results to retrieve after fusing the scores.\n:param weights: Tuple of floats. Weights for each retrieval result indicating their importance in the fusion process. The default is (0.5, 0.5), and the sum of the weights must equal 1. The length of this tuple must match the length of the ids and scores tuples.\n:return: Tuple containing two lists. The first list contains the fused ids, and the second list contains the fused scores, both selected based on the top_k parameter."}, "indent": 4}
{"namespace": "skfolio.utils.tools.format_measure", "type": "function", "completion_path": "skfolio/src/skfolio/utils/tools.py", "signature_position": [299, 299], "body_position": [315, 327], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_tools.py::TestFormatMeasure::test_float_maximum_decimal"], "requirement": {"Functionality": "Formats a given number into a user-friendly string, optionally converting it into a percentage format. It dynamically adjusts the number of decimal places based on the value of the number.\n", "Arguments": ":param x: float, The number to be formatted.\n:param percent: bool, default=False, Determines if the number should be formatted as a percentage. If True, the number is multiplied by 100 and a percentage sign is added.\n:return: str, The formatted string representation of the number. If the input number is NaN, it returns the string representation of NaN. Otherwise, it returns the number formatted with a dynamic number of decimal places, optionally as a percentage."}, "indent": 4}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [102, 102], "body_position": [103, 109], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/processing/test_data_processor.py::test_wait_for_disk_usage_higher_than_threshold"], "requirement": {"Functionality": "This function continuously checks the disk usage of a specified directory and waits until the free space is lower than a specified threshold in gigabytes. It periodically sleeps for a given amount of time before checking the disk usage again.\n", "Arguments": ":param input_dir: str, The directory whose disk usage is to be monitored.\n:param threshold_in_gb: int, The threshold for free disk space in gigabytes. The function waits until the free space is lower than this value. Defaults to 25 GB.\n:param sleep_time: int, The time in seconds that the function waits before checking the disk usage again. Defaults to 3 seconds.\n:return: No return values."}, "indent": 4}
{"namespace": "stepfun.pdf_to_weight", "type": "function", "completion_path": "camp_zipnerf/internal/stepfun.py", "signature_position": [53, 53], "body_position": [55, 56], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/stepfun_test.py::StepFunTest::test_weight_pdf_conversion_is_accurate"], "requirement": {"Functionality": "Converts a probability density function (PDF) that integrates to 1 into a vector of weights that also sums to 1. This is achieved by multiplying the PDF values by the difference between consecutive elements in the time or position vector.", "Arguments": ":param t: Array-like. Represents the time or position vector where the PDF is defined. It is used to calculate the differences between consecutive elements.\n:param p: Array-like. Represents the PDF values corresponding to the elements in 't'. These values are multiplied by the differences calculated from 't' to obtain the weights.\n:return: Array-like. A vector of weights that sums to 1, obtained from the input PDF."}, "indent": 2}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "type": "function", "completion_path": "nlm-ingestor/nlm_ingestor/ingestor/processors.py", "signature_position": [1225, 1225], "body_position": [1226, 1227], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_processor.py::MyTest::test_spaced_characters"], "requirement": {"Functionality": "Removes all whitespace characters from the input text and then segments the modified text into smaller parts or tokens.\n", "Arguments": ":param line_text: String. The text line to be processed. It is used to remove all spaces and then segment the text.\n:return: List of strings. The segmented parts of the modified input text."}, "indent": 4}
{"namespace": "skfolio.utils.stats.rand_weights", "type": "function", "completion_path": "skfolio/src/skfolio/utils/stats.py", "signature_position": [141, 141], "body_position": [158, 162], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_stats.py::TestRandWeights::test_weights_sum_to_one_n_1", "tests/test_portfolio/test_portfolio.py::test_portfolio_magic_methods", "tests/test_portfolio/test_multi_period_portfolio.py::test_portfolio_dominate"], "requirement": {"Functionality": "Generates a numpy array of n random weights that sum up to one, with an option to set a specified number of these weights to zero. This is useful for creating normalized weight distributions with optional zero-weight elements.\n", "Arguments": ":param n: Int. The total number of weights to generate. It determines the size of the output array.\n:param zeros: Int, default=0. The number of weights that should be randomly set to zero. It must not exceed n.\n:return: np.array. A numpy array containing the generated weights that sum to one, with the specified number of elements set to zero if applicable."}, "indent": 4}
{"namespace": "autorag.schema.module.Module.from_dict", "type": "method", "completion_path": "AutoRAG/autorag/schema/module.py", "signature_position": [20, 20], "body_position": [21, 24], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/autorag/schema/test_module_schema.py::test_module_from_dict_unsupported"], "requirement": {"Functionality": "This function creates an instance of the Module class from a dictionary representation. It extracts the module type from the dictionary, uses the remaining dictionary as module parameters, and returns a new Module instance initialized with these parameters.\n", "Arguments": ":param cls: The class object that this method is called on. It is used to create a new instance of the Module class.\n:param module_dict: Dict, The dictionary representation of a module, containing at least a 'module_type' key and possibly other parameters as key-value pairs. The 'module_type' is used to specify the type of the module, and the rest of the dictionary is used as parameters for the module.\n:return: Module, A new instance of the Module class initialized with the specified module type and parameters."}, "indent": 8}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "type": "function", "completion_path": "UniRef/detectron2/data/detection_utils.py", "signature_position": [534, 534], "body_position": [545, 561], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/data/test_detection_utils.py::TestTransformAnnotations::test_gen_crop_outside_boxes"], "requirement": {"Functionality": "Generates a CropTransform object that defines a cropping region in an image, ensuring that the region contains the center of a specified instance. The cropping region is determined based on the instance's bounding box and the desired crop size, with adjustments to ensure it fits within the image boundaries.\n", "Arguments": ":param crop_size: Tuple of int. The desired height and width of the crop in pixels.\n:param image_size: Tuple of int. The height and width of the image.\n:param instance: Dict. An annotation dictionary for a single instance, following Detectron2's dataset format. It includes the bounding box and its mode among other possible keys.\n:return: CropTransform. An object that specifies the parameters for the cropping operation, including the top-left corner and dimensions of the crop."}, "indent": 4}
{"namespace": "ref_utils.l2_normalize", "type": "function", "completion_path": "camp_zipnerf/internal/ref_utils.py", "signature_position": [45, 45], "body_position": [64, 71], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/ref_utils_test.py::RefUtilsTest::test_orientation_loss_gradients_are_finite"], "requirement": {"Functionality": "This function normalizes an array to unit length along its last axis while addressing potential issues with tiny or zero denominators that could cause gradients to explode during the backward pass. It achieves this by clamping the denominator to a minimum value during both the forward and backward passes, but with different thresholds, to ensure unit norm output without gradient explosion.\n", "Arguments": ":param x: Array. The array of values to be normalized. It is used as the input vector(s) for normalization.\n:param grad_eps: Float, optional. The minimum value to clamp the squared norm to before division in the backward pass to prevent exploding gradients. It defaults to approximately 1e-7.\n:return: Array. The normalized array, with each vector along the last axis of x normalized to unit length."}, "indent": 2}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "type": "method", "completion_path": "microagents/agents/agent_response.py", "signature_position": [122, 122], "body_position": [123, 127], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_agent_response.py::TestAgentResponse::test_parse_agent_info"], "requirement": {"Functionality": "Parses the agent information from a given response string, extracting the agent's name and any input text associated with it. The function assumes a specific format of the response string to correctly extract the information.\n", "Arguments": ":param response: str, the response string from which to parse the agent information. It is expected to contain the agent information in a specific format, delimited by 'Use Agent[' and ']', followed by the agent name and optional input text separated by a colon.\n:return: A tuple containing the agent's name as the first element and the associated input text as the second element. If no input text is present, the second element in the tuple will be an empty string."}, "indent": 8}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "type": "function", "completion_path": "UniRef/detectron2/data/detection_utils.py", "signature_position": [369, 369], "body_position": [385, 441], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/data/test_detection_utils.py::TestTransformAnnotations::test_transform_empty_annotation", "tests/data/test_detection_utils.py::TestTransformAnnotations::test_transform_RLE_resize"], "requirement": {"Functionality": "This function converts instance annotations from a dataset dictionary into an Instances object that is compatible with the models. It processes annotations related to bounding boxes, classes, segmentation masks, and keypoints, and organizes them into a structured format that the models can directly use.\n", "Arguments": ":param annos: list[dict]. A list of dictionaries where each dictionary contains information about one instance in an image, including bounding boxes, classes, segmentation, and keypoints.\n:param image_size: tuple. The size of the image as a tuple (height, width), used to scale and format the annotations correctly.\n:param mask_format: str, optional. The format of the segmentation masks in the annotations. It defaults to \"polygon\" and can also be \"bitmask\". This parameter determines how segmentation annotations are processed and converted.\n:return: Instances. An object containing structured fields like \"gt_boxes\", \"gt_classes\", \"gt_masks\", and \"gt_keypoints\", which are derived from the input annotations. This object is ready to be used by the models.\n"}, "indent": 4}
{"namespace": "skfolio.datasets._base.get_data_home", "type": "function", "completion_path": "skfolio/src/skfolio/datasets/_base.py", "signature_position": [24, 24], "body_position": [50, 54], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_dataset/test_dataset.py::TestGetDataHome::test_create_directory"], "requirement": {"Functionality": "This function returns the path to the skfolio data directory, which is used by certain dataset loaders to avoid redundant data downloads. It determines the directory based on an optional input, an environment variable, or a default location, and ensures the directory exists by creating it if necessary.\n", "Arguments": ":param data_home: str | Path | None. The desired path for the skfolio data directory. If not provided, the function looks for a path in the 'SKFOLIO_DATA' environment variable or defaults to '~/skfolio_data'.\n:return: str. The path to the skfolio data directory after ensuring it exists."}, "indent": 4}
{"namespace": "skfolio.utils.stats.cov_to_corr", "type": "function", "completion_path": "skfolio/src/skfolio/utils/stats.py", "signature_position": [256, 256], "body_position": [269, 273], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_stats.py::TestCovToCorr::test_valid_input", "tests/test_utils/test_stats.py::TestCovToCorr::test_3d_input"], "requirement": {"Functionality": "Converts a given covariance matrix into a correlation matrix and calculates the standard deviation for each variable. It ensures the input is a 2D array and performs the conversion using numpy operations.\n", "Arguments": ":param cov: np.ndarray. A 2D numpy array representing the covariance matrix of variables.\n:return: tuple[np.ndarray, np.ndarray]. A tuple containing two numpy arrays: the first is the correlation matrix derived from the covariance matrix, and the second is a 1D array of standard deviations for each variable."}, "indent": 4}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "type": "function", "completion_path": "UniRef/detectron2/export/torchscript_patch.py", "signature_position": [392, 392], "body_position": [398, 406], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/modeling/test_roi_heads.py::ROIHeadsTest::test_keypoint_head_scriptability"], "requirement": {"Functionality": "This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.", "Arguments": ":param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n:return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context."}, "indent": 4}
{"namespace": "iris.io.validators.are_shapes_equal", "type": "function", "completion_path": "open-iris/src/iris/io/validators.py", "signature_position": [213, 213], "body_position": [224, 230], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_validators.py::test_are_shapes_equal_raises_an_exception"], "requirement": {"Functionality": "This function generates a Pydantic validator that checks if two specified fields within a model have the same shape, primarily used for validating data structures like NumPy arrays to ensure they are compatible for operations that require matching dimensions.\n", "Arguments": ":param field1: str, The name of the first field to be checked. It is used to identify the first data structure whose shape is to be compared.\n:param field2: str, The name of the second field to be checked. It is used alongside field1 to compare the shapes of the two data structures.\n:return: Callable, A validator function that can be used within a Pydantic model to ensure the shapes of field1 and field2 match. If the shapes do not match, it raises a ValueError indicating the mismatch."}, "indent": 4}
{"namespace": "autorag.evaluate.util.cast_metrics", "type": "function", "completion_path": "AutoRAG/autorag/evaluate/util.py", "signature_position": [7, 7], "body_position": [14, 26], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/autorag/nodes/promptmaker/test_prompt_maker_run.py::test_evaluate_one_prompt_maker_node", "tests/autorag/evaluate/test_evaluate_util.py::test_cast_metrics"], "requirement": {"Functionality": "This function processes a list of metrics, which can either be strings or dictionaries, and returns a tuple containing a list of metric names and a list of dictionaries with metric parameters. It ensures that the input is correctly formatted and transforms it into a standardized format for further processing.\n", "Arguments": ":param metrics: Union[List[str], List[Dict]]. A list that can either contain strings representing metric names or dictionaries with metric details. It is used to extract metric names and parameters for further processing.\n:return: Tuple[List[str], List[Dict[str, Any]]]. A tuple where the first element is a list of metric names and the second element is a list of dictionaries containing metric parameters. This standardized format allows for consistent handling of metric information."}, "indent": 4}
{"namespace": "coord.construct_ray_warps", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [119, 119], "body_position": [135, 155], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_contract_matches_special_case", "tests/coord_test.py::CoordTest::test_construct_ray_warps_special_reciprocal"], "requirement": {"Functionality": "Constructs a pair of functions that establish a bijection between metric distances and normalized distances, based on a given function and its inverse. This is particularly useful for mapping distances in a 3D space to a normalized range [0, 1], and vice versa, as explained around Equation 11 in the provided reference.\n", "Arguments": ":param fn: Callable. The function used to transform ray distances. It is used in constructing the forward mapping from metric to normalized distances.\n:param t_near: Tensor. Represents the near-plane distances in the metric space. It is used as the lower bound for clipping distances to ensure they fall within a valid range.\n:param t_far: Tensor. Represents the far-plane distances in the metric space. It is used as the upper bound for clipping distances to ensure they fall within a valid range.\n:param fn_inv: Optional[Callable]. If provided, it is used as the inverse of the `fn` function for constructing the backward mapping from normalized to metric distances. If not provided, an attempt is made to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n:return: Tuple[Callable, Callable]. A tuple containing two functions: `t_to_s` and `s_to_t`. `t_to_s` maps metric distances to normalized distances in the range [0, 1], and `s_to_t` is its inverse, mapping normalized distances back to metric distances."}, "indent": 2}
{"namespace": "geometry.spherical_to_cartesian", "type": "function", "completion_path": "camp_zipnerf/internal/geometry.py", "signature_position": [208, 212], "body_position": [224, 228], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/geometry_test.py::GeometryTest::test_coordinate_transform"], "requirement": {"Functionality": "Converts spherical coordinates to cartesian coordinates. It calculates the cartesian coordinates (x, y, z) based on the given spherical coordinates (radius r, elevation theta, and azimuth phi).\n", "Arguments": ":param r: Numeric type. The radius in spherical coordinates. It represents how far away the point is from the origin.\n:param theta: Numeric type. The elevation angle in spherical coordinates. It represents the angle between the point and the positive z-axis.\n:param phi: Numeric type. The azimuth angle in spherical coordinates. It represents the angle between the projection of the point on the xy-plane and the positive x-axis.\n:return: Array of numeric type. The cartesian coordinates corresponding to the input spherical coordinates, organized as [x, y, z]."}, "indent": 2}
{"namespace": "linspline.integrate", "type": "function", "completion_path": "camp_zipnerf/internal/linspline.py", "signature_position": [39, 39], "body_position": [41, 42], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/linspline_test.py::LinsplineTest::test_blur_stepfun_and_integrate"], "requirement": {"Functionality": "Calculates the integral of the given data points (t, w) using the trapezoid rule. It ensures the input data points are valid for a linear spline before performing the integration.\n", "Arguments": ":param t: Array-like. Represents the x-coordinates of the data points. It is used along with w to compute the integral using the trapezoid rule.\n:param w: Array-like. Represents the y-coordinates of the data points. It is used along with t to compute the integral using the trapezoid rule.\n:return: Float. The computed integral of the data points using the trapezoid rule."}, "indent": 2}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "type": "function", "completion_path": "AutoRAG/autorag/nodes/retrieval/hybrid_cc.py", "signature_position": [56, 57], "body_position": [58, 63], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/autorag/nodes/retrieval/test_hybrid_cc.py::test_cc_non_overlap"], "requirement": {"Functionality": "This function calculates the weighted sum of scores for each ID, normalizes these scores, and then returns the top K IDs and their corresponding scores based on the weighted sum. It is designed to work with data in the form of tuples for IDs, scores, and weights.\n", "Arguments": ":param ids: Tuple of Lists of Strings. Each list within the tuple represents a set of IDs for a particular category or group.\n:param scores: Tuple of Lists of Floats. Each list within the tuple corresponds to the scores of the IDs in the same position in the 'ids' tuple. These scores are to be weighted and aggregated.\n:param weights: Tuple of Floats. Weights to be applied to the scores in each corresponding position across all categories or groups.\n:param top_k: Int. The number of top-scoring IDs to return based on the weighted sum of their scores.\n:return: Tuple containing a list of strings and a list of floats. The first list contains the top K IDs sorted by their weighted sum in descending order, and the second list contains the corresponding weighted sums of these top K IDs."}, "indent": 4}
{"namespace": "coord.track_linearize", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [44, 44], "body_position": [61, 65], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_track_isotropic", "tests/coord_test.py::CoordTest::test_track_linearize"], "requirement": {"Functionality": "This function applies a given function to a set of Gaussian means and covariances, using a linearization technique similar to that used in a Kalman filter. It linearizes the given function around the mean and then transforms the covariances accordingly.\n", "Arguments": ":param fn: Function. A function that can be applied to the mean. It is used to transform the Gaussian means and covariances.\n:param mean: Tensor. A tensor of Gaussian means, where the last axis represents the dimension of the means. It is used as the point around which the function is linearized.\n:param cov: Tensor. A tensor of covariances, where the last two axes represent the dimensions of the covariances. It is used along with the linearized function to transform the covariances.\n:return: Tuple (Tensor, Tensor). The first tensor is the transformed means (fn_mean), and the second tensor is the transformed covariances (fn_cov)."}, "indent": 2}
{"namespace": "skfolio.utils.tools.bisection", "type": "function", "completion_path": "skfolio/src/skfolio/utils/tools.py", "signature_position": [330, 330], "body_position": [343, 347], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_tools.py::TestBisection::test_empty_arrays"], "requirement": {"Functionality": "This function is a generator that iterates through a list of numpy arrays, bisecting each array into two halves, and yields these halves as a list of two arrays. It only bisects arrays that have more than one element.\n", "Arguments": ":param x: list of numpy.ndarray. A list containing numpy arrays to be bisected.\n:return: Iterator[list[numpy.ndarray, numpy.ndarray]]. Yields a list containing two bisected halves of the original array for each array in the input list that has more than one element."}, "indent": 4}
{"namespace": "skfolio.utils.stats.assert_is_square", "type": "function", "completion_path": "skfolio/src/skfolio/utils/stats.py", "signature_position": [204, 204], "body_position": [216, 217], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/test_utils/test_stats.py::TestAssertIsSquare::test_non_square_matrix_value_error"], "requirement": {"Functionality": "This function checks if the given numpy array is a square matrix. It raises a ValueError if the matrix is not square, ensuring that further operations requiring a square matrix can proceed safely.\n", "Arguments": ":param x: np.ndarray. The matrix to be checked. It is used to verify if the matrix is square by comparing its dimensions.\n:return: No return values. However, it raises a ValueError if the matrix is not square."}, "indent": 4}
{"namespace": "coord.pos_enc", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [193, 193], "body_position": [195, 207], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_pos_enc_matches_integrated_pos_enc_when_var_is_zero"], "requirement": {"Functionality": "This function generates a positional encoding for a given input, based on the method used in the original NeRF paper. It scales the input, applies a sine function, and optionally appends the original input to the result.\n", "Arguments": ":param x: Array. The input array to be encoded.\n:param min_deg: Integer. The minimum degree exponent for scaling the input.\n:param max_deg: Integer. The maximum degree exponent for scaling the input. The scales are generated as powers of 2 from min_deg to max_deg.\n:param append_identity: Bool, optional. Determines whether the original input array should be concatenated with the encoded array. Defaults to True.\n:return: Array. The resulting array after applying positional encoding. If append_identity is True, the original input is concatenated with the encoded features."}, "indent": 2}
{"namespace": "iris.io.validators.are_all_shapes_equal", "type": "function", "completion_path": "open-iris/src/iris/io/validators.py", "signature_position": [233, 233], "body_position": [247, 259], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_validators.py::test_are_all_shapes_equal_raises_an_exception"], "requirement": {"Functionality": "This function generates a Pydantic validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n", "Arguments": ":param field1: str. The name of the first field to be validated. It represents one of the lists of numpy arrays.\n:param field2: str. The name of the second field to be validated. It represents the other list of numpy arrays to compare against the first.\n:return: Callable. A function that acts as a Pydantic model validator. This validator function takes a class type and a dictionary of values, checks for shape equality between arrays in the specified fields, and returns the validated values if the check passes.\n"}, "indent": 4}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "type": "method", "completion_path": "EasyVolcap/easyvolcap/utils/gl_utils.py", "signature_position": [320, 320], "body_position": [321, 322], "dependency": {"intra_class": ["easyvolcap.utils.gl_utils.Mesh.render"], "intra_file": ["easyvolcap.utils.viewer_utils.Camera.H", "easyvolcap.utils.viewer_utils.Camera.W"], "cross_file": ["easyvolcap.utils.egl_utils.eglContextManager.resize", "easyvolcap.utils.viewer_utils.Camera"]}, "tests": ["tests/headless_opengl_tests.py::test_eglctx_auto_fbo_rbo", "tests/headless_opengl_tests.py::test_eglctx_auto_fbo_rbo"], "requirement": {"Functionality": "The function performs an offscreen rendering of the Mesh instance using a specified camera configuration. It first resizes the rendering context to match the camera's width and height, then renders the Mesh instance using the camera's settings.\n", "Arguments": ":param eglctx: eglContextManager, the rendering context manager used for offscreen rendering. It is resized to match the camera's dimensions before rendering.\n:param camera: Camera, the camera configuration used for rendering. Its width and height are used to resize the rendering context, and its settings are used during the rendering process.\n:return: No return values."}, "indent": 8}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "type": "function", "completion_path": "contrastors/src/contrastors/models/encoder/bert.py", "signature_position": [11, 11], "body_position": [12, 49], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["contrastors.models.encoder.configuration_nomic_bert.NomicBertConfig"]}, "tests": ["tests/test_flash_bert.py::test_bert_convert_to_hf"], "requirement": {"Functionality": "Converts a BertConfig object to a NomicBertConfig object with specific mappings and additional configurations. It adapts the configuration from a BERT model to be compatible with a Nomic model architecture, including some new arguments not present in the original configuration.\n", "Arguments": ":param bert_config: BertConfig. The configuration object for a BERT model. It contains various settings that define the model's architecture.\n:return: NomicBertConfig. A new configuration object tailored for a Nomic model, which includes both inherited settings from the BertConfig and new attributes specific to the Nomic model's requirements."}, "indent": 4}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "type": "method", "completion_path": "EasyVolcap/easyvolcap/utils/gl_utils.py", "signature_position": [324, 324], "body_position": [325, 354], "dependency": {"intra_class": ["easyvolcap.utils.gl_utils.Mesh.ebo", "easyvolcap.utils.gl_utils.Mesh.face_size", "easyvolcap.utils.gl_utils.Mesh.faces", "easyvolcap.utils.gl_utils.Mesh.mesh_program", "easyvolcap.utils.gl_utils.Mesh.point_program", "easyvolcap.utils.gl_utils.Mesh.render_type", "easyvolcap.utils.gl_utils.Mesh.upload_gl_uniforms", "easyvolcap.utils.gl_utils.Mesh.use_gl_program", "easyvolcap.utils.gl_utils.Mesh.vao", "easyvolcap.utils.gl_utils.Mesh.verts", "easyvolcap.utils.gl_utils.Mesh.visible"], "intra_file": ["easyvolcap.utils.gl_utils.Mesh.RenderType", "easyvolcap.utils.gl_utils.Mesh.RenderType.LINES", "easyvolcap.utils.gl_utils.Mesh.RenderType.QUADS", "easyvolcap.utils.gl_utils.Mesh.RenderType.STRIPS"], "cross_file": ["easyvolcap.utils.gl_utils.Mesh.RenderType.POINTS", "easyvolcap.utils.gl_utils.Mesh.RenderType.TRIS", "easyvolcap.utils.viewer_utils.Camera"]}, "tests": ["tests/headless_opengl_tests.py::test_eglctx_manual_fbo_rbo", "tests/headless_opengl_tests.py::test_eglctx_manual_fbo_rbo"], "requirement": {"Functionality": "The function renders a mesh object based on its visibility, render type, and associated OpenGL programs. It handles different rendering types such as points, lines, triangles, quads, and triangle strips by setting up the appropriate OpenGL state and issuing the correct draw call.\n", "Arguments": ":param self: Mesh. An instance of the Mesh class, which contains information about the mesh to be rendered, including visibility, render type, vertex array object (VAO), element buffer object (EBO), vertices, faces, and OpenGL programs.\n:param camera: Camera. The camera object used to set up the view and projection matrices in the shader programs. It is used in the upload_gl_uniforms method to pass these matrices to the GPU.\n:return: No return values. This function directly interacts with the GPU to render the mesh on the screen.\n\nThe function first checks if the mesh is visible. If it is not, the function returns immediately. Depending on the render type of the mesh, it selects the appropriate shader program (for point rendering or general mesh rendering) and binds it. It then uploads necessary uniforms to the GPU, binds the vertex array object of the mesh, and issues the appropriate OpenGL draw call based on the render type. For lines, triangles, quads, and triangle strips, it handles both indexed and non-indexed drawing. Finally, it unbinds the vertex array object to clean up."}, "indent": 8}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "type": "method", "completion_path": "EasyVolcap/easyvolcap/utils/gl_utils.py", "signature_position": [643, 643], "body_position": [644, 650], "dependency": {"intra_class": ["easyvolcap.utils.gl_utils.Quad.H", "easyvolcap.utils.gl_utils.Quad.W", "easyvolcap.utils.gl_utils.Quad.tex"], "intra_file": [], "cross_file": []}, "tests": ["tests/headless_opengl_tests.py::test_gl_tex_blit", "tests/headless_opengl_tests.py::test_gl_quad_blit", "tests/headless_opengl_tests.py::test_gl_quad_draw"], "requirement": {"Functionality": "This function uploads a portion or the entirety of a numpy array or a PyTorch tensor to a texture in OpenGL. It is designed to update the texture content starting from a specified position (x, y) and covering a specified width (w) and height (h). If width and height are not provided, it defaults to the object's width and height. The function handles the conversion from a PyTorch tensor to a numpy array before uploading.\n", "Arguments": ":param self: Quad. An instance of the Quad class, which contains the texture to be updated and the default dimensions (W, H) for the texture update.\n:param ptr: np.ndarray or torch.Tensor, the data source for the texture update. It is used as the pixel data to be uploaded to the texture.\n:param x: int, optional, the x-coordinate of the lower left corner where the texture update will start. Defaults to 0.\n:param y: int, optional, the y-coordinate of the lower left corner where the texture update will start. Defaults to 0.\n:param w: int, optional, the width of the portion of the texture to be updated. If 0, it defaults to the object's width (self.W).\n:param h: int, optional, the height of the portion of the texture to be updated. If 0, it defaults to the object's height (self.H).\n:return: No return values."}, "indent": 8}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/fcds_utils.py", "signature_position": [97, 103], "body_position": [104, 183], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.fcds_utils.matrix_to_rotation_6d", "easyvolcap.utils.fcds_utils.warn_once_about_pulsar_fxfy"], "cross_file": []}, "tests": ["tests/pulsar_points_rendering_tests.py::test_manual_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pulsar_rendering"], "requirement": {"Functionality": "This function calculates and returns the camera parameters for Pulsar, including camera position, rotation, and intrinsic parameters such as focal length and sensor width, based on the input rotation matrix, translation vector, camera intrinsic matrix, and image size. It ensures that all inputs are batched and validates their shapes and values before computing the parameters.\n", "Arguments": ":param R: torch.Tensor. A batch of rotation matrices of the camera, used to compute the camera's rotation in a different representation.\n:param tvec: torch.Tensor. A batch of translation vectors of the camera, used to compute the camera's position.\n:param camera_matrix: torch.Tensor. A batch of camera intrinsic matrices, used to derive focal lengths, principal points, and sensor width.\n:param image_size: torch.Tensor. A batch of image sizes, used to adjust the principal point offsets and normalize the focal length.\n:param znear: float, optional (default=0.1). The near clipping plane distance, used to adjust the focal length calculation.\n:return: torch.Tensor. A tensor containing the computed camera parameters for each instance in the batch, including camera position, rotation, and intrinsic parameters.\n"}, "indent": 4}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "type": "method", "completion_path": "EasyVolcap/easyvolcap/utils/gl_utils.py", "signature_position": [661, 661], "body_position": [666, 686], "dependency": {"intra_class": ["easyvolcap.utils.gl_utils.Quad.H", "easyvolcap.utils.gl_utils.Quad.W", "easyvolcap.utils.gl_utils.Quad.blit", "easyvolcap.utils.gl_utils.Quad.quad_program", "easyvolcap.utils.gl_utils.Quad.tex", "easyvolcap.utils.gl_utils.Quad.use_quad_draw", "easyvolcap.utils.gl_utils.Quad.verts"], "intra_file": ["easyvolcap.utils.gl_utils.Mesh.vao"], "cross_file": []}, "tests": ["tests/headless_opengl_tests.py::test_gl_quad_draw"], "requirement": {"Functionality": "The draw method in the Quad class is responsible for rendering a textured quadrilateral on the screen. It uses OpenGL conventions, specifically targeting the lower-left corner as the origin. If the use_quad_draw attribute is False, it falls back to a simpler blit method for drawing. Otherwise, it sets up a specific viewport and scissor box for rendering, activates a shader program, binds a texture, and draws the quadrilateral using vertex data. After drawing, it restores the viewport and scissor box to their original sizes.\n", "Arguments": ":param self: Quad. An instance of the Quad class. It uses attributes like use_quad_draw, W, H, quad_program, tex, and vao for drawing operations.\n:param x: int, optional. The x-coordinate of the lower-left corner where the quadrilateral will be drawn. Defaults to 0.\n:param y: int, optional. The y-coordinate of the lower-left corner where the quadrilateral will be drawn. Defaults to 0.\n:param w: int, optional. The width of the quadrilateral. If not provided or 0, the instance's W attribute is used. Defaults to 0.\n:param h: int, optional. The height of the quadrilateral. If not provided or 0, the instance's H attribute is used. Defaults to 0.\n:return: No return values. This method performs drawing operations and does not return any value."}, "indent": 8}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/fcds_utils.py", "signature_position": [196, 200], "body_position": [201, 212], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.fcds_utils.get_pytorch3d_ndc_K"], "cross_file": ["easyvolcap.utils.base_utils.dotdict", "easyvolcap.utils.base_utils.dotdict.H", "easyvolcap.utils.base_utils.dotdict.K", "easyvolcap.utils.base_utils.dotdict.R", "easyvolcap.utils.base_utils.dotdict.T", "easyvolcap.utils.base_utils.dotdict.W", "easyvolcap.utils.base_utils.dotdict.meta"]}, "tests": ["tests/pulsar_points_rendering_tests.py::test_pytorch3d_rendering", "tests/pulsar_points_rendering_tests.py::test_hybrid_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pytorch3d_rendering"], "requirement": {"Functionality": "Extracts and adjusts PyTorch3D camera parameters from a given batch input to align with PyTorch3D's coordinate system and conventions. It performs necessary transformations on rotation and translation matrices and computes the camera intrinsic matrix for normalized device coordinates (NDC).\n", "Arguments": ":param batch: dotdict. A batch of data containing camera parameters (R, T, K) and metadata (H, W) in a specific format. The rotation (R) and translation (T) matrices are adjusted to match PyTorch3D's requirements, and the intrinsic matrix (K) is recalculated for NDC.\n:return: Tuple containing the height (H) and width (W) of the images, the intrinsic matrix (K) for NDC, the adjusted rotation matrix (R), the adjusted translation vector (T), and the camera center (C) in the camera's coordinate system."}, "indent": 4}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "type": "method", "completion_path": "EasyVolcap/easyvolcap/utils/gl_utils.py", "signature_position": [688, 688], "body_position": [692, 699], "dependency": {"intra_class": ["easyvolcap.utils.gl_utils.Quad.H", "easyvolcap.utils.gl_utils.Quad.W", "easyvolcap.utils.gl_utils.Quad.fbo"], "intra_file": [], "cross_file": []}, "tests": ["tests/headless_opengl_tests.py::test_gl_quad_blit"], "requirement": {"Functionality": "The function blit copies a block of pixels from the source framebuffer to the destination framebuffer. It respects the OpenGL convention of using the lower left corner as the origin. This is typically used for operations like copying a texture, rendering to a texture, or performing a fast pixel transfer operation.\n", "Arguments": ":param self: Quad. An instance of the Quad class, which presumably holds information about a framebuffer object (FBO) and its dimensions.\n:param x: int, optional. The x-coordinate of the lower left corner where the copy starts. Defaults to 0.\n:param y: int, optional. The y-coordinate of the lower left corner where the copy starts. Defaults to 0.\n:param w: int, optional. The width of the pixel block to be copied. If 0, the width of the Quad instance (self.W) is used. Defaults to 0.\n:param h: int, optional. The height of the pixel block to be copied. If 0, the height of the Quad instance (self.H) is used. Defaults to 0.\n:return: No return values. This function performs operations directly on the framebuffer objects and does not return any value.\n\nNote: This function temporarily binds the Quad instance's framebuffer object (FBO) as the read framebuffer, performs the pixel copy operation, and then restores the previously bound read framebuffer. This ensures that the function's operations do not interfere with the current OpenGL state beyond the scope of the function."}, "indent": 8}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/loss_utils.py", "signature_position": [38, 38], "body_position": [40, 48], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["easyvolcap.utils.prop_utils.searchsorted"]}, "tests": ["tests/mipnerf360_tests.py::test_inner_outer", "tests/mipnerf360_tests.py::test_outer_measure_reference", "tests/mipnerf360_tests.py::test_inner_measure_reference"], "requirement": {"Functionality": "Constructs inner and outer measures based on cumulative sums for a given target time (t0) using the information from a source time (t1) and its corresponding values (y1). This is typically used in numerical methods or simulations where interpolation or approximation between time steps is required.\n", "Arguments": ":param t0: Tensor. The target times for which the measures are to be constructed.\n:param t1: Tensor. The source times that correspond to the given values (y1).\n:param y1: Tensor. The values associated with the source times (t1) that are used to construct the measures.\n:return: Tuple of Tensors. The first tensor is the inner measure, and the second tensor is the outer measure for the target time (t0)."}, "indent": 4}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/loss_utils.py", "signature_position": [53, 54], "body_position": [55, 62], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.loss_utils.inner_outer"], "cross_file": ["easyvolcap.utils.prop_utils.matchup_channels"]}, "tests": ["tests/mipnerf360_tests.py::test_lossfun_outer_self_zero", "tests/mipnerf360_tests.py::test_lossfun_outer_self_zero"], "requirement": {"Functionality": "This function calculates a scaled half-quadratic loss based on the input tensors representing target and environment weights and positions. It ensures that the proposal weight acts as an upper envelope over the nerf weight by penalizing weights outside this envelope.\n", "Arguments": ":param t: torch.Tensor. The target positions tensor, used along with weights to calculate the loss.\n:param w: torch.Tensor. The target weights tensor, representing the weights of the target positions.\n:param t_env: torch.Tensor. The environment positions tensor, used to calculate the upper envelope weights.\n:param w_env: torch.Tensor. The environment weights tensor, representing the weights of the environment positions.\n:param eps: torch.Tensor, optional. A small epsilon value to prevent division by zero, defaulting to the smallest positive number representable by torch.float32.\n:return: torch.Tensor. The calculated loss based on the difference between target weights and the upper envelope, scaled by a half-quadratic loss function."}, "indent": 4}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/loss_utils.py", "signature_position": [125, 126], "body_position": [127, 137], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["easyvolcap.utils.prop_utils.matchup_channels"]}, "tests": ["tests/mipnerf360_tests.py::test_distortion_loss_against_interval_distortion", "tests/mipnerf360_tests.py::test_distortion_loss_against_interval_distortion"], "requirement": {"Functionality": "Computes the distortion loss function for a given tensor of targets and weights. The function calculates both the inter-interval and intra-interval losses based on the provided tensors and combines them to produce the total distortion loss.\n", "Arguments": ":param t: torch.Tensor. The target tensor for which the distortion loss is to be calculated. It is expected that the last dimension of 't' is one more than that of 'w'.\n:param w: torch.Tensor. The weights tensor, used to weight the distortion loss calculation. The last dimension of 'w' should be one less than that of 't'.\n:return: torch.Tensor. The calculated distortion loss as a tensor. This combines both inter-interval and intra-interval losses."}, "indent": 4}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/prop_utils.py", "signature_position": [63, 63], "body_position": [65, 75], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.prop_utils.integrate_weights", "easyvolcap.utils.prop_utils.interpolate", "easyvolcap.utils.prop_utils.matchup_channels"], "cross_file": []}, "tests": ["tests/mipnerf360_tests.py::test_weighted_percentile_vectorized", "tests/mipnerf360_tests.py::test_weighted_percentile_vectorized"], "requirement": {"Functionality": "Computes the weighted percentiles of a step function using tensors. It matches up the channels of the tensors, integrates the weights, and then interpolates the integrated weights to find the weighted percentiles based on the provided percentile values.\n", "Arguments": ":param t: torch.Tensor. The tensor representing the values at which the step function changes.\n:param w: torch.Tensor. The tensor representing the weights associated with each value in 't'. The weights must sum to 1.\n:param ps: List[float]. A list of floats representing the percentile values for which the weighted percentiles are to be computed.\n:return: torch.Tensor. A tensor of the same shape as 't' and 'w', containing the computed weighted percentiles for the given percentile values."}, "indent": 4}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/prop_utils.py", "signature_position": [131, 136], "body_position": [157, 187], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.prop_utils.invert_cdf"], "cross_file": []}, "tests": ["tests/mipnerf360_tests.py::test_weighted_percentile", "tests/mipnerf360_tests.py::test_weighted_percentile"], "requirement": {"Functionality": "Performs importance sampling on a piecewise-constant probability density function (PDF) defined by bin endpoints and weights. It generates samples according to the specified PDF, with options for perturbation and jittering of samples.\n", "Arguments": ":param t: torch.Tensor. The tensor containing bin endpoint coordinates, which must be sorted. It represents the domain of the PDF.\n:param w: torch.Tensor. The tensor containing weights for each bin, used to define the piecewise-constant PDF.\n:param num_samples: int. The number of samples to generate from the PDF.\n:param perturb: bool, optional. If True, applies perturbation to the sampling process to avoid sample clustering at bin boundaries. Defaults to True.\n:param single_jitter: bool, optional. If True, applies the same jitter to every sample along each dimension. If False, applies independent jitter to each sample. Defaults to False.\n:return: torch.Tensor. A tensor of samples drawn from the specified PDF. The shape of the tensor is determined by the input tensors and the number of samples."}, "indent": 4}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/prop_utils.py", "signature_position": [202, 202], "body_position": [203, 217], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.prop_utils.matchup_channels"], "cross_file": []}, "tests": ["tests/mipnerf360_tests.py::test_max_dilate"], "requirement": {"Functionality": "Performs dilation (via max-pooling) on a non-negative step function. It dilates the time steps based on the specified dilation parameter and clips the dilated time steps within a given domain. The weights are also adjusted accordingly to match the dilated time steps.\n", "Arguments": ":param t: Tensor, the time steps of the step function, used as the basis for dilation.\n:param w: Tensor, the weights associated with each time step of the step function.\n:param dilation: Float, the amount by which to dilate the time steps.\n:param domain: Tuple of two floats, the minimum and maximum values to clip the dilated time steps to. Defaults to negative and positive infinity, allowing all dilated time steps.\n:return: A tuple of two Tensors. The first tensor is the dilated and clipped time steps, and the second tensor is the adjusted weights corresponding to the dilated time steps."}, "indent": 4}
{"namespace": "easyvolcap.utils.prop_utils.query", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/prop_utils.py", "signature_position": [262, 262], "body_position": [264, 267], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.prop_utils.searchsorted"], "cross_file": []}, "tests": ["tests/mipnerf360_tests.py::test_max_dilate"], "requirement": {"Functionality": "This function looks up the values of a step function defined by time-value pairs (t, y) at specified query times (tq). If a query time matches a step change, the function returns an outside value; otherwise, it interpolates the value at the query time based on the step function.\n", "Arguments": ":param tq: Tensor. The query times at which to evaluate the step function.\n:param t: Tensor. The times at which the step function changes value.\n:param y: Tensor. The values of the step function corresponding to the times in 't'.\n:param outside_value: Numeric, optional. The value to return for query times that exactly match a step change time. Defaults to 0.\n:return: Tensor. The interpolated or outside values of the step function at the query times."}, "indent": 4}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/prop_utils.py", "signature_position": [235, 240], "body_position": [241, 259], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.prop_utils.matchup_channels"], "cross_file": []}, "tests": ["tests/mipnerf360_tests.py::test_weight_annealing_zero_slope_noop"], "requirement": {"Functionality": "This function anneals the weights based on the training fraction and an annealing slope using Schlick's bias function. It adjusts the weights of a tensor based on the progression of training, making the weight adjustment more dynamic as training progresses. It ensures stability in the computation by handling cases where adjacent intervals have zero distance, setting their weight to zero, and preventing NaN values by using a softmax operation on the adjusted weights.\n", "Arguments": ":param t: torch.Tensor. The tensor representing time or another sequential dimension, used to align with the weights tensor.\n:param w: torch.Tensor. The weights tensor that will be adjusted based on the training fraction and anneal slope.\n:param train_frac: float. The fraction of training completed, used to calculate the annealing effect on weights.\n:param anneal_slope: float, optional. The slope of the annealing function, determining how sharply weights are adjusted. Defaults to 10.0.\n:param eps: torch.float32, optional. A very small number added to prevent division by zero and log of zero in computations. Defaults to a small epsilon value squared.\n:return: torch.Tensor. The adjusted weights tensor after applying the annealing process."}, "indent": 4}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/data_utils.py", "signature_position": [876, 876], "body_position": [877, 885], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.data_utils.to_cuda"], "cross_file": ["easyvolcap.utils.base_utils.dotdict"]}, "tests": ["tests/pulsar_points_rendering_tests.py::test_hybrid_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_hybrid_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pulsar_rendering"], "requirement": {"Functionality": "Converts the input batch to a CUDA tensor, recursively handling nested structures like tuples, lists, and dictionaries, except for dictionary keys named \"meta\". It ensures that the data is moved to the specified CUDA device for GPU processing.", "Arguments": ":param batch: tuple, list, dict, torch.Tensor, or other. The input data that needs to be converted to a CUDA tensor. It supports nested structures and handles them accordingly.\n:param device: str, default \"cuda\". The target CUDA device to which the data will be moved.\n:param ignore_list: bool, default False. A flag to indicate whether to ignore certain elements, though its usage is not directly apparent in the function's implementation.\n:return: torch.Tensor. The input batch converted to a CUDA tensor, ready for GPU processing."}, "indent": 4}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/chunk_utils.py", "signature_position": [166, 167], "body_position": [168, 172], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.chunk_utils.multi_gather"], "cross_file": []}, "tests": ["tests/bvh_vs_pytorch3d_knn_tests.py::test_bvh_dq_vs_10_knn_points"], "requirement": {"Functionality": "This function gathers triangles from a batch of vertices and faces tensors, computing the faces normals with respect to the vertices. It adjusts the dimensions of the faces tensor to match the batch dimension of the vertices tensor if necessary, and then reshapes the result to maintain the original faces tensor structure with additional dimensions for batch processing.\n", "Arguments": ":param v: torch.Tensor. The vertices tensor, potentially containing multiple batches of vertices. It is used to compute the normals of the faces.\n:param f: torch.Tensor. The faces tensor, indicating the indices of vertices forming each face. It may be expanded to match the batch dimension of the vertices tensor.\n:param dim: int, optional. The dimension along which the gathering of vertices is performed. Defaults to -2, indicating the second last dimension.\n:return: torch.Tensor. The tensor containing the gathered triangles, reshaped to maintain the structure of the original faces tensor with additional dimensions for batch processing."}, "indent": 4}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "type": "function", "completion_path": "EasyVolcap/easyvolcap/utils/data_utils.py", "signature_position": [982, 982], "body_position": [983, 991], "dependency": {"intra_class": [], "intra_file": ["easyvolcap.utils.data_utils.add_batch"], "cross_file": ["easyvolcap.utils.base_utils.dotdict"]}, "tests": ["tests/pulsar_points_rendering_tests.py::test_hybrid_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_hybrid_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pulsar_rendering"], "requirement": {"Functionality": "This function recursively adds a new dimension to the input data structure (which can be a batch of data in various formats such as tuple, list, dictionary, torch.Tensor, or numpy.ndarray) at the zeroth position. It is designed to handle nested data structures by applying the same operation to each element.\n", "Arguments": ":param batch: Union[tuple, list, dict, torch.Tensor, np.ndarray]. The input data which can be a single item or a batch of items in various formats. The function processes this data by adding a new dimension to it or its elements.\n:return: Union[torch.Tensor, np.ndarray]. The modified batch with an added dimension at the zeroth position. This return type ensures compatibility with PyTorch and NumPy data structures."}, "indent": 4}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "type": "method", "completion_path": "EasyVolcap/easyvolcap/utils/viewer_utils.py", "signature_position": [691, 691], "body_position": [692, 716], "dependency": {"intra_class": ["easyvolcap.utils.viewer_utils.Camera.H", "easyvolcap.utils.viewer_utils.Camera.K", "easyvolcap.utils.viewer_utils.Camera.R", "easyvolcap.utils.viewer_utils.Camera.T", "easyvolcap.utils.viewer_utils.Camera.W", "easyvolcap.utils.viewer_utils.Camera.bounds", "easyvolcap.utils.viewer_utils.Camera.f", "easyvolcap.utils.viewer_utils.Camera.mass", "easyvolcap.utils.viewer_utils.Camera.moment_of_inertia", "easyvolcap.utils.viewer_utils.Camera.movement_force", "easyvolcap.utils.viewer_utils.Camera.movement_speed", "easyvolcap.utils.viewer_utils.Camera.movement_torque", "easyvolcap.utils.viewer_utils.Camera.n", "easyvolcap.utils.viewer_utils.Camera.origin", "easyvolcap.utils.viewer_utils.Camera.t", "easyvolcap.utils.viewer_utils.Camera.v", "easyvolcap.utils.viewer_utils.Camera.world_up"], "intra_file": ["easyvolcap.utils.base_utils.dotdict.mass", "easyvolcap.utils.base_utils.dotdict.moment_of_inertia", "easyvolcap.utils.base_utils.dotdict.movement_force", "easyvolcap.utils.base_utils.dotdict.movement_speed", "easyvolcap.utils.base_utils.dotdict.movement_torque", "easyvolcap.utils.base_utils.dotdict.origin", "easyvolcap.utils.base_utils.dotdict.world_up"], "cross_file": ["easyvolcap.utils.base_utils.dotdict", "easyvolcap.utils.base_utils.dotdict.H", "easyvolcap.utils.base_utils.dotdict.K", "easyvolcap.utils.base_utils.dotdict.R", "easyvolcap.utils.base_utils.dotdict.T", "easyvolcap.utils.base_utils.dotdict.W", "easyvolcap.utils.base_utils.dotdict.bounds", "easyvolcap.utils.base_utils.dotdict.f", "easyvolcap.utils.base_utils.dotdict.meta", "easyvolcap.utils.base_utils.dotdict.n", "easyvolcap.utils.base_utils.dotdict.t", "easyvolcap.utils.base_utils.dotdict.update", "easyvolcap.utils.base_utils.dotdict.v"]}, "tests": ["tests/pulsar_points_rendering_tests.py::test_hybrid_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_hybrid_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_pulsar_rendering", "tests/pulsar_points_rendering_tests.py::test_resized_pulsar_rendering"], "requirement": {"Functionality": "The function converts camera parameters and GUI related elements into a batch format using tensors, suitable for processing with PyTorch. It organizes these parameters into a structured dictionary format for easy access and manipulation.\n", "Arguments": ":param self: Camera. An instance of the Camera class, containing various camera parameters and GUI related elements.\n:return: A dotdict instance containing all the camera parameters and GUI related elements converted into tensors. This structured dictionary includes both a direct mapping of parameters and a nested 'meta' dictionary with the same content.\n\nNote: The function assumes the existence of a 'dotdict' class or function that behaves similarly to a dictionary but allows access to its items through attributes (dot notation). It also assumes that 'torch' refers to the PyTorch library, used here for tensor operations."}, "indent": 8}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "type": "method", "completion_path": "microagents/agents/agent_persistence_manager.py", "signature_position": [16, 16], "body_position": [20, 22], "dependency": {"intra_class": ["agent_persistence_manager.AgentPersistenceManager.persistence"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_agent_persistance_manager.py::TestAgentPersistenceManager::test_save_agent_working_not_prime", "tests/test_agent_persistance_manager.py::TestAgentPersistenceManager::test_save_agent_not_working_or_prime"], "requirement": {"Functionality": "Serialize and save the agent state if it is a working agent and not a prime agent. This involves converting the agent's state into a dictionary format and then saving it using the persistence mechanism provided by the AgentPersistenceManager.\n", "Arguments": ":param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class, which is responsible for managing the persistence of agent states.\n:param agent: The agent object whose state is to be serialized and saved. This object should have attributes or methods to determine if it is a working agent and whether it is a prime agent.\n:return: No return values."}, "indent": 8}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "type": "method", "completion_path": "microagents/agents/agent_similarity.py", "signature_position": [69, 69], "body_position": [76, 93], "dependency": {"intra_class": ["agent_similarity.AgentSimilarity.agents", "agent_similarity.AgentSimilarity.get_embedding"], "intra_file": ["agent_similarity.Agent", "agent_similarity.logger"], "cross_file": []}, "tests": ["tests/test_agent_similarity.py::TestAgentSimilarity::test_find_closest_agent"], "requirement": {"Functionality": "Finds the closest agent to a given purpose embedding by calculating the cosine similarity between the purpose embedding of each agent and the given purpose embedding. It returns the agent with the highest similarity score to the given purpose embedding.\n", "Arguments": ":param purpose_embedding: np.ndarray, The embedding vector of the purpose for which the closest agent is being searched. It is used to compare against the purpose embeddings of the agents to find the closest match.\n:return: Tuple[Optional[Agent], float], A tuple containing the closest agent to the given purpose embedding and the highest similarity score. If no agents are found or an error occurs, it may return None for the agent and -inf for the similarity score."}, "indent": 8}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "type": "method", "completion_path": "microagents/agents/agent_lifecycle.py", "signature_position": [40, 40], "body_position": [42, 46], "dependency": {"intra_class": ["agent_lifecycle.AgentLifecycle.agents", "agent_lifecycle.AgentLifecycle.openai_wrapper"], "intra_file": ["agent_lifecycle.PRIME_AGENT_WEIGHT"], "cross_file": []}, "tests": ["tests/test_agent_lifecycle.py::TestAgentLifecycle::test_create_prime_agent_adds_agent"], "requirement": {"Functionality": "Creates the prime agent and adds it to the agent list. This prime agent is initialized with specific attributes such as prompt, name, weight, and flags indicating its prime status and another unspecified flag.", "Arguments": ":param self: AgentLifecycle. An instance of the AgentLifecycle class. It is used to access the agent list where the prime agent will be added.\n:return: No return values."}, "indent": 8}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "type": "method", "completion_path": "microagents/agents/agent_persistence_manager.py", "signature_position": [24, 24], "body_position": [28, 31], "dependency": {"intra_class": ["agent_persistence_manager.AgentPersistenceManager.persistence"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_agent_persistance_manager.py::TestAgentPersistenceManager::test_load_agent_existing", "tests/test_agent_persistance_manager.py::TestAgentPersistenceManager::test_load_agent_non_existing"], "requirement": {"Functionality": "Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n", "Arguments": ":param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n:param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n:param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n:param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n:return: An instance of the deserialized agent if found, otherwise None."}, "indent": 8}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "type": "method", "completion_path": "microagents/agents/agent_persistence_manager.py", "signature_position": [33, 33], "body_position": [37, 43], "dependency": {"intra_class": ["agent_persistence_manager.AgentPersistenceManager.load_agent", "agent_persistence_manager.AgentPersistenceManager.persistence"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_agent_persistance_manager.py::TestAgentPersistenceManager::test_load_all_agents"], "requirement": {"Functionality": "Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n", "Arguments": ":param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n:param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n:param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n:return: list. A list of agents that have been successfully loaded from the database."}, "indent": 8}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "type": "method", "completion_path": "microagents/agents/agent_lifecycle.py", "signature_position": [96, 96], "body_position": [98, 102], "dependency": {"intra_class": ["agent_lifecycle.AgentLifecycle.agent_persistence"], "intra_file": ["agent_lifecycle.logger"], "cross_file": []}, "tests": ["tests/test_agent_lifecycle.py::TestAgentLifecycle::test_save_agent_calls_persistence_save"], "requirement": {"Functionality": "Saves the given agent using the agent persistence mechanism. If an error occurs during the save operation, it logs the exception and re-raises the error.", "Arguments": ":param self: AgentLifecycle. An instance of the AgentLifecycle class.\n:param agent: MicroAgent, the agent to be saved. It is used by the agent persistence mechanism to save the agent.\n:return: No return values."}, "indent": 8}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "type": "method", "completion_path": "microagents/agents/microagent_manager.py", "signature_position": [38, 38], "body_position": [40, 41], "dependency": {"intra_class": ["microagent_manager.MicroAgentManager.agent_lifecycle", "microagent_manager.MicroAgentManager.cleanup_agents"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_microagent_manager.py::TestMicroAgentManager::test_get_agents"], "requirement": {"Functionality": "This function cleans up the agents and then returns the current list of agents managed by the MicroAgentManager instance.\n", "Arguments": ":param self: MicroAgentManager. An instance of the MicroAgentManager class, used to access the instance's methods and attributes.\n:return: List[Any]. The current list of agents after cleanup."}, "indent": 8}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "type": "method", "completion_path": "microagents/agents/agent_lifecycle.py", "signature_position": [113, 113], "body_position": [117, 126], "dependency": {"intra_class": ["agent_lifecycle.AgentLifecycle.openai_wrapper"], "intra_file": ["agent_lifecycle.logger"], "cross_file": []}, "tests": ["tests/test_agent_lifecycle.py::TestAgentLifecycle::test_generate_llm_prompt_returns_prompt"], "requirement": {"Functionality": "Generates a prompt for the Language Learning Model (LLM) based on a specified goal and sample input, then attempts to get a chat completion from an OpenAI wrapper. If an error occurs during this process, it logs the exception and returns an empty string.\n", "Arguments": ":param self: AgentLifecycle. An instance of the AgentLifecycle class.\n:param goal: str, The goal or objective that the prompt is intended to achieve.\n:param sample_input: str, A sample input to provide context or an example for the prompt generation.\n:return: str, The generated prompt from the LLM if successful, or an empty string if an error occurs."}, "indent": 8}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "type": "method", "completion_path": "microagents/integrations/sqlite_agent_persistence.py", "signature_position": [29, 29], "body_position": [33, 38], "dependency": {"intra_class": ["sqlite_agent_persistence.SQLiteAgentPersistence.filename"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_sqlite_agent_persistance.py::TestSQLiteAgentPersistence::test_save_agent", "tests/test_sqlite_agent_persistance.py::TestSQLiteAgentPersistence::test_fetch_agent", "tests/test_sqlite_agent_persistance.py::TestSQLiteAgentPersistence::test_load_all_purposes"], "requirement": {"Functionality": "Saves the serialized agent information into an SQLite database by replacing the existing record with the same ID or inserting a new record if the ID does not exist.", "Arguments": ":param self: SQLiteAgentPersistence. An instance of the SQLiteAgentPersistence class, which contains the filename of the SQLite database.\n:param agent_dict: dict, The dictionary containing the agent's information, including its 'id', 'purpose', and other data, which is used to insert or update the agent's record in the database.\n:return: No return values."}, "indent": 8}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "type": "method", "completion_path": "microagents/integrations/sqlite_agent_persistence.py", "signature_position": [40, 40], "body_position": [44, 48], "dependency": {"intra_class": ["sqlite_agent_persistence.SQLiteAgentPersistence.filename"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_sqlite_agent_persistance.py::TestSQLiteAgentPersistence::test_fetch_agent", "tests/test_sqlite_agent_persistance.py::TestSQLiteAgentPersistence::test_non_existent_agent"], "requirement": {"Functionality": "Fetches a serialized agent from the SQLite database based on its purpose and deserializes it. If no agent with the given purpose is found, None is returned.\n", "Arguments": ":param self: SQLiteAgentPersistence. An instance of the SQLiteAgentPersistence class, which contains the filename of the SQLite database.\n:param purpose: str, The purpose of the agent to be fetched. It is used to query the database for the corresponding agent data.\n:return: dict or None. The deserialized agent data as a dictionary if an agent with the given purpose is found; otherwise, None."}, "indent": 8}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "type": "method", "completion_path": "microagents/integrations/sqlite_agent_persistence.py", "signature_position": [50, 50], "body_position": [54, 57], "dependency": {"intra_class": ["sqlite_agent_persistence.SQLiteAgentPersistence.filename"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_sqlite_agent_persistance.py::TestSQLiteAgentPersistence::test_load_all_purposes"], "requirement": {"Functionality": "Loads all agent purposes from the SQLite database and returns them as a list. This function is used to retrieve a list of purposes for all agents stored in the database, which can be useful for various operations such as filtering or analysis.\n", "Arguments": ":param self: SQLiteAgentPersistence. An instance of the SQLiteAgentPersistence class, which contains the filename of the SQLite database as an attribute.\n:return: list. A list of purposes for all agents. Each element in the list is a string representing the purpose of an agent.\n\nNo additional input parameters are required for this function."}, "indent": 8}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "type": "method", "completion_path": "microagents/integrations/memoize.py", "signature_position": [56, 56], "body_position": [57, 60], "dependency": {"intra_class": ["memoize.SQLiteMemoization.connection"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_sqlite_memoization.py::TestSQLiteMemoization::test_caching_mechanism_stores_and_retrieves_data_correctly"], "requirement": {"Functionality": "Fetches the cached result from an SQLite database using the hash of the argument. If the result is found, it is loaded from JSON format and returned; otherwise, None is returned.", "Arguments": ":param self: SQLiteMemoization. An instance of the SQLiteMemoization class, which manages caching of function results in an SQLite database.\n:param arg_hash: str, The hash of the argument for which the cached result is being fetched. It is used to query the database for the corresponding cached result.\n:return: The result corresponding to the provided hash, loaded from JSON format if found; otherwise, None."}, "indent": 8}
{"namespace": "memoize.SQLiteMemoization._cache_result", "type": "method", "completion_path": "microagents/integrations/memoize.py", "signature_position": [67, 67], "body_position": [68, 73], "dependency": {"intra_class": ["memoize.SQLiteMemoization.connection"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_sqlite_memoization.py::TestSQLiteMemoization::test_caching_mechanism_stores_and_retrieves_data_correctly"], "requirement": {"Functionality": "This function caches the result of a computation in an SQLite database. It inserts a new row into the 'cache' table with the hash of the arguments as the key and the JSON-serialized result as the value.\n", "Arguments": ":param self: SQLiteMemoization. An instance of the SQLiteMemoization class, which manages caching of function results in an SQLite database.\n:param arg_hash: str, The hash of the arguments to the function whose result is being cached. It uniquely identifies the function call.\n:param result: Any, The result of the function call that is being cached. This result is serialized to JSON format before being stored in the database.\n:return: No return values."}, "indent": 8}
{"namespace": "run.execute_command_line_process", "type": "function", "completion_path": "XAgent/run.py", "signature_position": [41, 41], "body_position": [50, 66], "dependency": {"intra_class": [], "intra_file": ["run.start_command_line"], "cross_file": ["XAgent.config.ARGS", "XAgent.config.CONFIG"]}, "tests": ["tests/test_run.py::test_execute_command_line_process_normal_mode"], "requirement": {"Functionality": "Executes a command line process based on the provided arguments. It updates global configuration parameters with the provided arguments and, if quiet mode is enabled, redirects the standard output to a file instead of displaying it in the terminal.\n", "Arguments": ":param args: argparse.Namespace. Parsed command line arguments that dictate how the command line process should be executed.\n:param quiet_mode: bool, optional. Indicates whether the process should run in quiet mode, which involves redirecting output to a file rather than printing it to the terminal.\n:return: No return values."}, "indent": 4}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "type": "function", "completion_path": "XAgent/XAgent/ai_functions/request/openai.py", "signature_position": [114, 114], "body_position": [134, 211], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["XAgent.config.CONFIG", "XAgent.config.get_apiconfig_by_model", "XAgent.config.get_model_name", "XAgent.logs.logger"]}, "tests": ["tests/test_1106_model_openai.py::test_1106_model_openai"], "requirement": {"Functionality": "This function handles the operation of OpenAI's chat completion for version 1.x.x. It dynamically selects a model based on provided arguments and configuration, makes a request to the OpenAI API, and handles potential errors related to exceeding context length by attempting to use a higher-capacity model if available. If the context length limit is still exceeded or another error occurs, it raises an error.\n", "Arguments": ":param **kwargs: Variable keyword arguments. These include 'model' (str) which specifies the model to use for chat completion, among other settings that can be passed to configure the API request.\n:return: dict. A dictionary containing the response from the Chat API, structured according to the API's response format.\n\nRaises:\nBadRequestError: Raised if an error occurs during the chat completion operation, specifically if the context length limit is exceeded and no fallback models are available to handle the request."}, "indent": 8}
{"namespace": "litdata.streaming.client.S3Client.client", "type": "method", "completion_path": "litdata/litdata/streaming/client.py", "signature_position": [43, 43], "body_position": [44, 53], "dependency": {"intra_class": ["litdata.streaming.client.S3Client._client", "litdata.streaming.client.S3Client._create_client", "litdata.streaming.client.S3Client._last_time", "litdata.streaming.client.S3Client._refetch_interval"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_client.py::test_s3_client_without_cloud_space_id", "tests/streaming/test_client.py::test_s3_client_with_cloud_space_id"], "requirement": {"Functionality": "This function returns an S3 client instance. It creates a new S3 client if one does not already exist or if the credentials have expired based on a predefined interval. This ensures that the client is always up-to-date and valid for use.\n", "Arguments": ":param self: S3Client. An instance of the S3Client class. It uses the instance's attributes to manage the S3 client and its creation time.\n:return: Any. The S3 client instance that is either retrieved from the existing instance or newly created if necessary."}, "indent": 8}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "type": "method", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [314, 314], "body_position": [315, 336], "dependency": {"intra_class": ["litdata.streaming.dataset.StreamingDataset._state_dict", "litdata.streaming.dataset.StreamingDataset.current_epoch", "litdata.streaming.dataset.StreamingDataset.distributed_env", "litdata.streaming.dataset.StreamingDataset.drop_last", "litdata.streaming.dataset.StreamingDataset.input_dir", "litdata.streaming.dataset.StreamingDataset.item_loader", "litdata.streaming.dataset.StreamingDataset.seed", "litdata.streaming.dataset.StreamingDataset.shuffle"], "intra_file": [], "cross_file": ["litdata.streaming.item_loader.BaseItemLoader.state_dict", "litdata.streaming.resolver.Dir.path", "litdata.streaming.resolver.Dir.url", "litdata.utilities.env._DistributedEnv.world_size", "litdata.utilities.env._is_in_dataloader_worker"]}, "tests": ["tests/streaming/test_dataset.py::test_dataset_valid_state"], "requirement": {"Functionality": "The `state_dict` method generates and returns a dictionary representing the current state of the StreamingDataset instance, including information about the number of samples yielded, the number of workers, batch size, and other relevant dataset parameters. It raises an error if called from a DataLoader worker process.\n", "Arguments": ":param num_samples_yielded: int, the number of samples that have been yielded by the dataset.\n:param num_workers: int, the number of worker processes used for loading the data.\n:param batch_size: int, the size of the batches that data is divided into for processing.\n:return: Dict[str, Any], a dictionary containing the current state of the StreamingDataset instance, including details such as the number of samples yielded, number of workers, batch size, current epoch, input directory path and URL, item loader state (if applicable), whether the last batch is dropped, seed, world size of the distributed environment, and shuffle status."}, "indent": 8}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "type": "method", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [338, 338], "body_position": [339, 341], "dependency": {"intra_class": ["litdata.streaming.dataset.StreamingDataset._state_dict"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_dataset.py::test_dataset_valid_state"], "requirement": {"Functionality": "This function loads a given state dictionary into the StreamingDataset instance, effectively restoring its state based on the provided dictionary. This is particularly useful for resuming data streaming processes from a saved state.\n", "Arguments": ":param self: StreamingDataset. An instance of the StreamingDataset class.\n:param state_dict: Dict[str, Any]. A dictionary containing the state to be loaded into the StreamingDataset instance. It maps state keys to their corresponding values.\n:return: No return values."}, "indent": 8}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "type": "method", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [343, 343], "body_position": [344, 401], "dependency": {"intra_class": ["litdata.streaming.dataset.StreamingDataset._state_dict", "litdata.streaming.dataset.StreamingDataset.cache", "litdata.streaming.dataset.StreamingDataset.drop_last", "litdata.streaming.dataset.StreamingDataset.input_dir", "litdata.streaming.dataset.StreamingDataset.item_loader", "litdata.streaming.dataset.StreamingDataset.seed", "litdata.streaming.dataset.StreamingDataset.shuffle", "litdata.streaming.dataset.StreamingDataset.worker_env"], "intra_file": ["litdata.streaming.dataset._should_replace_path", "litdata.streaming.dataset._try_create_cache_dir", "litdata.utilities.env._WorkerEnv.world_size"], "cross_file": ["litdata.streaming.item_loader.BaseItemLoader.state_dict", "litdata.streaming.resolver.Dir.path", "litdata.streaming.resolver.Dir.url"]}, "tests": ["tests/streaming/test_dataset.py::test_dataset_valid_state"], "requirement": {"Functionality": "This function validates the state dictionary of a StreamingDataset instance against its current state. It checks for consistency across various parameters such as shuffle, num_workers, input directory path and URL, seed, item_loader state, and drop_last flag. If any of the parameters in the state dictionary do not match the current state of the StreamingDataset instance, a ValueError is raised indicating the mismatch.\n", "Arguments": ":param self: StreamingDataset. An instance of the StreamingDataset class. It uses various attributes of the instance such as _state_dict, worker_env, cache, shuffle, etc., to validate the state dictionary against the current state of the instance.\n:return: No return values. This function raises a ValueError if there is a mismatch between the state dictionary and the current state of the StreamingDataset instance."}, "indent": 8}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "type": "function", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [404, 404], "body_position": [405, 412], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["litdata.constants._DEFAULT_CACHE_DIR"]}, "tests": ["tests/streaming/test_dataset.py::test_try_create_cache_dir"], "requirement": {"Functionality": "Attempts to create a cache directory based on the input directory provided. It generates a unique directory name by hashing the input directory. If certain environment variables are not set, it creates the cache directory in a default location; otherwise, it creates it in a specified location.\n", "Arguments": ":param input_dir: Optional[str]. The directory path to be hashed and used for creating a unique cache directory. If None, an empty string is used for hashing.\n:return: Optional[str]. The path of the created cache directory. Returns None if the directory cannot be created, although this behavior is not explicitly handled in the function.\n"}, "indent": 4}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "type": "method", "completion_path": "litdata/litdata/streaming/downloader.py", "signature_position": [50, 50], "body_position": [51, 86], "dependency": {"intra_class": ["litdata.streaming.downloader.S3Downloader._client", "litdata.streaming.downloader.S3Downloader._s5cmd_available"], "intra_file": [], "cross_file": ["litdata.constants._INDEX_FILENAME", "litdata.streaming.client.S3Client.client"]}, "tests": ["tests/streaming/test_downloader.py::test_s3_downloader_fast"], "requirement": {"Functionality": "The function downloads a file from an S3 bucket to a local file path. It first checks if the given remote file path is an S3 URL and if the local file already exists. If the local file does not exist, it attempts to download the file using either the s5cmd command-line tool (if available) or the boto3 library. The function uses a file lock to prevent multiple processes from downloading the same file simultaneously.\n", "Arguments": ":param self: S3Downloader. An instance of the S3Downloader class.\n:param remote_filepath: str, The S3 URL of the file to be downloaded. It is used to parse the scheme, netloc, and path of the S3 file.\n:param local_filepath: str, The local file path where the downloaded file will be saved. It is used to check if the file already exists and to specify the download destination.\n:return: None. There are no return values, but the function may raise a ValueError if the remote file path does not use the \"s3\" scheme or a Timeout exception if the file lock cannot be acquired within the specified timeout."}, "indent": 8}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "type": "function", "completion_path": "litdata/litdata/streaming/dataset.py", "signature_position": [431, 433], "body_position": [434, 450], "dependency": {"intra_class": [], "intra_file": ["litdata.utilities.env._WorkerEnv.world_size"], "cross_file": ["litdata.utilities.env._WorkerEnv"]}, "tests": ["tests/streaming/test_dataset.py::test_replay_chunks_sampling"], "requirement": {"Functionality": "This function distributes chunks of data and their corresponding intervals across multiple workers based on the number of workers and a worker environment. It ensures that each worker is assigned a subset of chunks and intervals, following a distribution strategy that depends on the worker's index and the total world size defined in the worker environment.\n", "Arguments": ":param num_workers: int, the total number of workers among which the chunks and intervals are to be distributed.\n:param worker_env: _WorkerEnv, an instance or object representing the worker environment, which includes details like world size that are used in the distribution logic.\n:param chunks_replica: List[int], a list of chunk indices that need to be distributed among the workers.\n:param intervals_replica: List[Any], a list of intervals corresponding to each chunk in chunks_replica. Each interval represents the range or scope of the chunk it corresponds to.\n:return: A tuple containing two dictionaries. The first dictionary maps worker indices to their assigned chunks, and the second dictionary maps worker indices to the intervals corresponding to their assigned chunks. There is no explicit data type mentioned for the return value, but it is implied to be a tuple of two dictionaries based on the function's implementation."}, "indent": 4}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "type": "method", "completion_path": "litdata/litdata/streaming/downloader.py", "signature_position": [99, 99], "body_position": [100, 101], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.downloader.LocalDownloader", "litdata.streaming.downloader.LocalDownloader.download_file"], "cross_file": []}, "tests": ["tests/streaming/test_downloader.py::test_download_with_cache"], "requirement": {"Functionality": "The function modifies the remote file path by removing the prefix \"local:\" if present, and then calls the superclass's download_file method to download the file from the modified remote file path to the specified local file path.\n", "Arguments": ":param self: LocalDownloaderWithCache. An instance of the LocalDownloaderWithCache class.\n:param remote_filepath: str, The path of the file on the remote server. It may contain a \"local:\" prefix which will be removed before downloading.\n:param local_filepath: str, The path where the file should be saved locally after downloading.\n:return: No return values."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [74, 74], "body_position": [75, 79], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.serializers.Image"], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_pil_serializer"], "requirement": {"Functionality": "Serializes a PIL Image object into a bytes object containing the image's dimensions, mode, and raw pixel data. It returns this serialized data along with None, as a tuple. This can be useful for saving or transmitting the image data in a standardized format.\n", "Arguments": ":param item: Image, The PIL Image object to be serialized. It is used to extract the image's mode, dimensions, and raw pixel data for serialization.\n:return: Tuple[bytes, Optional[str]], A tuple where the first element is a bytes object containing the serialized image data and the second element is None. The serialized data includes the image's width, height, mode length as a bytes object, followed by the image mode encoded in UTF-8, and finally the raw pixel data."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [98, 98], "body_position": [99, 120], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.serializers.Image", "litdata.streaming.serializers.JpegImageFile", "litdata.streaming.serializers.PngImageFile"], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_jpeg_serializer"], "requirement": {"Functionality": "Serializes an image item into JPEG format. If the item is a JPEG and has a defined filename that exists, it reads the file directly. Otherwise, it converts the item into JPEG format in memory. It raises an error if the item is not a supported image type.\n", "Arguments": ":param item: Image. The image item to be serialized. It should be an instance of Image class or its subclasses.\n:return: A tuple containing the serialized image data in bytes and an optional error message as None. If the item is not a supported image type, a TypeError is raised.\n"}, "indent": 8}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [82, 82], "body_position": [83, 89], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.serializers.Image"], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_pil_serializer"], "requirement": {"Functionality": "The function deserializes a byte stream into an image object. It extracts the width, height, and mode of the image from the beginning of the byte stream, then uses these to reconstruct the image from the remaining bytes.\n", "Arguments": ":param cls: The class that is calling this method. It is used as a decorator to indicate that this method is intended to be a class method.\n:param data: bytes, The byte stream that contains the serialized image data. This data includes the image's width, height, mode, and the raw image bytes.\n:return: An image object reconstructed from the byte stream.\n\nNote: This function assumes the first 12 bytes (3 integers) of the data represent the width, height, and size of the mode string in that order, all as unsigned 32-bit integers. The mode string follows these integers and is of the length specified by the third integer. The rest of the data is the raw image data."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [169, 169], "body_position": [170, 180], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["litdata.constants._TORCH_DTYPES_MAPPING"]}, "tests": ["tests/streaming/test_serializer.py::test_tensor_serializer"], "requirement": {"Functionality": "The function deserializes a byte array into a PyTorch tensor. It extracts the data type and shape information encoded in the byte array, then reconstructs the tensor from the remaining bytes.\n", "Arguments": ":param data: bytes, the serialized tensor data including information about the tensor's dtype and shape, followed by the tensor's raw data.\n:return: torch.Tensor, the deserialized tensor reconstructed from the input byte array.\n"}, "indent": 8}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [160, 160], "body_position": [161, 167], "dependency": {"intra_class": ["litdata.streaming.serializers.TensorSerializer._dtype_to_indices"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_tensor_serializer", "tests/streaming/test_serializer.py::test_assert_bfloat16_tensor_serializer"], "requirement": {"Functionality": "Serializes a PyTorch tensor into a bytes object containing the tensor's dtype, shape, and raw data. This serialized format can be useful for saving or transmitting tensor data in a compact binary form. The function also returns None as the second element of the tuple, which could be used for additional metadata in future extensions.\n", "Arguments": ":param item: torch.Tensor, the tensor to be serialized. It is used to extract the dtype, shape, and raw data for serialization.\n:return: A tuple containing a bytes object and None. The bytes object is the serialized representation of the input tensor, including its dtype, shape, and raw data. The None value indicates that there is no additional metadata associated with this serialization."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [122, 122], "body_position": [123, 134], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.serializers.JpegImageFile", "litdata.streaming.serializers.PILSerializer", "litdata.streaming.serializers.PILSerializer.deserialize", "litdata.streaming.serializers._TORCH_VISION_AVAILABLE"], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_jpeg_serializer"], "requirement": {"Functionality": "The function attempts to deserialize the given byte data into a JPEG image. If PyTorch's torchvision is available and the data can be decoded as a JPEG, it returns a decoded JPEG image. If the decoding fails due to a runtime error (e.g., the data is actually a PNG with a JPEG extension), it falls back to using PIL to deserialize the data. If torchvision is available, it further converts the PIL image to a PyTorch tensor before returning.\n", "Arguments": ":param data: bytes, the byte data of the image to be deserialized.\n:return: Union[JpegImageFile, torch.Tensor], the deserialized image, which can be either a JpegImageFile object or a torch.Tensor, depending on whether torchvision is available and the type of image data."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [197, 197], "body_position": [198, 199], "dependency": {"intra_class": ["litdata.streaming.serializers.NoHeaderTensorSerializer._dtype_to_indices"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_assert_no_header_tensor_serializer"], "requirement": {"Functionality": "Serializes a PyTorch tensor into a bytes object and a string representing the tensor's data type. The serialization process converts the tensor to a NumPy array and then to bytes. The data type of the tensor is mapped to an index, which is included in the returned string for identification.\n", "Arguments": ":param self: NoHeaderTensorSerializer. An instance of the NoHeaderTensorSerializer class, which is responsible for serializing PyTorch tensors without including header information.\n:param item: torch.Tensor, The PyTorch tensor to be serialized. It is used to extract the tensor's data and data type for serialization.\n:return: Tuple[bytes, Optional[str]], A tuple where the first element is the serialized tensor data as bytes, and the second element is a string representing the tensor's data type through an index. The string is prefixed with \"no_header_tensor:\" followed by the data type index."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [201, 201], "body_position": [202, 203], "dependency": {"intra_class": ["litdata.streaming.serializers.NoHeaderTensorSerializer._dtype"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_assert_no_header_tensor_serializer"], "requirement": {"Functionality": "The function deserializes the given byte data into a PyTorch tensor using the specified data type stored in the instance.\n", "Arguments": ":param self: NoHeaderTensorSerializer. An instance of the NoHeaderTensorSerializer class, which must have a predefined data type (_dtype) for the tensor.\n:param data: bytes, The byte data to be deserialized into a PyTorch tensor.\n:return: torch.Tensor. The deserialized PyTorch tensor."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [225, 225], "body_position": [226, 239], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["litdata.constants._NUMPY_DTYPES_MAPPING"]}, "tests": ["tests/streaming/test_serializer.py::test_numpy_serializer"], "requirement": {"Functionality": "The function deserializes a byte array back into a numpy array. It first extracts the data type and shape information from the byte array, then reconstructs the numpy array based on this information.\n", "Arguments": ":param data: bytes, the serialized numpy array data that includes information about the data type, shape, and the array's actual data.\n:return: np.ndarray, the deserialized numpy array reconstructed from the input byte data.\n"}, "indent": 8}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [260, 260], "body_position": [261, 262], "dependency": {"intra_class": ["litdata.streaming.serializers.NoHeaderNumpySerializer._dtype"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_assert_no_header_numpy_serializer"], "requirement": {"Functionality": "Deserialize the given bytes data into a NumPy array using the predefined data type of the serializer instance.", "Arguments": ":param self: NoHeaderNumpySerializer. An instance of the NoHeaderNumpySerializer class. It uses the _dtype attribute of the instance to determine the data type for the NumPy array.\n:param data: bytes, The bytes data to be deserialized into a NumPy array.\n:return: np.ndarray, The deserialized data as a NumPy array."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [256, 256], "body_position": [257, 258], "dependency": {"intra_class": ["litdata.streaming.serializers.NoHeaderNumpySerializer._dtype_to_indices"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_assert_no_header_numpy_serializer"], "requirement": {"Functionality": "Serializes a NumPy array into a bytes object and generates a corresponding dtype identifier string. This method is designed for scenarios where header information is not required or desired in the serialized output.\n", "Arguments": ":param self: NoHeaderNumpySerializer. An instance of the NoHeaderNumpySerializer class.\n:param item: np.ndarray, The NumPy array to be serialized. It is used to convert the array into a bytes object and to determine the data type for generating the dtype identifier.\n:return: Tuple[bytes, Optional[str]], A tuple where the first element is the serialized bytes object of the NumPy array and the second element is a string representing the data type of the array in a custom format (\"no_header_numpy:{indice}\")."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [216, 216], "body_position": [217, 223], "dependency": {"intra_class": ["litdata.streaming.serializers.NumpySerializer._dtype_to_indices"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_numpy_serializer"], "requirement": {"Functionality": "Serializes a NumPy array into a bytes object, including metadata about the array's data type and shape, and returns it along with None as a placeholder for potential metadata that is not used in this implementation.", "Arguments": ":param item: np.ndarray, the NumPy array to be serialized, used to extract the data type, shape, and the array's binary content.\n:return: Tuple[bytes, Optional[str]], the serialized bytes object of the NumPy array and None. The bytes object includes the array's data type index, the number of dimensions, each dimension's size, and the array's binary data."}, "indent": 8}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "type": "method", "completion_path": "litdata/litdata/streaming/dataloader.py", "signature_position": [620, 620], "body_position": [621, 642], "dependency": {"intra_class": ["litdata.streaming.dataloader.StreamingDataLoader._latest_worker_idx", "litdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_combined", "litdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_streaming", "litdata.streaming.dataloader.StreamingDataLoader.batch_size", "litdata.streaming.dataloader.StreamingDataLoader.current_epoch", "litdata.streaming.dataloader.StreamingDataLoader.num_workers", "litdata.streaming.dataloader.StreamingDataLoader.restore"], "intra_file": [], "cross_file": ["litdata.streaming.dataset.StreamingDataset"]}, "tests": ["tests/streaming/test_dataset.py::test_resumable_dataset_two_workers", "tests/streaming/test_combined.py::test_combined_dataset_with_dataloader_and_one_worker", "tests/streaming/test_combined.py::test_combined_dataset_with_dataloader_2_epochs"], "requirement": {"Functionality": "This function generates and returns a dictionary containing the state of the StreamingDataLoader instance, which includes information about the dataset, current epoch, number of samples yielded, and the index of the latest worker. The structure of the returned state dictionary varies depending on whether the dataset is an instance of StreamingDataset.\n", "Arguments": ":param self: StreamingDataLoader. An instance of the StreamingDataLoader class. It is used to access the dataset, batch size, number of workers, current epoch, and other attributes to construct the state dictionary.\n:return: Dict[str, Any]. A dictionary representing the state of the StreamingDataLoader instance. The keys include \"dataset\", \"current_epoch\", \"num_samples_yielded\", and \"latest_worker_idx\". The structure of the \"dataset\" part of the dictionary depends on whether the dataset is a StreamingDataset or not."}, "indent": 8}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "type": "method", "completion_path": "litdata/litdata/streaming/serializers.py", "signature_position": [304, 304], "body_position": [305, 319], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.serializers._AV_AVAILABLE", "litdata.streaming.serializers._TORCH_VISION_AVAILABLE"], "cross_file": []}, "tests": ["tests/streaming/test_serializer.py::test_wav_deserialization"], "requirement": {"Functionality": "This function deserializes the given bytes data into a video object using the torchvision library. It first checks if the required libraries (torchvision and av) are installed, raises an exception if not, then writes the data to a temporary file and uses torchvision's read_video function to deserialize the video file into a video object.\n", "Arguments": ":param self: VideoSerializer. An instance of the VideoSerializer class.\n:param data: bytes, The video data in bytes format that needs to be deserialized.\n:return: Any, The deserialized video object. The exact type depends on the torchvision's read_video function's return type."}, "indent": 8}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "type": "method", "completion_path": "litdata/litdata/streaming/writer.py", "signature_position": [347, 347], "body_position": [349, 365], "dependency": {"intra_class": ["litdata.streaming.writer.BinaryWriter._is_done", "litdata.streaming.writer.BinaryWriter._serialized_items", "litdata.streaming.writer.BinaryWriter._should_write", "litdata.streaming.writer.BinaryWriter.filled", "litdata.streaming.writer.BinaryWriter.write_chunk", "litdata.streaming.writer.BinaryWriter.write_chunks_index"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_writer.py::test_binary_writer_with_ints_and_chunk_bytes", "tests/streaming/test_writer.py::test_binary_writer_with_ints_and_chunk_size", "tests/streaming/test_writer.py::test_binary_writer_with_jpeg_and_int", "tests/streaming/test_writer.py::test_binary_writer_with_jpeg_filepath_and_int"], "requirement": {"Functionality": "The function finalizes the writing process for a BinaryWriter instance. It attempts to write any remaining chunks of data to files and generates an index file for these chunks. It returns a list of file paths to the written chunks. The function marks the writing process as complete once it is done.\n", "Arguments": ":param self: BinaryWriter. An instance of the BinaryWriter class. It uses internal state to manage the writing process.\n:return: List[str]. A list of file paths to the chunks that have been written. This list is returned once the writing process is finalized and all chunks are written.\n\nNote: The function uses internal methods and properties of the BinaryWriter class, such as `_should_write`, `write_chunk`, and `write_chunks_index`, to manage the writing of chunks and the index file. It also checks if the writing process is already marked as complete by inspecting `self.filled` and marks it as complete by setting `self._is_done` to True at the end."}, "indent": 8}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "type": "method", "completion_path": "litdata/litdata/streaming/dataloader.py", "signature_position": [644, 644], "body_position": [653, 676], "dependency": {"intra_class": ["litdata.streaming.dataloader.StreamingDataLoader._latest_worker_idx", "litdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_combined", "litdata.streaming.dataloader.StreamingDataLoader._num_samples_yielded_streaming", "litdata.streaming.dataloader.StreamingDataLoader._worker_idx", "litdata.streaming.dataloader.StreamingDataLoader._worker_idx_iter", "litdata.streaming.dataloader.StreamingDataLoader.current_epoch", "litdata.streaming.dataloader.StreamingDataLoader.restore"], "intra_file": [], "cross_file": ["litdata.streaming.combined.CombinedStreamingDataset", "litdata.streaming.dataset.StreamingDataset"]}, "tests": ["tests/streaming/test_dataset.py::test_resumable_dataset_two_workers", "tests/streaming/test_combined.py::test_combined_dataset_with_dataloader_2_epochs"], "requirement": {"Functionality": "This function loads a dictionary containing the training state into the StreamingDataLoader instance. It is designed to be called from a non-worker process when resuming training. It updates the current epoch, the number of samples yielded, and the latest worker index based on the provided state. Additionally, it prepares the DataLoader for resuming by adjusting internal iterators and flags.\n", "Arguments": ":param self: StreamingDataLoader. An instance of the StreamingDataLoader class.\n:param obj: Dict[str, Any]. A dictionary containing the training state, including the current epoch, the number of samples yielded, and the latest worker index. It is used to update the state of the StreamingDataLoader and its dataset if applicable.\n:return: No return values. This method updates the state of the StreamingDataLoader instance in place.\n\nThis method also handles specific logic based on the type of dataset associated with the StreamingDataLoader, updating the dataset's state if it is a StreamingDataset or a CombinedStreamingDataset. It raises a RuntimeError if the dataset is neither."}, "indent": 8}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "type": "method", "completion_path": "litdata/litdata/streaming/combined.py", "signature_position": [101, 103], "body_position": [104, 108], "dependency": {"intra_class": ["litdata.streaming.combined.CombinedStreamingDataset._datasets", "litdata.streaming.combined.CombinedStreamingDataset._iterator"], "intra_file": ["litdata.streaming.combined._CombinedDatasetIterator.state_dict", "litdata.streaming.combined._state_dict"], "cross_file": []}, "tests": ["tests/streaming/test_combined.py::test_combined_dataset_state_dict"], "requirement": {"Functionality": "The function `state_dict` generates a state dictionary for the CombinedStreamingDataset instance. It returns an empty dictionary if the internal iterator is None and `num_samples_yielded` is not provided. Otherwise, it either returns a state dictionary created from the internal datasets or directly from the internal iterator's state dictionary, depending on whether the iterator is None.\n", "Arguments": ":param self: CombinedStreamingDataset. An instance of the CombinedStreamingDataset class.\n:param num_workers: int, The number of workers used in the dataset streaming process.\n:param batch_size: int, The size of the batch used in the dataset streaming process.\n:param num_samples_yielded: Optional[List[int]], A list of integers representing the number of samples yielded. This parameter is optional and defaults to None.\n:return: Dict[str, Any], A dictionary representing the state of the CombinedStreamingDataset instance. The dictionary is empty if the internal iterator is None and `num_samples_yielded` is not provided. Otherwise, it contains the state information relevant to the dataset streaming process."}, "indent": 8}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "type": "method", "completion_path": "litdata/litdata/streaming/combined.py", "signature_position": [110, 110], "body_position": [111, 125], "dependency": {"intra_class": ["litdata.streaming.combined.CombinedStreamingDataset._datasets", "litdata.streaming.combined.CombinedStreamingDataset._num_samples_yielded", "litdata.streaming.combined.CombinedStreamingDataset._use_streaming_dataloader"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_combined.py::test_combined_dataset_state_dict"], "requirement": {"Functionality": "This function loads the state of a CombinedStreamingDataset instance from a given state dictionary. It updates the state of each dataset within the CombinedStreamingDataset and, if applicable, updates the number of samples yielded by the streaming dataloader to avoid repeating samples.\n", "Arguments": ":param self: CombinedStreamingDataset. An instance of the CombinedStreamingDataset class.\n:param state_dict: Dict[str, Any]. A dictionary containing the state information for the CombinedStreamingDataset and its constituent datasets. It is used to restore the state of the CombinedStreamingDataset and its datasets.\n:return: None. There are no return values.\n"}, "indent": 8}
{"namespace": "litdata.streaming.resolver._resolve_dir", "type": "function", "completion_path": "litdata/litdata/streaming/resolver.py", "signature_position": [44, 44], "body_position": [45, 81], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.resolver.Dir", "litdata.streaming.resolver._resolve_datasets", "litdata.streaming.resolver._resolve_s3_connections", "litdata.streaming.resolver._resolve_studio", "litdata.streaming.resolver._resolve_time_template"], "cross_file": []}, "tests": ["tests/streaming/test_resolver.py::test_src_resolver_s3_connections", "tests/streaming/test_resolver.py::test_src_resolver_studios", "tests/streaming/test_resolver.py::test_src_resolver_datasets", "tests/streaming/test_resolver.py::test_dst_resolver_dataset_path", "tests/streaming/test_resolver.py::test_resolve_dir_absolute"], "requirement": {"Functionality": "Resolves a directory path into a Dir object, handling various path formats including local paths, S3 URLs, and specific project paths. It also processes paths based on their prefixes to determine the appropriate Dir object configuration.\n", "Arguments": ":param dir_path: Optional[Union[str, Dir]]. The directory path or Dir object to be resolved. It is used to determine the final Dir object based on the path's format and location.\n:return: Dir. The resolved Dir object with appropriately set path and/or URL attributes based on the input directory path or Dir object."}, "indent": 4}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "type": "function", "completion_path": "litdata/litdata/streaming/resolver.py", "signature_position": [206, 206], "body_position": [207, 232], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.resolver.Dir", "litdata.streaming.resolver.Dir.path", "litdata.streaming.resolver.Dir.url"], "cross_file": []}, "tests": ["tests/streaming/test_resolver.py::test_assert_dir_is_empty"], "requirement": {"Functionality": "This function checks if a specified directory, particularly in an S3 bucket, is empty. It raises errors if the directory is not a Dir object, does not start with \"s3://\", or already contains data. Currently, it does not support appending or overwriting data in the directory.\n", "Arguments": ":param output_dir: Dir. The directory to be checked. It must be an instance of the Dir class.\n:param append: Bool, optional. Indicates if appending data to the directory is allowed. Currently not implemented.\n:param overwrite: Bool, optional. Indicates if overwriting data in the directory is allowed. Currently not implemented.\n:return: No return values. This function either completes successfully or raises an error."}, "indent": 4}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "type": "function", "completion_path": "litdata/litdata/streaming/resolver.py", "signature_position": [235, 235], "body_position": [236, 277], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.resolver.Dir", "litdata.streaming.resolver.Dir.path", "litdata.streaming.resolver.Dir.url"], "cross_file": []}, "tests": ["tests/streaming/test_resolver.py::test_assert_dir_has_index_file"], "requirement": {"Functionality": "This function asserts that a given directory, expected to be an S3 bucket directory, does not contain an index file named \"index.json\". If the directory is not an S3 bucket or already contains an index file, it raises an error. Additionally, if an index file is not found, it deletes all objects within the specified prefix in the bucket.\n", "Arguments": ":param output_dir: Dir. The directory object which is expected to be an S3 bucket directory. It is used to check for the presence of an \"index.json\" file and to perform operations based on its existence.\n:return: None. There are no return values from this function."}, "indent": 4}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "type": "method", "completion_path": "litdata/litdata/streaming/writer.py", "signature_position": [367, 367], "body_position": [370, 393], "dependency": {"intra_class": ["litdata.streaming.writer.BinaryWriter._cache_dir", "litdata.streaming.writer.BinaryWriter._distributed_env", "litdata.streaming.writer.BinaryWriter._merge_no_wait", "litdata.streaming.writer.BinaryWriter.rank"], "intra_file": [], "cross_file": ["litdata.constants._INDEX_FILENAME", "litdata.utilities.env._DistributedEnv.world_size"]}, "tests": ["tests/streaming/test_writer.py::test_binary_writer_with_ints_and_chunk_bytes", "tests/streaming/test_writer.py::test_binary_writer_with_ints_and_chunk_size", "tests/streaming/test_writer.py::test_binary_writer_with_jpeg_and_int", "tests/streaming/test_writer.py::test_binary_writer_with_jpeg_filepath_and_int"], "requirement": {"Functionality": "The merge function is responsible for combining individual index files created by multiple workers into a single, unified index file. This is particularly useful in distributed environments where each worker generates a part of the index. The function waits until all parts are available, then proceeds with the merge only if it's the master node (rank 0).\n", "Arguments": ":param self: BinaryWriter. An instance of the BinaryWriter class, which manages the merging process.\n:param num_workers: int, optional. The number of workers that are expected to contribute to the index. It defaults to 1, assuming a non-distributed environment unless specified otherwise. It's used to determine when all parts of the index are ready for merging.\n:param node_rank: Optional[int], optional. The rank of the node in the distributed environment. This parameter is used to identify the master node (rank 0) which is responsible for the actual merging process. If None, the function assumes a non-distributed environment or that the rank determination is handled externally.\n:return: No return values. The function's purpose is to perform the merge operation, and it does not return any value.\n\nNote: The function includes a mechanism to wait for all index parts to be available by periodically checking the presence of index files in a cache directory. It also includes a special condition for non-master nodes (rank != 0) to wait until the merged index file is available."}, "indent": 8}
{"namespace": "litdata.streaming.resolver._execute", "type": "function", "completion_path": "litdata/litdata/streaming/resolver.py", "signature_position": [298, 303], "body_position": [306, 347], "dependency": {"intra_class": [], "intra_file": ["litdata.streaming.resolver.Machine", "litdata.streaming.resolver._LIGHTNING_SDK_AVAILABLE"], "cross_file": []}, "tests": ["tests/streaming/test_resolver.py::test_execute"], "requirement": {"Functionality": "This function remotely executes the current operator by creating a data preparation machine job through the Studio API. It continuously checks the job status and prints the job URL when it starts. The function raises exceptions if the required SDK is not available or if the job fails.\n", "Arguments": ":param name: str, The name of the job to be executed.\n:param num_nodes: int, The number of instances (nodes) required for the job.\n:param machine: Optional[Machine], The machine configuration for the job. If not provided, a default machine configuration is fetched.\n:param command: Optional[str], The command to be executed in the job. If not provided, a default command that includes the current working directory and environment variables is used.\n:return: No return values."}, "indent": 4}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "type": "method", "completion_path": "litdata/litdata/streaming/reader.py", "signature_position": [81, 81], "body_position": [83, 84], "dependency": {"intra_class": ["litdata.streaming.reader.PrepareChunksThread._to_delete_queue"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_reader.py::test_prepare_chunks_thread_eviction"], "requirement": {"Functionality": "Deletes the specified chunks by adding their indices to a deletion queue. This is typically used to manage chunks of data that are no longer needed for the current epoch in a threaded environment.\n", "Arguments": ":param self: PrepareChunksThread. An instance of the PrepareChunksThread class.\n:param chunk_indexes: List[int], The indices of the chunks to be deleted. These indices are added to a queue for deletion.\n:return: No return values."}, "indent": 8}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "type": "method", "completion_path": "litdata/litdata/streaming/reader.py", "signature_position": [198, 198], "body_position": [200, 201], "dependency": {"intra_class": ["litdata.streaming.reader.BinaryReader._cache_dir", "litdata.streaming.reader.BinaryReader._config", "litdata.streaming.reader.BinaryReader._item_loader", "litdata.streaming.reader.BinaryReader._remote_input_dir", "litdata.streaming.reader.BinaryReader._serializers"], "intra_file": [], "cross_file": ["litdata.streaming.config.ChunksConfig", "litdata.streaming.config.ChunksConfig.load"]}, "tests": ["tests/streaming/test_reader.py::test_prepare_chunks_thread_eviction"], "requirement": {"Functionality": "This function attempts to load the chunks configuration for a BinaryReader instance if the index files are available. It updates the instance's configuration with the loaded ChunksConfig object.", "Arguments": ":param self: BinaryReader. An instance of the BinaryReader class. It uses its attributes such as _cache_dir, _serializers, _remote_input_dir, and _item_loader to attempt to load the configuration.\n:return: Optional[ChunksConfig]. The loaded ChunksConfig object if the configuration could be successfully loaded, otherwise None."}, "indent": 8}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "type": "method", "completion_path": "litdata/litdata/streaming/reader.py", "signature_position": [76, 76], "body_position": [78, 79], "dependency": {"intra_class": ["litdata.streaming.reader.PrepareChunksThread._to_download_queue"], "intra_file": [], "cross_file": []}, "tests": ["tests/streaming/test_reader.py::test_prepare_chunks_thread_eviction"], "requirement": {"Functionality": "The download function takes a list of chunk indices and enqueues them into a download queue for processing. This is typically used to prepare chunks of data for downloading in a multi-threaded or asynchronous environment, where each chunk is identified by its index.\n", "Arguments": ":param self: PrepareChunksThread. An instance of the PrepareChunksThread class, which should have access to a queue attribute for storing the chunk indices to be downloaded.\n:param chunk_indexes: List[int]. A list of integers where each integer represents the index of a chunk that needs to be downloaded. These indices are used to identify and enqueue the specific chunks for downloading.\n:return: No return values. This method modifies the state of the _to_download_queue attribute by adding the chunk indices to it but does not return any value."}, "indent": 8}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "type": "method", "completion_path": "litdata/litdata/streaming/reader.py", "signature_position": [204, 204], "body_position": [205, 207], "dependency": {"intra_class": ["litdata.streaming.reader.BinaryReader._config"], "intra_file": [], "cross_file": ["litdata.streaming.config.ChunksConfig"]}, "tests": ["tests/streaming/test_reader.py::test_prepare_chunks_thread_eviction"], "requirement": {"Functionality": "This function returns the configuration for a BinaryReader instance. If the configuration is not set (None), it raises a RuntimeError indicating that the configuration should be defined before accessing it.", "Arguments": ":param self: BinaryReader. An instance of the BinaryReader class. It uses the _config attribute to determine if the configuration is set.\n:return: ChunksConfig. The configuration of the BinaryReader instance."}, "indent": 8}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "type": "method", "completion_path": "litdata/litdata/streaming/reader.py", "signature_position": [217, 217], "body_position": [225, 271], "dependency": {"intra_class": ["litdata.streaming.reader.BinaryReader._config", "litdata.streaming.reader.BinaryReader._distributed_env", "litdata.streaming.reader.BinaryReader._item_loader", "litdata.streaming.reader.BinaryReader._last_chunk_index", "litdata.streaming.reader.BinaryReader._max_cache_size", "litdata.streaming.reader.BinaryReader._prepare_thread", "litdata.streaming.reader.BinaryReader._try_load_config", "litdata.streaming.reader.BinaryReader.config"], "intra_file": ["litdata.streaming.reader.PrepareChunksThread", "litdata.streaming.reader.PrepareChunksThread.__init__", "litdata.streaming.reader.PrepareChunksThread.delete", "litdata.streaming.reader.PrepareChunksThread.download", "litdata.streaming.reader.PrepareChunksThread.stop", "litdata.streaming.sampler.ChunkedIndex.chunk_indexes", "litdata.streaming.sampler.ChunkedIndex.index", "litdata.streaming.sampler.ChunkedIndex.is_last_index"], "cross_file": ["litdata.streaming.item_loader.PyTreeLoader.load_item_from_chunk", "litdata.streaming.sampler.ChunkedIndex", "litdata.streaming.sampler.ChunkedIndex.chunk_index"]}, "tests": ["tests/streaming/test_writer.py::test_binary_writer_with_ints_and_chunk_bytes", "tests/streaming/test_writer.py::test_binary_writer_with_ints_and_chunk_size", "tests/streaming/test_writer.py::test_binary_writer_with_jpeg_and_int", "tests/streaming/test_writer.py::test_binary_writer_with_jpeg_filepath_and_int"], "requirement": {"Functionality": "The read function in the BinaryReader class is designed to read and return an item from a specified chunk. It ensures that the chunk is available either locally or in memory, and if not, it initiates its download. The function supports prefetching to minimize wait times and handles the lifecycle of chunks, including their deletion once they are fully consumed to avoid errors such as segmentation faults.\n", "Arguments": ":param self: BinaryReader. An instance of the BinaryReader class.\n:param index: ChunkedIndex, The index of the chunk from which an item is to be read. This index is used to determine the specific chunk and the item within that chunk to be loaded.\n:return: Any, The item read from the specified chunk. The type of the item can vary depending on the implementation of the item loader.\n\nNote: The function raises a ValueError if the provided index is not an instance of ChunkedIndex. It also raises an Exception if the reader's index configuration is not defined. Additionally, the function asserts the presence of a prepare thread during certain operations to ensure the thread is correctly managing the chunks."}, "indent": 8}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "type": "function", "completion_path": "litdata/litdata/utilities/broadcast.py", "signature_position": [140, 140], "body_position": [142, 144], "dependency": {"intra_class": [], "intra_file": ["litdata.utilities.broadcast._ImmutableDistributedMap", "litdata.utilities.broadcast._ImmutableDistributedMap.__init__", "litdata.utilities.broadcast._ImmutableDistributedMap.set_and_get"], "cross_file": []}, "tests": ["tests/utilities/test_broadcast.py::test_broadcast"], "requirement": {"Functionality": "This function broadcasts an object across machines in a distributed environment. It checks if the application is running in an environment with an external URL (indicating a distributed setting) and uses an immutable distributed map to share the object. If not in such an environment, it simply returns the object as is.\n", "Arguments": ":param key: String, the key associated with the object to be broadcasted. It is used to identify the object in the distributed map.\n:param obj: Any, the object to be broadcasted. This can be of any type and is the object that is either shared across machines or returned directly.\n:return: Any. The object that was either broadcasted and retrieved from the distributed map or the original object if not in a distributed environment."}, "indent": 4}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "type": "function", "completion_path": "litdata/litdata/utilities/shuffle.py", "signature_position": [8, 13], "body_position": [14, 28], "dependency": {"intra_class": [], "intra_file": ["litdata.utilities.env._DistributedEnv.world_size"], "cross_file": ["litdata.utilities.env._DistributedEnv", "litdata.utilities.env._DistributedEnv.num_nodes"]}, "tests": ["tests/utilities/test_shuffle.py::test_intra_node_chunk_shuffle"], "requirement": {"Functionality": "This function shuffles the chunks of data assigned to each node in a distributed environment. It ensures that each node receives a randomized set of chunk indexes based on the provided seed and the current epoch, aiming to distribute data more evenly and randomly across nodes for each training epoch.\n", "Arguments": ":param distributed_env: _DistributedEnv. The environment of the distributed system, containing information like the number of nodes and the world size.\n:param chunks_per_ranks: List[List[int]]. A list where each sublist contains the chunk indexes assigned to a specific rank.\n:param seed: int. The seed used for random number generation to ensure reproducibility of the shuffle.\n:param current_epoch: int. The current epoch number, used to modify the seed for each epoch to ensure different shuffles.\n:return: List[int]. A flattened list of shuffled chunk indexes across all nodes."}, "indent": 4}
{"namespace": "litdata.processing.functions._get_input_dir", "type": "function", "completion_path": "litdata/litdata/processing/functions.py", "signature_position": [54, 54], "body_position": [55, 71], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.functions._get_indexed_paths"], "cross_file": []}, "tests": ["tests/processing/test_functions.py::test_get_input_dir"], "requirement": {"Functionality": "Determines the input directory from a sequence of inputs by extracting and resolving indexed paths. It checks the first two elements of the input sequence for valid file paths, raises an error if inconsistent file paths are found, and formats the path to include the project root or a specified depth in the file system.\n", "Arguments": ":param inputs: Sequence[Any]. A sequence of inputs from which file paths are to be extracted and analyzed.\n:return: Optional[str]. The absolute path to the input directory determined from the inputs, or None if no valid file paths are found."}, "indent": 4}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "type": "function", "completion_path": "litdata/litdata/processing/utilities.py", "signature_position": [113, 113], "body_position": [114, 120], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.utilities.optimize_dns"], "cross_file": []}, "tests": ["tests/processing/test_utilities.py::test_optimize_dns_context"], "requirement": {"Functionality": "This function is designed to temporarily enable or disable DNS optimization based on the input parameter. It attempts to perform operations within a context where DNS optimization is enabled or disabled as specified, and ensures that DNS optimization is always disabled after these operations, even if an exception occurs.\n", "Arguments": ":param enable: Bool, indicates whether DNS optimization should be enabled (True) or disabled (False) during the execution of the context.\n:return: No return values. This function is a context manager that does not return any value but ensures DNS optimization is set according to the 'enable' parameter and then reset to disabled afterwards."}, "indent": 4}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "type": "function", "completion_path": "litdata/litdata/utilities/shuffle.py", "signature_position": [31, 36], "body_position": [37, 79], "dependency": {"intra_class": [], "intra_file": ["litdata.utilities.env._DistributedEnv.world_size"], "cross_file": ["litdata.utilities.env._DistributedEnv"]}, "tests": ["tests/utilities/test_shuffle.py::test_associate_chunks_and_internals_to_ranks"], "requirement": {"Functionality": "This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n", "Arguments": ":param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n:param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n:param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n:param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n:return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank."}, "indent": 4}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "type": "method", "completion_path": "litdata/litdata/processing/functions.py", "signature_position": [95, 95], "body_position": [96, 113], "dependency": {"intra_class": ["litdata.processing.functions.LambdaDataTransformRecipe._contains_device", "litdata.processing.functions.LambdaDataTransformRecipe._contains_is_last", "litdata.processing.functions.LambdaDataTransformRecipe._device", "litdata.processing.functions.LambdaDataTransformRecipe._find_device", "litdata.processing.functions.LambdaDataTransformRecipe._fn"], "intra_file": [], "cross_file": []}, "tests": ["tests/processing/test_data_processor.py::test_lambda_transform_recipe", "tests/processing/test_data_processor.py::test_lambda_transform_recipe_class"], "requirement": {"Functionality": "The function 'prepare_item' prepares an item by applying a transformation function ('_fn') to the item's metadata, potentially including additional context like device information and a flag indicating if it's the last item. This is used within a data transformation recipe context, where items are processed and transformed according to a specified lambda function or callable.\n", "Arguments": ":param item_metadata: Any, the metadata of the item to be transformed. It is used as the first argument in the transformation function.\n:param output_dir: str, the directory where the transformed item's output should be stored. It is passed as the second argument to the transformation function.\n:param is_last: bool, a flag indicating whether the item is the last one in the sequence to be processed. It is conditionally added to the keyword arguments passed to the transformation function if '_contains_is_last' is True.\n:return: No return values. The function directly calls the transformation function ('_fn') with the provided arguments and keyword arguments, affecting external state or outputs rather than returning a value."}, "indent": 8}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [90, 90], "body_position": [92, 99], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["litdata.streaming.client.S3Client", "litdata.streaming.client.S3Client.client"]}, "tests": ["tests/processing/test_data_processor.py::test_wait_for_file_to_exist"], "requirement": {"Functionality": "This function continuously checks if a specified file exists in an S3 bucket. It attempts to retrieve the file metadata using the head_object method. If the file is not found, it waits for a specified amount of time before trying again. If any other error occurs, the error is raised.", "Arguments": ":param s3: S3Client. The S3 client used to interact with AWS S3.\n:param obj: parse.ParseResult. The parsed result of the S3 object URL, containing the bucket name and object key.\n:param sleep_time: int, optional. The amount of time in seconds to wait before retrying if the file does not exist. Defaults to 2 seconds.\n:return: Any. The response from the head_object method if the file exists. This typically includes metadata about the S3 object."}, "indent": 4}
{"namespace": "litdata.processing.functions.optimize", "type": "function", "completion_path": "litdata/litdata/processing/functions.py", "signature_position": [257, 274], "body_position": [297, 369], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.functions.LambdaDataChunkRecipe", "litdata.processing.functions.LambdaDataChunkRecipe.__init__", "litdata.processing.functions._get_default_num_workers", "litdata.processing.functions._get_input_dir"], "cross_file": ["litdata.constants._IS_IN_STUDIO", "litdata.processing.data_processor.DataProcessor", "litdata.processing.data_processor.DataProcessor.run", "litdata.processing.readers.BaseReader", "litdata.processing.utilities.optimize_dns_context", "litdata.streaming.dataloader.StreamingDataLoader", "litdata.streaming.resolver.Dir", "litdata.streaming.resolver.Dir.path", "litdata.streaming.resolver.Dir.url", "litdata.streaming.resolver._assert_dir_has_index_file", "litdata.streaming.resolver._execute", "litdata.streaming.resolver._resolve_dir"]}, "tests": ["tests/streaming/test_dataset.py::test_dataset_for_text_tokens_distributed_num_workers_end_to_end", "tests/processing/test_data_processor.py::test_data_processing_optimize", "tests/processing/test_data_processor.py::test_data_processing_optimize_yield", "tests/processing/test_data_processor.py::test_data_processing_optimize_class", "tests/processing/test_data_processor.py::test_data_processing_optimize_class_yield", "tests/processing/test_data_processor.py::test_empty_optimize", "tests/processing/test_data_processor.py::test_load_torch_audio", "tests/processing/test_data_processor.py::test_load_torch_audio_from_wav_file"], "requirement": {"Functionality": "This function is designed to optimize a dataset by converting it into chunks, which can be processed in a distributed manner if required. It applies a given function to each element of the input sequence, supports various configurations for chunking, compression, and parallel processing, and handles both local and remote execution environments.\n", "Arguments": ":param fn: Callable[[Any], Any]. A function that will be executed over each element of the inputs.\n:param inputs: Sequence[Any]. A sequence of inputs that the `fn` function will process. Each input should contain at least a valid filepath.\n:param output_dir: str. The directory where the processed data will be stored.\n:param weights: Optional[List[int]]. Weights associated with each input, used for balancing work among workers. Default is None.\n:param chunk_size: Optional[int]. The maximum number of elements each chunk should contain. Default is None.\n:param chunk_bytes: Optional[Union[int, str]]. The maximum size in bytes that each chunk should be. Default is None.\n:param compression: Optional[str]. The compression algorithm to use for the chunks. Default is None.\n:param num_workers: Optional[int]. The number of workers to use for processing. Default is None.\n:param fast_dev_run: bool. If True, processes only a subset of the inputs for quick development testing. Default is False.\n:param num_nodes: Optional[int]. The number of nodes to use for remote execution, supported only on https://lightning.ai/. Default is None.\n:param machine: Optional[str]. Specifies the machine type for remote execution, supported only on https://lightning.ai/. Default is None.\n:param num_downloaders: Optional[int]. The number of downloaders per worker. Default is None.\n:param num_uploaders: Optional[int]. The number of uploaders per worker. Default is None.\n:param reorder_files: bool. If True, reorders files by size to distribute work equally among workers; otherwise, preserves the order of processing. Default is True.\n:param reader: Optional[BaseReader]. The reader to use for reading inputs. Default is None.\n:param batch_size: Optional[int]. Groups the inputs into batches of this size. Default is None.\n:return: None. This function does not return any value."}, "indent": 4}
{"namespace": "litdata.processing.functions.map", "type": "function", "completion_path": "litdata/litdata/processing/functions.py", "signature_position": [157, 172], "body_position": [193, 254], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.functions.LambdaDataTransformRecipe", "litdata.processing.functions.LambdaDataTransformRecipe.__init__", "litdata.processing.functions._get_default_num_workers", "litdata.processing.functions._get_input_dir"], "cross_file": ["litdata.constants._IS_IN_STUDIO", "litdata.processing.data_processor.DataProcessor", "litdata.processing.data_processor.DataProcessor.run", "litdata.processing.readers.BaseReader", "litdata.processing.utilities.optimize_dns_context", "litdata.streaming.dataloader.StreamingDataLoader", "litdata.streaming.resolver.Dir", "litdata.streaming.resolver.Dir.path", "litdata.streaming.resolver.Dir.url", "litdata.streaming.resolver._assert_dir_is_empty", "litdata.streaming.resolver._execute", "litdata.streaming.resolver._resolve_dir"]}, "tests": ["tests/processing/test_data_processor.py::test_data_processing_map", "tests/processing/test_data_processor.py::test_data_processing_map_without_input_dir", "tests/processing/test_data_processor.py::test_data_processing_map_weights_mismatch", "tests/processing/test_data_processor.py::test_data_processing_map_without_input_dir_and_folder", "tests/processing/test_data_processor.py::test_data_processing_map_non_absolute_path", "tests/processing/test_data_processor.py::test_map_error_when_not_empty", "tests/processing/test_data_processor.py::test_map_is_last", "tests/processing/test_data_processor.py::test_map_batch_size", "tests/processing/test_data_processor.py::test_streaming_dataset_in_map"], "requirement": {"Functionality": "This function maps a callable over a collection of inputs, potentially in a distributed manner, and processes them according to the specified parameters. It supports various configurations for parallel processing, handling of input batches, and customization of the processing environment, including remote execution options.\n", "Arguments": ":param fn: Callable[[str, Any], None]. A function to be executed over each input element. It defines the processing logic for each input.\n:param inputs: Sequence[Any]. A sequence of inputs to be processed by the `fn` function. Each input should contain at least a valid filepath, and it can be a mix of different data types.\n:param output_dir: Union[str, Dir]. The directory where the processed data should be stored. It can be a string path or a Dir object.\n:param weights: Optional[List[int]]. Provides an associated weight to each input, used to balance work among workers. Default is None.\n:param num_workers: Optional[int]. Specifies the number of workers to use during processing. Default is None, which means the system decides.\n:param fast_dev_run: Union[bool, int]. If True or a positive integer, only a subset of the inputs will be processed. This is useful for development or testing. Default is False.\n:param num_nodes: Optional[int]. Specifies the number of nodes to use for remote execution. Only supported on https://lightning.ai/. Default is None.\n:param machine: Optional[str]. Specifies the machine type to use for remote execution. Only supported on https://lightning.ai/. Default is None.\n:param num_downloaders: Optional[int]. The number of downloaders per worker. Default is None.\n:param num_uploaders: Optional[int]. The number of uploaders per worker. Default is None.\n:param reorder_files: bool. If True, reorders the files by file size to distribute work equally among all workers. If False, preserves the order of processing. Default is True.\n:param error_when_not_empty: bool. If True, raises an error if the output directory is not empty. Default is False.\n:param reader: Optional[BaseReader]. Specifies a custom reader for processing the inputs. Default is None.\n:param batch_size: Optional[int]. Groups the inputs into batches of this size for processing. Default is None.\n:return: No return values. The function processes the inputs and stores the results in the specified output directory."}, "indent": 4}
{"namespace": "litdata.processing.data_processor._download_data_target", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [112, 112], "body_position": [114, 166], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold"], "cross_file": ["litdata.streaming.client.S3Client", "litdata.streaming.client.S3Client.client"]}, "tests": ["tests/processing/test_data_processor.py::test_download_data_target"], "requirement": {"Functionality": "This function downloads data from a remote directory to a local cache directory to optimize reading. It continuously fetches download tasks from an input queue, checks if the files are already downloaded, downloads missing files, and then signals completion by putting the task index into an output queue.\n", "Arguments": ":param input_dir: Dir. The directory object representing the source directory from which files are to be downloaded. It is used to determine the source path or URL for the files.\n:param cache_dir: str. The path to the local cache directory where files are to be downloaded. It is used as the destination for the downloaded files.\n:param queue_in: Queue. The input queue from which download tasks are fetched. Each task includes an index and a list of file paths to download.\n:param queue_out: Queue. The output queue where the index of a completed download task is put to signal that the files for that index are available.\n:return: None. There are no return values as the function's purpose is to perform side effects (downloading files and communicating via queues)."}, "indent": 4}
{"namespace": "litdata.processing.data_processor._upload_fn", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [192, 192], "body_position": [194, 245], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["litdata.streaming.client.S3Client", "litdata.streaming.client.S3Client.client"]}, "tests": ["tests/processing/test_data_processor.py::test_upload_fn", "tests/processing/test_data_processor.py::test_upload_s3_fn"], "requirement": {"Functionality": "This function is responsible for uploading optimized chunks from a local directory to a remote dataset directory. It supports uploading to S3 cloud storage or moving files within the local filesystem based on the output directory's scheme. The function continuously processes items from the upload queue until a termination signal is received.\n", "Arguments": ":param upload_queue: Queue. The queue containing data chunks to be uploaded. Each item can be a file path or a tuple containing a temporary directory and a file path.\n:param remove_queue: Queue. The queue to which file paths are sent for removal after successful upload.\n:param cache_dir: str. The local directory where files are cached before being uploaded. It is prepended to file paths that don't start with it.\n:param output_dir: Dir. The target directory where files should be uploaded. It can be a local directory or an S3 bucket, determined by its URL scheme.\n:return: None. This function does not return any value."}, "indent": 4}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [281, 287], "body_position": [288, 305], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.data_processor._get_node_rank", "litdata.processing.data_processor._get_num_nodes"], "cross_file": ["litdata.utilities.packing._pack_greedily"]}, "tests": ["tests/processing/test_data_processor.py::test_map_items_to_workers_weighted"], "requirement": {"Functionality": "This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node and returns a list of items for each worker, with the items shuffled.\n", "Arguments": ":param num_workers: Int. The number of workers per node.\n:param user_items: List[Any]. The items to be distributed among the workers.\n:param weights: Optional[List[int]]. The weights for each item, used for distribution. If not provided, all items are considered to have equal weight.\n:param file_size: Bool. A flag indicating whether to consider the items as files with sizes for printing purposes. If True, sizes are printed in megabytes; otherwise, the total weight of items is printed.\n:return: List[List[Any]]. A list of lists, where each sublist contains the items assigned to a worker, shuffled randomly."}, "indent": 4}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [248, 248], "body_position": [249, 278], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.data_processor._get_node_rank", "litdata.processing.data_processor._get_num_nodes"], "cross_file": []}, "tests": ["tests/processing/test_data_processor.py::test_map_items_to_workers_sequentially"], "requirement": {"Functionality": "This function distributes a list of items among workers in a distributed computing environment sequentially. It calculates the number of items each worker should process based on the total number of workers across all nodes and the total number of items. It ensures an even distribution of items and handles any remainder by distributing extra items to the last workers in the list.\n", "Arguments": ":param num_workers: Int. The number of workers per node. It is used to calculate the total number of workers across all nodes.\n:param user_items: List[Any]. A list of items that need to be distributed among the workers. \n:return: List[List[Any]]. A list of lists, where each sublist contains the items assigned to a worker. This ensures each worker knows which items they are responsible for processing.\n\nInternal workings and additional notes:\n- The function first calculates the total number of workers across all nodes and then determines how many items each worker should process.\n- It adjusts for any remainder by adding extra items to the workers starting from the end of the list, ensuring an even distribution as much as possible.\n- It uses cumulative sum to efficiently calculate the start and end indices for each worker's items.\n- The function ensures that the output list has a length equal to the number of workers; otherwise, it raises a RuntimeError indicating improper assignment.\n- This function relies on two other functions: `_get_num_nodes()` to get the total number of nodes in the environment, and `_get_node_rank()` to identify the current node's rank within the environment."}, "indent": 4}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "type": "method", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [1056, 1056], "body_position": [1057, 1071], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.data_processor._get_cache_data_dir", "litdata.processing.data_processor._get_cache_dir"], "cross_file": []}, "tests": ["tests/processing/test_data_processor.py::test_cache_dir_cleanup"], "requirement": {"Functionality": "The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n", "Arguments": ":param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n:return: No return values. This method performs operations on the filesystem but does not return any value."}, "indent": 8}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [324, 324], "body_position": [326, 335], "dependency": {"intra_class": [], "intra_file": ["litdata.processing.data_processor._get_num_bytes"], "cross_file": []}, "tests": ["tests/processing/test_data_processor.py::test_get_item_filesizes"], "requirement": {"Functionality": "Computes and returns a list of file sizes for each item in the given list by parallelizing the file size retrieval process to improve performance. It uses a ThreadPoolExecutor to execute the file size retrieval in parallel, adjusting the number of workers based on the CPU count.\n", "Arguments": ":param items: List[Any]. A list of items for which to compute file sizes. The nature of these items is not specified, but they are used to determine file sizes in some way.\n:param base_path: str, optional. A base path to be prepended to each item's path before computing its file size. Defaults to an empty string, indicating no base path is used.\n:return: List[int]. A list of file sizes corresponding to each item in the input list. Each size is an integer representing the number of bytes."}, "indent": 4}
{"namespace": "litdata.processing.data_processor._is_path", "type": "function", "completion_path": "litdata/litdata/processing/data_processor.py", "signature_position": [342, 342], "body_position": [343, 354], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["litdata.constants._IS_IN_STUDIO"]}, "tests": ["tests/processing/test_data_processor.py::test_is_path_valid_in_studio"], "requirement": {"Functionality": "Determines if a given element is a path that exists or starts with a specified input directory. It specifically checks if the element is a string that represents a path within the input directory or if it exists in the file system. This function also handles absolute path conversion when necessary.\n", "Arguments": ":param input_dir: Optional[str]. The base directory against which the element is checked. It is used to determine if the element is a subpath of this directory.\n:param element: Any. The element to be checked if it's a path. It is converted to a string and checked against the input directory or the file system for existence.\n:return: Bool. Indicates whether the element is a valid path that exists or starts with the specified input directory."}, "indent": 4}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "type": "method", "completion_path": "gaussian-splatting-lightning/internal/utils/network_factory.py", "signature_position": [43, 51], "body_position": [52, 82], "dependency": {"intra_class": ["internal.utils.network_factory.NetworkFactory._get_seed", "internal.utils.network_factory.NetworkFactory._get_torch_layer", "internal.utils.network_factory.NetworkFactory.tcnn"], "intra_file": [], "cross_file": []}, "tests": ["tests/network_factory_test.py::NetworkFactoryTestCase::test_torch"], "requirement": {"Functionality": "This function generates and returns a neural network model based on the specified parameters. It supports creating networks using either the tinycudann library (if `self.tcnn` is True) or PyTorch. The network configuration includes the number of input and output dimensions, the number of layers and neurons per layer, and the activation functions for the hidden and output layers.\n", "Arguments": ":param self: NetworkFactory. An instance of the NetworkFactory class.\n:param n_input_dims: int, The number of input dimensions for the network.\n:param n_output_dims: int, The number of output dimensions for the network.\n:param n_layers: int, The total number of layers in the network, including the output layer. Must be greater than 0.\n:param n_neurons: int, The number of neurons in each hidden layer. Must be greater than 0.\n:param activation: Literal[\"ReLU\", \"None\"], The activation function to use for the hidden layers.\n:param output_activation: Literal[\"ReLU\", \"Sigmoid\", \"None\"], The activation function to use for the output layer.\n:return: A neural network model. The type of the model depends on whether tinycudann or PyTorch is used for its creation.\n\nNote: The function asserts that both `n_layers` and `n_neurons` are greater than 0 to ensure a valid network configuration. It dynamically selects the network type in tinycudann based on the number of neurons. For PyTorch, it constructs the network sequentially, layer by layer, using the specified number of input/output dimensions, neurons, and activation functions."}, "indent": 8}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "type": "method", "completion_path": "open-iris/src/iris/nodes/geometry_refinement/smoothing.py", "signature_position": [228, 228], "body_position": [241, 252], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["iris.io.errors.GeometryRefinementError"]}, "tests": ["tests/unit_tests/nodes/geometry_refinement/test_smoothing.py::test_rolling_median_raises_an_error_when_not_1D_signal"], "requirement": {"Functionality": "This function calculates the rolling median of a 1D signal array. It shifts the signal by a range defined by the kernel offset, computes the median of these shifted signals, and then trims the resulting median array to account for edge effects introduced by the shifting process.\n", "Arguments": ":param signal: np.ndarray, The input signal array for which the rolling median is to be computed. It is used to generate shifted versions of itself, which are then used to calculate the rolling median.\n:param kernel_offset: int, The offset for the kernel, determining how far the signal is shifted in both directions to compute the rolling median. It defines the range of shifts applied to the signal for median calculation.\n:return: np.ndarray, The rolling median of the input signal. This array will be shorter than the input signal by 2 * kernel_offset elements due to the trimming process to remove edge effects."}, "indent": 8}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "type": "function", "completion_path": "open-iris/src/iris/nodes/matcher/utils.py", "signature_position": [108, 114], "body_position": [127, 201], "dependency": {"intra_class": [], "intra_file": ["iris.io.dataclasses.IrisTemplate.iris_codes", "iris.nodes.matcher.utils.count_nonmatchbits", "iris.nodes.matcher.utils.count_sqrt_totalbits", "iris.nodes.matcher.utils.normalized_HD"], "cross_file": ["iris.io.dataclasses.IrisTemplate", "iris.io.dataclasses.IrisTemplate.mask_codes", "iris.io.errors.MatcherError"]}, "tests": ["tests/unit_tests/nodes/matcher/test_matcher_utils.py::test_hamming_distance_with_weights"], "requirement": {"Functionality": "This function calculates the Hamming distance between two iris templates, considering an allowed rotation shift. It supports optional parameters for normalized and weighted Hamming distance calculations. The function returns the minimum Hamming distance and the corresponding rotation shift that achieves this minimum distance.\n", "Arguments": ":param template_probe: IrisTemplate. The iris template from the probe, used as one of the inputs for the Hamming distance calculation.\n:param template_gallery: IrisTemplate. The iris template from the gallery, used as the other input for the Hamming distance calculation.\n:param rotation_shift: int. The amount of rotation allowed in the matching process, which is converted to columns for the calculation.\n:param nm_dist: Optional[float] = None. The nonmatch distance, an optional parameter for calculating a normalized Hamming distance. Defaults to None.\n:param weights: Optional[List[np.ndarray]] = None. A list of weights tables, an optional parameter for calculating a weighted Hamming distance. Defaults to None.\n:return: Tuple[float, int]. The function returns a tuple containing the minimum Hamming distance and the corresponding rotation shift that achieves this minimum distance."}, "indent": 4}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "type": "method", "completion_path": "open-iris/src/iris/nodes/eye_properties_estimation/bisectors_method.py", "signature_position": [84, 86], "body_position": [100, 140], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["iris.io.class_configs.Algorithm.params", "iris.io.errors.EyeCentersEstimationError"]}, "tests": ["tests/unit_tests/nodes/eye_properties_estimation/test_bisectors_method.py::test_calculate_perpendicular_bisectors_raises_an_exception"], "requirement": {"Functionality": "This function calculates the perpendicular bisectors of a specified number of randomly chosen pairs of points from a polygon's vertices, ensuring that each pair of points has a distance greater than a specified minimum. It is designed to find the center of a circular shape by generating perpendicular bisectors that ideally converge towards the center.\n", "Arguments": ":param self: BisectorsMethod. An instance of the BisectorsMethod class, which contains parameters like the number of bisectors to calculate and the maximum iterations for the random selection process.\n:param polygon: np.ndarray. An array representing the vertices of the polygon from which pairs of points are chosen. It is used as the basis for calculating perpendicular bisectors.\n:param min_distance_between_sector_points_in_px: float. The minimum distance between any two points in a pair. This parameter ensures that the chosen points are not too close to each other, allowing for a more accurate estimation of the center.\n:return: Tuple[np.ndarray, np.ndarray]. A tuple containing two numpy arrays. The first array represents the starting points of the calculated perpendicular bisectors, and the second array represents the ending points of these bisectors. These bisectors can be used to estimate the center of a circular shape.\n\nRaises.\nEyeCentersEstimationError: This exception is raised if the function fails to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed. This indicates that it may not be possible to accurately estimate the center of the shape."}, "indent": 8}
{"namespace": "iris.io.class_configs.Algorithm.execute", "type": "method", "completion_path": "open-iris/src/iris/io/class_configs.py", "signature_position": [69, 69], "body_position": [75, 83], "dependency": {"intra_class": ["iris.io.class_configs.Algorithm._callbacks", "iris.io.class_configs.Algorithm.run"], "intra_file": [], "cross_file": []}, "tests": ["tests/unit_tests/io/test_class_configs.py::test_parametrized_model_validation_hook_raising_an_error"], "requirement": {"Functionality": "The execute method runs the main algorithm of the Algorithm class, allowing for pre-execution and post-execution hooks to be called. These hooks are methods from objects in the _callbacks list, which are executed before and after the main algorithm run method, respectively.\n", "Arguments": ":param self: Algorithm. An instance of the Algorithm class.\n:param *args: Any. Variable length argument list passed to both the hooks and the run method of the algorithm.\n:param **kwargs: Any. Arbitrary keyword arguments passed to both the hooks and the run method of the algorithm.\n:return: Any. The result of the algorithm's run method, which can be of any type depending on the specific implementation.\n"}, "indent": 8}
{"namespace": "tanuki.validator.Validator.validate_output", "type": "method", "completion_path": "tanuki_py/src/tanuki/validator.py", "signature_position": [80, 80], "body_position": [81, 86], "dependency": {"intra_class": ["tanuki.validator.Validator.check_type"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_validator/test_validate_output.py::test_validate_output_dataclass"], "requirement": {"Functionality": "The function validates the output string by deserializing it from JSON format and then checking if it matches a given type definition. If the output cannot be deserialized or does not match the type definition, it returns False.\n", "Arguments": ":param self: Validator. An instance of the Validator class.\n:param output: str, The JSON string that needs to be validated against the type definition.\n:param type_definition: Any, The type definition that the deserialized output is expected to match. This could be any type or structure that the implementation of check_type can handle.\n:return: bool, Indicates whether the output matches the type definition. Returns True if it matches, False otherwise."}, "indent": 8}
{"namespace": "tanuki.register.Register.load_function_description", "type": "method", "completion_path": "tanuki_py/src/tanuki/register.py", "signature_position": [111, 111], "body_position": [117, 192], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["tanuki.models.embedding.Embedding", "tanuki.models.function_description.FunctionDescription", "tanuki.models.function_type.FunctionType", "tanuki.models.function_type.FunctionType.EMBEDDABLE", "tanuki.models.function_type.FunctionType.SYMBOLIC"]}, "tests": ["tests/test_configure_MP.py::test_configurability", "tests/test_configure_MP.py::test_finetuning", "tests/test_token_counter.py::test_token_counter_non_finetunable_1", "tests/test_token_counter.py::test_error_raise", "tests/test_register/test_output_type_definitions.py::test_output_pydantic_classes"], "requirement": {"Functionality": "This function creates a description of a given function object, including its name, docstring, input and output type hints, and class definitions for those types. It is designed to facilitate the registration of the function by providing detailed metadata about it. The function also determines the function type based on the output type hint, distinguishing between symbolic functions and those that can be embedded.\n", "Arguments": ":param func_object: The function object to create a description for. It is used to extract the signature, type hints, and docstring of the function.\n:return: FunctionDescription. An instance of FunctionDescription containing the function's metadata, including its name, docstring, input and output type hints, class definitions for input and output types, and the function type (symbolic or embeddable).\n\nAdditional details:\n- The function uses the `inspect.signature` method to get the function's signature and `get_type_hints` to fetch the type hints.\n- It differentiates between input and output type hints based on the function's signature.\n- A helper function `get_class_definition` is used to fetch class definitions for the type hints, handling generic types and ensuring that class definitions are fetched for non-built-in types.\n- The function checks if the output type hint is a class or a subclass of a Union and processes it accordingly to determine the function type and output class definition.\n- The function type can be either SYMBOLIC or EMBEDDABLE, determined based on whether the output type hint is a subclass of an Embedding class or part of a Union type.\n- The function's output, `FunctionDescription`, is a structured representation of the function's metadata, designed for use in function registration processes."}, "indent": 8}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "type": "method", "completion_path": "tanuki_py/src/tanuki/bloom_filter.py", "signature_position": [60, 60], "body_position": [61, 64], "dependency": {"intra_class": ["tanuki.bloom_filter.BloomFilter.bit_array", "tanuki.bloom_filter.BloomFilter.hash_count", "tanuki.bloom_filter.BloomFilter.hash_functions", "tanuki.bloom_filter.BloomFilter.size"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_bloom_filter.py::test_bloom_filter_persistence"], "requirement": {"Functionality": "Adds a string to the Bloom filter by calculating multiple hash values and setting the bits at the calculated indices in the bit array to 1. This is used to efficiently check whether an item might be in a set without storing the actual items.", "Arguments": ":param self: BloomFilter. An instance of the BloomFilter class.\n:param string: str, The string to be added to the Bloom filter. It is used to calculate the hash values which determine the indices in the bit array that should be set to 1.\n:return: No return values."}, "indent": 8}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "type": "method", "completion_path": "tanuki_py/src/tanuki/bloom_filter.py", "signature_position": [70, 70], "body_position": [71, 78], "dependency": {"intra_class": ["tanuki.bloom_filter.BloomFilter.bit_array", "tanuki.bloom_filter.BloomFilter.indices", "tanuki.bloom_filter.BloomFilter.init_bit_array", "tanuki.bloom_filter.BloomFilter.persistence", "tanuki.bloom_filter.BloomFilter.save", "tanuki.bloom_filter.BloomFilter.size"], "intra_file": [], "cross_file": ["tanuki.persistence.filter.bloom_interface.IBloomFilterPersistence.load"]}, "tests": ["tests/test_bloom_filter.py::test_bit_array_length", "tests/test_bloom_filter.py::test_simple_test_case"], "requirement": {"Functionality": "Loads the bit array from persistence into the BloomFilter instance. It checks if the loaded bit array's length matches the expected length based on the BloomFilter size. If there is a mismatch, indicating potential corruption, it logs a warning and reinitializes the bit array and indices, then saves the new state.", "Arguments": ":param self: BloomFilter. An instance of the BloomFilter class. It uses self.persistence to load the bit array, self.size to calculate the expected length, and self.init_bit_array and self.save methods for reinitialization and saving if necessary.\n:return: No return values."}, "indent": 8}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "type": "method", "completion_path": "tanuki_py/src/tanuki/bloom_filter.py", "signature_position": [50, 50], "body_position": [51, 58], "dependency": {"intra_class": ["tanuki.bloom_filter.BloomFilter.bit_array", "tanuki.bloom_filter.BloomFilter.hash_count", "tanuki.bloom_filter.BloomFilter.hash_functions", "tanuki.bloom_filter.BloomFilter.size"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_bloom_filter.py::test_bloom_filter_persistence"], "requirement": {"Functionality": "The lookup function checks if a given string is possibly in the Bloom Filter by using its hash functions to generate indices and then checking if all bits at these indices are set in the filter's bit array. If any bit is not set, it returns False, indicating the string is definitely not in the filter. Otherwise, it returns True, indicating the string might be in the filter (with a possibility of false positives).\n", "Arguments": ":param self: BloomFilter. An instance of the BloomFilter class. It uses the instance's hash_functions, hash_count, size, and bit_array attributes to perform the lookup.\n:param string: str, The string to check for presence in the Bloom Filter. It is used as input to the hash functions to generate indices in the bit array.\n:return: bool, Indicates whether the string is possibly in the Bloom Filter (True) or definitely not in the Bloom Filter (False)."}, "indent": 8}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "type": "method", "completion_path": "tanuki_py/src/tanuki/models/function_config.py", "signature_position": [34, 34], "body_position": [42, 49], "dependency": {"intra_class": ["tanuki.models.function_config.FunctionConfig.current_model_stats", "tanuki.models.function_config.FunctionConfig.current_training_run", "tanuki.models.function_config.FunctionConfig.distilled_model", "tanuki.models.function_config.FunctionConfig.last_training_run", "tanuki.models.function_config.FunctionConfig.nr_of_training_runs", "tanuki.models.function_config.FunctionConfig.teacher_models"], "intra_file": ["tanuki.models.function_config.config_factory"], "cross_file": ["tanuki.constants.DISTILLED_MODEL", "tanuki.constants.TEACHER_MODEL", "tanuki.language_models.llm_configs.model_config_factory.ModelConfigFactory.create_config"]}, "tests": ["tests/test_func_configs.py::test_update_config_various", "tests/test_func_configs.py::test_update_config_from_string"], "requirement": {"Functionality": "This function loads configuration settings for a function from a given dictionary. It updates the instance of FunctionConfig with various attributes such as distilled model, current model stats, last training run, current training run, number of training runs, and teacher models based on the provided dictionary.\n", "Arguments": ":param self: FunctionConfig. An instance of the FunctionConfig class. It is used to update the instance with configuration settings from the dictionary.\n:param json_dict: dict, The dictionary from which to load the function configuration. It should contain keys like 'distilled_model', 'current_model_stats', 'last_training_run', 'current_training_run', 'nr_of_training_runs', and optionally 'teacher_models'.\n:return: FunctionConfig. The instance of FunctionConfig class after updating it with the configuration settings from the provided dictionary. No new object is created; the method updates the existing instance."}, "indent": 8}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "type": "method", "completion_path": "tanuki_py/src/tanuki/language_models/openai_api.py", "signature_position": [60, 60], "body_position": [70, 142], "dependency": {"intra_class": ["tanuki.language_models.openai_api.OpenAI_API.api_key", "tanuki.language_models.openai_api.OpenAI_API.check_api_key"], "intra_file": ["tanuki.language_models.openai_api.LLM_GENERATION_PARAMETERS", "tanuki.language_models.openai_api.OPENAI_URL"], "cross_file": []}, "tests": ["tests/test_model_providers/test_openai_api.py::TestOpenAI_API::test_invalid_api_key"], "requirement": {"Functionality": "This function generates a response from the OpenAI API based on the provided model, system message, prompt, and additional parameters. It handles API key verification, parameter validation, and retries on failure up to 5 times with exponential backoff. It also processes the response to remove any parsing helper tokens before returning the final text.\n", "Arguments": ":param self: OpenAI_API. An instance of the OpenAI_API class.\n:param model: OpenAIConfig, The model configuration to use for generation, including the model name and any parsing helper tokens.\n:param system_message: str, The system message to include in the generation request.\n:param prompt: str, The user prompt to include in the generation request.\n:param **kwargs: dict, Additional optional parameters for generation such as temperature, top_p, frequency_penalty, presence_penalty, and max_new_tokens.\n:return: str, The generated text response from the OpenAI API after removing any parsing helper tokens."}, "indent": 8}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "type": "function", "completion_path": "skfolio/src/skfolio/utils/stats.py", "signature_position": [220, 220], "body_position": [232, 234], "dependency": {"intra_class": [], "intra_file": ["skfolio.utils.stats.assert_is_square"], "cross_file": []}, "tests": ["tests/test_utils/test_stats.py::TestAssertIsSymmetric::test_non_square_matrix"], "requirement": {"Functionality": "This function checks if a given matrix is symmetric. It first verifies if the matrix is square and then checks if the matrix is equal to its transpose. If the matrix is not symmetric, it raises a ValueError.\n", "Arguments": ":param x: np.ndarray. The matrix to be checked for symmetry. It should be a 2-dimensional numpy array.\n:return: No return values. Raises a ValueError if the matrix is not symmetric."}, "indent": 4}
{"namespace": "skfolio.utils.stats.assert_is_distance", "type": "function", "completion_path": "skfolio/src/skfolio/utils/stats.py", "signature_position": [237, 237], "body_position": [249, 253], "dependency": {"intra_class": [], "intra_file": ["skfolio.utils.stats.assert_is_symmetric"], "cross_file": []}, "tests": ["tests/test_utils/test_stats.py::TestAssertIsDistance::test_non_square_matrix"], "requirement": {"Functionality": "This function checks if the given matrix is a distance matrix by ensuring it is symmetric and its diagonal elements are close to zero. If these conditions are not met, it raises a ValueError.\n", "Arguments": ":param x: np.ndarray. The matrix to be checked if it's a distance matrix. It should be a square matrix (n, n) where 'n' is the number of elements.\n:return: No return values. However, it raises a ValueError if the matrix is not a distance matrix by the defined criteria."}, "indent": 4}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "type": "method", "completion_path": "tanuki_py/src/tanuki/language_models/language_model_manager.py", "signature_position": [124, 124], "body_position": [130, 164], "dependency": {"intra_class": ["tanuki.language_models.language_model_manager.LanguageModelManager.choose_model_from_tokens", "tanuki.language_models.language_model_manager.LanguageModelManager.construct_prompt", "tanuki.language_models.language_model_manager.LanguageModelManager.default_generation_length", "tanuki.language_models.language_model_manager.LanguageModelManager.function_modeler", "tanuki.language_models.language_model_manager.LanguageModelManager.initialized_functions", "tanuki.language_models.language_model_manager.LanguageModelManager.suitable_for_finetuning_token_check"], "intra_file": [], "cross_file": ["tanuki.function_modeler.FunctionModeler.get_models", "tanuki.function_modeler.FunctionModeler.get_symbolic_alignments", "tanuki.utils.approximate_token_count"]}, "tests": ["tests/test_token_counter.py::test_token_counter_non_finetunable_1", "tests/test_token_counter.py::test_error_raise"], "requirement": {"Functionality": "This function determines the appropriate language model and prompt for a given function description and its arguments. It decides whether to use a distilled model for zero-shot prompting or a teacher model for fine-tuning based on the suitability for distillation and token count requirements. It initializes function-specific data if not already done, updates examples for fine-tuning if necessary, and constructs the prompt to be used for generation.\n", "Arguments": ":param self: LanguageModelManager. An instance of the LanguageModelManager class.\n:param args: Tuple. The arguments passed to the function for which the generation case is being determined.\n:param kwargs: Dictionary. The keyword arguments passed to the function for which the generation case is being determined.\n:param function_description: Object. An object containing the description of the function for which the generation case is being determined. It is used to identify the model and construct the prompt.\n:param llm_parameters: Dictionary. Parameters for the language model generation, such as the maximum number of new tokens to generate.\n:param func_hash: Hashable. A unique identifier for the function, used to track initialization and examples for fine-tuning.\n:return: A tuple containing the constructed prompt, the selected model, a boolean indicating if the model is suitable for distillation, and a boolean indicating if the function is already initialized and does not require saving examples for fine-tuning."}, "indent": 8}
{"namespace": "skfolio.utils.stats.cov_nearest", "type": "function", "completion_path": "skfolio/src/skfolio/utils/stats.py", "signature_position": [304, 304], "body_position": [344, 376], "dependency": {"intra_class": [], "intra_file": ["skfolio.utils.stats._CLIPPING_VALUE", "skfolio.utils.stats.assert_is_square", "skfolio.utils.stats.assert_is_symmetric", "skfolio.utils.stats.corr_to_cov", "skfolio.utils.stats.cov_to_corr", "skfolio.utils.stats.is_cholesky_dec", "skfolio.utils.stats.is_positive_definite"], "cross_file": []}, "tests": ["tests/test_utils/test_stats.py::test_cov_nearest_cov_non_psd", "tests/test_utils/test_stats.py::test_corr_nearest_non_psd", "tests/test_utils/test_stats.py::TestCovNearest::test_raise_value_error_if_input_covariance_matrix_not_square"], "requirement": {"Functionality": "This function computes the nearest covariance matrix that is positive definite and allows for a Cholesky decomposition, keeping the variance unchanged. It either uses the Higham & Nick (2002) algorithm or clips eigenvalues based on the specified parameters to ensure the resulting matrix is positive definite.\n", "Arguments": ":param cov: np.ndarray. The input covariance matrix to be adjusted. It is used as the base for computing the nearest positive definite covariance matrix.\n:param higham: bool, default=False. Determines the method used to find the nearest positive definite matrix. If True, the Higham & Nick (2002) algorithm is used; otherwise, eigenvalues are clipped.\n:param higham_max_iteration: int, default=100. Specifies the maximum number of iterations for the Higham & Nick (2002) algorithm when it is used.\n:return: np.ndarray. The nearest covariance matrix that is positive definite and allows for a Cholesky decomposition.\n\nReferences:\n- \"Computing the nearest correlation matrix - a problem from finance\" IMA Journal of Numerical Analysis Higham & Nick (2002)"}, "indent": 4}
{"namespace": "skfolio.datasets._base.clear_data_home", "type": "function", "completion_path": "skfolio/src/skfolio/datasets/_base.py", "signature_position": [57, 57], "body_position": [66, 67], "dependency": {"intra_class": [], "intra_file": ["skfolio.datasets._base.get_data_home"], "cross_file": []}, "tests": ["tests/test_dataset/test_dataset.py::TestClearDataHome::test_delete_content_default_path"], "requirement": {"Functionality": "Deletes all the content of the data home cache directory. It first determines the correct data home directory path and then removes all its contents.\n", "Arguments": ":param data_home: str or path-like, optional. The path to the scikit-learn data directory. If not provided, it defaults to `~/skfolio_data`. This parameter specifies the location of the cache to be cleared.\n:return: No return values."}, "indent": 4}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "type": "function", "completion_path": "UniRef/detectron2/export/flatten.py", "signature_position": [158, 158], "body_position": [168, 183], "dependency": {"intra_class": [], "intra_file": ["detectron2.export.flatten.DictSchema", "detectron2.export.flatten.IdentitySchema", "detectron2.export.flatten.IdentitySchema.flatten", "detectron2.export.flatten.InstancesSchema", "detectron2.export.flatten.ListSchema", "detectron2.export.flatten.TensorWrapSchema", "detectron2.export.flatten.TupleSchema"], "cross_file": []}, "tests": ["tests/test_export_torchscript.py::TestTorchscriptUtils::test_flatten_instances_boxes"], "requirement": {"Functionality": "Flattens an object into a tuple format suitable for PyTorch tracing and provides a schema to rebuild the original object. This is useful for serialization and deserialization of complex objects in a format that PyTorch can work with.\n", "Arguments": ":param obj: The object to be flattened. It can be of various types including str, bytes, list, tuple, mapping collections, Instances, Boxes, or ROIMasks. The type of the object determines how it will be flattened.\n:return: A tuple containing two elements. The first element is the flattened result (res) which can be used as tracing outputs in PyTorch. The second element is a schema object which has a `__call__` method. This method can reconstruct the original object from the flattened result. The schema is essentially a dataclass that can be serialized."}, "indent": 4}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "type": "function", "completion_path": "skfolio/src/skfolio/utils/equations.py", "signature_position": [18, 24], "body_position": [85, 113], "dependency": {"intra_class": [], "intra_file": ["skfolio.utils.equations._string_to_equation"], "cross_file": ["skfolio.exceptions.GroupNotFoundError"]}, "tests": ["tests/test_prior/test_black_litterman.py::test_views_to_matrix"], "requirement": {"Functionality": "This function converts a list of linear equations into the left and right matrices of the inequality A <= B, based on the provided groups and equations. It is useful for mathematical and financial modeling, particularly in scenarios where asset groups and their relationships are analyzed through linear inequalities.\n", "Arguments": ":param groups: array-like of shape (n_groups, n_assets). It represents 2D array of assets groups, where each row can be a different categorization of assets (e.g., by type or region).\n:param equations: array-like of shape (n_equations,). It is a 1D array of string representations of linear equations that describe the relationships between different groups of assets.\n:param sum_to_one: bool, default=False. Indicates whether all elements in a group should sum to one. This is a constraint often used in financial models like the Black-Litterman model.\n:param raise_if_group_missing: bool, default=False. Determines whether to raise an error if a group mentioned in the equations is not found in the groups array. If False, a warning is issued instead.\n:param names: tuple[str, str], default=('groups', 'equations'). Names for the 'groups' and 'equations' parameters used in error messages, to improve clarity.\n:return: tuple[np.ndarray, np.ndarray]. The function returns two numpy arrays. The first array (left) is of shape (n_equations, n_assets) representing the left side of the inequality A <= B, and the second array (right) is of shape (n_equations,) representing the right side of the inequality. If none of the groups in the equations are part of the input groups, `None` is returned.\n"}, "indent": 4}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "type": "function", "completion_path": "UniRef/detectron2/export/torchscript_patch.py", "signature_position": [51, 51], "body_position": [58, 87], "dependency": {"intra_class": [], "intra_file": ["detectron2.export.torchscript_patch._add_instances_conversion_methods", "detectron2.export.torchscript_patch._clear_jit_cache", "detectron2.export.torchscript_patch._gen_instance_module", "detectron2.export.torchscript_patch._import"], "cross_file": []}, "tests": ["tests/modeling/test_roi_heads.py::ROIHeadsTest::test_keypoint_head_scriptability"], "requirement": {"Functionality": "This function acts as a context manager to temporarily replace the 'Instances' class in detectron2 with a new, statically-typed, scriptable class defined by the input 'fields'. It generates a new module for the class, writes it to a temporary file, imports it, and sets up the environment so that torchscript can recognize and work with this new class. After exiting the context, it cleans up by removing modifications.\n", "Arguments": ":param fields: List of tuples or similar structure. Defines the fields of the new 'Instances' class, where each element in the list represents a field in the class.\n:return: Yields a new class that replaces 'Instances' for the duration of the context. No return value outside of the context manager's yield."}, "indent": 4}
{"namespace": "detectron2.data.detection_utils.read_image", "type": "function", "completion_path": "UniRef/detectron2/data/detection_utils.py", "signature_position": [166, 166], "body_position": [180, 185], "dependency": {"intra_class": [], "intra_file": ["detectron2.data.detection_utils._apply_exif_orientation", "detectron2.data.detection_utils.convert_PIL_to_numpy"], "cross_file": ["detectron2.utils.file_io.PathManager"]}, "tests": ["tests/data/test_detection_utils.py::TestTransformAnnotations::test_read_exif_orientation"], "requirement": {"Functionality": "Reads an image from a file, applies any necessary orientation corrections based on the image's EXIF data, and converts it to a specified format.\n", "Arguments": ":param file_name: str, the path to the image file that needs to be read.\n:param format: str, optional, specifies the desired format for the output image. It can be one of the supported image modes in PIL (Python Imaging Library), \"BGR\", or \"YUV-BT.601\". If not provided, the image's original format is used.\n:return: np.ndarray, the image after being read, possibly oriented correctly, and converted to the specified format. The data type of the array is uint8 for 0-255 ranges in supported PIL modes or \"BGR\", and float for \"YUV-BT.601\" with a range of 0-1 for Y."}, "indent": 4}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "type": "function", "completion_path": "UniRef/detectron2/data/detection_utils.py", "signature_position": [257, 259], "body_position": [281, 318], "dependency": {"intra_class": [], "intra_file": ["detectron2.data.detection_utils.transform_keypoint_annotations"], "cross_file": []}, "tests": ["tests/data/test_detection_utils.py::TestTransformAnnotations::test_flip_keypoints", "tests/data/test_detection_utils.py::TestTransformAnnotations::test_transform_RLE"], "requirement": {"Functionality": "This function applies specified transformations to the bounding box, segmentation, and keypoints annotations of a single instance in an image. It modifies the input annotation dictionary in-place to include the transformed annotations. The function supports transforming bounding boxes to a specific format, applying transformations to segmentation polygons or run-length encodings (RLE), and transforming keypoints based on the provided transformations and image size.\n", "Arguments": ":param annotation: dict, the annotations of a single instance which include bounding box, segmentation, and keypoints. This dictionary is modified in-place with transformed annotations.\n:param transforms: TransformList or list[Transform], the transformations to be applied to the annotations. These can include transformations for boxes, coordinates for segmentation polygons, and keypoints.\n:param image_size: tuple, the height and width of the image after transformation. This is used to clip the transformed bounding box and ensure segmentation masks match the transformed image size.\n:param keypoint_hflip_indices: ndarray[int], optional, indices to handle flipping of keypoints horizontally. This is used when applying horizontal flip transformations to keypoints.\n:return: dict, the same input dictionary with transformed \"bbox\", \"segmentation\", \"keypoints\" annotations, and the \"bbox_mode\" field set to XYXY_ABS."}, "indent": 4}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "type": "method", "completion_path": "UniRef/detectron2/data/transforms/transform.py", "signature_position": [210, 210], "body_position": [214, 217], "dependency": {"intra_class": ["detectron2.data.transforms.transform.RotationTransform.rm_coords"], "intra_file": [], "cross_file": []}, "tests": ["tests/data/test_rotation_transform.py::TestRotationTransform::test_inverse_transform"], "requirement": {"Functionality": "Applies a rotation transformation to a set of coordinates. This function transforms each coordinate in the input array according to a predefined rotation matrix associated with the instance. If the rotation angle is a multiple of 360 degrees or the input is empty, the original coordinates are returned without modification.\n", "Arguments": ":param self: RotationTransform. An instance of the RotationTransform class, which should have a predefined rotation matrix (self.rm_coords) and an angle attribute.\n:param coords: array-like, a N * 2 array containing N pairs of (x, y) points that represent the coordinates to be transformed. These coordinates are expected to undergo a rotation transformation based on the instance's rotation matrix.\n:return: array-like, a N * 2 array of transformed coordinates. If the input coordinates are empty or the rotation angle is a multiple of 360 degrees, the original coordinates are returned."}, "indent": 8}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "type": "function", "completion_path": "UniRef/detectron2/utils/analysis.py", "signature_position": [71, 71], "body_position": [96, 100], "dependency": {"intra_class": [], "intra_file": ["detectron2.utils.analysis.FlopCountAnalysis", "detectron2.utils.analysis.FlopCountAnalysis.__init__"], "cross_file": []}, "tests": ["tests/test_model_analysis.py::FasterRCNNTest::test_flop", "tests/test_model_analysis.py::MaskRCNNTest::test_flop"], "requirement": {"Functionality": "This function calculates the floating-point operations (flops) count for each operator in a given model using just-in-time (jit) compilation. It is specifically designed to support standard detection models in detectron2 by running the model with a given input and computing the flops. It emphasizes that the computed flops might vary with different inputs, suggesting averaging across multiple inputs for more accurate estimation.\n", "Arguments": ":param model: nn.Module. A detectron2 model that is expected to take a list of dictionaries as input. The model is used to compute the flops by running it with the provided inputs.\n:param inputs: list of dictionaries. These are the inputs to the model, adhering to detectron2's standard input format. Only the \"image\" key within these dictionaries is utilized for the flops computation.\n:return: DefaultDict[str, float]. A dictionary where keys are operator names and values are the corresponding Gflop counts. This provides an overview of the computational cost associated with each operator in the model."}, "indent": 4}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "type": "method", "completion_path": "UniRef/detectron2/data/transforms/transform.py", "signature_position": [200, 200], "body_position": [204, 208], "dependency": {"intra_class": ["detectron2.data.transforms.transform.RotationTransform.rm_image"], "intra_file": [], "cross_file": []}, "tests": ["tests/data/test_rotation_transform.py::TestRotationTransform::test_inverse_transform"], "requirement": {"Functionality": "Applies a rotation transformation to an image represented as a numpy array. The transformation is based on a predefined angle stored in the instance. If the image is empty or the angle results in no change (multiples of 360 degrees), the original image is returned unchanged. Otherwise, the image is rotated using OpenCV's warpAffine function.\n", "Arguments": ":param self: RotationTransform. An instance of the RotationTransform class, which contains the rotation matrix (rm_image), the original image dimensions (h, w), the bounding dimensions after rotation (bound_w, bound_h), and the default interpolation method (interp).\n:param img: numpy array, The input image to be transformed, expected to be in the format Height * Width * Nchannels.\n:param interp: [optional] The interpolation method to be used for the rotation. If not provided, the instance's default interpolation method is used.\n:return: numpy array. The rotated image, which may have different dimensions from the input image depending on the rotation angle and the bounding dimensions."}, "indent": 8}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "type": "method", "completion_path": "UniRef/detectron2/utils/visualizer.py", "signature_position": [390, 390], "body_position": [402, 441], "dependency": {"intra_class": ["detectron2.utils.visualizer.Visualizer._create_grayscale_image", "detectron2.utils.visualizer.Visualizer._instance_mode", "detectron2.utils.visualizer.Visualizer._jitter", "detectron2.utils.visualizer.Visualizer.metadata", "detectron2.utils.visualizer.Visualizer.output", "detectron2.utils.visualizer.Visualizer.overlay_instances"], "intra_file": ["detectron2.utils.visualizer.ColorMode", "detectron2.utils.visualizer.ColorMode.SEGMENTATION", "detectron2.utils.visualizer.GenericMask", "detectron2.utils.visualizer.GenericMask.__init__", "detectron2.utils.visualizer.VisImage.height", "detectron2.utils.visualizer.VisImage.reset_image", "detectron2.utils.visualizer.VisImage.width", "detectron2.utils.visualizer._create_text_labels"], "cross_file": ["detectron2.utils.visualizer.ColorMode.IMAGE_BW"]}, "tests": ["tests/test_visualizer.py::TestVisualizer::test_BWmode_nomask", "tests/test_visualizer.py::TestVisualizer::test_draw_no_metadata"], "requirement": {"Functionality": "This function visualizes instance-level prediction results on an image. It processes the predictions to extract and visualize bounding boxes, classes, scores, masks, and keypoints. Depending on the instance mode, it may also adjust the visualization style (e.g., segmentation colors, grayscale image).\n", "Arguments": ":param self: Visualizer. An instance of the Visualizer class, which contains methods and properties for visualizing prediction results.\n:param predictions: Instances. The output of an instance detection/segmentation model. It uses fields like \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" or \"pred_masks_rle\" to draw the visualizations on the image.\n:return: VisImage. An image object with the visualizations drawn on it."}, "indent": 8}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "type": "method", "completion_path": "UniRef/detectron2/utils/visualizer.py", "signature_position": [317, 317], "body_position": [324, 335], "dependency": {"intra_class": ["detectron2.utils.visualizer.VisImage.canvas"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_visualizer.py::TestVisualizer::test_overlay_instances_no_boxes", "tests/test_visualizer.py::TestVisualizer::test_correct_output_shape", "tests/test_visualizer.py::TestVisualizer::test_draw_soft_mask", "tests/test_visualizer.py::TestVisualizer::test_border_polygons"], "requirement": {"Functionality": "This function retrieves the visualized image from the VisImage instance, converting it from RGBA to RGB format, and returns it as a numpy ndarray of uint8 type. The image dimensions are determined by the current state of the canvas associated with the VisImage instance.\n", "Arguments": ":param self: VisImage. An instance of the VisImage class, which contains the canvas attribute used to generate the visualized image.\n:return: ndarray. The visualized image in RGB format as a numpy ndarray of uint8 type. The shape of the returned image is (H, W, 3), where H and W are the height and width of the image, respectively."}, "indent": 8}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "type": "method", "completion_path": "UniRef/detectron2/utils/visualizer.py", "signature_position": [545, 545], "body_position": [555, 612], "dependency": {"intra_class": ["detectron2.utils.visualizer.Visualizer._instance_mode", "detectron2.utils.visualizer.Visualizer._jitter", "detectron2.utils.visualizer.Visualizer.draw_panoptic_seg", "detectron2.utils.visualizer.Visualizer.draw_sem_seg", "detectron2.utils.visualizer.Visualizer.metadata", "detectron2.utils.visualizer.Visualizer.output", "detectron2.utils.visualizer.Visualizer.overlay_instances"], "intra_file": ["detectron2.utils.visualizer.ColorMode", "detectron2.utils.visualizer.ColorMode.SEGMENTATION", "detectron2.utils.visualizer._create_text_labels"], "cross_file": ["detectron2.utils.file_io.PathManager"]}, "tests": ["tests/test_visualizer.py::TestVisualizer::test_draw_rotated_dataset_dict"], "requirement": {"Functionality": "This function visualizes the annotations/segmentations of an image based on the Detectron2 Dataset format. It handles different types of annotations including segmentation masks, keypoints, bounding boxes, semantic segmentation, and panoptic segmentation. The visualizations are drawn on the image, and the modified image object is returned.\n", "Arguments": ":param self: Visualizer. An instance of the Visualizer class, which contains methods for drawing different types of annotations on images.\n:param dic: dict, The annotation/segmentation data for a single image, formatted according to the Detectron2 Dataset specifications. This dictionary may contain keys for annotations, semantic segmentation, and panoptic segmentation, among others.\n:return: VisImage, The image object with the visualizations drawn on it. This includes any combination of segmentation masks, keypoints, bounding boxes, semantic segmentation, and panoptic segmentation based on the input dictionary.\n"}, "indent": 8}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "type": "method", "completion_path": "UniRef/detectron2/utils/visualizer.py", "signature_position": [1042, 1044], "body_position": [1061, 1091], "dependency": {"intra_class": ["detectron2.utils.visualizer.Visualizer._change_color_brightness", "detectron2.utils.visualizer.Visualizer._draw_text_in_mask", "detectron2.utils.visualizer.Visualizer.draw_polygon", "detectron2.utils.visualizer.Visualizer.output"], "intra_file": ["detectron2.utils.visualizer.GenericMask", "detectron2.utils.visualizer.GenericMask.__init__", "detectron2.utils.visualizer.GenericMask.has_holes", "detectron2.utils.visualizer.GenericMask.mask", "detectron2.utils.visualizer.GenericMask.polygons", "detectron2.utils.visualizer.VisImage.ax", "detectron2.utils.visualizer.VisImage.height", "detectron2.utils.visualizer.VisImage.width"], "cross_file": ["detectron2.utils.colormap.random_color"]}, "tests": ["tests/test_visualizer.py::TestVisualizer::test_border_mask_with_holes"], "requirement": {"Functionality": "This function draws a binary mask on an image. It supports drawing regular masks as polygons and masks with holes using a different approach. It can also draw text on the mask if specified. The function allows for customization of the mask's color, edge color, transparency, and the minimum area of components to be drawn.\n", "Arguments": ":param self: Visualizer. An instance of the Visualizer class.\n:param binary_mask: ndarray. A numpy array of shape (H, W) representing the binary mask, where H is the image height and W is the image width. Each value in the array is either 0 or 1 of uint8 type.\n:param color: Optional. The color of the mask. If None, a random color is chosen. Accepts formats listed in `matplotlib.colors`.\n:param edge_color: Optional. The color of the polygon edges. Accepts formats listed in `matplotlib.colors`.\n:param text: str, Optional. The text to be drawn on the mask. If None, no text is drawn.\n:param alpha: float, Optional. The blending efficiency for the mask, with smaller values leading to more transparent masks. Defaults to 0.5.\n:param area_threshold: float, Optional. The minimum area of connected components to be shown. Defaults to 10.\n:return: VisImage. The image object with the mask drawn on it."}, "indent": 8}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "type": "function", "completion_path": "UniRef/detectron2/utils/testing.py", "signature_position": [86, 86], "body_position": [93, 130], "dependency": {"intra_class": [], "intra_file": ["detectron2.utils.testing.convert_scripted_instances"], "cross_file": []}, "tests": ["tests/modeling/test_roi_heads.py::ROIHeadsTest::test_mask_head_scriptability", "tests/modeling/test_roi_heads.py::ROIHeadsTest::test_StandardROIHeads_scriptability"], "requirement": {"Functionality": "This function asserts that two instances of a custom class (presumably named 'Instances') are close or equal to each other in terms of their properties and values. It checks if the image sizes are the same (either as tuples or tensors, based on the input parameter) and verifies that all fields within the instances are equal or close to each other, with special handling for fields that are of type Boxes, ROIMasks, or torch.Tensor.\n", "Arguments": ":param input: Instances. The first instance to compare.\n:param other: Instances. The second instance to compare against the first.\n:param rtol: float, optional. The relative tolerance parameter for comparison of tensor fields.\n:param msg: str, optional. A custom message prefix to append to the default error messages.\n:param size_as_tensor: bool, optional. A flag indicating whether to compare the image_size of the Instances as tensors instead of tuples. This is useful for comparing outputs of tracing.\n:return: No return values. However, it raises an AssertionError if the instances do not match according to the specified criteria or a ValueError if it encounters a field type it does not know how to compare."}, "indent": 4}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "type": "method", "completion_path": "UniRef/detectron2/structures/rotated_boxes.py", "signature_position": [236, 236], "body_position": [243, 245], "dependency": {"intra_class": ["detectron2.structures.rotated_boxes.RotatedBoxes.tensor"], "intra_file": [], "cross_file": []}, "tests": ["tests/structures/test_rotated_boxes.py::TestRotatedBoxesStructure::test_clip_area_arbitrary_angle"], "requirement": {"Functionality": "Computes and returns the area of all the rotated boxes represented by the class instance. The area is calculated by multiplying the width and height of each box.\n", "Arguments": ":param self: RotatedBoxes. An instance of the RotatedBoxes class, which contains the boxes' data in a tensor.\n:return: torch.Tensor. A tensor that contains the computed area of each box."}, "indent": 8}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "type": "function", "completion_path": "UniRef/detectron2/modeling/proposal_generator/build.py", "signature_position": [15, 15], "body_position": [20, 24], "dependency": {"intra_class": [], "intra_file": ["detectron2.modeling.proposal_generator.build.PROPOSAL_GENERATOR_REGISTRY"], "cross_file": []}, "tests": ["tests/modeling/test_roi_heads.py::ROIHeadsTest::test_rroi_heads"], "requirement": {"Functionality": "Builds a proposal generator based on the configuration provided. If the configuration specifies \"PrecomputedProposals\" as the proposal generator name, it returns None, indicating no proposal generator is used. Otherwise, it retrieves and initializes a proposal generator from a registry using the specified name and configuration.\n", "Arguments": ":param cfg: Configuration object. The configuration that contains the proposal generator's name and other relevant settings.\n:param input_shape: The input shape for the proposal generator. It is used to initialize the proposal generator if one is created.\n:return: A proposal generator instance or None. If the proposal generator's name is \"PrecomputedProposals\", it returns None, indicating no proposal generator is used. Otherwise, it returns an instance of the specified proposal generator initialized with the given configuration and input shape."}, "indent": 4}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "type": "method", "completion_path": "UniRef/detectron2/modeling/roi_heads/fast_rcnn.py", "signature_position": [278, 278], "body_position": [289, 318], "dependency": {"intra_class": ["detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.box_reg_loss", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.loss_weight"], "intra_file": ["detectron2.modeling.roi_heads.fast_rcnn._log_classification_stats"], "cross_file": []}, "tests": ["tests/modeling/test_fast_rcnn.py::FastRCNNTest::test_fast_rcnn_rotated"], "requirement": {"Functionality": "This function calculates and returns the classification and box regression losses for object detection, based on the predictions from the model and the ground truth proposals.\n", "Arguments": ":param self: FastRCNNOutputLayers. An instance of the FastRCNNOutputLayers class.\n:param predictions: Tuple containing the scores and proposal deltas returned by the `forward()` method. The scores are used for calculating classification loss, and the proposal deltas are used for calculating box regression loss.\n:param proposals: list[Instances]. A list of Instances objects that match the features used to compute predictions. These objects are expected to contain the fields `proposal_boxes`, `gt_boxes`, and `gt_classes`, which are used for calculating the losses.\n:return: Dict[str, Tensor]. A dictionary of losses where the keys are strings indicating the type of loss ('loss_cls' for classification loss and 'loss_box_reg' for box regression loss) and the values are Tensors representing the calculated losses, scaled by their respective weights defined in `self.loss_weight`."}, "indent": 8}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "type": "function", "completion_path": "UniRef/detectron2/tracking/base_tracker.py", "signature_position": [53, 53], "body_position": [62, 64], "dependency": {"intra_class": [], "intra_file": ["detectron2.tracking.base_tracker.BaseTracker", "detectron2.tracking.base_tracker.TRACKER_HEADS_REGISTRY"], "cross_file": ["detectron2.config.config.CfgNode"]}, "tests": ["tests/tracking/test_vanilla_hungarian_bbox_iou_tracker.py::TestVanillaHungarianBBoxIOUTracker::test_from_config"], "requirement": {"Functionality": "Builds a tracker head object based on the configuration specified in the `cfg` parameter. It retrieves the tracker name from the configuration, looks up the corresponding tracker class from a registry, and then instantiates and returns an object of that class using the configuration.\n", "Arguments": ":param cfg: CfgNode. The configuration node containing tracker information, specifically the name of the tracker to be built.\n:return: BaseTracker. An instance of the tracker class as specified in the configuration."}, "indent": 4}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "type": "method", "completion_path": "UniRef/detectron2/modeling/box_regression.py", "signature_position": [78, 78], "body_position": [88, 116], "dependency": {"intra_class": ["detectron2.modeling.box_regression.Box2BoxTransform.scale_clamp", "detectron2.modeling.box_regression.Box2BoxTransform.weights"], "intra_file": [], "cross_file": []}, "tests": ["tests/modeling/test_box2box_transform.py::TestBox2BoxTransform::test_apply_deltas_tracing"], "requirement": {"Functionality": "This function applies transformation deltas (dx, dy, dw, dh) to a set of bounding boxes. It adjusts the position and size of each box based on the provided deltas, effectively decoding the predicted bounding box transformations.\n", "Arguments": ":param self: Box2BoxTransform. An instance of the Box2BoxTransform class.\n:param deltas: Tensor, the transformation deltas of shape (N, k*4), where k >= 1. Each delta[i] represents k potentially different class-specific box transformations for the single box boxes[i]. It is used to calculate the new positions and sizes of the boxes.\n:param boxes: Tensor, the original boxes to transform, of shape (N, 4). It provides the starting point for each box before applying the deltas.\n:return: Tensor, the transformed boxes with the same shape as the input deltas. This tensor represents the new positions and sizes of the boxes after applying the deltas."}, "indent": 8}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "type": "method", "completion_path": "scepter/scepter/modules/annotator/utils.py", "signature_position": [103, 103], "body_position": [104, 114], "dependency": {"intra_class": ["scepter.scepter.modules.annotator.utils.AnnotatorProcessor.general_ins"], "intra_file": [], "cross_file": []}, "tests": ["tests/tools/test_annotators.py::AnnotatorTest::test_annotator_processor"], "requirement": {"Functionality": "The function processes an image with a general annotation instruction and optionally filters the output based on the annotation type(s) specified. If a specific annotation type is requested and found in the output, only that annotation is returned. If multiple annotation types are requested, it returns a dictionary of those types found in the output. If no specific annotation type is requested, it returns the entire processed output.\n", "Arguments": ":param self: AnnotatorProcessor. An instance of the AnnotatorProcessor class.\n:param image: The image to be processed.\n:param anno_type: str or list of str, optional. The type(s) of annotations to filter the output by. If None, all annotations in the output are returned.\n:return: Depending on the anno_type parameter, this function can return a single annotation, a dictionary of annotations, or the entire processed output."}, "indent": 8}
{"namespace": "microsearch.engine.SearchEngine.search", "type": "method", "completion_path": "microsearch/src/microsearch/engine.py", "signature_position": [60, 60], "body_position": [61, 66], "dependency": {"intra_class": ["microsearch.engine.SearchEngine.bm25"], "intra_file": ["microsearch.engine.normalize_string", "microsearch.engine.update_url_scores"], "cross_file": []}, "tests": ["tests/test_engine.py::test_search_engine"], "requirement": {"Functionality": "The function performs a search based on the input query. It normalizes the query string, splits it into keywords, and then calculates the BM25 score for each keyword across URLs. The scores for URLs are aggregated and returned as a dictionary where each URL is a key and its aggregated score is the value.\n", "Arguments": ":param self: SearchEngine. An instance of the SearchEngine class.\n:param query: str, The search query input by the user. It is normalized and split into keywords for scoring.\n:return: dict[str, float], A dictionary where each key is a URL and its value is the aggregated BM25 score based on the input query's keywords."}, "indent": 8}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "type": "method", "completion_path": "microsearch/src/microsearch/engine.py", "signature_position": [76, 76], "body_position": [77, 78], "dependency": {"intra_class": ["microsearch.engine.SearchEngine.index"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_engine.py::test_search_engine"], "requirement": {"Functionality": "The bulk_index function takes a list of documents, where each document is represented as a tuple containing a URL and its corresponding content. It then indexes each document by calling the 'index' method with the URL and content of each document.\n", "Arguments": ":param self: SearchEngine. An instance of the SearchEngine class.\n:param documents: list[tuple[str, str]]. A list of tuples, where each tuple contains a URL (as a string) and its corresponding content (as a string). These documents are to be indexed.\n:return: No return values."}, "indent": 8}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "type": "method", "completion_path": "UniRef/detectron2/structures/rotated_boxes.py", "signature_position": [253, 253], "body_position": [277, 301], "dependency": {"intra_class": ["detectron2.structures.rotated_boxes.RotatedBoxes.normalize_angles", "detectron2.structures.rotated_boxes.RotatedBoxes.tensor"], "intra_file": [], "cross_file": []}, "tests": ["tests/structures/test_rotated_boxes.py::TestRotatedBoxesStructure::test_clip_area_arbitrary_angle"], "requirement": {"Functionality": "The function clips rotated boxes in place by limiting their x and y coordinates to ensure they fit within a specified box size. It specifically targets boxes that are almost horizontal, determined by a given angle threshold, to maintain backward compatibility. This is done to avoid the complexities and ambiguities involved in clipping highly rotated boxes.\n", "Arguments": ":param box_size: Tuple[int, int], The size of the box within which the rotated boxes are to be clipped, specified as (height, width).\n:param clip_angle_threshold: float, optional, The maximum absolute angle (in degrees) for which the clipping operation is performed on nearly horizontal boxes. Defaults to 1.0.\n:return: No return values. The function modifies the tensor attribute of the RotatedBoxes instance in place.\n\nAdditional Notes:\n- The function first normalizes the angles of the boxes to be within the range (-180, 180] degrees.\n- It then identifies the indices of the boxes that are nearly horizontal based on the clip_angle_threshold.\n- For these identified boxes, it converts their representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2), where (x1, y1) and (x2, y2) are the coordinates of the top-left and bottom-right corners, respectively.\n- The x and y coordinates are then clamped to ensure they do not exceed the specified box_size limits.\n- Finally, the boxes are converted back to their original representation, ensuring that any numerical errors do not increase their sizes."}, "indent": 8}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "type": "method", "completion_path": "UHGEval/uhgeval/dataset/xinhua.py", "signature_position": [32, 32], "body_position": [33, 36], "dependency": {"intra_class": ["xinhua.XinhuaHallucinations.data"], "intra_file": [], "cross_file": []}, "tests": ["tests/dataset/test_xinhua.py::TestXinhuaHallucinations::test_statistics"], "requirement": {"Functionality": "Calculates and returns the statistics of different types of objects contained in the XinhuaHallucinations instance's data. It counts the occurrences of each type ('doc', 'gen', 'kno', 'num') and updates the statistics accordingly.\n", "Arguments": ":param self: XinhuaHallucinations. An instance of the XinhuaHallucinations class. It uses the 'data' attribute of the instance, which is expected to be a list of dictionaries, each with a 'type' key.\n:return: dict. A dictionary with keys corresponding to the types ('doc', 'gen', 'kno', 'num') and values representing the count of each type found in the 'data' attribute of the instance."}, "indent": 8}
{"namespace": "mmdet3d.models.builder.build_neck", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/models/builder.py", "signature_position": [39, 39], "body_position": [41, 44], "dependency": {"intra_class": [], "intra_file": ["mmdet3d.models.builder.NECKS"], "cross_file": []}, "tests": ["tests/test_models/test_necks/test_necks.py::test_imvoxel_neck", "tests/test_models/test_necks/test_necks.py::test_dla_neck"], "requirement": {"Functionality": "Builds and returns a neck model component based on the configuration provided. It checks if the specified neck type is available in the NECKS module dictionary; if so, it builds the neck using NECKS. Otherwise, it builds the neck using MMDET_NECKS.\n", "Arguments": ":param cfg: Dict. The configuration dictionary for the neck model, which must include a 'type' key that specifies the type of neck to be built.\n:return: Object. The built neck model instance."}, "indent": 4}
{"namespace": "mmdet3d.models.builder.build_loss", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/models/builder.py", "signature_position": [71, 71], "body_position": [73, 78], "dependency": {"intra_class": [], "intra_file": ["mmdet3d.models.builder.LOSSES"], "cross_file": []}, "tests": ["tests/test_metrics/test_losses.py::test_multibin_loss"], "requirement": {"Functionality": "Builds and returns a loss function based on the configuration provided. It checks the type of loss specified in the configuration and selects the appropriate loss function from predefined libraries or modules.", "Arguments": ":param cfg: Dict. The configuration dictionary containing the type of loss function to be built and possibly other relevant settings.\n:return: A loss function object. The specific loss function built according to the provided configuration."}, "indent": 4}
{"namespace": "mmdet3d.models.builder.build_head", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/models/builder.py", "signature_position": [63, 63], "body_position": [65, 68], "dependency": {"intra_class": [], "intra_file": ["mmdet3d.models.builder.HEADS"], "cross_file": []}, "tests": ["tests/test_models/test_heads/test_paconv_decode_head.py::test_paconv_decode_head_loss", "tests/test_models/test_heads/test_heads.py::test_point_rcnn_rpnhead_getboxes", "tests/test_models/test_heads/test_heads.py::test_smoke_mono3d_head", "tests/test_models/test_heads/test_heads.py::test_point_rcnn_bbox_head", "tests/test_models/test_heads/test_heads.py::test_point_rcnn_roi_head", "tests/test_models/test_heads/test_heads.py::test_primitive_head", "tests/test_models/test_heads/test_heads.py::test_center_head", "tests/test_models/test_heads/test_heads.py::test_ssd3d_head", "tests/test_models/test_heads/test_heads.py::test_groupfree3d_head", "tests/test_models/test_heads/test_heads.py::test_monoflex_head", "tests/test_models/test_heads/test_pointnet2_decode_head.py::test_pn2_decode_head_loss"], "requirement": {"Functionality": "Builds and returns a head object based on the configuration provided. It checks if the specified head type is available in the HEADS module dictionary. If it is, it builds the head using the HEADS module; otherwise, it uses the MMDET_HEADS module to build the head.\n", "Arguments": ":param cfg: Dict. The configuration dictionary that contains the type of head to be built and possibly other relevant configuration information.\n:return: An instance of the head object built based on the provided configuration."}, "indent": 4}
{"namespace": "mmdet3d.models.builder.build_segmentor", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/models/builder.py", "signature_position": [99, 99], "body_position": [101, 110], "dependency": {"intra_class": [], "intra_file": ["mmdet3d.models.builder.SEGMENTORS"], "cross_file": []}, "tests": ["tests/test_models/test_segmentors.py::test_pointnet2_msg", "tests/test_models/test_segmentors.py::test_paconv_cuda_ssg"], "requirement": {"Functionality": "Builds a segmentor model based on the given configuration. It warns the user if the training or testing configurations are specified both in the function arguments and the model configuration, and ensures that these configurations are not duplicated.\n", "Arguments": ":param cfg: Dict. The configuration dictionary for the segmentor model. It is used to configure the model.\n:param train_cfg: Dict, optional. The training configuration. If specified, it should not be duplicated in the cfg dictionary.\n:param test_cfg: Dict, optional. The testing configuration. If specified, it should not be duplicated in the cfg dictionary.\n:return: A segmentor model instance. The segmentor model built based on the provided configurations."}, "indent": 4}
{"namespace": "mmdet3d.models.builder.build_detector", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/models/builder.py", "signature_position": [81, 81], "body_position": [83, 96], "dependency": {"intra_class": [], "intra_file": ["mmdet3d.models.builder.DETECTORS"], "cross_file": []}, "tests": ["tests/test_models/test_detectors.py::test_voxel_net", "tests/test_models/test_detectors.py::test_vote_net", "tests/test_models/test_detectors.py::test_centerpoint", "tests/test_models/test_detectors.py::test_groupfree3dnet", "tests/test_models/test_detectors.py::test_point_rcnn", "tests/test_models/test_detectors.py::test_sassd"], "requirement": {"Functionality": "Builds a detector model based on the configuration provided. It warns the user if train_cfg or test_cfg is passed directly to the function since these configurations should be specified in the model. It asserts to ensure that train_cfg and test_cfg are not specified in both the outer field and model field. Depending on the type of detector specified in the cfg, it builds the detector using either the DETECTORS or MMDET_DETECTORS registry.\n", "Arguments": ":param cfg: Dict. The configuration dictionary for the detector model. It must include the 'type' of the detector and may include 'train_cfg' and 'test_cfg' among other settings.\n:param train_cfg: Dict, optional. The training configuration. If specified, it should not be duplicated in the cfg dictionary.\n:param test_cfg: Dict, optional. The testing configuration. If specified, it should not be duplicated in the cfg dictionary.\n:return: An instance of the built detector model. The specific type of the model depends on the 'type' specified in the cfg dictionary."}, "indent": 4}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/core/evaluation/indoor_eval.py", "signature_position": [203, 209], "body_position": [231, 309], "dependency": {"intra_class": [], "intra_file": ["mmdet3d.core.evaluation.indoor_eval.eval_map_recall"], "cross_file": []}, "tests": ["tests/test_metrics/test_indoor_eval.py::test_indoor_eval_less_classes"], "requirement": {"Functionality": "This function evaluates the detection results by comparing the detection annotations with the ground truth annotations. It calculates the mean Average Precision (mAP) and mean Average Recall (mAR) for different IoU thresholds and organizes the evaluation results into a dictionary and a formatted table for logging.\n", "Arguments": ":param gt_annos: list[dict]. Ground truth annotations containing information about the true bounding boxes and their labels.\n:param dt_annos: list[dict]. Detection annotations containing detected bounding boxes, their labels, and scores.\n:param metric: list[float]. A list of IoU thresholds used for computing average precisions and recalls.\n:param label2cat: dict. A mapping from numerical labels to category names.\n:param logger: logging.Logger or str, optional. Specifies how the mAP summary is printed. If None, the default logging method is used.\n:param box_type_3d: class, optional. The type of 3D bounding box used, which determines how the boxes are converted and processed.\n:param box_mode_3d: str, optional. The mode or format in which the 3D bounding boxes are represented.\n:return: dict[str, float]. A dictionary containing the evaluation results, including class-wise AP and AR for each IoU threshold, as well as overall mAP and mAR.\n"}, "indent": 4}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "type": "function", "completion_path": "Generalizable-BEV/mmdet3d/core/bbox/structures/utils.py", "signature_position": [142, 142], "body_position": [156, 172], "dependency": {"intra_class": [], "intra_file": ["mmdet3d.core.bbox.structures.box_3d_mode.Box3DMode.CAM", "mmdet3d.core.bbox.structures.box_3d_mode.Box3DMode.DEPTH", "mmdet3d.core.bbox.structures.box_3d_mode.Box3DMode.LIDAR"], "cross_file": []}, "tests": ["tests/test_utils/test_box3d.py::test_get_box_type::test_bad_box_type"], "requirement": {"Functionality": "Determines the specific 3D box type and mode based on the given box structure type. It maps a string identifier to a corresponding 3D box class and mode, raising an error if the identifier is not recognized.\n", "Arguments": ":param box_type: str, The identifier for the type of box structure. It should be one of \"LiDAR\", \"Camera\", or \"Depth\". This parameter is used to select the appropriate 3D box class and mode.\n:return: tuple, A pair consisting of the 3D box class and its mode corresponding to the input box type. The first element of the tuple is the class representing the 3D box type, and the second element is the mode indicating the context (LiDAR, Camera, or Depth) in which the box is used."}, "indent": 4}
{"namespace": "ollama._client.Client.chat", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [145, 153], "body_position": [164, 189], "dependency": {"intra_class": ["ollama._client.Client._request_stream"], "intra_file": ["ollama._client._encode_image"], "cross_file": ["ollama._types.Message", "ollama._types.Options", "ollama._types.RequestError"]}, "tests": ["tests/test_client.py::test_client_chat_stream"], "requirement": {"Functionality": "The function initiates a chat response using a specified model. It validates the input parameters, raises errors for invalid inputs, and sends a request to an API endpoint to create a chat response. Depending on the 'stream' parameter, it either returns a single ChatResponse or a generator for ChatResponses.\n", "Arguments": ":param self: Client. An instance of the Client class.\n:param model: str, optional. The model identifier used for generating chat responses. It is required and used to specify which model to use for the chat.\n:param messages: Optional[Sequence[Message]], optional. A sequence of messages (or dict-like objects) to be included in the chat. Each message must contain a 'role' (one of 'system', 'user', 'assistant') and 'content'. If 'images' are included, they are encoded.\n:param stream: bool, optional, default False. Determines the type of response: if False, a single ChatResponse is returned; if True, a generator of ChatResponses is returned.\n:param format: Literal['', 'json'], optional, default ''. Specifies the format of the response. Currently supports '' (default) and 'json'.\n:param options: Optional[Options], optional. Additional options for the chat request.\n:param keep_alive: Optional[Union[float, str]], optional. Specifies the keep-alive parameter for the chat session.\n:return: Union[Mapping[str, Any], Iterator[Mapping[str, Any]]]. Returns a single ChatResponse if 'stream' is False, otherwise returns a generator of ChatResponses.\n\nRaises `RequestError` if a model is not provided or if messages do not meet the required structure.\nRaises `TypeError` if messages are not a list of Message or dict-like objects.\nRaises `ResponseError` if the request could not be fulfilled."}, "indent": 4}
{"namespace": "ollama._client.Client.pull", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [209, 214], "body_position": [220, 229], "dependency": {"intra_class": ["ollama._client.Client._request_stream"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_client.py::test_client_pull_stream"], "requirement": {"Functionality": "The function sends a POST request to pull a model from a server. It can operate in either a streaming or non-streaming mode based on the `stream` parameter. If the request fails, it raises a `ResponseError`. Depending on the `stream` parameter, it returns either a single `ProgressResponse` or an iterator of `ProgressResponse` objects.\n", "Arguments": ":param self: Client. An instance of the Client class.\n:param model: str, The name of the model to be pulled from the server.\n:param insecure: bool, Optional. Specifies whether the request should be made over an insecure connection. Defaults to False.\n:param stream: bool, Optional. Determines the mode of the response. If True, the response is streamed and an iterator of `ProgressResponse` objects is returned. If False, a single `ProgressResponse` object is returned. Defaults to False.\n:return: Union[Mapping[str, Any], Iterator[Mapping[str, Any]]], The function returns a `ProgressResponse` if `stream` is False. If `stream` is True, it returns an iterator of `ProgressResponse` objects."}, "indent": 4}
{"namespace": "ollama._client.Client.generate", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [99, 112], "body_position": [123, 143], "dependency": {"intra_class": ["ollama._client.Client._request_stream"], "intra_file": ["ollama._client._encode_image"], "cross_file": ["ollama._types.Options", "ollama._types.RequestError"]}, "tests": ["tests/test_client.py::test_client_generate_stream"], "requirement": {"Functionality": "The function generates a response based on the specified model and parameters. It raises exceptions if the model is not provided or if the request cannot be fulfilled. Depending on the 'stream' parameter, it either returns a single response or a generator for streaming responses.\n", "Arguments": ":param self: Client. An instance of the Client class.\n:param model: str, optional, default is an empty string. The model to use for generating a response. It is required, and an exception is raised if not provided.\n:param prompt: str, optional, default is an empty string. The prompt to pass to the model for generating a response.\n:param system: str, optional, default is an empty string. The system identifier to use for the request.\n:param template: str, optional, default is an empty string. The template identifier to use for formatting the response.\n:param context: Optional[Sequence[int]], optional. A sequence of integers representing the context for the request. Defaults to None, which is treated as an empty list.\n:param stream: bool, optional, default is False. Determines whether the response should be streamed. If True, the function returns a generator.\n:param raw: bool, optional, default is False. Determines whether the response should include raw data.\n:param format: Literal['', 'json'], optional, default is an empty string. Specifies the format of the response. Can be either an empty string or 'json'.\n:param images: Optional[Sequence[AnyStr]], optional. A sequence of images to include in the request. Defaults to None.\n:param options: Optional[Options], optional. Additional options for the request. Defaults to None.\n:param keep_alive: Optional[Union[float, str]], optional. Specifies the keep-alive parameter for the request. Can be either a float, a string, or None.\n:return: Union[Mapping[str, Any], Iterator[Mapping[str, Any]]]. Returns a single response or a generator for streaming responses, depending on the 'stream' parameter."}, "indent": 4}
{"namespace": "ollama._client.Client.push", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [231, 236], "body_position": [242, 251], "dependency": {"intra_class": ["ollama._client.Client._request_stream"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_client.py::test_client_push_stream"], "requirement": {"Functionality": "The function sends a POST request to the '/api/push' endpoint with the specified parameters. It handles the request differently based on the 'stream' parameter: if 'stream' is False, it returns a single 'ProgressResponse'; if 'stream' is True, it returns a generator yielding 'ProgressResponse' objects. It raises a 'ResponseError' if the request cannot be fulfilled.\n", "Arguments": ":param self: Client. An instance of the Client class.\n:param model: str, The name of the model to be pushed.\n:param insecure: bool, Optional. Specifies whether the request should be made over an insecure connection. Defaults to False.\n:param stream: bool, Optional. Determines the type of response: a single 'ProgressResponse' object if False, or a generator of 'ProgressResponse' objects if True. Defaults to False.\n:return: Union[Mapping[str, Any], Iterator[Mapping[str, Any]]], The function returns a 'ProgressResponse' if 'stream' is False, or a generator yielding 'ProgressResponse' objects if 'stream' is True."}, "indent": 4}
{"namespace": "ollama._client.Client.create", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [253, 259], "body_position": [265, 281], "dependency": {"intra_class": ["ollama._client.Client._parse_modelfile", "ollama._client.Client._request_stream"], "intra_file": ["ollama._client._as_path"], "cross_file": ["ollama._types.RequestError"]}, "tests": ["tests/test_client.py::test_client_create_path_relative", "tests/test_client.py::test_client_create_modelfile", "tests/test_client.py::test_client_create_from_library"], "requirement": {"Functionality": "The `create` function in the `Client` class initiates a request to create a model based on the provided model file or path. It handles the request either as a single response or as a stream of responses, depending on the `stream` parameter.\n", "Arguments": ":param self: Client. An instance of the Client class.\n:param model: str, The name of the model to be created.\n:param path: Optional[Union[str, PathLike]], The file system path where the model file is located. Used to read the model file if `modelfile` is not directly provided.\n:param modelfile: Optional[str], The content of the model file as a string. Used if `path` is not provided.\n:param stream: bool, Determines the mode of the response. If `False`, a single `ProgressResponse` is returned. If `True`, a generator yielding `ProgressResponse` objects is returned.\n:return: Union[Mapping[str, Any], Iterator[Mapping[str, Any]]], The function returns a `ProgressResponse` if `stream` is `False`. If `stream` is `True`, it returns a generator of `ProgressResponse` objects. Raises `ResponseError` if the request could not be fulfilled.\n\nNote: The function raises a `RequestError` if neither `path` nor `modelfile` is provided, indicating that one of them is required for the operation."}, "indent": 4}
{"namespace": "ollama._client.Client._create_blob", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [301, 301], "body_position": [302, 321], "dependency": {"intra_class": ["ollama._client.Client._request"], "intra_file": [], "cross_file": ["ollama._types.ResponseError"]}, "tests": ["tests/test_client.py::test_client_create_blob_exists"], "requirement": {"Functionality": "This function calculates the SHA-256 checksum of a file specified by the path, checks if a blob with that checksum already exists on the server by making a HEAD request, and if not found (404 status code), uploads the file as a new blob using a POST request. Finally, it returns the digest of the file.\n", "Arguments": ":param self: Client. An instance of the Client class.\n:param path: Union[str, Path]. The file path of the blob to be created. It is used to open the file, calculate its SHA-256 checksum, and upload the file if it does not already exist on the server.\n:return: str. The SHA-256 digest of the file in the format 'sha256:<hexdigest>'."}, "indent": 4}
{"namespace": "ollama._client.AsyncClient.generate", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [381, 394], "body_position": [404, 424], "dependency": {"intra_class": ["ollama._client.AsyncClient._request_stream"], "intra_file": ["ollama._client._encode_image"], "cross_file": ["ollama._types.Options", "ollama._types.RequestError"]}, "tests": ["tests/test_client.py::test_async_client_generate_stream"], "requirement": {"Functionality": "This function asynchronously generates a response using the specified model and parameters. It handles the request to the server, sending all necessary data, and processes the response. If the 'stream' parameter is set to True, it returns an asynchronous generator that yields the response; otherwise, it returns a single response object. It raises specific errors if the model is not provided or if the request cannot be fulfilled.\n", "Arguments": ":param self: AsyncClient. An instance of the AsyncClient class.\n:param model: str, default ''. The model identifier used for generating the response. It is required and an error is raised if not provided.\n:param prompt: str, default ''. The prompt or input text for the model to process.\n:param system: str, default ''. An identifier for the system or environment where the model is run.\n:param template: str, default ''. A template identifier to format the model's response.\n:param context: Optional[Sequence[int]], default None. A sequence of integers representing the context or additional data for the model.\n:param stream: bool, default False. Determines if the response should be streamed asynchronously. If True, the function returns an asynchronous generator.\n:param raw: bool, default False. Indicates if the response should be returned in raw format.\n:param format: Literal['', 'json'], default ''. Specifies the format of the response; can be empty or 'json'.\n:param images: Optional[Sequence[AnyStr]], default None. A sequence of images to be included in the request, encoded as strings.\n:param options: Optional[Options], default None. Additional options for the request.\n:param keep_alive: Optional[Union[float, str]], default None. Specifies the keep-alive parameter for the connection.\n:return: Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]. If 'stream' is False, returns a single response object as a dictionary. If 'stream' is True, returns an asynchronous generator yielding response objects."}, "indent": 4}
{"namespace": "ollama._client.AsyncClient.pull", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [491, 496], "body_position": [502, 511], "dependency": {"intra_class": ["ollama._client.AsyncClient._request_stream"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_client.py::test_async_client_pull_stream"], "requirement": {"Functionality": "The function asynchronously requests to pull data for a specified model from an API endpoint. It can operate in either a secure or insecure mode and has the option to stream the response. If the request cannot be fulfilled, it raises a `ResponseError`. Depending on the `stream` parameter, it either returns a single `ProgressResponse` or a generator yielding `ProgressResponse` objects.\n", "Arguments": ":param self: AsyncClient. An instance of the AsyncClient class.\n:param model: str, The name of the model to pull data for.\n:param insecure: bool, Optional. Determines whether the request should be made in an insecure manner. Defaults to False.\n:param stream: bool, Optional. Determines whether the response should be streamed. Defaults to False.\n:return: Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]], A single `ProgressResponse` if `stream` is False, or a generator yielding `ProgressResponse` objects if `stream` is True."}, "indent": 4}
{"namespace": "ollama._client.AsyncClient.chat", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [426, 434], "body_position": [444, 469], "dependency": {"intra_class": ["ollama._client.AsyncClient._request_stream"], "intra_file": ["ollama._client._encode_image"], "cross_file": ["ollama._types.Message", "ollama._types.Options", "ollama._types.RequestError"]}, "tests": ["tests/test_client.py::test_async_client_chat_stream"], "requirement": {"Functionality": "This function asynchronously creates a chat response using the specified model. It validates the input parameters, raises errors for invalid inputs, and then makes an asynchronous request to generate a chat response. If the 'stream' parameter is False, it returns a single ChatResponse; otherwise, it returns an asynchronous generator of ChatResponse.\n", "Arguments": ":param self: AsyncClient. An instance of the AsyncClient class.\n:param model: str, default to an empty string. The model identifier used for generating chat responses. It is required for making the request.\n:param messages: Optional[Sequence[Message]], default to None. A sequence of messages to be included in the chat. Each message should be a dictionary containing at least 'role' and 'content', and optionally 'images'. Validates each message for the correct structure and content.\n:param stream: bool, default to False. Determines the type of response. If False, a single ChatResponse is returned. If True, an asynchronous generator of ChatResponse is returned.\n:param format: Literal['', 'json'], default to an empty string. Specifies the format of the response. Currently supports '' (default) and 'json'.\n:param options: Optional[Options], default to None. Additional options for the chat request.\n:param keep_alive: Optional[Union[float, str]], default to None. Specifies the keep-alive parameter for the chat session. Can be a float representing seconds or a string.\n:return: Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]. If 'stream' is False, returns a single ChatResponse as a mapping. If 'stream' is True, returns an asynchronous generator of ChatResponse mappings."}, "indent": 4}
{"namespace": "ollama._client.AsyncClient.push", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [513, 518], "body_position": [524, 533], "dependency": {"intra_class": ["ollama._client.AsyncClient._request_stream"], "intra_file": [], "cross_file": []}, "tests": ["tests/test_client.py::test_async_client_push_stream"], "requirement": {"Functionality": "The function asynchronously sends a POST request to the '/api/push' endpoint with the specified model name, insecurity flag, and stream flag. It handles the response by either returning a single `ProgressResponse` object or a generator yielding `ProgressResponse` objects, based on the value of the `stream` parameter.\n", "Arguments": ":param self: AsyncClient. An instance of the AsyncClient class.\n:param model: str, The name of the model to be pushed in the request.\n:param insecure: bool, Optional. A flag indicating whether the request should be made over an insecure connection. Defaults to False.\n:param stream: bool, Optional. A flag indicating whether the response should be streamed. If True, the function returns a generator of `ProgressResponse` objects; otherwise, it returns a single `ProgressResponse` object. Defaults to False.\n:return: Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]], The function returns a `ProgressResponse` object if `stream` is False. If `stream` is True, it returns a generator yielding `ProgressResponse` objects."}, "indent": 4}
{"namespace": "ollama._client.AsyncClient._create_blob", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [583, 583], "body_position": [584, 610], "dependency": {"intra_class": ["ollama._client.AsyncClient._request"], "intra_file": [], "cross_file": ["ollama._types.ResponseError"]}, "tests": ["tests/test_client.py::test_async_client_create_blob_exists"], "requirement": {"Functionality": "This function calculates the SHA256 checksum of a file specified by the path, checks if a blob with that checksum already exists on the server by making a HEAD request, and if not found (404 status code), uploads the file in chunks to the server using a POST request. Finally, it returns the digest of the file.\n", "Arguments": ":param self: AsyncClient. An instance of the AsyncClient class.\n:param path: Union[str, Path]. The file path of the file to create a blob for. It is used to read the file in binary mode and calculate its SHA256 checksum, and if necessary, to upload the file to the server.\n:return: str. The digest of the file in the format 'sha256:<checksum>'."}, "indent": 4}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "type": "method", "completion_path": "Python-Type-Challenges/views/challenge.py", "signature_position": [151, 153], "body_position": [154, 222], "dependency": {"intra_class": [], "intra_file": ["challenge.TypeCheckResult"], "cross_file": []}, "tests": ["tests/test_solutions.py::test_solution_valid"], "requirement": {"Functionality": "This function performs a type check on a combination of user-provided code and test code using Pyright, identifies lines with expected type errors, and returns a result indicating whether the type check passed or failed along with relevant error messages.\n", "Arguments": ":param cls: The class that this method belongs to, used to access class-specific attributes and methods.\n:param user_code: str, The user-provided code that needs to be type-checked.\n:param test_code: str, The test code that is combined with the user code for type checking.\n:return: TypeCheckResult, An object containing the result of the type check, including a message detailing the outcome and a boolean indicating if the type check passed.\n"}, "indent": 8}
{"namespace": "ollama._client.AsyncClient.create", "type": "method", "completion_path": "ollama-python/ollama/_client.py", "signature_position": [535, 541], "body_position": [547, 563], "dependency": {"intra_class": ["ollama._client.AsyncClient._parse_modelfile", "ollama._client.AsyncClient._request_stream"], "intra_file": ["ollama._client._as_path"], "cross_file": ["ollama._types.RequestError"]}, "tests": ["tests/test_client.py::test_async_client_create_path_relative", "tests/test_client.py::test_async_client_create_modelfile", "tests/test_client.py::test_async_client_create_from_library"], "requirement": {"Functionality": "The `create` method in the `AsyncClient` class asynchronously handles the creation of a model by sending a POST request to the server. It supports reading model data from a file path or directly from a string. If `stream` is set to `True`, it returns an asynchronous iterator for streaming responses; otherwise, it returns a single mapping object representing the response.\n", "Arguments": ":param self: AsyncClient. An instance of the AsyncClient class.\n:param model: str, The name of the model to be created.\n:param path: Optional[Union[str, PathLike]], The file system path to a file containing model data. If provided, the content of the file is read as the model data. Defaults to None.\n:param modelfile: Optional[str], A string containing the model data. Used if `path` is not provided. Defaults to None.\n:param stream: bool, Indicates whether the response should be streamed. If `True`, the method returns an asynchronous iterator; otherwise, it returns a single response object. Defaults to False.\n:return: Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]], The response from the server. The type of the response depends on the value of the `stream` parameter.\n\nRaises `ResponseError` if the request could not be fulfilled. It also raises `RequestError` if neither `path` nor `modelfile` is provided, indicating that one of them must be specified for the request."}, "indent": 4}
{"namespace": "sfast.utils.aot_printer.aot_printer", "type": "function", "completion_path": "stable-fast/src/sfast/utils/aot_printer.py", "signature_position": [36, 36], "body_position": [37, 44], "dependency": {"intra_class": [], "intra_file": ["sfast.utils.aot_printer.get_compiler_fn"], "cross_file": []}, "tests": ["tests/operators/test_cudnn_convolution.py::test_conv_bias", "tests/operators/test_cudnn_convolution.py::test_conv_bias_relu"], "requirement": {"Functionality": "The function checks if the input argument is an instance of a PyTorch neural network module. If it is, it compiles the module using a forward and backward compiler specific for modules. If the input is not a module, it assumes it's a function and compiles it using a forward and backward compiler specific for functions.\n", "Arguments": ":param fn: torch.nn.Module or function. The neural network module or function to be compiled.\n:return: The compiled module or function using the specified forward and backward compilers."}, "indent": 4}
{"namespace": "autorag.deploy.extract_best_config", "type": "function", "completion_path": "AutoRAG/autorag/deploy.py", "signature_position": [90, 90], "body_position": [102, 113], "dependency": {"intra_class": [], "intra_file": ["autorag.deploy.summary_df_to_yaml"], "cross_file": ["autorag.utils.util.load_summary_file"]}, "tests": ["tests/autorag/test_deploy.py::test_extract_best_config"], "requirement": {"Functionality": "Extracts the optimal pipeline configuration from a given trial directory and optionally saves it to a YAML file. It reads the summary and configuration files from the trial directory to construct a dictionary of the best pipeline configuration.\n", "Arguments": ":param trial_path: str, The path to the trial directory from which the optimal pipeline configuration is to be extracted. The directory must contain a summary.csv file with the evaluation results.\n:param output_path: Optional[str], The file path where the extracted pipeline configuration should be saved in YAML format. If not specified or None, the function will not save the configuration to a file but will still return the configuration dictionary. The file extension must be .yaml or .yml if provided.\n:return: Dict, The dictionary containing the extracted optimal pipeline configuration."}, "indent": 4}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "type": "function", "completion_path": "stable-fast/src/sfast/jit/trace_helper.py", "signature_position": [33, 33], "body_position": [34, 72], "dependency": {"intra_class": [], "intra_file": ["sfast.jit.trace_helper.to_module"], "cross_file": []}, "tests": ["tests/jit/test_trace_helper.py::test_lazy_trace_with_generator"], "requirement": {"Functionality": "This function dynamically traces a given function or PyTorch module, caching the traced versions to optimize future calls. It wraps the original function or module's forward method, traces it with optional compiler enhancements, and caches the result for identical future calls to reduce overhead.\n", "Arguments": ":param func: Function or torch.nn.Module. The function or PyTorch module to be traced. It is used to either directly trace the function or the module's forward method if a module is provided.\n:param ts_compiler: Callable, optional. A compiler function that can optionally be used to further process the traced module. It is used if provided to compile the traced module or its call helper with additional arguments.\n:param kwargs_: Dict. Additional keyword arguments that are passed to the tracing function. These are used to customize the tracing process.\n:return: Function. A wrapped version of the original function or module's forward method that uses cached traces for efficiency.\n\nNote: The function uses a lock to ensure thread safety when accessing or updating the cache of traced modules."}, "indent": 4}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "type": "method", "completion_path": "AutoRAG/autorag/deploy.py", "signature_position": [143, 143], "body_position": [152, 153], "dependency": {"intra_class": ["autorag.deploy.Runner.__init__"], "intra_file": ["autorag.deploy.extract_best_config"], "cross_file": []}, "tests": ["tests/autorag/test_deploy.py::test_runner_full"], "requirement": {"Functionality": "This function loads a Runner instance from a specified trial folder that has already been evaluated using the Evaluator class. It initializes the Runner with the best configuration found in the trial folder and sets the project directory to the parent directory of the trial folder.\n", "Arguments": ":param cls: The class method is called on. It is used to instantiate the Runner with the extracted configuration and project directory.\n:param trial_path: str, The path of the trial folder from which the Runner is to be loaded. It is used to locate the trial folder and extract the best configuration for initializing the Runner.\n:return: Initialized Runner. An instance of the Runner class initialized with the best configuration from the trial folder and the project directory set to the parent directory of the trial folder."}, "indent": 8}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "type": "function", "completion_path": "AutoRAG/autorag/nodes/retrieval/run.py", "signature_position": [15, 20], "body_position": [33, 123], "dependency": {"intra_class": [], "intra_file": ["autorag.nodes.retrieval.run.edit_summary_df_params", "autorag.nodes.retrieval.run.get_hybrid_execution_times", "autorag.nodes.retrieval.run.get_ids_and_scores", "autorag.nodes.retrieval.run.get_module_params", "autorag.nodes.retrieval.run.select_result_for_hybrid"], "cross_file": ["autorag.strategy.filter_by_threshold", "autorag.strategy.select_best_average"]}, "tests": ["tests/autorag/nodes/retrieval/test_run_retrieval_node.py::test_run_retrieval_node", "tests/autorag/schema/test_node_schema.py::test_from_dict"], "requirement": {"Functionality": "This function evaluates and selects the best module among retrieval node results by running each module with given parameters, measuring their execution times, and applying specified strategies. It saves the results and a summary of the execution times and evaluation metrics to disk.\n", "Arguments": ":param modules: List of Callable. These are the retrieval modules to be executed.\n:param module_params: List of Dict. Parameters for each retrieval module in the 'modules' list.\n:param previous_result: DataFrame. The result from a previous step, which could be from query expansion or QA data. This is used as part of the evaluation context.\n:param node_line_dir: String. The directory path for this node line, where results and summaries will be saved.\n:param strategies: Dict. Strategies for evaluating and selecting the best retrieval node result, including metrics and possibly speed thresholds.\n:return: DataFrame. The best result dataframe, which combines the previous result columns with the selected retrieval node's result columns."}, "indent": 4}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "type": "function", "completion_path": "AutoRAG/autorag/nodes/queryexpansion/run.py", "signature_position": [17, 22], "body_position": [39, 121], "dependency": {"intra_class": [], "intra_file": ["autorag.nodes.queryexpansion.run.evaluate_one_query_expansion_node", "autorag.nodes.queryexpansion.run.make_retrieval_callable_params"], "cross_file": ["autorag.strategy.filter_by_threshold", "autorag.strategy.measure_speed", "autorag.strategy.select_best_average"]}, "tests": ["tests/autorag/nodes/queryexpansion/test_query_expansion_run.py::test_run_query_expansion_node_default"], "requirement": {"Functionality": "This function evaluates and selects the best module among query expansion node results by running each module with given parameters, measuring their execution times, and evaluating their performance based on specified strategies. It saves the results and a summary, including execution times and evaluation metrics, to the specified directory. Finally, it selects and saves the best result based on the evaluation.\n", "Arguments": ":param modules: List[Callable]. A list of query expansion modules to be executed.\n:param module_params: List[Dict]. Parameters for each query expansion module in the modules list.\n:param previous_result: pd.DataFrame. The dataframe containing the results from the previous step, used as input for query expansion.\n:param node_line_dir: str. The directory path where the results and summaries will be saved.\n:param strategies: Dict. A dictionary containing strategies for selecting the best query expansion module, including metrics, speed thresholds, and other evaluation criteria.\n:return: pd.DataFrame. The dataframe containing the best result after evaluating all query expansion modules according to the specified strategies."}, "indent": 4}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "type": "function", "completion_path": "AutoRAG/autorag/nodes/promptmaker/run.py", "signature_position": [16, 21], "body_position": [43, 124], "dependency": {"intra_class": [], "intra_file": ["autorag.nodes.promptmaker.run.evaluate_one_prompt_maker_node", "autorag.nodes.promptmaker.run.make_generator_callable_params"], "cross_file": ["autorag.evaluate.util.cast_metrics", "autorag.strategy.filter_by_threshold", "autorag.strategy.measure_speed", "autorag.strategy.select_best_average"]}, "tests": ["tests/autorag/nodes/promptmaker/test_prompt_maker_run.py::test_run_prompt_maker_node_default"], "requirement": {"Functionality": "This function runs a series of prompt maker modules with given parameters and evaluates their performance to select the best prompt maker module based on specified strategies. It saves the results and a summary, including execution times and evaluation metrics, to the specified directory. The function integrates with a default or specified generator module for evaluation purposes and handles the creation of necessary directories.\n", "Arguments": ":param modules: List[Callable]. A list of prompt maker modules to be executed.\n:param module_params: List[Dict]. Parameters for each prompt maker module in the modules list.\n:param previous_result: pd.DataFrame. The dataframe containing the results from previous operations, which will be combined with the best prompt maker's result.\n:param node_line_dir: str. The directory path where the node's output will be stored. It includes creating necessary subdirectories.\n:param strategies: Dict. A dictionary containing strategies for selecting the best prompt maker module. It may include metrics for evaluation, speed thresholds, and generator module specifications.\n:return: pd.DataFrame. The dataframe containing the combined results of the previous operation and the best prompt maker's output."}, "indent": 4}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "type": "function", "completion_path": "AutoRAG/autorag/schema/node.py", "signature_position": [89, 89], "body_position": [98, 99], "dependency": {"intra_class": [], "intra_file": ["autorag.schema.node.Node", "autorag.schema.node.extract_values"], "cross_file": []}, "tests": ["tests/autorag/schema/test_node_schema.py::test_find_llm_models"], "requirement": {"Functionality": "Extracts values associated with a specified key from a list of nodes, ensuring each value is unique by removing duplicates.\n", "Arguments": ":param nodes: List[Node]. The nodes from which you want to extract values. These nodes are expected to have a structure that includes modules and module_params from which values are extracted.\n:param key: String. The key corresponding to the value you want to extract from each node's module_param.\n:return: List[String]. A list of unique values extracted from the nodes based on the specified key."}, "indent": 4}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "type": "function", "completion_path": "AutoRAG/autorag/evaluate/metric/generation.py", "signature_position": [92, 92], "body_position": [105, 113], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["autorag.embedding_models", "autorag.evaluate.metric.util.calculate_cosine_similarity"]}, "tests": ["tests/autorag/evaluate/metric/test_generation_metric.py::test_sem_score_other_model"], "requirement": {"Functionality": "Computes the semantic similarity score between a list of ground truth strings and a predicted string using cosine similarity. It uses an embedding model to convert strings into embeddings and then calculates the maximum cosine similarity between the predicted string and each ground truth string.\n", "Arguments": ":param generation_gt: List[str]. A list of ground truth strings. These strings are compared against the prediction to find the maximum cosine similarity.\n:param pred: str. The model's prediction string. Its semantic similarity is calculated against each string in the ground truth list.\n:param embedding_model: Optional[BaseEmbedding]. The embedding model used to compute the cosine similarity. If not provided, a default model ('all-mpnet-base-v2') is used. This model converts the input strings into embeddings for similarity calculation.\n:return: float. The maximum semantic similarity score between the predicted string and the ground truth strings."}, "indent": 4}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "type": "function", "completion_path": "stable-diffusion-webui-forge/modules/gfpgan_model.py", "signature_position": [56, 56], "body_position": [57, 60], "dependency": {"intra_class": [], "intra_file": ["gfpgan_model.gfpgan_face_restorer", "gfpgan_model.logger"], "cross_file": []}, "tests": ["test/test_face_restorers.py::test_face_restorers"], "requirement": {"Functionality": "This function attempts to restore faces in an image using the GFPGAN face restorer. If the GFPGAN face restorer is not set up, it logs a warning and returns the original image.\n", "Arguments": ":param np_image: The image as a NumPy array on which face restoration is attempted.\n:return: The image as a NumPy array. It is either the original image or the restored image if the GFPGAN face restorer is set up and used successfully."}, "indent": 4}
{"namespace": "codeformer_model.setup_model", "type": "function", "completion_path": "stable-diffusion-webui-forge/modules/codeformer_model.py", "signature_position": [58, 58], "body_position": [59, 64], "dependency": {"intra_class": [], "intra_file": ["codeformer_model.FaceRestorerCodeFormer"], "cross_file": []}, "tests": ["test/test_face_restorers.py::test_face_restorers"], "requirement": {"Functionality": "This function attempts to set up a model for face restoration by initializing a FaceRestorerCodeFormer instance with the given directory name. It adds the initialized instance to a global list of face restorers. If an error occurs during setup, it reports the error.\n", "Arguments": ":param dirname: str, The directory name where the model or necessary files for the FaceRestorerCodeFormer are located. It is used to initialize the FaceRestorerCodeFormer instance.\n:return: No return values."}, "indent": 4}
{"namespace": "gfpgan_model.setup_model", "type": "function", "completion_path": "stable-diffusion-webui-forge/modules/gfpgan_model.py", "signature_position": [63, 63], "body_position": [64, 71], "dependency": {"intra_class": [], "intra_file": ["gfpgan_model.FaceRestorerGFPGAN"], "cross_file": []}, "tests": ["test/test_face_restorers.py::test_face_restorers"], "requirement": {"Functionality": "Sets up the GFPGAN model for face restoration by patching the facexlib with the given directory and initializing the GFPGAN face restorer with the model located in the specified directory. It also handles any exceptions that occur during the setup process and reports them.\n", "Arguments": ":param dirname: str, The directory path where the GFPGAN model is located. This directory is used to patch the facexlib and initialize the GFPGAN face restorer.\n:return: No return values."}, "indent": 4}
{"namespace": "quaternion.rotate", "type": "function", "completion_path": "camp_zipnerf/internal/quaternion.py", "signature_position": [76, 76], "body_position": [79, 80], "dependency": {"intra_class": [], "intra_file": ["quaternion.conjugate", "quaternion.im", "quaternion.multiply"], "cross_file": []}, "tests": ["tests/rigid_body_test.py::RigidBodyTest::test_exp_se3_only_rotation"], "requirement": {"Functionality": "Rotates a vector in 3D space using a quaternion to represent the rotation. The function performs the rotation by converting the vector into a quaternion format, applying the rotation, and then converting it back.\n", "Arguments": ":param q: Array-like. The quaternion representing the rotation, used to rotate the vector.\n:param v: Array-like. The vector to be rotated, represented as an array or similar data structure.\n:return: Array-like. The rotated vector, after applying the quaternion rotation."}, "indent": 2}
{"namespace": "quaternion.from_axis_angle", "type": "function", "completion_path": "camp_zipnerf/internal/quaternion.py", "signature_position": [240, 242], "body_position": [253, 264], "dependency": {"intra_class": [], "intra_file": ["quaternion._safe_sqrt"], "cross_file": []}, "tests": ["tests/rigid_body_test.py::RigidBodyTest::test_exp_se3_only_rotation", "tests/quaternion_test.py::QuaternionTest::test_quaternion_axis_angle_round_trip"], "requirement": {"Functionality": "Constructs a quaternion from a given axis-angle representation of a rotation, ensuring numerical stability for small angles by using a small epsilon value.\n", "Arguments": ":param axis_angle: A 3-vector (array-like). It represents the axis of rotation (direction) and the magnitude of rotation (angle). It is used to calculate the quaternion.\n:param eps: Float, optional. A small number to avoid division by zero and ensure numerical stability for rotations close to zero. It defaults to the machine epsilon for float32.\n:return: A quaternion (array-like). It encodes the rotation specified by the input axis-angle vector.\n"}, "indent": 2}
{"namespace": "openlogprobs.extract.topk_search", "type": "function", "completion_path": "openlogprobs/openlogprobs/extract.py", "signature_position": [78, 79], "body_position": [80, 105], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["openlogprobs.models.Model", "openlogprobs.models.Model.argmax", "openlogprobs.models.Model.topk"]}, "tests": ["test/test_logprobs.py::test_topk_consistency"], "requirement": {"Functionality": "Performs a top-k search on a given model with a specific prefix and index, adjusting the search bias until the desired index is the most probable. It calculates the log probability of the index being the top result under the adjusted conditions.\n", "Arguments": ":param model: Model. The model on which the top-k search is performed. It is used to get top-k words, their indices, and to adjust search biases.\n:param prefix: str. The prefix string used for the search in the model. It guides the search process.\n:param idx: int. The target index for which the function tries to maximize its probability in the top-k results.\n:param k: int, optional. The number of top results to consider. Defaults to 1.\n:param high: int, optional. The initial high bias value for the target index. Defaults to 40.\n:return: Tuple. A tuple containing the log probability of the target index being the top result and the number of calls made to the model."}, "indent": 4}
{"namespace": "resample.resample_3d", "type": "function", "completion_path": "camp_zipnerf/internal/resample.py", "signature_position": [48, 56], "body_position": [78, 141], "dependency": {"intra_class": [], "intra_file": ["resample.gather_volume"], "cross_file": []}, "tests": ["tests/resample_test.py::Resample3dTest::test_resample_3d_edges_zero_outside", "tests/resample_test.py::Resample3dTest::test_resample_3d_nearest_neighbor_matches_trilinear_convolution"], "requirement": {"Functionality": "This function resamples input 3D data at specified locations using either trilinear interpolation or nearest neighbor interpolation. It supports edge behavior handling and can adjust for half-pixel centering.\n", "Arguments": ":param data: A tensor with shape [D, H, W, C], representing the volume from which to sample. It is the input volume data.\n:param locations: A tensor with shape [D, ..., 3], containing floating point locations at which to sample the data. These locations assume voxel centers at integer coordinates.\n:param edge_behavior: String, specifying how to handle sample points outside the input volume. Options are 'CONSTANT_OUTSIDE' (pads the volume and interpolates towards a constant value outside the tensor) and 'CLAMP' (clamps sample points to the volume). Default is 'CONSTANT_OUTSIDE'.\n:param constant_values: Float, the constant value to use when 'edge_behavior' is set to 'CONSTANT_OUTSIDE'. Default is 0.0.\n:param coordinate_order: String, indicating whether the sample locations are in 'xyz' or 'zyx' order. Default is 'xyz'.\n:param method: String, specifying the interpolation method to use. Must be either 'TRILINEAR' for trilinear interpolation or 'NEAREST' for nearest neighbor interpolation. Default is 'TRILINEAR'.\n:param half_pixel_center: Bool, determines if half-pixel centering is used, which affects the calculation of sample locations. Default is False.\n:return: A tensor of shape [D, ..., C] containing the resampled values. This tensor contains the output volume data after resampling at the specified locations.\n"}, "indent": 2}
{"namespace": "math.plus_eps", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [51, 51], "body_position": [52, 54], "dependency": {"intra_class": [], "intra_file": ["math.tiny_val"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_plus_eps", "tests/math_test.py::MathTest::test_plus_eps_grad", "tests/math_test.py::MathTest::test_safe_log1p_out_of_bounds"], "requirement": {"Functionality": "The function adjusts very small values of x (close to zero) to a minimum threshold or calculates the next representable floating-point value towards positive infinity for x. This is useful for avoiding issues with values that are too small to be represented accurately in computations.\n", "Arguments": ":param x: The input value that needs adjustment. It is used to check against a tiny threshold value and to compute the next representable floating-point value if necessary.\n:return: The adjusted value of x. If x is smaller than a tiny threshold, the threshold value is returned. Otherwise, the next floating-point value towards positive infinity is returned."}, "indent": 2}
{"namespace": "math.minus_eps", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [58, 58], "body_position": [59, 61], "dependency": {"intra_class": [], "intra_file": ["math.tiny_val"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_minus_eps", "tests/math_test.py::MathTest::test_minus_eps_grad"], "requirement": {"Functionality": "This function adjusts the input value `x` by returning a slightly smaller value than `x` if `x` is not smaller than a very small value (`tiny_val`). If `x` is smaller than `tiny_val`, it returns `-tiny_val` instead. This is useful for numerical computations where a value slightly less than `x` is needed, but avoiding values too close to zero.\n", "Arguments": ":param x: The input value to be adjusted. It is used to check against `tiny_val` and to compute the next smaller floating-point number towards negative infinity.\n:return: A floating-point number that is slightly smaller than `x` if `x` is not smaller than `tiny_val`, otherwise `-tiny_val`. This ensures the returned value is always slightly less than the input, avoiding values too close to zero."}, "indent": 2}
{"namespace": "math.safe_exp", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [215, 215], "body_position": [216, 220], "dependency": {"intra_class": [], "intra_file": ["math.generate_safe_fn", "math.max_val", "math.min_val"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_safe_exp_correct", "tests/math_test.py::MathTest::test_safe_exp_finite"], "requirement": {"Functionality": "The function creates a safe exponential function that avoids overflow issues by limiting the input range. It uses a helper function to generate this safe version of the exponential function, applying a custom gradient function for backpropagation in automatic differentiation contexts.\n", "Arguments": ":param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n:return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors."}, "indent": 2}
{"namespace": "math.safe_log", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [207, 207], "body_position": [208, 212], "dependency": {"intra_class": [], "intra_file": ["math.generate_safe_fn", "math.max_val", "math.tiny_val"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_safe_log_correct", "tests/math_test.py::MathTest::test_safe_log_finite", "tests/spin_math_test.py::SpinMathTest::test_safe_log_grad"], "requirement": {"Functionality": "This function creates a safe version of the logarithm function that can handle edge cases or specific conditions defined by `generate_safe_fn`. It wraps the JAX numpy logarithm function (`jnp.log`) with additional logic to manage derivatives and bounds, ensuring stability and safety in computations.\n", "Arguments": ":param x: The input value or array for which the logarithm is to be calculated. It is used as the input to the safe logarithm function created by `generate_safe_fn`.\n:return: The result of applying the safe logarithm function to `x`. This could be a single value or an array of values, depending on the input `x`."}, "indent": 2}
{"namespace": "math.safe_sqrt", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [223, 223], "body_position": [224, 228], "dependency": {"intra_class": [], "intra_file": ["math.generate_safe_fn", "math.max_val", "math.tiny_val"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_safe_sqrt_correct", "tests/math_test.py::MathTest::test_safe_sqrt_finite", "tests/spin_math_test.py::SpinMathTest::test_safe_sqrt"], "requirement": {"Functionality": "This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n", "Arguments": ":param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n:return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes."}, "indent": 2}
{"namespace": "math.power_ladder_max_output", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [269, 269], "body_position": [271, 277], "dependency": {"intra_class": [], "intra_file": ["math.safe_div", "math.select"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_power_ladder_max_output"], "requirement": {"Functionality": "Calculates the limit of the power_ladder function as its input x approaches infinity, based on the provided power parameter p. It uses conditions to determine the output based on the value of p.", "Arguments": ":param p: A numeric value representing the power parameter in the power_ladder function. It determines the behavior of the function as x approaches infinity.\n:return: A numeric value representing the limit of the power_ladder function as x goes to infinity, which varies depending on the value of p."}, "indent": 2}
{"namespace": "geopoly.generate_basis", "type": "function", "completion_path": "camp_zipnerf/internal/geopoly.py", "signature_position": [79, 81], "body_position": [98, 161], "dependency": {"intra_class": [], "intra_file": ["geopoly.compute_sq_dist", "geopoly.tesselate_geodesic"], "cross_file": []}, "tests": ["tests/geopoly_test.py::GeopolyTest::test_generate_basis_golden"], "requirement": {"Functionality": "Generates a 3D basis by tessellating a geometric polyhedron, optionally removing symmetric basis columns to avoid redundant projections. The function supports tessellation of tetrahedron, icosahedron, or octahedron shapes and returns a matrix representing the 3D basis.\n", "Arguments": ":param base_shape: string, the name of the starting polyhedron. It must be either 'tetrahedron', 'icosahedron', or 'octahedron'. This parameter determines the initial shape to be tessellated.\n:param angular_tesselation: int, the number of times the polyhedron is tessellated. A value of 1 means no tessellation is applied. This parameter controls the complexity of the resulting basis.\n:param remove_symmetries: bool, optional (default=True). If True, symmetric basis columns are removed to prevent redundant negative copies in projections. This parameter affects the uniqueness of the basis vectors.\n:param eps: float, optional (default=1e-4). A small number used to determine when two vertices are considered symmetric. This parameter helps in identifying and removing symmetries with a tolerance defined by eps.\n:return: A matrix with shape [3, n], where n is the number of vertices after tessellation (and symmetry removal, if applicable). This matrix represents the 3D basis generated from the specified polyhedron."}, "indent": 2}
{"namespace": "math.safe_log1p", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [231, 231], "body_position": [232, 236], "dependency": {"intra_class": [], "intra_file": ["math.generate_safe_fn", "math.max_val"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_safe_log1p_correct", "tests/math_test.py::MathTest::test_safe_log1p_out_of_bounds", "tests/math_test.py::MathTest::test_safe_log1p_output_and_gradient_are_finite"], "requirement": {"Functionality": "The function creates a safe version of the log1p function, which calculates the natural logarithm of 1 plus the input value, x. It ensures that the input value is within a safe range to avoid numerical errors or undefined behavior. The function also specifies how the derivative of this operation should be computed for automatic differentiation.\n", "Arguments": ":param x: The input value for which the natural logarithm of 1 plus x is computed. \n:return: The result of the safe log1p operation on the input value x."}, "indent": 2}
{"namespace": "math.power_ladder", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [280, 280], "body_position": [283, 301], "dependency": {"intra_class": [], "intra_file": ["math.clip_finite_nograd", "math.remove_zero", "math.safe_expm1", "math.safe_log1p", "math.safe_sign", "math.select", "math.tiny_val"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_power_ladder_special_cases", "tests/math_test.py::MathTest::test_power_ladder_misc_properties", "tests/math_test.py::MathTest::test_power_ladder_is_odd", "tests/math_test.py::MathTest::test_power_ladder_and_inverse_pre_post_multipliers_are_correct"], "requirement": {"Functionality": "Implements Tukey's power ladder transformation on the input x, with optional pre-multiplication and post-multiplication adjustments. This transformation is used for data normalization and handling special cases for the power parameter p.\n", "Arguments": ":param x: Numeric or array-like. The input data to be transformed.\n:param p: Numeric. The power parameter of the transformation. Special cases are handled for p values of 1, 0, -inf, and inf.\n:param premult: Numeric or None. An optional pre-multiplication factor applied to x before the transformation. If None, no pre-multiplication is applied.\n:param postmult: Numeric or None. An optional post-multiplication factor applied to the transformed data. If None, no post-multiplication is applied.\n:return: The transformed data after applying Tukey's power ladder and any specified pre- and post-multiplications."}, "indent": 2}
{"namespace": "math.inv_power_ladder", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [304, 304], "body_position": [306, 326], "dependency": {"intra_class": [], "intra_file": ["math.clip_finite_nograd", "math.minus_eps", "math.override_gradient", "math.power_ladder_max_output", "math.remove_zero", "math.safe_div", "math.safe_expm1", "math.safe_log1p", "math.safe_sign", "math.select"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_power_ladder_misc_properties", "tests/math_test.py::MathTest::test_power_ladder_and_inverse_pre_post_multipliers_are_correct"], "requirement": {"Functionality": "Performs the inverse operation of a power ladder transformation on the input `y` using a specified power `p`, with optional pre-multiplication and post-multiplication adjustments. It applies various mathematical operations based on the value of `p` to compute the inverse transformation.\n", "Arguments": ":param y: The input value(s) to be transformed. It is used as the base for the inverse power ladder operation.\n:param p: The power value used in the transformation. It determines the specific operation to be applied to `y`.\n:param premult: Optional. A scalar value to pre-multiply the result after the inverse transformation is applied. It is used to adjust the scale of the output before the transformation.\n:param postmult: Optional. A scalar value to post-multiply the input `y` before the inverse transformation is applied. It is used to adjust the scale of the input before the transformation.\n:return: The result of the inverse power ladder transformation applied to `y`, adjusted by optional pre-multiplication and post-multiplication factors."}, "indent": 2}
{"namespace": "math.learning_rate_decay", "type": "function", "completion_path": "camp_zipnerf/internal/math.py", "signature_position": [348, 350], "body_position": [371, 378], "dependency": {"intra_class": [], "intra_file": ["math.log_lerp"], "cross_file": []}, "tests": ["tests/math_test.py::MathTest::test_learning_rate_decay"], "requirement": {"Functionality": "This function calculates the learning rate at a given optimization step with the option to include a delay at the start. It uses a log-linear interpolation (or exponential decay) between an initial and final learning rate over a specified number of steps. If a delay is specified, the initial learning rate is scaled down by a multiplier and gradually returns to the normal rate after the delay period.\n", "Arguments": ":param step: int, the current optimization step, used to determine the learning rate based on the progression of steps.\n:param lr_init: float, the initial learning rate at the start of optimization.\n:param lr_final: float, the final learning rate to be reached at the end of optimization.\n:param max_steps: int, the total number of steps in the optimization process, used to calculate the progression and the learning rate decay.\n:param lr_delay_steps: int, optional, the number of steps to delay before applying the full learning rate, allows for a gradual increase to the initial learning rate.\n:param lr_delay_mult: float, optional, the multiplier applied to the learning rate during the delay period, affects the starting learning rate when a delay is applied.\n:return: float, the calculated learning rate for the current step, adjusted for any specified delay."}, "indent": 2}
{"namespace": "utils.dummy_rays", "type": "function", "completion_path": "camp_zipnerf/internal/utils.py", "signature_position": [135, 139], "body_position": [140, 154], "dependency": {"intra_class": [], "intra_file": ["utils.generate_random_rays"], "cross_file": []}, "tests": ["tests/utils_test.py::UtilsTest::test_dummy_rays"], "requirement": {"Functionality": "Generates a set of random rays with specified parameters and options for including exposure index, exposure values, and device index. It utilizes a predefined function `generate_random_rays` to create these rays based on the input conditions.\n", "Arguments": ":param include_exposure_idx: Bool, optional. Determines whether to include the exposure index in the generated rays.\n:param include_exposure_values: Bool, optional. Specifies if the exposure values should be included in the generated rays.\n:param include_device_idx: Bool, optional. Indicates whether to include the device index in the generated rays.\n:return: The result from `generate_random_rays`. The type and structure of the return value depend on the implementation of `generate_random_rays`, typically a collection of generated rays with specified properties and optional information."}, "indent": 2}
{"namespace": "camera_utils.points_to_pixels", "type": "function", "completion_path": "camp_zipnerf/internal/camera_utils.py", "signature_position": [1065, 1072], "body_position": [1095, 1131], "dependency": {"intra_class": [], "intra_file": ["camera_utils.ProjectionType", "camera_utils.ProjectionType.PERSPECTIVE", "camera_utils._radial_and_tangential_distort"], "cross_file": []}, "tests": ["tests/camera_utils_test.py::CameraUtilsTest::test_points_to_pixels"], "requirement": {"Functionality": "This function calculates the 2D pixel coordinates and depth values from 3D point coordinates using camera intrinsics, extrinsics, and optionally distortion parameters. It supports vectorized operations over the leading dimensions of the input arrays and can work with either numpy or jax.numpy for computations.\n", "Arguments": ":param points: float array, 3D coordinates of points to project. These are the world coordinates that need to be projected onto the 2D image plane.\n:param pixtocams: float array, inverse of the camera intrinsics matrices. These are used to transform points from pixel coordinates to camera coordinates.\n:param camtoworlds: float array, camera extrinsics matrices. These describe the position and orientation of the camera in the world coordinate system.\n:param distortion_params: dict of floats or float arrays, optional. These parameters are used to model the camera lens distortion. If provided, the function will correct for radial and tangential distortion.\n:param camtype: camera_utils.ProjectionType, specifies the type of camera projection model to use. Currently, only perspective projection is supported.\n:param xnp: module, either numpy or jax.numpy. This parameter allows the function to be used for computations on different devices (CPU or GPU/TPU).\n:return: A tuple containing two elements:\n         coordinates: float array, the computed 2D pixel coordinates of the input 3D points.\n         depth: float array, the depth values of the points in the camera coordinate system, useful for depth-aware applications."}, "indent": 2}
{"namespace": "rigid_body.exp_se3", "type": "function", "completion_path": "camp_zipnerf/internal/rigid_body.py", "signature_position": [157, 159], "body_position": [174, 190], "dependency": {"intra_class": [], "intra_file": ["rigid_body._safe_sqrt", "rigid_body.exp_so3", "rigid_body.rp_to_se3", "rigid_body.skew"], "cross_file": []}, "tests": ["tests/rigid_body_test.py::RigidBodyTest::test_exp_se3_only_translation", "tests/rigid_body_test.py::RigidBodyTest::test_exp_se3_pure_translation"], "requirement": {"Functionality": "This function computes the exponential map from the Lie algebra se3 to the Lie group SE3, which is used to represent the motion in 3D space. It calculates the homogeneous transformation matrix that represents the motion of a body given a screw axis and a small motion magnitude.\n", "Arguments": ":param screw_axis: A 6-vector (numpy array). It encodes a screw axis of motion, which can be divided into [w, v] where w is an angle-axis rotation and v represents a translation. The magnitude of w corresponds to the magnitude of motion.\n:param eps: Float. An epsilon value for numerical stability, used to avoid division by zero or other numerical issues. It defaults to the smallest positive representable number in float32.\n:return: A (4, 4) numpy array. The homogeneous transformation matrix that represents the motion of a body for one second about the screw axis S with magnitude theta.\n"}, "indent": 2}
{"namespace": "rigid_body.exp_so3", "type": "function", "completion_path": "camp_zipnerf/internal/rigid_body.py", "signature_position": [104, 106], "body_position": [119, 136], "dependency": {"intra_class": [], "intra_file": ["rigid_body._safe_sqrt", "rigid_body.skew"], "cross_file": []}, "tests": ["tests/rigid_body_test.py::RigidBodyTest::test_exp_so3_rotation", "tests/rigid_body_test.py::RigidBodyTest::test_so3_round_trip"], "requirement": {"Functionality": "This function computes the exponential map from the Lie algebra so3 to the Lie group SO3, using Rodrigues' formula. It translates a 3D axis-angle representation of a rotation into a 3x3 rotation matrix. The function includes a numerical stability mechanism for small angles of rotation.\n", "Arguments": ":param axis_angle: A 3-vector (numpy array). Represents the axis of rotation and the magnitude of rotation. It is used to compute the rotation matrix.\n:param eps: Float. A small epsilon value for numerical stability, used to avoid division by zero or very small values that could lead to numerical instability. Defaults to the machine epsilon for float32.\n:return: A (3, 3) numpy array. An orthonormal rotation matrix representing the same rotation as the input axis-angle representation."}, "indent": 2}
{"namespace": "render.conical_frustum_to_gaussian", "type": "function", "completion_path": "camp_zipnerf/internal/render.py", "signature_position": [62, 62], "body_position": [78, 81], "dependency": {"intra_class": [], "intra_file": ["render.gaussianize_frustum", "render.lift_gaussian"], "cross_file": []}, "tests": ["tests/render_test.py::RenderTest::test_conical_frustum_scaling", "tests/render_test.py::RenderTest::test_conical_frustum", "tests/render_test.py::RenderTest::test_rotated_conic_frustums", "tests/render_test.py::RenderTest::test_conical_frustum_to_gaussian_outputs_are_finite"], "requirement": {"Functionality": "This function approximates a 3D conical frustum as a Gaussian distribution by calculating its mean and covariance. It takes into account the axis of the cone, the starting and ending distances of the frustum, the scale of the radius as a function of distance, and whether the Gaussian should have a diagonal or full-covariance.\n", "Arguments": ":param d: jnp.float32 3-vector. The axis of the cone. It is used to determine the direction of the conical frustum.\n:param t0: float. The starting distance of the frustum from the origin. It marks the beginning of the conical section being approximated.\n:param t1: float. The ending distance of the frustum from the origin. It marks the end of the conical section being approximated.\n:param base_radius: float. The scale of the radius as a function of distance from the origin. It determines how the radius of the frustum changes with distance.\n:param diag: boolean. Indicates whether the resulting Gaussian distribution will have a diagonal covariance matrix or a full-covariance matrix. This affects the complexity and the information contained in the covariance.\n:return: A tuple containing the mean (a jnp.float32 3-vector) and the covariance (a matrix) of the approximated Gaussian distribution. These represent the central tendency and the spread of the distribution, respectively."}, "indent": 2}
{"namespace": "render.cylinder_to_gaussian", "type": "function", "completion_path": "camp_zipnerf/internal/render.py", "signature_position": [84, 84], "body_position": [100, 103], "dependency": {"intra_class": [], "intra_file": ["render.lift_gaussian"], "cross_file": []}, "tests": ["tests/render_test.py::RenderTest::test_cylinder_scaling", "tests/render_test.py::RenderTest::test_cylinder"], "requirement": {"Functionality": "This function approximates a cylinder as a Gaussian distribution by calculating its mean and covariance based on the cylinder's axis, start and end distances, radius, and whether the Gaussian should be diagonal or full-covariance. It utilizes another function `lift_gaussian` to perform the actual conversion.\n", "Arguments": ":param d: jnp.float32 3-vector. The axis of the cylinder, used to define the direction and orientation of the cylinder.\n:param t0: float. The starting distance of the cylinder from the origin, used to calculate the mean of the Gaussian.\n:param t1: float. The ending distance of the cylinder from the origin, used along with t0 to calculate the mean and variance of the Gaussian.\n:param radius: float. The radius of the cylinder, used to calculate the variance of the Gaussian.\n:param diag: boolean. Indicates whether the resulting Gaussian will have a diagonal covariance matrix or a full-covariance matrix.\n:return: A tuple representing a Gaussian distribution, which includes the mean and covariance of the approximated cylinder."}, "indent": 2}
{"namespace": "camera_utils.pixels_to_rays", "type": "function", "completion_path": "camp_zipnerf/internal/camera_utils.py", "signature_position": [925, 934], "body_position": [966, 1062], "dependency": {"intra_class": [], "intra_file": ["camera_utils.ProjectionType", "camera_utils.ProjectionType.FISHEYE", "camera_utils.ProjectionType.PANORAMIC", "camera_utils.ProjectionType.PERSPECTIVE", "camera_utils._radial_and_tangential_undistort", "camera_utils.convert_to_ndc"], "cross_file": []}, "tests": ["tests/camera_utils_test.py::CameraUtilsTest::test_points_to_pixels"], "requirement": {"Functionality": "This function calculates 3D camera rays given 2D pixel coordinates, camera inverse intrinsics, and extrinsics. It supports optional distortion correction, different camera projection types (e.g., fisheye, panoramic), and can work with either numpy or jax.numpy for computations. The function is vectorized to handle arrays of pixel coordinates and camera parameters, and it computes ray origins, directions, normalized view directions, ray differential radii, and image plane coordinates.\n", "Arguments": ":param pix_x_int: int array, shape SH, the x coordinates of image pixels, used as part of the input to compute ray directions.\n:param pix_y_int: int array, shape SH, the y coordinates of image pixels, used alongside pix_x_int to compute ray directions.\n:param pixtocams: float array, broadcastable to SH + [3, 3], the inverse intrinsics of the cameras, used to transform pixel coordinates to camera coordinates.\n:param camtoworlds: float array, broadcastable to SH + [3, 4], the extrinsics of the cameras, used to transform camera coordinates to world coordinates.\n:param distortion_params: dict of floats, optional, camera distortion parameters for correcting lens distortion in the computed rays.\n:param pixtocam_ndc: float array, [3, 3], optional, inverse intrinsics for Normalized Device Coordinates (NDC) projection, used when converting ray origins and directions into NDC space.\n:param camtype: camera_utils.ProjectionType, the type of camera projection (e.g., fisheye or perspective), affects how rays are computed based on the camera model.\n:param xnp: module, either numpy or jax.numpy, the numerical library used for computations, allowing the function to be used with different backends.\n\n:return: A tuple containing:\n         - origins: float array, shape SH + [3], the origin points of the rays in world coordinates.\n         - directions: float array, shape SH + [3], the direction vectors of the rays in world coordinates.\n         - viewdirs: float array, shape SH + [3], the normalized direction vectors of the rays.\n         - radii: float array, shape SH + [1], the differential radii of the rays, useful for mip-NeRF cones.\n         - imageplane: float array, shape SH + [2], the xy coordinates on the image plane, representing the projection of pixel coordinates in world space."}, "indent": 2}
{"namespace": "render.compute_alpha_weights", "type": "function", "completion_path": "camp_zipnerf/internal/render.py", "signature_position": [151, 156], "body_position": [158, 161], "dependency": {"intra_class": [], "intra_file": ["render.compute_alpha_weights_helper"], "cross_file": []}, "tests": ["tests/render_test.py::RenderTest::test_compute_alpha_weights_with_huge_deltas"], "requirement": {"Functionality": "This function calculates the alpha compositing weights based on the given density, distance between points (tdist), and direction vectors (dirs). It computes the product of density and the adjusted distance between points to use as input for another helper function that computes the alpha weights.\n", "Arguments": ":param density: Array-like. The density values for each point in the space, used to compute the weights.\n:param tdist: Array-like. The distances between consecutive points along a path or direction, used to adjust the density values.\n:param dirs: Array-like. The direction vectors for each point or segment, used to calculate the norm-adjusted distances between points.\n:param **kwargs: Arbitrary keyword arguments. These are passed directly to the helper function for computing alpha weights.\n:return: The return value from the helper function that computes the alpha weights. The data type and specifics depend on the implementation of the helper function."}, "indent": 2}
{"namespace": "stepfun.sample", "type": "function", "completion_path": "camp_zipnerf/internal/stepfun.py", "signature_position": [92, 100], "body_position": [118, 138], "dependency": {"intra_class": [], "intra_file": ["stepfun.invert_cdf"], "cross_file": []}, "tests": ["tests/stepfun_test.py::StepFunTest::test_distortion_loss_against_sampling", "tests/stepfun_test.py::StepFunTest::test_sample_large_flat", "tests/stepfun_test.py::StepFunTest::test_sample_single_bin", "tests/stepfun_test.py::StepFunTest::test_weighted_percentile"], "requirement": {"Functionality": "This function samples from a piecewise-constant probability density function (PDF) defined by bin endpoints and corresponding weights. It supports both deterministic and random sampling methods, with options for jittering samples and focusing on the center of the distribution.\n", "Arguments": ":param rng: A random number generator or None. Used for generating random samples. If None, deterministic sampling is used based on linspace.\n:param t: jnp.ndarray. The bin endpoint coordinates, which must be sorted. It defines the intervals of the step function.\n:param w_logits: jnp.ndarray. The logits corresponding to the weights of each bin in the step function.\n:param num_samples: int. The number of samples to generate.\n:param single_jitter: bool. Determines if each sample should be jittered by the same amount (True) or independently (False) when using random sampling.\n:param deterministic_center: bool. Controls the behavior of deterministic sampling. If False, samples span the entire PDF. If True, samples are centered in each interval of the PDF.\n:param eps: float. A small value added for numerical stability and to adjust the range of uniform sampling.\n:return: jnp.ndarray(float32). An array of sampled values from the step function, with shape [batch_size, num_samples]."}, "indent": 2}
{"namespace": "stepfun.sample_intervals", "type": "function", "completion_path": "camp_zipnerf/internal/stepfun.py", "signature_position": [141, 148], "body_position": [163, 183], "dependency": {"intra_class": [], "intra_file": ["stepfun.sample"], "cross_file": []}, "tests": ["tests/stepfun_test.py::StepFunTest::test_sample_intervals_unbiased", "tests/stepfun_test.py::StepFunTest::test_sample_single_interval"], "requirement": {"Functionality": "This function samples intervals from a step function based on provided bin endpoint coordinates, bin weights (logits), and other parameters. It generates a set of interval samples by first sampling points from the step function, then calculating midpoints between adjacent samples, and finally adjusting the first and last intervals to ensure they are within the specified domain.\n", "Arguments": ":param rng: Random number generator or None. Used for generating random samples. If None, 'linspace' sampling is implied.\n:param t: jnp.ndarray. An array of bin endpoint coordinates, which must be sorted. It represents the edges of the bins in the step function.\n:param w_logits: jnp.ndarray. An array of logits corresponding to the weights of each bin in the step function.\n:param num_samples: int. The number of intervals to sample from the step function.\n:param single_jitter: bool, optional. Determines whether to jitter every sample by the same amount or each sample independently in the inverse CDF. Defaults to False.\n:param domain: tuple of two floats (minval, maxval). Specifies the valid range of values for 't', ensuring sampled intervals are within this domain.\n:return: jnp.ndarray(float32). An array of sampled intervals from the step function, adjusted to fit within the specified domain."}, "indent": 2}
{"namespace": "stepfun.weighted_percentile", "type": "function", "completion_path": "camp_zipnerf/internal/stepfun.py", "signature_position": [201, 201], "body_position": [203, 209], "dependency": {"intra_class": [], "intra_file": ["stepfun.integrate_weights"], "cross_file": []}, "tests": ["tests/stepfun_test.py::StepFunTest::test_weighted_percentile", "tests/stepfun_test.py::StepFunTest::test_weighted_percentile_vectorized"], "requirement": {"Functionality": "Computes the weighted percentiles of a step function by interpolating into the integrated weights based on the given percentiles. It ensures that the weights sum to 1 and uses these weights to calculate the weighted percentiles.\n", "Arguments": ":param t: Array-like. The values at which the step function changes. It is used as the x-coordinates for interpolation.\n:param w: Array-like. The weights associated with the values in 't', which must sum to 1. These weights are integrated and used in the interpolation process.\n:param ps: Array-like. The percentiles to compute, given as values between 0 and 100. These are the target y-values for which the corresponding x-values are found through interpolation.\n:return: Array-like. The computed weighted percentiles corresponding to the input percentiles 'ps'."}, "indent": 2}
{"namespace": "stepfun.blur_and_resample_weights", "type": "function", "completion_path": "camp_zipnerf/internal/stepfun.py", "signature_position": [247, 247], "body_position": [249, 267], "dependency": {"intra_class": [], "intra_file": ["stepfun.weight_to_pdf"], "cross_file": []}, "tests": ["tests/stepfun_test.py::StepFunTest::test_blur_and_resample_weights_pool_two_spikes", "tests/stepfun_test.py::StepFunTest::test_blur_and_resample_weights_extend_boundaries", "tests/stepfun_test.py::StepFunTest::test_blur_and_resample_weights_no_op"], "requirement": {"Functionality": "Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n", "Arguments": ":param tq: Array-like. The new time points at which the histogram is to be resampled.\n:param t: Array-like. The original time points of the histogram.\n:param w: Array-like. The weights or values associated with the original time points.\n:param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n:return: Array-like. The resampled weights corresponding to the new time points `tq`."}, "indent": 2}
{"namespace": "spin_math.apply_homogeneous_transform", "type": "function", "completion_path": "camp_zipnerf/internal/spin_math.py", "signature_position": [144, 145], "body_position": [155, 157], "dependency": {"intra_class": [], "intra_file": ["spin_math.from_homogeneous", "spin_math.matmul", "spin_math.to_homogeneous"], "cross_file": []}, "tests": ["tests/rigid_body_test.py::RigidBodyTest::test_exp_se3_only_translation", "tests/rigid_body_test.py::RigidBodyTest::test_exp_se3_pure_translation"], "requirement": {"Functionality": "Applies a homogeneous transformation to a collection of vectors, effectively transforming the given 3D points according to the specified transformation matrix.\n", "Arguments": ":param transform: (C+1,C+1) A homogeneous transformation matrix that defines how each point in the collection should be transformed.\n:param vectors: (*,C) An array containing 3D points that are to be transformed. The array can have an arbitrary shape, but the last dimension must be the dimensionality of the points (C).\n:return: (*,C) An array of the same shape as the input 'vectors', containing the transformed points."}, "indent": 2}
{"namespace": "stepfun.resample", "type": "function", "completion_path": "camp_zipnerf/internal/stepfun.py", "signature_position": [212, 212], "body_position": [230, 244], "dependency": {"intra_class": [], "intra_file": ["stepfun.resample"], "cross_file": []}, "tests": ["tests/stepfun_test.py::StepFunTest::test_resample_self_noop", "tests/stepfun_test.py::StepFunTest::test_resample_2x_downsample", "tests/stepfun_test.py::StepFunTest::test_resample_entire_interval", "tests/stepfun_test.py::StepFunTest::test_resample_entire_domain", "tests/stepfun_test.py::StepFunTest::test_resample_single_span", "tests/stepfun_test.py::StepFunTest::test_resample_vectorized"], "requirement": {"Functionality": "Resamples a step function defined by pairs of time points (tp) and their corresponding values (vp) into new intervals specified by t. It supports both summation and averaging methods for resampling.\n", "Arguments": ":param t: Tensor. The endpoints into which the step function is resampled. It is used as the new time points for resampling.\n:param tp: Tensor. The original time points of the step function being resampled. It defines the intervals of the original step function.\n:param vp: Tensor. The values of the step function at the original time points (tp). These values are resampled according to the new intervals defined by t.\n:param use_avg: Bool, optional. Determines the resampling method. If False, the function sums the values of the step function for each interval in `t`. If True, it returns the average value, weighted by the width of each interval in `t`.\n:return: Tensor. The values of the resampled step function at the new intervals defined by t."}, "indent": 2}
{"namespace": "coord.integrated_pos_enc", "type": "function", "completion_path": "camp_zipnerf/internal/coord.py", "signature_position": [163, 163], "body_position": [175, 183], "dependency": {"intra_class": [], "intra_file": ["coord.expected_sin"], "cross_file": []}, "tests": ["tests/coord_test.py::CoordTest::test_integrated_pos_enc_when_degrees_are_large"], "requirement": {"Functionality": "This function encodes input coordinates using sinusoidal functions scaled by powers of 2 within a specified range of degrees. It scales the mean and variance of the coordinates, concatenates them, and then applies a sinusoidal encoding.\n", "Arguments": ":param mean: tensor, the mean coordinates to be encoded. It is used as the base for scaling before encoding.\n:param var: tensor, the variance of the coordinates to be encoded. It is scaled and used in the encoding to adjust the spread of the encoded values.\n:param min_deg: int, the minimum degree (inclusive) of the encoding scale. It determines the lower bound of the scaling factor for the encoding.\n:param max_deg: int, the maximum degree (exclusive) of the encoding scale. It sets the upper limit for the scaling factor in the encoding process.\n:return: jnp.ndarray, the encoded variables resulting from the sinusoidal encoding of the scaled mean and variance."}, "indent": 2}
{"namespace": "ref_utils.generate_dir_enc_fn", "type": "function", "completion_path": "camp_zipnerf/internal/ref_utils.py", "signature_position": [196, 196], "body_position": [205, 211], "dependency": {"intra_class": [], "intra_file": ["ref_utils.generate_ide_fn"], "cross_file": []}, "tests": ["tests/ref_utils_test.py::RefUtilsTest::test_spherical_harmonics"], "requirement": {"Functionality": "Generates a directional encoding function based on the specified number of spherical harmonics degrees. This function internally creates another function that evaluates the directional encoding for given inputs.\n", "Arguments": ":param deg_view: Int. The number of spherical harmonics degrees to use for generating the directional encoding function. It determines the complexity and accuracy of the encoding.\n:return: Function. A function that takes a 3D point (or points) as input and returns its directional encoding. This returned function internally uses a generated integrated directional encoding function with the specified degree of spherical harmonics.\n"}, "indent": 2}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "type": "function", "completion_path": "nlm-ingestor/nlm_ingestor/ingestor/processors.py", "signature_position": [79, 79], "body_position": [80, 177], "dependency": {"intra_class": [], "intra_file": ["nlm_ingestor.ingestor.processors.clean_line", "nlm_ingestor.ingestor.processors.fix_spaced_characters", "nlm_ingestor.ingestor.processors.logger", "nlm_ingestor.ingestor.processors.should_skip"], "cross_file": ["nlm_ingestor.ingestor.formatter", "nlm_ingestor.ingestor.formatter.connect", "nlm_ingestor.ingestor.line_parser", "nlm_ingestor.ingestor.line_parser.Line", "nlm_ingestor.ingestor.line_parser.Line.continuing_line", "nlm_ingestor.ingestor.line_parser.Line.ends_with_period", "nlm_ingestor.ingestor.line_parser.Line.has_spaced_characters", "nlm_ingestor.ingestor.line_parser.Line.incomplete_line", "nlm_ingestor.ingestor.line_parser.Line.is_list_or_row", "nlm_ingestor.ingestor.line_parser.Line.line_type", "nlm_ingestor.ingestor.line_parser.Line.text"]}, "tests": ["tests/test_processor.py::MyTest::test_line_join"], "requirement": {"Functionality": "This function processes a list of text lines, cleans them, and organizes them into blocks based on their content and structure. It removes duplicate lines (ignoring numbers), fixes spaced characters, connects incomplete lines, and categorizes lines into paragraphs, headers, or list items. Each block of text is then appended to a result list with metadata about the block type, index, and related header block index if applicable.\n", "Arguments": ":param lines: List of strings. The text lines to be processed and cleaned.\n:param xml: Bool, optional. A flag indicating whether the processing should consider XML-specific formatting rules.\n:return: List of dictionaries. Each dictionary represents a block of text with metadata including the block index, text, type, starting index of the text group, list of blocks if any, the index of the associated header block, and the level of indentation or list."}, "indent": 4}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "type": "function", "completion_path": "nlm-ingestor/nlm_ingestor/ingestor_utils/utils.py", "signature_position": [93, 93], "body_position": [94, 141], "dependency": {"intra_class": [], "intra_file": ["nlm_ingestor.ingestor_utils.utils.bracket_rule", "nlm_ingestor.ingestor_utils.utils.nltk_tokenzier", "nlm_ingestor.ingestor_utils.utils.quotation_pattern", "nlm_ingestor.ingestor_utils.utils.rules", "nlm_ingestor.ingestor_utils.utils.space_rule"], "cross_file": []}, "tests": ["tests/test_sent_tokenizer.py::PreProcessingTests::test_sentence_tokenizer"], "requirement": {"Functionality": "The function tokenizes a given text into sentences, handling special cases such as paragraphs separated by new lines, punctuation at the beginning of the text, and ensuring that sentences within brackets are not broken. It also normalizes quotation marks within the text.\n", "Arguments": ":param org_texts: String. The original text that needs to be tokenized into sentences. It is used as the input to apply various tokenization and normalization rules.\n:return: List of Strings. The tokenized sentences from the original text. If the input text is empty or None, it returns the input as is.\n\nNote: The function uses several predefined rules and patterns (such as `space_rule`, `bracket_rule`, `rules`, and `quotation_pattern`) and an instance of a tokenizer (`nltk_tokenzier`) to perform the tokenization and normalization. These components are assumed to be defined outside the function."}, "indent": 4}
{"namespace": "searcharray.postings.SearchArray.positions", "type": "method", "completion_path": "searcharray/searcharray/postings.py", "signature_position": [557, 557], "body_position": [559, 562], "dependency": {"intra_class": ["searcharray.postings.SearchArray.posns", "searcharray.postings.SearchArray.term_dict", "searcharray.postings.SearchArray.term_mat"], "intra_file": [], "cross_file": []}, "tests": ["test/test_phrase_matches.py::test_positions", "test/test_phrase_matches.py::test_positions_mask", "test/test_phrase_matches.py::test_positions_mask_single"], "requirement": {"Functionality": "This function retrieves and returns a list of numpy arrays representing the positions of a given term within documents. If a specific document key is provided, it returns the positions for that document only; otherwise, it returns positions across all documents.\n", "Arguments": ":param self: SearchArray. An instance of the SearchArray class.\n:param token: str, The term for which positions are to be found.\n:param key: The specific document key to search within. If None, positions are searched across all documents. Defaults to None.\n:return: List[np.ndarray], A list of numpy arrays where each array contains the positions of the given term. The positions are specific to a document if a key is provided, or across all documents if no key is provided."}, "indent": 8}
{"namespace": "searcharray.solr.parse_min_should_match", "type": "function", "completion_path": "searcharray/searcharray/solr.py", "signature_position": [10, 10], "body_position": [25, 60], "dependency": {"intra_class": [], "intra_file": ["searcharray.solr.parse_min_should_match"], "cross_file": []}, "tests": ["test/test_solr.py::test_standard_percentage", "test/test_solr.py::test_over_100_percentage", "test/test_solr.py::test_negative_percentage", "test/test_solr.py::test_standard_integer", "test/test_solr.py::test_negative_integer", "test/test_solr.py::test_integer_exceeding_clause_count", "test/test_solr.py::test_conditional_spec_less_than_clause_count", "test/test_solr.py::test_conditional_spec_greater_than_clause_count", "test/test_solr.py::test_complex_conditional_spec", "test/test_solr.py::test_invalid_spec_percentage", "test/test_solr.py::test_invalid_spec_integer", "test/test_solr.py::test_invalid_spec_conditional", "test/test_solr.py::test_empty_spec", "test/test_solr.py::test_complex_conditional_spec_with_percentage"], "requirement": {"Functionality": "This function parses the \"min should match\" specification (commonly referred to as 'mm' spec) from Solr's search engine configuration and calculates the minimum number of clauses that must match based on the given specification and the total number of clauses. It supports both absolute and percentage-based specifications, as well as conditional specifications using '<'.\n", "Arguments": ":param num_clauses: int, the total number of clauses in the query.\n:param spec: str, the 'min should match' specification as a string, which can include absolute numbers, percentages, and conditional expressions.\n:return: int, the calculated minimum number of clauses that must match according to the 'mm' spec."}, "indent": 4}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "type": "method", "completion_path": "searcharray/searcharray/postings.py", "signature_position": [578, 578], "body_position": [579, 589], "dependency": {"intra_class": ["searcharray.postings.SearchArray.phrase_freq_every_diff", "searcharray.postings.SearchArray.posns", "searcharray.postings.SearchArray.term_dict", "searcharray.postings.SearchArray.term_mat"], "intra_file": [], "cross_file": ["searcharray.term_dict.TermMissingError"]}, "tests": ["test/test_phrase_matches.py::test_phrase_different_posns", "test/test_phrase_matches.py::test_phrase_scattered_posns", "test/test_phrase_matches.py::test_phrase_scattered_posns3", "test/test_phrase_matches.py::test_phrase_too_many_posns_with_truncate"], "requirement": {"Functionality": "This function calculates the frequency of a phrase within a SearchArray instance based on the provided tokens and slop. If the slop is 1 and all tokens are unique, it attempts to directly calculate the phrase frequencies using the positions of terms. If the slop is not 1 or tokens are not unique, it delegates the calculation to another method that handles different slops or non-unique tokens.\n", "Arguments": ":param self: SearchArray. An instance of the SearchArray class.\n:param tokens: List[str]. A list of tokens (words) for which the phrase frequency is to be calculated.\n:param slop: int, optional. The maximum distance between tokens in the phrase. Defaults to 1, meaning tokens must be adjacent.\n:return: np.ndarray. An array of phrase frequencies corresponding to the tokens in the SearchArray instance."}, "indent": 8}
{"namespace": "searcharray.postings.SearchArray.index", "type": "method", "completion_path": "searcharray/searcharray/postings.py", "signature_position": [222, 223], "body_position": [225, 238], "dependency": {"intra_class": ["searcharray.postings.SearchArray.__init__", "searcharray.postings.SearchArray.avg_doc_length", "searcharray.postings.SearchArray.doc_lens", "searcharray.postings.SearchArray.posns", "searcharray.postings.SearchArray.term_dict", "searcharray.postings.SearchArray.term_mat"], "intra_file": ["searcharray.postings.ws_tokenizer"], "cross_file": ["searcharray.indexing.build_index_from_tokenizer"]}, "tests": ["test/test_tmdb.py::test_tokenize_tmdb", "test/test_tmdb.py::test_batch_sizes_give_same", "test/test_tmdb.py::test_index_benchmark", "test/test_tmdb.py::test_index_benchmark_1k_random", "test/test_tmdb.py::test_eq_benchmark", "test/test_phrase_matches.py::test_phrase_different_posns", "test/test_phrase_matches.py::test_phrase_scattered_posns", "test/test_phrase_matches.py::test_phrase_scattered_posns3", "test/test_phrase_matches.py::test_phrase_too_many_posns", "test/test_phrase_matches.py::test_phrase_too_many_posns_with_truncate", "test/test_phrase_matches.py::test_positions", "test/test_phrase_matches.py::test_positions_mask", "test/test_phrase_matches.py::test_positions_mask_single"], "requirement": {"Functionality": "Indexes an array of strings using a specified tokenizer and returns an instance of SearchArray containing the indexed data. It builds an index from the given array and tokenizer, handling large arrays by processing them in batches if necessary.\n", "Arguments": ":param cls: The class SearchArray itself. It is used to create a new instance of SearchArray that will hold the indexed data.\n:param array: Iterable, the array of strings to be indexed. It is the input data that needs to be processed and indexed.\n:param tokenizer: A function used for tokenizing the strings in the array. It defaults to ws_tokenizer if not specified. This tokenizer is applied to each string in the array to generate tokens for indexing.\n:param truncate: bool, indicates whether to truncate the data to fit within memory constraints. It defaults to False, meaning the data will not be truncated unless specified.\n:param batch_size: int, the size of batches to process the array in. This is useful for processing large arrays without loading everything into memory at once. Defaults to 100000.\n:param avoid_copies: bool, indicates whether to avoid making copies of the data to save memory. Defaults to True, optimizing memory usage during the indexing process.\n:return: An instance of SearchArray, which contains the indexed data including term matrix, positions, term dictionary, average document length, and document lengths."}, "indent": 8}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "type": "method", "completion_path": "deluder/deluder/interceptors/proxifier/interceptor.py", "signature_position": [42, 42], "body_position": [43, 54], "dependency": {"intra_class": ["deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.connections", "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.lock", "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.server"], "intra_file": [], "cross_file": ["deluder.interceptor.MessageInterceptor.config", "deluder.interceptor.MessageInterceptor.logger", "deluder.interceptors.proxifier.server.Server", "deluder.interceptors.proxifier.server.Server.start"]}, "tests": ["tests/interceptors/proxifier/test_proxifier_length_strategy.py::test_proxifier_interceptor_length_strategy", "tests/interceptors/proxifier/test_proxifier_suffix_strategy.py::test_proxifier_interceptor_suffix_strategy"], "requirement": {"Functionality": "Initializes an instance of the ProxifierMessageInterceptor class by setting up a server with specific configurations and starting it. It also initializes a dictionary to keep track of connections and a lock for thread safety.\n", "Arguments": ":param self: ProxifierMessageInterceptor. An instance of the ProxifierMessageInterceptor class. It uses its configuration and logger attributes to set up the server and for logging purposes.\n:return: No return values."}, "indent": 8}
{"namespace": "searcharray.utils.bitcount.bit_count64", "type": "function", "completion_path": "searcharray/searcharray/utils/bitcount.py", "signature_position": [24, 24], "body_position": [26, 34], "dependency": {"intra_class": [], "intra_file": ["searcharray.utils.bitcount._1", "searcharray.utils.bitcount._2", "searcharray.utils.bitcount._4", "searcharray.utils.bitcount.all_but_one_bit", "searcharray.utils.bitcount.s01", "searcharray.utils.bitcount.s0F", "searcharray.utils.bitcount.s33", "searcharray.utils.bitcount.s55"], "cross_file": []}, "tests": ["test/test_bitcount64.py::test_bitcount64"], "requirement": {"Functionality": "Counts the number of bits set to 1 (also known as the Hamming weight or the population count) in each element of a 64-bit integer array using bitwise operations. This is a step in the process of efficiently calculating the bit count for each element in the array.", "Arguments": ":param arr: Array of 64-bit integers. The array whose elements' bit counts are to be calculated. It is modified in-place to contain the bit counts as its elements.\n:return: Array of integers. The same array passed as input, but now each element has been replaced with its corresponding bit count."}, "indent": 4}
{"namespace": "searcharray.solr.edismax", "type": "function", "completion_path": "searcharray/searcharray/solr.py", "signature_position": [169, 177], "body_position": [202, 240], "dependency": {"intra_class": [], "intra_file": ["searcharray.solr._edismax_field_centric", "searcharray.solr._edismax_term_centric", "searcharray.solr.get_field", "searcharray.solr.parse_field_boosts", "searcharray.solr.parse_query_terms"], "cross_file": ["searcharray.postings.SearchArray.score", "searcharray.similarity.Similarity", "searcharray.similarity.default_bm25"]}, "tests": ["test/test_solr.py::test_edismax", "test/test_solr.py::test_edismax_custom_similarity"], "requirement": {"Functionality": "This function performs an Extended Disjunction Max Query (edismax) search over a given DataFrame, considering various search parameters such as query fields, minimum match specifications, and phrase, bigram, and trigram matches. It supports both term-centric and field-centric approaches to calculate the relevance scores of documents based on the input query and returns the scores along with an explanation of how they were computed.\n", "Arguments": ":param frame: pd.DataFrame. The DataFrame over which the search is performed.\n:param q: str. The query string used for the search.\n:param qf: List[str]. The fields in the DataFrame to search against.\n:param mm: Optional[str]. The minimum should match specification, which dictates the minimum number of query terms that must match. Defaults to None.\n:param pf: Optional[List[str]]. The fields to search for phrase matches. Defaults to None.\n:param pf2: Optional[List[str]]. The fields to search for bigram matches. Defaults to None.\n:param pf3: Optional[List[str]]. The fields to search for trigram matches. Defaults to None.\n:param q_op: str. The default operator for the query, which can be \"OR\" or \"AND\". Defaults to \"OR\".\n:param similarity: Similarity. The similarity measure to use for scoring documents. Defaults to a BM25 similarity measure.\n:return: Tuple[np.ndarray, str]. A tuple containing an array of search results scores and a string explaining the scoring."}, "indent": 4}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "type": "method", "completion_path": "deluder/deluder/interceptors/proxifier/interceptor.py", "signature_position": [56, 56], "body_position": [57, 62], "dependency": {"intra_class": ["deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor._get_connection", "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor._handle_connection_close_message"], "intra_file": [], "cross_file": ["deluder.interceptors.proxifier.connection.Connection.c2s", "deluder.interceptors.proxifier.connection.Connection.s2c"]}, "tests": ["tests/interceptors/proxifier/test_proxifier_buffer_strategy.py::test_proxifier_interceptor_buffer_strategy"], "requirement": {"Functionality": "Intercepts a message in a ProxifierMessageInterceptor instance and modifies it based on its type. If the message is a SendMessage, it transforms the message data using the connection's c2s method. If it's a RecvMessage, it uses the s2c method. For CloseMessage, it handles the connection closure without modifying the message data directly.\n", "Arguments": ":param self: ProxifierMessageInterceptor. An instance of the ProxifierMessageInterceptor class.\n:param process: Process, the process associated with the message, used to identify or manage the process-specific logic or data.\n:param message: Message, the message to be intercepted and processed. Depending on its type (SendMessage, RecvMessage, CloseMessage), the message is handled differently.\n:return: No return values. The function modifies the message object in place."}, "indent": 8}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "type": "method", "completion_path": "deluder/deluder/interceptors/proxifier/interceptor.py", "signature_position": [64, 64], "body_position": [65, 69], "dependency": {"intra_class": ["deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.connections", "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.server"], "intra_file": [], "cross_file": ["deluder.interceptors.proxifier.server.Server.stop"]}, "tests": ["tests/interceptors/proxifier/test_proxifier_buffer_strategy.py::test_proxifier_interceptor_buffer_strategy"], "requirement": {"Functionality": "The destroy method is responsible for stopping all connections managed by the ProxifierMessageInterceptor instance and then stopping the server associated with it, if any. This method ensures that all active connections are properly terminated before stopping the server.\n", "Arguments": ":param self: ProxifierMessageInterceptor. An instance of the ProxifierMessageInterceptor class. It uses its 'connections' attribute to manage individual connections and a 'server' attribute to manage the server instance.\n:return: No return values."}, "indent": 8}
